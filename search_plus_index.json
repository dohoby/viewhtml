{"./":{"url":"./","title":"Introduction","keywords":"","body":"Java实用笔记 Base Collection Hashmap ConcurrentHashMap相关 HashMap相关 HashMap相关2 HashMap相关tx jdk1.7hashmap源码分析 jdk1.8ConcurrentHashMap源码分析 jdk1.8hashmap源码分析 Jdk1.8源码解析-java.util.HashMap#putTreeVal Jdk1.8源码解析-java.util.HashMap#resize Jdk1.8源码解析-java.util.HashMap#treeifyBin链表转化为红黑树 其他map hashmap源码 jdk1.7ConcurrentHashMap源码 jdk1.7HashMap源码 jdk1.8ConcurrentHashMap源码 jdk1.8HashMap源码 Hashset hashset源码学习 集合类学习 String 集合StringBuffer和StringBuilder string源码 string源码学习 字符串string学习 字符串string学习2 线程 ThreadPoolExecutor详解 ThreadPoolExecutor 的参数含义及源码执行流程 线程的状态 网络 http和tcp Test 集合基本类型装箱拆箱 基本类型装箱拆箱2 Design Patterns Spring spring享元模式 spring代理模式 spring策略模式 Jvm jvm内存模型 jvm参数 Mybatis mybatis分页插件 mybatis插件引入 Page Page Plugin Reflect Helper mybatis分页插件2 mybatis插件引入 Page Interceptor Pagination Mybatis View 1 mybatis学习 mybatis源码 mybatis源码学习总结1 mybatis源码学习总结2 mybatis用法 常见mybatis用法 Mybatis Spring Mybatis-Spring源码分析 Redis redis相关 缓存 Spring Aop aop动态代理学习 mybatis学习 JdkDanymicAopProxy源码 Ioc bean实例化过程 spring-ioc原理 事务 spring事务管理 Springboot springboot配置 springboot配置文件加载分析 修改springboot默认加载application.properties文件 监听器 springboot监听器 SpringApplication源码分析 Zother 1 Java Face Notes Dubbo IO NIO Java基础 java集合 JVM Kafka Linux Mybatis My Sql Nginx Redis Spring Spring Boot Spring Cloud 多线程 异常&反射 简历 Zother 5 Java Interview Java基础学习 Java集合面试题及答案 一、Java基础 七、JavaWeb 三、Java 集合 九、操作系统 二、设计模式 二十、Spring源码解析 二十一、Tomcat源码解析 五、JVM 八、Mysql Zother 7 Java Interview Data Structures Algorithms 算法面试真题汇总 算法题目难点题目总结 递归套路总结 Interview 已投公司情况 自我介绍和项目介绍 Interview Experience 各大公司面经 面试常见知识 面试常见问题分类汇总 Microservice 微服务相关资料 Network http面试问题全解析 Operating System 后端程序员必备的Linux基础知识 操作系统、计算机网络相关知识 Project 消息中间件面试题 秒杀项目总结 学习路线 "},"base/collection/hashmap/ConcurrentHashMap相关.html":{"url":"base/collection/hashmap/ConcurrentHashMap相关.html","title":"ConcurrentHashMap相关","keywords":"","body":" ConcurrentHashMap有哪些构造函数？ 一共有五个，作用及代码如下： //无参构造函数 public ConcurrentHashMap() { } //可传初始容器大小的构造函数 public ConcurrentHashMap(int initialCapacity) { if (initialCapacity = (MAXIMUM_CAPACITY >>> 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity >>> 1) + 1)); this.sizeCtl = cap; } //可传入map的构造函数 public ConcurrentHashMap(Map m) { this.sizeCtl = DEFAULT_CAPACITY; putAll(m); } //可设置阈值和初始容量 public ConcurrentHashMap(int initialCapacity, float loadFactor) { this(initialCapacity, loadFactor, 1); } //可设置初始容量和阈值和并发级别 public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) { if (!(loadFactor > 0.0f) || initialCapacity = (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); this.sizeCtl = cap; } ConcurrentHashMap使用什么技术来保证线程安全？ jdk1.7：Segment+HashEntry来进行实现的； jdk1.8：放弃了Segment臃肿的设计，采用Node+CAS+Synchronized来保证线程安全； ConcurrentHashMap的get方法是否要加锁，为什么？ 不需要，get方法采用了unsafe方法，来保证线程安全。 ConcurrentHashMap迭代器是强一致性还是弱一致性？HashMap呢？ 弱一致性，hashmap强一直性。 ConcurrentHashMap可以支持在迭代过程中，向map添加新元素，而HashMap则抛出了ConcurrentModificationException， 因为HashMap包含一个修改计数器，当你调用他的next()方法来获取下一个元素时，迭代器将会用到这个计数器。 ConcurrentHashMap1.7和1.8的区别： jdk1.8的实现降低锁的粒度，jdk1.7锁的粒度是基于Segment的，包含多个HashEntry，而jdk1.8锁的粒度就是Node 数据结构：jdk1.7 Segment+HashEntry；jdk1.8 数组+链表+红黑树+CAS+synchronized ConcurrentHashMap "},"base/collection/hashmap/HashMap相关.html":{"url":"base/collection/hashmap/HashMap相关.html","title":"HashMap相关","keywords":"","body":" 下面直接来干货，先说这三个Map的区别： HashTable HashMap ConcurrentHashMap区别 HashMap,LinkedHashMap,TreeMap都属于Map LinkedHashMap也是一个HashMap,但是内部维持了一个双向链表,可以保持顺序 TreeMap 可以用于排序 HashMap可以用 Collections的synchronizedMap方法使HashMap具有同步的能力 HashTable 底层数组+链表实现，无论key还是value都不能为null，线程安全，实现线程安全的方式是在修改数据时锁住整个HashTable，效率低，ConcurrentHashMap做了相关优化 初始size为11，扩容：newsize = olesize*2+1 计算index的方法：index = (hash & 0x7FFFFFFF) % tab.length HashMap 底层数组+链表实现，可以存储null键和null值，线程不安全 初始size为16，扩容：newsize = oldsize*2，size一定为2的n次幂 扩容针对整个Map，每次扩容时，原来数组中的元素依次重新计算存放位置，并重新插入 插入元素后才判断该不该扩容，有可能无效扩容（插入后如果扩容，如果没有再次插入，就会产生无效扩容） 当Map中元素总数超过Entry数组的75%，触发扩容操作，为了减少链表长度，元素分配更均匀 计算index方法：index = hash & (tab.length – 1) HashMap的初始值还要考虑加载因子: 哈希冲突：若干Key的哈希值按数组大小取模后，如果落在同一个数组下标上，将组成一条Entry链，对Key的查找需要遍历Entry链上的每个元素执行equals()比较。 加载因子：为了降低哈希冲突的概率，默认当HashMap中的键值对达到数组大小的75%时，即会触发扩容。因此，如果预估容量是100，即需要设定100/0.75＝134的数组大小。 空间换时间：如果希望加快Key查找的时间，还可以进一步降低加载因子，加大初始大小，以降低哈希冲突的概率。 HashMap和Hashtable都是用hash算法来决定其元素的存储，因此HashMap和Hashtable的hash表包含如下属性： 容量（capacity）：hash表中桶的数量 初始化容量（initial capacity）：创建hash表时桶的数量，HashMap允许在构造器中指定初始化容量 尺寸（size）：当前hash表中记录的数量 负载因子（load factor）：负载因子等于“size/capacity”。负载因子为0，表示空的hash表，0.5表示半满的散列表，依此类推。轻负载的散列表具有冲突少、适宜插入与查询的特点（但是使用Iterator迭代元素时比较慢） 除此之外，hash表里还有一个“负载极限”，“负载极限”是一个0～1的数值，“负载极限”决定了hash表的最大填满程度。当hash表中的负载因子达到指定的“负载极限”时，hash表会自动成倍地增加容量（桶的数量），并将原有的对象重新分配，放入新的桶内，这称为rehashing。 HashMap和Hashtable的构造器允许指定一个负载极限，HashMap和Hashtable默认的“负载极限”为0.75，这表明当该hash表的3/4已经被填满时，hash表会发生rehashing。 “负载极限”的默认值（0.75）是时间和空间成本上的一种折中： 较高的“负载极限”可以降低hash表所占用的内存空间，但会增加查询数据的时间开销，而查询是最频繁的操作（HashMap的get()与put()方法都要用到查询） 较低的“负载极限”会提高查询数据的性能，但会增加hash表所占用的内存开销 程序猿可以根据实际情况来调整“负载极限”值。 ConcurrentHashMap 底层采用分段的数组+链表实现，线程安全 通过把整个Map分为N个Segment，可以提供相同的线程安全，但是效率提升N倍，默认提升16倍。(读操作不加锁，由于HashEntry的value变量是 volatile的，也能保证读取到最新的值。) Hashtable的synchronized是针对整张Hash表的，即每次锁住整张表让线程独占，ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术 有些方法需要跨段，比如size()和containsValue()，它们可能需要锁定整个表而而不仅仅是某个段，这需要按顺序锁定所有段，操作完毕后，又按顺序释放所有段的锁 扩容：段内扩容（段内元素超过该段对应Entry数组长度的75%触发扩容，不会对整个Map进行扩容），插入前检测需不需要扩容，有效避免无效扩容 Hashtable和HashMap都实现了Map接口，但是Hashtable的实现是基于Dictionary抽象类的。Java5提供了ConcurrentHashMap，它是HashTable的替代，比HashTable的扩展性更好。 在HashMap中，null可以作为键，这样的键只有一个，但可以有一个或多个键所对应的值为null。当get()方法返回null值时，即可以表示HashMap中没有该key，也可以表示该key所对应的value为null。因此，在HashMap中不能由get()方法来判断HashMap中是否存在某个key，应该用containsKey()方法来判断。而在Hashtable中，无论是key还是value都不能为null。 Hashtable是线程安全的，它的方法是同步的，可以直接用在多线程环境中。而HashMap则不是线程安全的，在多线程环境中，需要手动实现同步机制。 Hashtable与HashMap另一个区别是HashMap的迭代器（Iterator）是fail-fast迭代器，而Hashtable的enumerator迭代器不是fail-fast的。所以当有其它线程改变了HashMap的结构（增加或者移除元素），将会抛出ConcurrentModificationException，但迭代器本身的remove()方法移除元素则不会抛出ConcurrentModificationException异常。但这并不是一个一定发生的行为，要看JVM。 参考： https://mp.weixin.qq.com/s?__biz=MzUxNDA1NDI3OA==&mid=2247486460&idx=1&sn=dc272f151d7e49cb33b8aa8559424ae8&chksm=f94a8a15ce3d03035a741e7e383d1bf6939b45f3e1d821ccb316672dfec6e8c24c51d34b80d2&mpshare=1&scene=24&srcid=&sharer_sharetime=1588342568180&sharer_shareid=05fe8a67423e445e481692cdff57b1ab#rd 锁分段技术 锁分段技术：首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。 ConcurrentHashMap提供了与Hashtable和SynchronizedMap不同的锁机制。Hashtable中采用的锁机制是一次锁住整个hash表，从而在同一时刻只能由一个线程对其进行操作；而ConcurrentHashMap中则是一次锁住一个桶。 ConcurrentHashMap默认将hash表分为16个桶，诸如get、put、remove等常用操作只锁住当前需要用到的桶。这样，原来只能一个线程进入，现在却能同时有16个写线程执行，并发性能的提升是显而易见的。 Concurrenthashmap https://blog.liangqingxiang.top/view2/zother2-interview/java/collection/Concurrenthashmap/ Hashtable与HashMap的区别 https://mp.weixin.qq.com/s/YHSLdfCh-ivI__sdDaL8og https://my.oschina.net/u/1458864/blog/267591 Hashmap的结构，1.7和1.8有哪些区别 https://blog.csdn.net/qq_36520235/article/details/82417949 Java: Map里面的键和值可以为空吗？ Java: Map里面的键和值可以为空吗？ "},"base/collection/hashmap/HashMap相关2.html":{"url":"base/collection/hashmap/HashMap相关2.html","title":"HashMap相关2","keywords":"","body":" 请说明一下HashMap扩容的过程 假如HashMap里的元素有100w个，请问第二维链表的长度大概是多少？ 嗦嘎！链表的长度很短，相比总元素的个数可以忽略不计。这个时候小伙伴们的眼睛通常会开始发光，很童贞。作为面试官是很喜欢看到这种眼神的。我使用反射统计过HashMap里面链表的长度，在HashMap里放了100w个随机字符串键值对，发现链表的长度几乎从来没有超过7这个数字，当我增大loadFactor的时候，才会偶尔冒出几个长度为8的链表来。于是问题又来了。 既然链表如此短，为啥Java8要对HashMap的链表进行改造，使用红黑树来代替链表呢？ 有很多同学都没具体去深入关注Java8的新特性，如果他们回答不上来，我也不会感到失望。因为到这个问题的时候，已经只剩下15%的同学不到了，如果再打击他们，看着他们落寞的身影走出了大门，我都要对自己感到失望了，怎么招个人就如此困难？ 这道题的关键在于如果Key的hashcode不是随机的，而是人为特殊构造的话，那么第二维链表可能会无比的长，而且分布极为不均匀，这个时候就会出现性能问题。比如我们把对象的hashcode都统一返回一个常量，最终的结果就是hashmap会退化为一维链表，Get方法的性能巨降为O(n)，使用红黑树可以将性能提升到O(log(n))。 参考 Get方法的时间复杂度是多少？ 小伙伴们在回答这道题是有很多人会开始怀疑人生，他们的脑细胞这个时候会出现短路现象。明明是O(1)啊，平时都记得牢牢的，可是刚才Get方法的流程里需要遍历链表，遍历的时间复杂度难道不是O(n)么？此刻观察这些孩子们的表情是非常卡哇伊呢的。当然还有些甚至是科班的小伙伴居然没听过时间复杂度，想到这里我也开始怀疑人生了。当他们卡壳的时候，我会稍微提醒一下，问下面的这一道题。 loadFactor负载因子 负载因子，默认值是0.75。负载因子表示一个散列表的空间的使用程度，有这样一个公式：initailCapacity*loadFactor=HashMap的容量。 所以负载因子越大则散列表的装填程度越高，也就是能容纳更多的元素，元素多了，链表大了，所以此时索引效率就会降低。反之，负载因子越小则链表中的数据量就越稀疏，此时会对空间造成烂费，但是此时索引效率高。 当桶中元素到达8个的时候，概率已经变得非常小，也就是说用0.75作为加载因子，每个碰撞位置的链表长度超过８个是几乎不可能的。当桶中元素到达8个的时候，概率已经变得非常小，也就是说用0.75作为加载因子，每个碰撞位置的链表长度超过８个是几乎不可能的。 解决地址冲突 在数据结构中，我们处理hash冲突常使用的方法有：开发定址法、再哈希法、链地址法、建立公共溢出区。而hashMap中处理hash冲突的方法就是链地址法。 https://zhuanlan.zhihu.com/p/79219960 扩容 resize final Node[] resize() { Node[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //第一部分：扩容 if (oldCap > 0) { if (oldCap >= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } else if ((newCap = oldCap = DEFAULT_INITIAL_CAPACITY) newThr = oldThr 0) // initial capacity was placed in threshold newCap = oldThr; else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap [] newTab = (Node[])new Node[newCap]; table = newTab; //第三部分：旧数据保存在新数组里面 if (oldTab != null) { for (int j = 0; j e; if ((e = oldTab[j]) != null) { oldTab[j] = null; //只有一个节点，通过索引位置直接映射 if (e.next == null) newTab[e.hash & (newCap - 1)] = e; //如果是红黑树，需要进行树拆分然后映射 else if (e instanceof TreeNode) ((TreeNode)e).split(this, newTab, j, oldCap); else { //如果是多个节点的链表，将原链表拆分为两个链表 Node loHead = null, loTail = null; Node hiHead = null, hiTail = null; Node next; do { next = e.next; if ((e.hash & oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); //链表1存于原索引 if (loTail != null) { loTail.next = null; newTab[j] = loHead; } //链表2存于原索引加上原hash桶长度的偏移量 if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } （1）第一部分： //第一部分：扩容 if (oldCap > 0) { if (oldCap >= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } else if ((newCap = oldCap = DEFAULT_INITIAL_CAPACITY) newThr = oldThr 根据代码也能看明白：首先如果超过了数组的最大容量，那么就直接将阈值设置为整数最大值，然后如果没有超过，那就扩容为原来的2倍，这里要注意是oldThr （2）第二部分： //第二部分：设置阈值 else if (oldThr > 0) //阈值已经初始化了，就直接使用 newCap = oldThr; else { // 没有初始化阈值那就初始化一个默认的容量和阈值 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap [] newTab = (Node[])new Node[newCap]; table = newTab; 首先第一个else if表示如果阈值已经初始化过了，那就直接使用旧的阈值。然后第二个else表示如果没有初始化，那就初始化一个新的数组容量和新的阈值。 （3）第三部分 第三部分同样也很复杂，就是把旧数据复制到新数组里面。这里面需要注意的有下面几种情况： A：扩容后，若hash值新增参与运算的位=0，那么元素在扩容后的位置=原始位置 B：扩容后，若hash值新增参与运算的位=1，那么元素在扩容后的位置=原始位置+扩容后的旧位置。 hash值新增参与运算的位是什么呢？我们把hash值转变成二进制数字，新增参与运算的位就是倒数第五位。 这里面有一个非常好的设计理念，扩容后长度为原hash表的2倍，于是把hash表分为两半，分为低位和高位，如果能把原链表的键值对， 一半放在低位，一半放在高位，而且是通过e.hash & oldCap == 0来判断，这个判断有什么优点呢？ 举个例子：n = 16，二进制为10000，第5位为1，e.hash & oldCap 是否等于0就取决于e.hash第5 位是0还是1，这就相当于有50%的概率放在新hash表低位，50%的概率放在新hash表高位。 OK，到这一步基本上就算是把扩容这一部分讲完了，还有一个问题没有解决，也就是说存储的原理讲明白了，存储的元素多了如何扩容也明白了，扩容之后出现了地址冲突怎么办呢？ 什么是红黑树呢？ 红黑树是一个自平衡的二叉查找树，也就是说红黑树的查找效率是非常的高，查找效率会从链表的o(n)降低为o(logn)。如果之前没有了解过红黑树的话，也没关系，你就记住红黑树的查找效率很高就OK了。 HashMap在并发编程环境下有什么问题啊? (1)多线程扩容，引起的死循环问题 (2)多线程put的时候可能导致元素丢失 (3)put非null元素后get出来的却是null 在jdk1.8中还有这些问题么? 在jdk1.8中，死循环问题已经解决。其他两个问题还是存在。 你一般怎么解决这些问题的？ 比如ConcurrentHashmap，Hashtable等线程安全等集合类。 你一般用什么作为HashMap的key? 健可以为Null值么? 你一般用什么作为HashMap的key? 我用可变类当HashMap的key有什么问题? 如果让你实现一个自定义的class作为HashMap的key该如何实现？ 健可以为Null值么? 必须可以，key为null的时候，hash算法最后的值以0来计算，也就是放在数组的第一个位置。 你一般用什么作为HashMap的key? 一般用Integer、String这种不可变类当HashMap当key，而且String最为常用。 (1)因为字符串是不可变的，所以在它创建的时候hashcode就被缓存了，不需要重新计算。这就使得字符串很适合作为Map中的键，字符串的处理速度要快过其它的键对象。这就是HashMap中的键往往都使用字符串。 (2)因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法是非常重要的,这些类已经很规范的覆写了hashCode()以及equals()方法。 我用可变类当HashMap的key有什么问题? 我用可变类当HashMap的key有什么问题? hashcode可能发生改变，导致put进去的值，无法get出，如下所示 HashMap, Object> changeMap = new HashMap<>(); List list = new ArrayList<>(); list.add(\"hello\"); Object objectValue = new Object(); changeMap.put(list, objectValue); System.out.println(changeMap.get(list)); list.add(\"hello world\");//hashcode发生了改变 System.out.println(changeMap.get(list)); 输出值如下 java.lang.Object@74a14482 null 如果让你实现一个自定义的class作为HashMap的key该如何实现？ 此题考察两个知识点 重写hashcode和equals方法注意什么? 如何设计一个不变类 针对问题一，记住下面四个原则即可 (1)两个对象相等，hashcode一定相等 (2)两个对象不等，hashcode不一定不等 (3)hashcode相等，两个对象不一定相等 (4)hashcode不等，两个对象一定不等 针对问题二，记住如何写一个不可变类 (1)类添加final修饰符，保证类不被继承。 如果类可以被继承会破坏类的不可变性机制，只要继承类覆盖父类的方法并且继承类可以改变成员变量值，那么一旦子类以父类的形式出现时，不能保证当前类是否可变。 (2)保证所有成员变量必须私有，并且加上final修饰 通过这种方式保证成员变量不可改变。但只做到这一步还不够，因为如果是对象成员变量有可能再外部改变其值。所以第4点弥补这个不足。 (3)不提供改变成员变量的方法，包括setter 避免通过其他接口改变成员变量的值，破坏不可变特性。 (4)通过构造器初始化所有成员，进行深拷贝(deep copy) 如果构造器传入的对象直接赋值给成员变量，还是可以通过对传入对象的修改进而导致改变内部变量的值。例如： public final class ImmutableDemo { private final int[] myArray; public ImmutableDemo(int[] array) { this.myArray = array; // wrong } } 这种方式不能保证不可变性，myArray和array指向同一块内存地址，用户可以在ImmutableDemo之外通过修改array对象的值来改变myArray内部的值。 为了保证内部的值不被修改，可以采用深度copy来创建一个新内存保存传入的值。正确做法： public final class MyImmutableDemo { private final int[] myArray; public MyImmutableDemo(int[] array) { this.myArray = array.clone(); } } (5)在getter方法中，不要直接返回对象本身，而是克隆对象，并返回对象的拷贝 这种做法也是防止对象外泄，防止通过getter获得内部可变成员对象后对成员变量直接操作，导致成员变量发生改变。 https://zhuanlan.zhihu.com/p/76735726 "},"base/collection/hashmap/HashMap相关tx.html":{"url":"base/collection/hashmap/HashMap相关tx.html","title":"HashMap相关tx","keywords":"","body":" HashMap特点 特点： 1.存取无序的, HashMap 的实现不是同步的，这意味着它不是线程安全的 2.键和值位置都可以是null，但是键位置只能是一个null 3.键位置是唯一的，底层的数据结构控制键的位置 4.jdk1.8前数据结构是：链表 + 数组 jdk1.8之后是：链表 + 数组 + 红黑树 5.阈值(边界值) > 8 并且数组长度大于64，才将链表转换为红黑树，变为红黑树的目的是为了高效的查询。 面试题：HashMap中hash函数是怎么实现的？还有哪些hash函数的实现方式？ 对于key的hashCode做hash操作，无符号右移16位然后做异或运算。除此以外，还可以用取余数法、伪随机数法等，但是这些效率都比较低，而无符号右移16我异或运算效率是最高的。 面试题：当两个对象的hashCode相等时会怎么样？ 会发生哈希碰撞。若key值内容相同，则替换旧的value，否则连接到链表后面。 链表长度超过8，数组长度超过64的时候，会转成红黑树 面试题：何时发生哈希碰撞和什么是哈希碰撞,如何解决哈希碰撞？ 只要两个元素的key计算的哈希值相同就会发生哈希碰撞，jdk8之前使用链表解决哈希碰撞，jdk8之后使用链表+红黑树解决哈希碰撞 面试题：如果两个键的hashcode相同，如何存储键值对？ hashCode相同，通过equals方法比较内容是否相同 相同：新的value覆盖旧的value，返回旧的value 不相同：将新的键值对添加到链表中 流程图 说明： 1.size表示 HashMap中K-V的实时数量， 注意这个不等于数组的长度 。 2.threshold( 临界值) =capacity(容量) * loadFactor( 加载因子 0.75 )。这个值是当前已占用数组长度的最大值。size超过这个临界值就重新resize(扩容)，扩容后的 HashMap 容量是之前容量的两倍。在我们实际开发中，如果对效率要求很高，应当尽可能避免hashMap的扩容 问题： 为什么必须是2的n次幂？如果输入值不是2的幂比如10会怎么样？ HashMap构造方法还可以指定集合的初始化容量大小： /** * 指定容量去初始化一个HashMap */ public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } 根据上述讲解我们已经知道，当向HashMap中添加一个元素的时候，需要根据key的hash值，去确定其在数组中的具体位置。 HashMap 为了存取高效，要尽量较少碰撞，就是要尽量把数据分配均匀，每个链表长度大致相同，这个实现就在把数据存到哪个链表中的算法。 这个算法实际就是取模，hash%length，计算机中直接求余效率不如位移运算(这点上述已经讲解)。所以源码中做了优化,使用 hash&(length-1)，而实际上hash%length等于hash&(length-1)的前提是length是2的n次幂。 为什么这样能均匀分布减少碰撞呢？2的n次方实际就是1后面n个0，2的n次方-1 实际就是n个1； 举例： 如果不考虑效率，直接求余数的话，就不需要要求长度是2的n次幂小结：1.由上面可以看出，当我们根据key的hash确定其在数组的位置时，如果数组长度为2的n次幂，就可以保证数据的均匀插入 。如果不是2的n次幂，可能数组的一些位置永远不会插入数据，浪费数组的空间，加大了hash碰撞。2.另一方面，一般我们可能会想通过 % 求余数来确定位置，这样做其实也是可以的，但是性能不如 & 位与运算。而且当数组长度是2的n次幂时，hash & (length - 1)==hash % length3.因此，，HashMap容量是2的n次幂的原因，是为了数据的均匀分布，减少Hash冲突。Hash冲突越大，代表数组中一个链表就越长，这样会降低hashmap的性能。3.Hash冲突越大，代表数组中一个链表就越长，这样会降低hashmap的性能。 如果在创建HashMap对象的时候，指定的容量不是2的n次幂，比如10，HashMap会通过一系列位移运算和或运算得到一个2的n次幂，这个数字是离我们指定容量最近的数字 /** * 用指定的容量和负载因子初始化一个HashMap * * @param initialCapacity 初始值 * @param loadFactor 负载因子 * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity MAXIMUM_CAPACITY) { initialCapacity = MAXIMUM_CAPACITY; } if (loadFactor >> 1; n |= n >>> 2; n |= n >>> 4; n |= n >>> 8; n |= n >>> 16; return (n = MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } 说明：由此可以看到，当在实例化HashMap的时候，如果给定了initialCapacity（假设是10），由于HashMap的initialCapacity必须都是2的n次幂，因此这个方法用于找到大于等于initialCapacity的最小的是2的n次幂。如果initialCapacity已经是2的n次幂，则直接返回这个数。 下面我们分析一下这个算法。 1.首先，为什么要进行cap - 1 的操作。Int n = cap - 1.这是为了防止cap已经是2的n次幂。如果cap已经是2的n次幂，又没有执行减1的操作，则执行完后面的几条无符号右移操作之后，返回的capacity将是这个数的2倍。 2.如果这时候n是0，最后返回的是capacity是1，因为最后有个n+1的操作。 3.按位或运算符 | 运算规则：相同位置的二进制数位上，如果都是0，那么结果就是0，否则为1 为什么转化为红黑树临界值是8 /** * 使用TreeNode的临界值，默认=8 * 经过大量的计算得知，链表中元素复合泊松分布 * 0: 0.60653066 * 1: 0.30326533 * 2: 0.07581633 * 3: 0.01263606 * 4: 0.00157952 * 5: 0.00015795 * 6: 0.00001316 * 7: 0.00000094 * 8: 0.00000006 * 上面是链表中，每个节点可能会存放元素的概率 * * 网上还有另一种说法，红黑树的平均查找长度是log(n)，如果长度为8，平均查找长度是log(8) = 3 * 链表平均查找长度是 n/2，如果长度是8的情况下，8/2=4，效率低于红黑树，所以需要转换为红黑树。 * 如果链表长度小于等于6， 6/2=3.而log(6) ≈ 2.6，虽然比链表快，但是效率差距并不大 * 而且，链表转换为红黑树也需要一定的时间，所以这时候并不会转换为红黑树 */ static final int TREEIFY_THRESHOLD = 8; 64 当Map里面的数量超过这个值时，表中的桶才能进行树形化 ，否则桶内元素太多时会扩容，而不是树形化 /** * 当数组长度大于这个数时才会转红黑树，否则只是扩容 */ static final int MIN_TREEIFY_CAPACITY = 64; #### 哈希表的加载因子 /** * 加载因子，初始值=0.75，与扩容有关 * * @serial */ final float loadFactor; 说明 1.loadFactor负载因子，是用来衡量HashMap满的程度，标识HashMap的疏密程度，影响hash操作到同一个数组位置的概率，计算HashMap的实时加载因子的方法为：size / capacity 2.loadFactor太大会导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。0.75是官方经过大量的数据测试，得出的最好的数字 3.当HashMap中容纳的元素超过边界值时，认为HashMap太挤了，就需要扩容。这个扩容的过程涉及到rehash、复制等操作，非常的消耗性能，所以开发中尽量减少扩容的次数，可以通过创建HashMap时指定初始化容量来尽量的避免 比如：我们需要存放1000个元素到HashMap，那么我们可能需要new HashMap(1024)。但是1024*0.75=768 "},"base/collection/hashmap/jdk1.7hashmap源码分析.html":{"url":"base/collection/hashmap/jdk1.7hashmap源码分析.html","title":"jdk1.7hashmap源码分析","keywords":"","body":" JDK1.7版本HashMap的扩容机制 https://blog.csdn.net/chen13333336677/article/details/99706702 扩容必须满足两个条件： 1、 存放新值的时候当前已有元素的个数必须大于等于阈值 2、 存放新值的时候当前存放数据发生hash碰撞（当前key计算的hash值换算出来的数组下标位置已经存在值） 首先是put()方法 public V put(K key, V value) { 　　　　//判断当前Hashmap(底层是Entry数组)是否存值（是否为空数组） 　　　　if (table == EMPTY_TABLE) { 　　　　　　inflateTable(threshold);//如果为空，则初始化 　　　　} 　　　　 　　　　//判断key是否为空 　　　　if (key == null) 　　　　　　return putForNullKey(value);//hashmap允许key为空 　　　　 　　　　//计算当前key的哈希值　　　　 　　　　int hash = hash(key); 　　　　//通过哈希值和当前数据长度，算出当前key值对应在数组中的存放位置 　　　　int i = indexFor(hash, table.length); 　　　　for (Entry e = table[i]; e != null; e = e.next) { 　　　　　　Object k; 　　　　　　//如果计算的哈希位置有值（及hash冲突），且key值一样，则覆盖原值value，并返回原值value 　　　　　　if (e.hash == hash && ((k = e.key) == key || key.equals(k))) { 　　　　　　　　V oldValue = e.value; 　　　　　　　　e.value = value; 　　　　　　　　e.recordAccess(this); 　　　　　　　　return oldValue; 　　　　　　} 　　　　} 　　　　modCount++; 　　　　//存放值的具体方法 　　　　addEntry(hash, key, value, i); 　　　　return null; 　　} private void inflateTable(int toSize) { // Find a power of 2 >= toSize int capacity = roundUpToPowerOf2(toSize); threshold = (int) Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1); table = new Entry[capacity]; initHashSeedAsNeeded(capacity); } 在put()方法中有调用addEntry()方法，这个方法里面是具体的存值，在存值之前还有判断是否需要扩容 void addEntry(int hash, K key, V value, int bucketIndex) { 　　　　//1、判断当前个数是否大于等于阈值 　　　　//2、当前存放是否发生哈希碰撞 　　　　//如果上面两个条件否发生，那么就扩容 　　　　if ((size >= threshold) && (null != table[bucketIndex])) { 　　　　　　//扩容，并且把原来数组中的元素重新放到新数组中 　　　　　　resize(2 * table.length); 　　　　　　hash = (null != key) ? hash(key) : 0; 　　　　　　bucketIndex = indexFor(hash, table.length); 　　　　} 　　　　createEntry(hash, key, value, bucketIndex); 　　} size定义 /** * The number of key-value mappings contained in this map. */ transient int size; 个人分析： 默认构造函数中hashmap的容量初始化为16，这个16是指桶数组的大小，当为16时，加载因子为0.75时，则扩容阈值为1216表示是数组的长度，并不表示hashmap只能存16个数 size表示hashmap总共存了多少数,每调用一次createEntry创建一个节点时，都会size++ void createEntry(int hash, K key, V value, int bucketIndex) { Entry e = table[bucketIndex]; table[bucketIndex] = new Entry<>(hash, key, value, e); size++; } 上面addEntry方法中先调用resize()方法，再调用createEntry方法，说明先扩容，再新增节点，这样减少了一次旧数据的移植 如果需要扩容，调用扩容的方法resize() void resize(int newCapacity) { 　　　　Entry[] oldTable = table; 　　　　int oldCapacity = oldTable.length; 　　　　//判断是否有超出扩容的最大值，如果达到最大值则不进行扩容操作 　　　　if (oldCapacity == MAXIMUM_CAPACITY) { 　　　　　　threshold = Integer.MAX_VALUE; 　　　　　　return; 　　　　} 　　　　Entry[] newTable = new Entry[newCapacity]; 　　　　// transfer()方法把原数组中的值放到新数组中 　　　　transfer(newTable, initHashSeedAsNeeded(newCapacity)); 　　　　//设置hashmap扩容后为新的数组引用 　　　　table = newTable; 　　　　//设置hashmap扩容新的阈值 　　　　threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1); 　　} transfer()在实际扩容时候把原来数组中的元素放入新的数组中 void transfer(Entry[] newTable, boolean rehash) { 　　　　int newCapacity = newTable.length; 　　　　for (Entry e : table) { 　　　　　　while(null != e) { 　　　　　　　　Entry next = e.next;//我:将当前e节点的下一个节点next保存起来 　　　　　　　　if (rehash) { 　　　　　　　　　　e.hash = null == e.key ? 0 : hash(e.key); 　　　　　　　　} 　　　　　　　　//通过key值的hash值和新数组的大小算出在当前数组中的存放位置 　　　　　　　　int i = indexFor(e.hash, newCapacity);//我：找到e节点在新桶数组中的下标 　　　　　　　　e.next = newTable[i];//我：（newTable[i]一般表示为该链表的首个节点），即为将链表首节点改为节点e的下一个节点， //也就是将新节点插入到链表头，也就是e变成了头节点 　　　　　　　　newTable[i] = e;//将e赋值给新桶数组newTable的头节点 　　　　　　　　e = next;//继续迭代原来数组的下一个节点 　　　　　　} 　　　　} 　　} 个人分析： Entry next = e.next; //我：将当前e节点的下一个节点next保存起来 int i = indexFor(e.hash, newCapacity); //我：找到e节点在新桶数组中的下标 newTable[i]表示对应下标为i的链表（newTable[i]一般表示为该链表的首个节点），即为将链表首节点改为节点e的下一个节点， 也就是将新节点插入到链表头，也就是e变成了头节点 newTable[i] = e;//将e赋值给新桶数组newTable的头节点 链表移植后顺序变为倒序了，因为都是从旧桶数组的头节点开始遍历复制到新桶数组的头节点，每次迭代都是复制到头节点 其他：Hashmap的扩容需要满足两个条件：当前数据存储的数量（即size()）大小必须大于等于阈值；当前加入的数据是否发生了hash冲突。 因为上面这两个条件，所以存在下面这些情况 （1）、就是hashmap在存值的时候（默认大小为16，负载因子0.75，阈值12），可能达到最后存满16个值的时候，再存入第17个值才会发生扩容现象，因为前16个值，每个值在底层数组中分别占据一个位置，并没有发生hash碰撞。 （2）、当然也有可能存储更多值（超多16个值，最多可以存26个值）都还没有扩容。原理：前11个值全部hash碰撞，存到数组的同一个位置（这时元素个数小于阈值12，不会扩容），后面所有存入的15个值全部分散到数组剩下的15个位置（这时元素个数大于等于阈值，但是每次存入的元素并没有发生hash碰撞，所以不会扩容），前面11+15=26，所以在存入第27个值的时候才同时满足上面两个条件，这时候才会发生扩容现象。 死循环如何出现的 /** * Transfers all entries from current table to newTable. */ void transfer(Entry[] newTable, boolean rehash) { int newCapacity = newTable.length; for (Entry e : table) { while(null != e) { Entry next = e.next; if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } } } 模拟死循环是怎么发生的jdk1.7的transfer是用头插法，新的链表和原来的是倒着的，所以这时候假如有两个线程，第一个线程只执行到Entry next = e.next;然后就第二个线程执行了，等到第二个线程执行完，其实这时候已经完成了扩容的任务，且链表里的顺序 已经倒置了，这时候第一个线程继续执行，这时候就把尾巴又指向头了，然后就造成了环。 必看： https://www.jianshu.com/p/1e9cf0ac07f4 下面这个也可以看看，但分析得不够上面的清晰 https://www.cnblogs.com/dongguacai/p/5599100.html hashmap扩容后,原来的数组还有值吗 扩容后新的newTable会赋值给旧的table HashMap初始容量为什么是2的n次幂及扩容为什么是2倍的形式 HashMap的容量为什么是2的n次幂，和这个(n - 1) & hash的计算方法有着千丝万缕的关系，符号&是按位与的计算，这是位运算，计算机能直接运算，特别高效，按位与&的计算方法是，只有当对应位置的数据都为1时，运算结果也为1，当HashMap的容量是2的n次幂时，(n-1)的2进制也就是1111111*111这样形式的，这样与添加元素的hash值进行位运算时，能够充分的散列，使得添加的元素均匀分布在HashMap的每个位置上，减少hash碰撞 终上所述，HashMap计算添加元素的位置时，使用的位运算，这是特别高效的运算；另外，HashMap的初始容量是2的n次幂，扩容也是2倍的形式进行扩容，是因为容量是2的n次幂，可以使得添加的元素均匀分布在HashMap中的数组上，减少hash碰撞，避免形成链表的结构，使得查询效率降低！ https://blog.csdn.net/apeopl/article/details/88935422 get操作 public V get(Object key) { if (key == null) return getForNullKey(); Entry entry = getEntry(key); return null == entry ? null : entry.getValue(); } private V getForNullKey() { if (size == 0) { return null; } for (Entry e = table[0]; e != null; e = e.next) { if (e.key == null) return e.value; } return null; } final Entry getEntry(Object key) { if (size == 0) { return null; } int hash = (key == null) ? 0 : hash(key); for (Entry e = table[indexFor(hash, table.length)]; e != null; e = e.next) { Object k; if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) return e; } return null; } static int indexFor(int h, int length) { // assert Integer.bitCount(length) == 1 : \"length must be a non-zero power of 2\"; return h & (length-1); } 美团面试题：Hashmap的结构，1.7和1.8有哪些区别，史上最深入的分析 https://blog.csdn.net/qq_36520235/article/details/82417949 HashMap六连问，被面试官问懵了 https://mp.weixin.qq.com/s/kSoqwhytXh0Z9qLPK_ZDRw 请从源码的角度，来说说Hashtable与HashMap的区别？ https://mp.weixin.qq.com/s/YHSLdfCh-ivI__sdDaL8og "},"base/collection/hashmap/jdk1.8ConcurrentHashMap源码分析.html":{"url":"base/collection/hashmap/jdk1.8ConcurrentHashMap源码分析.html","title":"jdk1.8ConcurrentHashMap源码分析","keywords":"","body":" Segment static class Segment extends ReentrantLock implements Serializable { private static final long serialVersionUID = 2249069246763182397L; final float loadFactor; Segment(float lf) { this.loadFactor = lf; } } "},"base/collection/hashmap/jdk1.8HashMap源码分析.html":{"url":"base/collection/hashmap/jdk1.8HashMap源码分析.html","title":"jdk1.8hashmap源码分析","keywords":"","body":" JDK1.8之后的HashMap底层数据结构 public class HashMap extends AbstractMap implements Map, Cloneable, Serializable { // 序列号 private static final long serialVersionUID = 362498820763181265L; // 默认的初始容量是16 static final int DEFAULT_INITIAL_CAPACITY = 1 [] table; // 存放具体元素的集 transient Set> entrySet; // 存放元素的个数，注意这个不等于数组的长度。 transient int size; // 每次扩容和更改map结构的计数器 transient int modCount; // 临界值 当实际大小(容量*填充因子)超过临界值时，会进行扩容 int threshold; // 加载因子 final float loadFactor; } DEFAULT_INITIAL_CAPACITY： table数组默认大小为16，即在应用代码中，创建一个HashMap时，不指定容量，则之后在第一次添加数据到该HashMap，首先分配一个大小为16的数组table。 MAXIMUM_CAPACITY：数组最大大小，超过则不再进行拓容，大小为2的30次方。 DEFAULT_LOAD_FACTOR：数组拓容阈值，即在当前数组存放的数据节点达到容量的0.75时，则进行数组拓容，具体拓容逻辑在resize方法定义。 数据结构 在JDK1.8之前，HashMap是基于链式哈希实现的，而在JDK1.8之后，为了提高冲突节点的访问性能，在链式哈希实现的基础上，在哈希表大小超过64时，针对冲突节点链条，如果节点数量超过8个，则升级为红黑树，小于等于6个时，则降级为链表结构。 size表示 HashMap中K-V的实时数量， 注意这个不等于数组的长度 。 loadFactor加载因子0.75 loadFactor加载因子是控制数组存放数据的疏密程度，loadFactor越趋近于1，那么 数组中存放的数据(entry)也就越多，也就越密，也就是会让链表的长度增加，loadFactor越小，也就是趋近于0，数组中存放的数据(entry)也就越少，也就越稀疏。 loadFactor太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor的默认值为0.75f是官方给出的一个比较好的临界值。 给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 16 * 0.75 = 12 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。 /** * 加载因子，初始值=0.75，与扩容有关 * * @serial */ final float loadFactor; 说明 1.loadFactor负载因子，是用来衡量HashMap满的程度，标识HashMap的疏密程度，影响hash操作到同一个数组位置的概率，计算HashMap的实时加载因子的方法为：size / capacity 2.loadFactor太大会导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。0.75是官方经过大量的数据测试，得出的最好的数字 3.当HashMap中容纳的元素超过边界值时，认为HashMap太挤了，就需要扩容。这个扩容的过程涉及到rehash、复制等操作，非常的消耗性能，所以开发中尽量减少扩容的次数，可以通过创建HashMap时指定初始化容量来尽量的避免 比如：我们需要存放1000个元素到HashMap，那么我们可能需要new HashMap(1024)。但是1024*0.75=768 threshold threshold = capacity * loadFactor，当Size>=threshold的时候，那么就要考虑对数组的扩增了，也就是说，这个的意思就是 衡量数组是否需要扩增的一个标准。 为什么hashmap的容量大小是2的倍数 1、h&(length-1)等同于h与数组长度的取模运算，而位运算效率比取模运算高2、使数据均匀分布，减少hash碰撞,如果是2的倍数，也就是2的n次方，2的n次方是1后面n个0, 则length-1则是2的n次方-1，也就是n个1，后面全是1，与hash作位运算不会影响hash值，否则不是2的倍数，则会影响 hash值计算 //Java 8中的散列值优化函数 static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); //key.hashCode()为哈希算法，返回初始哈希值 } 这段代码叫“扰动函数”。 大概意思： 哈希表（HashMap表）容量大多是 2 的 N次方，当元素hashCode()很多时候，低位是相同的， 这将导致hash冲突（碰撞）； 因此Jdk1.8+对 h = key.hashCode()，对h 和 h无符号右移16位，做了 按位异或 运算。 h ^ (h >>> 16)。 这样 进行 (n-1)& hash时，避免了 只靠低位数据来计算哈希时导致的冲突，计算结果由高低位 结合决定，可以更合理的分散数据存储。 https://mp.weixin.qq.com/s/uIfdFZy2pWqFVOwrpzfQVQ https://www.zhihu.com/question/20733617/answer/111577937?utm_source=wechat_session&utm_medium=social&s_s_i=yLxd3TPgxK5hrPWP1bBUT9B9RegUZxkR%2FBkt6AUn0bY%3D&s_r=1 key.hashCode() //key.hashCode() 是根据 K 的类型选择调用那个hashCode方法 //此处 K为String时，调用 java.lang.String#hashCode //jdk1.8 String的hash算法 // todo n=value.length 算法 // todo hash算法 = val[0]*31^(n-1) + val[1]*31^(n-2) + … + val[n-1] public int hashCode() { int h = hash; if (h == 0 && value.length > 0) { char val[] = value;// todo 要put的 String key，转为 char[] val //todo 此处进行hash算法 for (int i = 0; i https://mp.weixin.qq.com/s?__biz=MzIwMDAzNTUwMg==&mid=2247483718&idx=1&sn=f243930ba9d5756a4f34044dc6b53803&chksm=96821c87a1f595917934eb405ac40e1730e18678096d36db55b999663552a21da0472ba0e10a&scene=21#wechat_redirect 为什么在JDK1.8中进行对HashMap优化的时候，把链表转化为红黑树的阈值是8,而不是7或者不是20呢（面试蘑菇街问过）？ 如果选择6和8（如果链表小于等于6树还原转为链表，大于等于8转为树），中间有个差值7可以有效防止链表和树频繁转换。假设一下，如果设计成链表个数超过8则链表转换成树结构，链表个数小于8则树结构转换成链表，如果一个HashMap不停的插入、删除元素，链表个数在8左右徘徊，就会频繁的发生树转链表、链表转树，效率会很低。 还有一点重要的就是由于treenodes的大小大约是常规节点的两倍，因此我们仅在容器包含足够的节点以保证使用时才使用它们，当它们变得太小（由于移除或调整大小）时，它们会被转换回普通的node节点，容器中节点分布在hash桶中的频率遵循泊松分布，桶的长度超过8的概率非常非常小。所以作者应该是根据概率统计而选择了8作为阀值 https://blog.csdn.net/qq_36520235/article/details/82417949 泊松分布，概率分布，当阈值为8时，发生的碰撞最低，接近0，后面再增加，概率就增加，所以取8，在hashmap源码里这个变量定义上面有解析： The first values are: * * 0: 0.60653066 * 1: 0.30326533 * 2: 0.07581633 * 3: 0.01263606 * 4: 0.00157952 * 5: 0.00015795 * 6: 0.00001316 * 7: 0.00000094 * 8: 0.00000006 * more: less than 1 in ten million 网上还有另一种说法，红黑树的平均查找长度是log(n)，如果长度为8，平均查找长度是log(8) = 3 链表平均查找长度是 n/2，如果长度是8的情况下，8/2=4，效率低于红黑树，所以需要转换为红黑树。 如果链表长度小于等于6， 6/2=3.而log(6) ≈ 2.6，虽然比链表快，但是效率差距并不大 而且，链表转换为红黑树也需要一定的时间，所以这时候并不会转换为红黑树 put源码 // 存储元素的数组，总是2的幂次倍 transient Node[] table; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) {//todo tab：用于容器存值；p: 取出 table[index = (hash%n)] 容器这个index中的值，待用 Node[] tab; Node p; int n, i;// todo tab的作用：1、扩容把oldValue，重新赋给tab；2：把tab[i]的值取出，赋给 p if ((tab = table) == null || (n = tab.length) == 0)//todo 判断当前hash表是否是空，如果为空，进行扩容 n = (tab = resize()).length;//todo 扩容（扩容之后，把原来的值赋给新容器）后的容器长度 if ((p = tab[i = (n - 1) & hash]) == null)//todo 取出当前值 赋给 p = tab[i] tab[i] = newNode(hash, key, value, null);//todo tab[i] = newNode(hash,key,value,null); else {//todo tab[i] != null, 则 当前index下已经有值存在 Node e; K k;// todo k：取出当前值 p.key; e： 临时Node节点 if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k)))) e = p;// todo 把 tab[i]值赋给 临时节点e [改变e的值，就相当于改变了 当前值 p] todo 0、【可能存在key=null，value=null 的情况， --> 交给 2 替换】 else if (p instanceof TreeNode)//如果数组中, 这个元素 p 是TreeNode类型 e = ((TreeNode)p).putTreeVal(this, tab, hash, key, value);//判定成功则在红黑树中查找符合的条件的节点并返回此节点 else {//todo 若以上条件均判断失败，则执行以下代码 单向链表put新值 p.next = newNode(hash,key,value,null) for (int binCount = 0; ; ++binCount) {//todo 向Node单向链表中添加数据 if ((e = p.next) == null) {// todo 取出当前节点p的 下一个节点 值， 为null newNode p.next = newNode(hash, key, value, null); if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st //todo 循环7次，统一个数组中的链表 数 为 8 treeifyBin(tab, hash);//todo 数组元素数为8时，尝试转换为红黑树 break; } if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) //todo 1、e的key 和 新值 key 相等 break；交给 2 统一 replace 值 break; p = e;//p记录下一个节点 todo e = p.next 取出的p的下一个节点有值，则把这个节点e赋给临时变量p，再一次循环取 p.nex put 新值 value 给 p.next (p的下下一个值) } } if (e != null) { // existing mapping for key todo 这里处理链表添加时，e.key = key(新)， 替换掉 value；返回 oldValue V oldValue = e.value; if (!onlyIfAbsent || oldValue == null)// todo onlyIfAbsent = false e.value = value;//todo 2 把新传过来的 value值，赋给 e.value = value afterNodeAccess(e); return oldValue;//todo key相同，覆盖value值后，返回原来的 老值 oldValue。 } } ++modCount;//并发修改计数器 ,有并发修改就抛异常 ConcurrentModificationException todo 记录集合被修改的次数 （put的次数） if (++size > threshold)//todo 新put一次 容器长度(size + 1), 并判断是否需要扩容，【size 容器总长度】 resize(); afterNodeInsertion(evict);// todo Callbacks to allow LinkedHashMap post-actions return null; } Jdk1.8 源码解析：java.util.HashMap#putVal resize源码 每次扩容时，原数据的存储位置，要么还在原来位置存储，要么原来位置挪动 oldCap后存储。 java.util.HashMap#resize final Node[] resize() {/** 重新设置table大小/扩容 并返回扩容的Node数组即HashMap的最新数据 **/ Node[] oldTab = table;//todo 把table（容器值）赋给 临时容器 oldTab，待用，用完之后，重置oldTab = null； int oldCap = (oldTab == null) ? 0 : oldTab.length;//原数组如果为null，则长度赋值0 int oldThr = threshold; int newCap, newThr = 0; if (oldCap > 0) {//todo 如果原数组长度大于0，说明原数组已经存在元素 if (oldCap >= MAXIMUM_CAPACITY) {//todo 数组大小如果已经大于等于最大值(2^30) threshold = Integer.MAX_VALUE;//todo 修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 return oldTab; }////原数组长度大于等于初始化长度16，并且原数组长度扩大1倍也小于2^30次方 else if ((newCap = oldCap = DEFAULT_INITIAL_CAPACITY) newThr = oldThr 0) // initial capacity was placed in threshold todo //原来的数组没有数据，故new容量 = oldThr 老阀值 newCap = oldThr; else { // zero initial threshold signifies using defaults todo 阀值等于0,oldCap也等于0(集合未进行初始化） newCap = DEFAULT_INITIAL_CAPACITY; /** new capacity 取 默认值 16 **/ newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); /**new threshold阀值default 12**/ } if (newThr == 0) {//todo //如果扩容（扩容没有设置阀值）阀值为0，计算新的阀值上限 float ft = (float)newCap * loadFactor; // todo //通过新表大小*负载因子获取 newThr = (newCap [] newTab = (Node[])new Node[newCap];//todo 扩表 table = newTab;//todo 将当前表（newTable）赋予table if (oldTab != null) {//todo 若oldTab中有值需要通过循环将oldTab中的值保存到新表newTab中 for (int j = 0; j e; // todo 临时节点 if ((e = oldTab[j]) != null) {//获取老表中第j个元素 赋予e todo 如果当前位置元素不为空，那么需要转移该元素到新数组 oldTab[j] = null;//todo 元数据j位置置为null，便于垃圾回收 if (e.next == null)//todo //数组没有下一个引用（不是链表）,单元素 newTab[e.hash & (newCap - 1)] = e; else if (e instanceof TreeNode)//todo 红黑树 ((TreeNode)e).split(this, newTab, j, oldCap);//todo todo 分割树，将新表和旧表分割成两个树，并判断索引处节点的长度是否需要转换成红黑树放入新表存储 else { // preserve order todo // 之所以定义两个头两个尾对象,是由于链表中的元素的下标在扩容后,要么是原下标+oldCap,要么不变。 Node loHead = null, loTail = null;//todo loHead：用于存储原下标未变的，链表结构；loTail：新加一个节点，并把 当前节点 变成 尾部节点，方便下一次追加 Node hiHead = null, hiTail = null; Node next; do {//todo 遍历桶中的链表 特别牛逼的设计思想 next = e.next; if ((e.hash & oldCap) == 0) {//原索引 下标没有改变 todo if (loTail == null) loHead = e;// 第一个节点 else loTail.next = e;// 加入到尾部 loTail = e;// todo 调整尾部元素 为 当前新加入的节点 } else {// todo 原索引+oldCap 下标改变 if (hiTail == null) hiHead = e;// 第一个节点 else hiTail.next = e;//todo 从第一个节点依次追加入Node hiHead的 Node next中， 加入到尾部 hiTail = e;// 调整尾部元素 } } while ((e = next) != null); if (loTail != null) {// 原下标对应的链表 loTail.next = null;// 尾部节点next设置为null，代码严谨 newTab[j] = loHead; } if (hiTail != null) {// 新下标对应的链表 hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } 链表迁移个人分析： 若原来e = oldTab[j])对应的链表为a b c 1 2 3 abc定义e.hash & oldCap计算为0，而123为1 也就是abc是低位链表，迁移后不变，而123是高位链表， 先定义低位链表头部loHead和尾部loTail, if (loTail == null) loHead = e;// (1)e->a时，当loTail为null，也就是没有节点时，将loHead指向e，也就是loHead指向a else loTail.next = e;// (2)e->b时， 尾节点loTail的下一个节点指向e，也就是b loTail = e;//(1)e->a时，将loTail也指向e，也就是a //步骤(2)将loTail节点指向e，也就是b,即为尾节点下移 （1）当e为a时， 当loTail为null，也就是没有节点时，将loHead指向e，也就是loHead指向a同时将loTail也指向e，也就是a （2）当e为b时(e指向a的下一个节点为b) 低位链表头节点loHead不变，还是指向a，而尾节点loTail的下一个节点指向e，也就是b,同时将尾节点loTail指向e，也就是b这时候就是loHead指向a，而loTail指向b，这样低位链表的节点就起来了 （3）同理当e为c时 Node loHead = null, loTail = null;//todo loHead：用于存储原下标未变的，链表结构；loTail：新加一个节点，并把 当前节点 变成 尾部节点，方便下一次追加 Node hiHead = null, hiTail = null; Node next; do {//todo 遍历桶中的链表 特别牛逼的设计思想 next = e.next; if ((e.hash & oldCap) == 0) {//原索引 下标没有改变 todo if (loTail == null) loHead = e;// 第一个节点 else loTail.next = e;// 加入到尾部 loTail = e;// todo 调整尾部元素 为 当前新加入的节点 } else {// todo 原索引+oldCap 下标改变 if (hiTail == null) hiHead = e;// 第一个节点 else hiTail.next = e;//todo 从第一个节点依次追加入Node hiHead的 Node next中， 加入到尾部 hiTail = e;// 调整尾部元素 } } while ((e = next) != null); if (loTail != null) {// 原下标对应的链表 loTail.next = null;// 尾部节点next设置为null，代码严谨 newTab[j] = loHead; } if (hiTail != null) {// 新下标对应的链表 hiTail.next = null; newTab[j + oldCap] = hiHead; } 下标计算 个人分析： 原来数组长度4 下标0 1 2 3 新数组长度8 下标0 1 2 3 4 5 6 7 hash值为1时 旧数组 00000000000000000 0000000000000001 00000000000000000 0000000000000011 00000000000000000 0000000000000001 下标1 新数组 00000000000000000 0000000000000001 00000000000000000 0000000000000111 00000000000000000 0000000000000001 下标1 上面用取模或与运算计算出下标相同， 而e.hash & oldCap计算如下： 00000000000000000 0000000000000001 00000000000000000 0000000000000100 00000000000000000 0000000000000000 计算结果等于0，当判断为0时，则下标迁移到新数组还是一样 hash值为4时 旧数组 00000000000000000 0000000000000100 00000000000000000 0000000000000011 00000000000000000 0000000000000000 下标0 新数组 00000000000000000 0000000000000100 00000000000000000 0000000000000111 00000000000000000 0000000000000100 下标4 上面用取模或与运算计算出下标不相同 而e.hash & oldCap计算如下： 00000000000000000 0000000000000100 00000000000000000 0000000000000100 00000000000000000 0000000000000100 计算结果等于4，当判断不为0时，则下标迁移到新数组变为原来下标+原来数组长度， 也就是0+4=4 java.util.HashMap.TreeNode#split 第一个节点为红黑树时，则扩容原来的红黑树 /** * 分割树，将新表和旧表分割成两个树，并判断索引处节点的长度是否需要转换成红黑树放入新表存储 * 处理方式和链表处理大的方法类似 **/ final void split(HashMap map, Node[] tab, int index, int bit) { TreeNode b = this; // Relink into lo and hi lists, preserving order TreeNode loHead = null, loTail = null; TreeNode hiHead = null, hiTail = null; int lc = 0, hc = 0; for (TreeNode e = b, next; e != null; e = next) { next = (TreeNode)e.next; e.next = null; if ((e.hash & bit) == 0) { if ((e.prev = loTail) == null) loHead = e; else loTail.next = e; loTail = e; ++lc; } else { if ((e.prev = hiTail) == null) hiHead = e; else hiTail.next = e; hiTail = e; ++hc; } } if (loHead != null) { if (lc https://mp.weixin.qq.com/s/wZn-dh3tqFneGCDjtlhdmA get public V get(Object key) { Node e; return (e = getNode(hash(key), key)) == null ? null : e.value; } final Node getNode(int hash, Object key) { Node[] tab; Node first, e; int n; K k; if ((tab = table) != null && (n = tab.length) > 0 && (first = tab[(n - 1) & hash]) != null) { if (first.hash == hash && // always check first node ((k = first.key) == key || (key != null && key.equals(k)))) return first; if ((e = first.next) != null) { if (first instanceof TreeNode) return ((TreeNode)first).getTreeNode(hash, key); do { if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } 构造函数和变为2的倍数 public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor tableSizeFor /** * Returns a power of two size for the given target capacity. */ static final int tableSizeFor(int cap) { int n = cap - 1; n |= n >>> 1; n |= n >>> 2; n |= n >>> 4; n |= n >>> 8; n |= n >>> 16; return (n = MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } Node节点类源码: 链表节点 如下为链表节点的数据结构设计：包括key，value，key的hash值，当前节点在链表中的下一个节点next。 该链表内的所有节点的key的hash值是相同的，链表头结点存放在哈希表table中，基于hash值与table大小获取该链表头结点在table中的位置下标。 在红黑树中获取某个节点的时间复杂度为O(logN) /** * Basic hash bin node, used for most entries. (See below for * TreeNode subclass, and in LinkedHashMap for its Entry subclass.) */ static class Node implements Map.Entry { final int hash;// 哈希值，存放元素到hashmap中时用来与其他元素hash值比较 final K key;//键 V value;//值 // 指向下一个节点 Node next; Node(int hash, K key, V value, Node next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + \"=\" + value; } // 重写hashCode()方法 public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } // 重写 equals() 方法 public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry e = (Map.Entry)o; if (Objects.equals(key, e.getKey()) && Objects.equals(value, e.getValue())) return true; } return false; } } 参考：https://mp.weixin.qq.com/s?__biz=MzU3MzQ3MzUyNw==&mid=2247485929&idx=2&sn=f33af98df0ee5e5006bf95c8f54cfff5&chksm=fcc05e71cbb7d76706e4d665789367f907139f43a9e750e0db7e1a36d853e163cda8546e9d19&scene=21#wechat_redirect 树节点类源码: 与链表节点一样，红黑树中存放key的hash值相同的节点集合，其中红黑树根节点root存放在哈希表table中，对应的table数组下标也是基于key的hash值和数组table大小获取。 static final class TreeNode extends LinkedHashMap.Entry { TreeNode parent; // 父 TreeNode left; // 左 TreeNode right; // 右 TreeNode prev; // needed to unlink next upon deletion boolean red; // 判断颜色 TreeNode(int hash, K key, V val, Node next) { super(hash, key, val, next); } // 返回根节点 final TreeNode root() { for (TreeNode r = this, p;;) { if ((p = r.parent) == null) return r; r = p; } 参考 Java基础系列：JDK1.8源码分析之HashMap JDK1.8源码分析：可重入锁ReentrantLock和Condition的实现原理 hashmap视频 HashMap和ConcurrentHashMap原理详解【图灵学院】 数据结构精华-HashMap+ConcurrentHashMap【图灵学院】 【JDK8】HashMap实现原理 HashMap常见面试题及解答【鲁班学院】 腾讯课堂-hashmap视频 ConcurrentHashMap1.8源码分析,java高级,java架构进阶_咕泡学院 红黑树 http://note.youdao.com/noteshare?id=a356dd5d38a4bff0dba7b512a9c7fcd9 "},"base/collection/hashmap/Jdk1.8源码解析HashMap-putTreeVal.html":{"url":"base/collection/hashmap/Jdk1.8源码解析HashMap-putTreeVal.html","title":"Jdk1.8源码解析-java.util.HashMap#putTreeVal","keywords":"","body":" java.util.HashMap.TreeNode#putTreeVal /** * todo 当存在hash碰撞的时候，且元素数量大于8个时候，就会以红黑树的方式将这些元素组织起来 * @param map 当前节点所在的HashMap对象; * @param tab 当前HashMap对象的元素数组; * @param h 指定key的hash值; * @param k 指定key; * @param v 指定key上要写入的值; * @return 返回：指定key所匹配到的节点对象，针对这个对象去修改V（返回空说明创建了一个新节点） * Tree version of putVal. **/ final TreeNode putTreeVal(HashMap map, Node[] tab, int h, K k, V v) { //todo 定义k的Class对象 Class kc = null; //todo 标识是否已经遍历过一次树，未必是从根节点遍历的，但是遍历路径上一定已经包含了后续需要比对的所有节点。 boolean searched = false; //todo 父节点不为空那么查找根节点，为空那么自身就是根节点 TreeNode root = (parent != null) ? root() : this; // todo 从根节点开始遍历，没有终止条件，只能从内部退出 for (TreeNode p = root;;) { //todo // 声明方向、当前节点hash值、当前节点的键对象 int dir, ph; K pk; // 如果当前节点hash 大于 指定key的hash值 if ((ph = p.hash) > h) dir = -1;// todo 要添加的元素应该放置在当前节点的左侧 else if (ph q, ch; searched = true;// 标识已经遍历过一次了 //todo 红黑树也是二叉树，所以只要沿着左右两侧遍历寻找就可以了， //这是个短路或（||）运算，如果先从左侧就已经找到了，右侧就不需要遍历了 //find 方法内部还会有递归调用。参见：find方法解析 找到了指定key键对应的 if (((ch = p.left) != null && (q = ch.find(h, k, kc)) != null) || ((ch = p.right) != null && (q = ch.find(h, k, kc)) != null)) return q;// } // 走到这里就说明，遍历了所有子节点也没有找到和当前键equals相等的节点 todo 再比较一下当前节点键和指定key键的大小 dir = tieBreakOrder(k, pk); } TreeNode xp = p;// 定义xp指向当前节点 /** * 1、如果dir小于等于0，那么看当前节点的左节点是否为空，如果为空，就可以把要添加的元素作为当前节点的左节点，如果不为空，还需要下一轮继续比较； * 2、如果dir大于等于0，那么看当前节点的右节点是否为空，如果为空，就可以把要添加的元素作为当前节点的右节点，如果不为空，还需要下一轮继续比较； * 3、如果以上两条当中有一个子节点不为空，这个if中还做了一件事，那就是把p已经指向了对应的不为空的子节点，开始下一轮的比较。 */ // 如果恰好要添加的方向上的子节点为空，此时节点p已经指向了这个空的子节点 if ((p = (dir xpn = xp.next;// 获取当前节点的next节点 TreeNode x = map.newTreeNode(h, k, v, xpn);// 创建一个新的树节点 if (dir )xpn).prev = x;// 那么原来的next节点的前节点指向到新的树节点 moveRootToFront(tab, balanceInsertion(root, x));//todo 重新平衡，以及新的根节点置顶 return null;// todo 返回空，意味着产生了一个新节点 } } } java.util.HashMap.TreeNode#root /** * Returns root of tree containing this node. * 找到 TreeNode 的根结点 **/ final TreeNode root() { for (TreeNode r = this, p;;) { //若r的父节点 == null，则 r为根结点，return r if ((p = r.parent) == null) return r; //若p = r.parent，r的父节点不为空，把 p 赋给 r，继续循环，直到找到根结点，并返回 r = p; } } java.util.HashMap#comparableClassFor /** * Returns x's Class if it is of the form \"class C implements * Comparable\", else null. todo 如果对象x的类是C，如果C实现了Comparable接口，那么返回C，否则返回null */ static Class comparableClassFor(Object x) { if (x instanceof Comparable) { Class c; Type[] ts, as; Type t; ParameterizedType p; if ((c = x.getClass()) == String.class) // bypass checks todo 如果x是个字符串对象 return c;// todo 返回String.class；others：为什么如果x是个字符串就直接返回c了呢 ? 因为String 实现了 Comparable 接口，可参考如下String类的定义public final class String implements java.io.Serializable, Comparable, CharSequence //todo 如果 c 不是字符串类，获取c直接实现的接口（如果是泛型接口则附带泛型信息）**/ if ((ts = c.getGenericInterfaces()) != null) {// 遍历接口数组 for (int i = 0; i } } return null;// 如果c并没有实现 Comparable 那么返回空 } java.util.HashMap#compareComparables /** * Returns k.compareTo(x) if x matches kc (k's screened comparable * class), else 0. * todo 如果x所属的类是kc，返回k.compareTo(x)的比较结果, * todo 如果x为空，或者其所属的类不是kc，返回0 */ @SuppressWarnings({\"rawtypes\",\"unchecked\"}) // for cast to Comparable static int compareComparables(Class kc, Object k, Object x) { return (x == null || x.getClass() != kc ? 0 : ((Comparable)k).compareTo(x)); } java.util.HashMap#newTreeNode // Create a tree bin node TreeNode newTreeNode(int hash, K key, V value, Node next) { return new TreeNode<>(hash, key, value, next); } java.util.HashMap.TreeNode#TreeNode TreeNode(int hash, K key, V val, Node next) { super(hash, key, val, next); } java.util.LinkedHashMap.Entry#Entry Entry(int hash, K key, V value, Node next) { super(hash, key, value, next); } java.util.HashMap.TreeNode#putTreeVal "},"base/collection/hashmap/Jdk1.8源码解析HashMap-resize.html":{"url":"base/collection/hashmap/Jdk1.8源码解析HashMap-resize.html","title":"Jdk1.8源码解析-java.util.HashMap#resize","keywords":"","body":" resize在HashMap中，同样相当重要，每到数据存储的数量达到，限制阀值时，就需要把原来的HashMap数组扩大一次或多次，已满足可以继续存储数据。与此同时，每次扩容时，原数据的存储位置，要么还在原来位置存储，要么原来位置挪动 oldCap后存储。 java.util.HashMap#resize final Node[] resize() {/** 重新设置table大小/扩容 并返回扩容的Node数组即HashMap的最新数据 **/ Node[] oldTab = table;//todo 把table（容器值）赋给 临时容器 oldTab，待用，用完之后，重置oldTab = null； int oldCap = (oldTab == null) ? 0 : oldTab.length;//原数组如果为null，则长度赋值0 int oldThr = threshold; int newCap, newThr = 0; if (oldCap > 0) {//todo 如果原数组长度大于0，说明原数组已经存在元素 if (oldCap >= MAXIMUM_CAPACITY) {//todo 数组大小如果已经大于等于最大值(2^30) threshold = Integer.MAX_VALUE;//todo 修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 return oldTab; }////原数组长度大于等于初始化长度16，并且原数组长度扩大1倍也小于2^30次方 else if ((newCap = oldCap = DEFAULT_INITIAL_CAPACITY) newThr = oldThr 0) // initial capacity was placed in threshold todo //原来的数组没有数据，故new容量 = oldThr 老阀值 newCap = oldThr; else { // zero initial threshold signifies using defaults todo 阀值等于0,oldCap也等于0(集合未进行初始化） newCap = DEFAULT_INITIAL_CAPACITY; /** new capacity 取 默认值 16 **/ newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); /**new threshold阀值default 12**/ } if (newThr == 0) {//todo //如果扩容（扩容没有设置阀值）阀值为0，计算新的阀值上限 float ft = (float)newCap * loadFactor; // todo //通过新表大小*负载因子获取 newThr = (newCap [] newTab = (Node[])new Node[newCap];//todo 扩表 table = newTab;//todo 将当前表（newTable）赋予table if (oldTab != null) {//todo 若oldTab中有值需要通过循环将oldTab中的值保存到新表newTab中 for (int j = 0; j e; // todo 临时节点 if ((e = oldTab[j]) != null) {//获取老表中第j个元素 赋予e todo 如果当前位置元素不为空，那么需要转移该元素到新数组 oldTab[j] = null;//todo 元数据j位置置为null，便于垃圾回收 if (e.next == null)//todo //数组没有下一个引用（不是链表）,单元素 newTab[e.hash & (newCap - 1)] = e; else if (e instanceof TreeNode)//todo 红黑树 ((TreeNode)e).split(this, newTab, j, oldCap);//todo todo 分割树，将新表和旧表分割成两个树，并判断索引处节点的长度是否需要转换成红黑树放入新表存储 else { // preserve order todo // 之所以定义两个头两个尾对象,是由于链表中的元素的下标在扩容后,要么是原下标+oldCap,要么不变。 Node loHead = null, loTail = null;//todo loHead：用于存储原下标未变的，链表结构；loTail：新加一个节点，并把 当前节点 变成 尾部节点，方便下一次追加 Node hiHead = null, hiTail = null; Node next; do {//todo 遍历桶中的链表 特别牛逼的设计思想 next = e.next; if ((e.hash & oldCap) == 0) {//原索引 下标没有改变 todo if (loTail == null) loHead = e;// 第一个节点 else loTail.next = e;// 加入到尾部 loTail = e;// todo 调整尾部元素 为 当前新加入的节点 } else {// todo 原索引+oldCap 下标改变 if (hiTail == null) hiHead = e;// 第一个节点 else hiTail.next = e;//todo 从第一个节点依次追加入Node hiHead的 Node next中， 加入到尾部 hiTail = e;// 调整尾部元素 } } while ((e = next) != null); if (loTail != null) {// 原下标对应的链表 loTail.next = null;// 尾部节点next设置为null，代码严谨 newTab[j] = loHead; } if (hiTail != null) {// 新下标对应的链表 hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } java.util.HashMap.TreeNode#split /** * 分割树，将新表和旧表分割成两个树，并判断索引处节点的长度是否需要转换成红黑树放入新表存储 * 处理方式和链表处理大的方法类似 **/ final void split(HashMap map, Node[] tab, int index, int bit) { TreeNode b = this; // Relink into lo and hi lists, preserving order TreeNode loHead = null, loTail = null; TreeNode hiHead = null, hiTail = null; int lc = 0, hc = 0; for (TreeNode e = b, next; e != null; e = next) { next = (TreeNode)e.next; e.next = null; if ((e.hash & bit) == 0) { if ((e.prev = loTail) == null) loHead = e; else loTail.next = e; loTail = e; ++lc; } else { if ((e.prev = hiTail) == null) hiHead = e; else hiTail.next = e; hiTail = e; ++hc; } } if (loHead != null) { if (lc Jdk1.8 源码解析：java.util.HashMap#resize() Segment static class Segment extends ReentrantLock implements Serializable { private static final long serialVersionUID = 2249069246763182397L; final float loadFactor; Segment(float lf) { this.loadFactor = lf; } } "},"base/collection/hashmap/Jdk1.8源码解析HashMap-treeifyBin.html":{"url":"base/collection/hashmap/Jdk1.8源码解析HashMap-treeifyBin.html","title":"Jdk1.8源码解析-java.util.HashMap#treeifyBin链表转化为红黑树","keywords":"","body":" java.util.HashMap#treeifyBin链表转化为红黑树 除了链表的长度大于等于8，还有hashmap数组长度要大于等于64才会转化为红黑树 /** * Replaces all linked nodes in bin at index for given hash unless * table is too small, in which case resizes instead. */ final void treeifyBin(Node[] tab, int hash) {//链表的树形化方法：如果链表的长度超过了8，则调用该方法 int n, index; Node e; if (tab == null || (n = tab.length) hd = null, tl = null; do {// 3. 从链表的第一个节点开始构建树，e是链表的第一个节点 TreeNode p = replacementTreeNode(e, null);// 将该节点转换为 树节点 if (tl == null)// 如果尾节点为空，说明还没有根节点 hd = p;// 首节点（根节点）指向 当前节点 else {// todo 尾节点不为空，以下两行是一个双向链表结构 p.prev = tl;// 当前树节点的 前一个节点指向 尾节点 tl.next = p;// 尾节点的 后一个节点指向 当前节点 } tl = p;// 把当前节点设为尾节点 } while ((e = e.next) != null);//todo 循环结束后 // 到目前为止 也只是把Node对象转换成了TreeNode对象，把单向链表转换成了双向链表 if ((tab[index] = hd) != null)// 把转换后的双向链表，替换原来位置上的单向链表 todo 4. 让数组的第一个元素指向红黑树的头节点，以后这个数组里的元素就是红黑树，而不是链表了 hd.treeify(tab); } } Jdk1.8 源码解析：java.util.HashMap#treeifyBin java.util.HashMap#replacementTreeNode // For treeifyBin TreeNode replacementTreeNode(Node p, Node next) { return new TreeNode<>(p.hash, p.key, p.value, next); } java.util.HashMap.TreeNode#treeify /** * todo 参数为HashMap的元素数组 * Forms tree of the nodes linked from this node. * @return root of tree */ final void treeify(Node[] tab) { TreeNode root = null;// 定义树的根节点 for (TreeNode x = this, next; x != null; x = next) {// 遍历链表，x指向当前节点、next指向下一个节点 next = (TreeNode)x.next;// 下一个节点 x.left = x.right = null;// todo 设置当前节点的左右节点为空 if (root == null) {// 如果还没有根节点 x.parent = null;// 当前节点的父节点设为空 x.red = false;// 当前节点的红色属性设为false（把当前节点设为黑色） root = x;// 根节点指向到当前节点 } else {// 如果已经存在根节点了 K k = x.key;// 取得当前链表节点的key int h = x.hash;// 取得当前链表节点的hash值 Class kc = null;// 定义key所属的Class for (TreeNode p = root;;) {// todo 从根节点开始遍历，此遍历没有设置边界，只能从内部跳出 int dir, ph;//（ GOTO1） todo dir 标识方向（左右）、ph标识当前树节点的hash值 K pk = p.key;// 当前树节点的key if ((ph = p.hash) > h)// 如果当前树节点hash值 大于 当前链表节点的hash值 dir = -1;// 标识当前链表节点会放到当前树节点的左侧 else if (ph xp = p;// todo 保存当前树节点 /** * 1、如果dir 小于等于0 ： 当前链表节点一定放置在当前树节点的左侧，但不一定是该树节点的左孩子，也可能是左孩子的右孩子 或者 更深层次的节点。 * 2、如果dir 大于0 ： 当前链表节点一定放置在当前树节点的右侧，但不一定是该树节点的右孩子，也可能是右孩子的左孩子 或者 更深层次的节点。 * 3、如果当前树节点不是叶子节点，那么最终会以当前树节点的 左孩子 或者 右孩子 为 起始节点 再从GOTO1 处开始 重新寻找自己（当前链表节点）的位置。 * 4、如果当前树节点就是叶子节点，那么根据dir的值，就可以把当前链表节点挂载到当前树节点的左或者右侧了。 * 5、挂载之后，还需要重新把树进行平衡。平衡之后，就可以针对下一个链表节点进行处理了。 **/ if ((p = (dir Segment static class Segment extends ReentrantLock implements Serializable { private static final long serialVersionUID = 2249069246763182397L; final float loadFactor; Segment(float lf) { this.loadFactor = lf; } } "},"base/collection/hashmap/其他map.html":{"url":"base/collection/hashmap/其他map.html","title":"其他map","keywords":"","body":" LinkedHashMap 继承了HashMap public class LinkedHashMap extends HashMap implements Map 双向链表，Entry继承HashMap.Node /** * HashMap.Node subclass for normal LinkedHashMap entries. */ static class Entry extends HashMap.Node { Entry before, after; Entry(int hash, K key, V value, Node next) { super(hash, key, value, next); } } private static final long serialVersionUID = 3801124242820219131L; /** * The head (eldest) of the doubly linked list. */ transient LinkedHashMap.Entry head; /** * The tail (youngest) of the doubly linked list. */ transient LinkedHashMap.Entry tail; linkedMap在于存储数据你想保持进入的顺序与被取出的顺序一致的话，优先考虑LinkedMap，hashMap键只能允许为一条为空，value可以允许为多条为空，键唯一，但值可以多个。 经本人测试linkedMap键和值都不可以为空 "},"base/collection/hashmap源码/jdk1.7ConcurrentHashMap源码.html":{"url":"base/collection/hashmap源码/jdk1.7ConcurrentHashMap源码.html","title":"jdk1.7ConcurrentHashMap源码","keywords":"","body":" 1、 /* * ORACLE PROPRIETARY/CONFIDENTIAL. Use is subject to license terms. * * * * * * * * * * * * * * * * * * * * */ /* * * * * * * Written by Doug Lea with assistance from members of JCP JSR-166 * Expert Group and released to the public domain, as explained at * http://creativecommons.org/publicdomain/zero/1.0/ */ package java.util.concurrent; import java.util.concurrent.locks.*; import java.util.*; import java.io.Serializable; import java.io.IOException; import java.io.ObjectInputStream; import java.io.ObjectOutputStream; import java.io.ObjectStreamField; /** * A hash table supporting full concurrency of retrievals and * adjustable expected concurrency for updates. This class obeys the * same functional specification as {@link java.util.Hashtable}, and * includes versions of methods corresponding to each method of * Hashtable. However, even though all operations are * thread-safe, retrieval operations do not entail locking, * and there is not any support for locking the entire table * in a way that prevents all access. This class is fully * interoperable with Hashtable in programs that rely on its * thread safety but not on its synchronization details. * * Retrieval operations (including get) generally do not * block, so may overlap with update operations (including * put and remove). Retrievals reflect the results * of the most recently completed update operations holding * upon their onset. For aggregate operations such as putAll * and clear, concurrent retrievals may reflect insertion or * removal of only some entries. Similarly, Iterators and * Enumerations return elements reflecting the state of the hash table * at some point at or since the creation of the iterator/enumeration. * They do not throw {@link ConcurrentModificationException}. * However, iterators are designed to be used by only one thread at a time. * * The allowed concurrency among update operations is guided by * the optional concurrencyLevel constructor argument * (default 16), which is used as a hint for internal sizing. The * table is internally partitioned to try to permit the indicated * number of concurrent updates without contention. Because placement * in hash tables is essentially random, the actual concurrency will * vary. Ideally, you should choose a value to accommodate as many * threads as will ever concurrently modify the table. Using a * significantly higher value than you need can waste space and time, * and a significantly lower value can lead to thread contention. But * overestimates and underestimates within an order of magnitude do * not usually have much noticeable impact. A value of one is * appropriate when it is known that only one thread will modify and * all others will only read. Also, resizing this or any other kind of * hash table is a relatively slow operation, so, when possible, it is * a good idea to provide estimates of expected table sizes in * constructors. * * This class and its views and iterators implement all of the * optional methods of the {@link Map} and {@link Iterator} * interfaces. * * Like {@link Hashtable} but unlike {@link HashMap}, this class * does not allow null to be used as a key or value. * * This class is a member of the * * Java Collections Framework. * * @since 1.5 * @author Doug Lea * @param the type of keys maintained by this map * @param the type of mapped values */ public class ConcurrentHashMap extends AbstractMap implements ConcurrentMap, Serializable { private static final long serialVersionUID = 7249069246763182397L; /* * The basic strategy is to subdivide the table among Segments, * each of which itself is a concurrently readable hash table. To * reduce footprint, all but one segments are constructed only * when first needed (see ensureSegment). To maintain visibility * in the presence of lazy construction, accesses to segments as * well as elements of segment's table must use volatile access, * which is done via Unsafe within methods segmentAt etc * below. These provide the functionality of AtomicReferenceArrays * but reduce the levels of indirection. Additionally, * volatile-writes of table elements and entry \"next\" fields * within locked operations use the cheaper \"lazySet\" forms of * writes (via putOrderedObject) because these writes are always * followed by lock releases that maintain sequential consistency * of table updates. * * Historical note: The previous version of this class relied * heavily on \"final\" fields, which avoided some volatile reads at * the expense of a large initial footprint. Some remnants of * that design (including forced construction of segment 0) exist * to ensure serialization compatibility. */ /* ---------------- Constants -------------- */ /** * The default initial capacity for this table, * used when not otherwise specified in a constructor. */ static final int DEFAULT_INITIAL_CAPACITY = 16; /** * The default load factor for this table, used when not * otherwise specified in a constructor. */ static final float DEFAULT_LOAD_FACTOR = 0.75f; /** * The default concurrency level for this table, used when not * otherwise specified in a constructor. */ static final int DEFAULT_CONCURRENCY_LEVEL = 16; /** * The maximum capacity, used if a higher value is implicitly * specified by either of the constructors with arguments. MUST * be a power of two Unlike the other hash map implementations we do not implement a * threshold for regulating whether alternative hashing is used for * String keys. Alternative hashing is either enabled for all instances * or disabled for all instances. */ static final boolean ALTERNATIVE_HASHING; static { // Use the \"threshold\" system property even though our threshold // behaviour is \"ON\" or \"OFF\". String altThreshold = java.security.AccessController.doPrivileged( new sun.security.action.GetPropertyAction( \"jdk.map.althashing.threshold\")); int threshold; try { threshold = (null != altThreshold) ? Integer.parseInt(altThreshold) : Integer.MAX_VALUE; // disable alternative hashing if -1 if (threshold == -1) { threshold = Integer.MAX_VALUE; } if (threshold [] segments; transient Set keySet; transient Set> entrySet; transient Collection values; /** * ConcurrentHashMap list entry. Note that this is never exported * out as a user-visible Map.Entry. */ static final class HashEntry { final int hash; final K key; volatile V value; volatile HashEntry next; HashEntry(int hash, K key, V value, HashEntry next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } /** * Sets next field with volatile write semantics. (See above * about use of putOrderedObject.) */ final void setNext(HashEntry n) { UNSAFE.putOrderedObject(this, nextOffset, n); } // Unsafe mechanics static final sun.misc.Unsafe UNSAFE; static final long nextOffset; static { try { UNSAFE = sun.misc.Unsafe.getUnsafe(); Class k = HashEntry.class; nextOffset = UNSAFE.objectFieldOffset (k.getDeclaredField(\"next\")); } catch (Exception e) { throw new Error(e); } } } /** * Gets the ith element of given table (if nonnull) with volatile * read semantics. Note: This is manually integrated into a few * performance-sensitive methods to reduce call overhead. */ @SuppressWarnings(\"unchecked\") static final HashEntry entryAt(HashEntry[] tab, int i) { return (tab == null) ? null : (HashEntry) UNSAFE.getObjectVolatile (tab, ((long)i void setEntryAt(HashEntry[] tab, int i, HashEntry e) { UNSAFE.putOrderedObject(tab, ((long)i >> 10); h += (h >> 6); h += (h >> 16); } /** * Segments are specialized versions of hash tables. This * subclasses from ReentrantLock opportunistically, just to * simplify some locking and avoid separate construction. */ static final class Segment extends ReentrantLock implements Serializable { /* * Segments maintain a table of entry lists that are always * kept in a consistent state, so can be read (via volatile * reads of segments and tables) without locking. This * requires replicating nodes when necessary during table * resizing, so the old lists can be traversed by readers * still using old version of table. * * This class defines only mutative methods requiring locking. * Except as noted, the methods of this class perform the * per-segment versions of ConcurrentHashMap methods. (Other * methods are integrated directly into ConcurrentHashMap * methods.) These mutative methods use a form of controlled * spinning on contention via methods scanAndLock and * scanAndLockForPut. These intersperse tryLocks with * traversals to locate nodes. The main benefit is to absorb * cache misses (which are very common for hash tables) while * obtaining locks so that traversal is faster once * acquired. We do not actually use the found nodes since they * must be re-acquired under lock anyway to ensure sequential * consistency of updates (and in any case may be undetectably * stale), but they will normally be much faster to re-locate. * Also, scanAndLockForPut speculatively creates a fresh node * to use in put if no node is found. */ private static final long serialVersionUID = 2249069246763182397L; /** * The maximum number of times to tryLock in a prescan before * possibly blocking on acquire in preparation for a locked * segment operation. On multiprocessors, using a bounded * number of retries maintains cache acquired while locating * nodes. */ static final int MAX_SCAN_RETRIES = Runtime.getRuntime().availableProcessors() > 1 ? 64 : 1; /** * The per-segment table. Elements are accessed via * entryAt/setEntryAt providing volatile semantics. */ transient volatile HashEntry[] table; /** * The number of elements. Accessed only either within locks * or among other volatile reads that maintain visibility. */ transient int count; /** * The total number of mutative operations in this segment. * Even though this may overflows 32 bits, it provides * sufficient accuracy for stability checks in CHM isEmpty() * and size() methods. Accessed only either within locks or * among other volatile reads that maintain visibility. */ transient int modCount; /** * The table is rehashed when its size exceeds this threshold. * (The value of this field is always (int)(capacity * * loadFactor).) */ transient int threshold; /** * The load factor for the hash table. Even though this value * is same for all segments, it is replicated to avoid needing * links to outer object. * @serial */ final float loadFactor; Segment(float lf, int threshold, HashEntry[] tab) { this.loadFactor = lf; this.threshold = threshold; this.table = tab; } final V put(K key, int hash, V value, boolean onlyIfAbsent) { HashEntry node = tryLock() ? null : scanAndLockForPut(key, hash, value); V oldValue; try { HashEntry[] tab = table; int index = (tab.length - 1) & hash; HashEntry first = entryAt(tab, index); for (HashEntry e = first;;) { if (e != null) { K k; if ((k = e.key) == key || (e.hash == hash && key.equals(k))) { oldValue = e.value; if (!onlyIfAbsent) { e.value = value; ++modCount; } break; } e = e.next; } else { if (node != null) node.setNext(first); else node = new HashEntry(hash, key, value, first); int c = count + 1; if (c > threshold && tab.length node) { /* * Reclassify nodes in each list to new table. Because we * are using power-of-two expansion, the elements from * each bin must either stay at same index, or move with a * power of two offset. We eliminate unnecessary node * creation by catching cases where old nodes can be * reused because their next fields won't change. * Statistically, at the default threshold, only about * one-sixth of them need cloning when a table * doubles. The nodes they replace will be garbage * collectable as soon as they are no longer referenced by * any reader thread that may be in the midst of * concurrently traversing table. Entry accesses use plain * array indexing because they are followed by volatile * table write. */ HashEntry[] oldTable = table; int oldCapacity = oldTable.length; int newCapacity = oldCapacity [] newTable = (HashEntry[]) new HashEntry[newCapacity]; int sizeMask = newCapacity - 1; for (int i = 0; i e = oldTable[i]; if (e != null) { HashEntry next = e.next; int idx = e.hash & sizeMask; if (next == null) // Single node on list newTable[idx] = e; else { // Reuse consecutive sequence at same slot HashEntry lastRun = e; int lastIdx = idx; for (HashEntry last = next; last != null; last = last.next) { int k = last.hash & sizeMask; if (k != lastIdx) { lastIdx = k; lastRun = last; } } newTable[lastIdx] = lastRun; // Clone remaining nodes for (HashEntry p = e; p != lastRun; p = p.next) { V v = p.value; int h = p.hash; int k = h & sizeMask; HashEntry n = newTable[k]; newTable[k] = new HashEntry(h, p.key, v, n); } } } } int nodeIndex = node.hash & sizeMask; // add the new node node.setNext(newTable[nodeIndex]); newTable[nodeIndex] = node; table = newTable; } /** * Scans for a node containing given key while trying to * acquire lock, creating and returning one if not found. Upon * return, guarantees that lock is held. UNlike in most * methods, calls to method equals are not screened: Since * traversal speed doesn't matter, we might as well help warm * up the associated code and accesses as well. * * @return a new node if key not found, else null */ private HashEntry scanAndLockForPut(K key, int hash, V value) { HashEntry first = entryForHash(this, hash); HashEntry e = first; HashEntry node = null; int retries = -1; // negative while locating node while (!tryLock()) { HashEntry f; // to recheck first below if (retries (hash, key, value, null); retries = 0; } else if (key.equals(e.key)) retries = 0; else e = e.next; } else if (++retries > MAX_SCAN_RETRIES) { lock(); break; } else if ((retries & 1) == 0 && (f = entryForHash(this, hash)) != first) { e = first = f; // re-traverse if entry changed retries = -1; } } return node; } /** * Scans for a node containing the given key while trying to * acquire lock for a remove or replace operation. Upon * return, guarantees that lock is held. Note that we must * lock even if the key is not found, to ensure sequential * consistency of updates. */ private void scanAndLock(Object key, int hash) { // similar to but simpler than scanAndLockForPut HashEntry first = entryForHash(this, hash); HashEntry e = first; int retries = -1; while (!tryLock()) { HashEntry f; if (retries MAX_SCAN_RETRIES) { lock(); break; } else if ((retries & 1) == 0 && (f = entryForHash(this, hash)) != first) { e = first = f; retries = -1; } } } /** * Remove; match on key only if value null, else match both. */ final V remove(Object key, int hash, Object value) { if (!tryLock()) scanAndLock(key, hash); V oldValue = null; try { HashEntry[] tab = table; int index = (tab.length - 1) & hash; HashEntry e = entryAt(tab, index); HashEntry pred = null; while (e != null) { K k; HashEntry next = e.next; if ((k = e.key) == key || (e.hash == hash && key.equals(k))) { V v = e.value; if (value == null || value == v || value.equals(v)) { if (pred == null) setEntryAt(tab, index, next); else pred.setNext(next); ++modCount; --count; oldValue = v; } break; } pred = e; e = next; } } finally { unlock(); } return oldValue; } final boolean replace(K key, int hash, V oldValue, V newValue) { if (!tryLock()) scanAndLock(key, hash); boolean replaced = false; try { HashEntry e; for (e = entryForHash(this, hash); e != null; e = e.next) { K k; if ((k = e.key) == key || (e.hash == hash && key.equals(k))) { if (oldValue.equals(e.value)) { e.value = newValue; ++modCount; replaced = true; } break; } } } finally { unlock(); } return replaced; } final V replace(K key, int hash, V value) { if (!tryLock()) scanAndLock(key, hash); V oldValue = null; try { HashEntry e; for (e = entryForHash(this, hash); e != null; e = e.next) { K k; if ((k = e.key) == key || (e.hash == hash && key.equals(k))) { oldValue = e.value; e.value = value; ++modCount; break; } } } finally { unlock(); } return oldValue; } final void clear() { lock(); try { HashEntry[] tab = table; for (int i = 0; i Segment segmentAt(Segment[] ss, int j) { long u = (j ) UNSAFE.getObjectVolatile(ss, u); } /** * Returns the segment for the given index, creating it and * recording in segment table (via CAS) if not already present. * * @param k the index * @return the segment */ @SuppressWarnings(\"unchecked\") private Segment ensureSegment(int k) { final Segment[] ss = this.segments; long u = (k seg; if ((seg = (Segment)UNSAFE.getObjectVolatile(ss, u)) == null) { Segment proto = ss[0]; // use segment 0 as prototype int cap = proto.table.length; float lf = proto.loadFactor; int threshold = (int)(cap * lf); HashEntry[] tab = (HashEntry[])new HashEntry[cap]; if ((seg = (Segment)UNSAFE.getObjectVolatile(ss, u)) == null) { // recheck Segment s = new Segment(lf, threshold, tab); while ((seg = (Segment)UNSAFE.getObjectVolatile(ss, u)) == null) { if (UNSAFE.compareAndSwapObject(ss, u, null, seg = s)) break; } } } return seg; } // Hash-based segment and entry accesses /** * Get the segment for the given hash */ @SuppressWarnings(\"unchecked\") private Segment segmentForHash(int h) { long u = (((h >>> segmentShift) & segmentMask) ) UNSAFE.getObjectVolatile(segments, u); } /** * Gets the table entry for the given segment and hash */ @SuppressWarnings(\"unchecked\") static final HashEntry entryForHash(Segment seg, int h) { HashEntry[] tab; return (seg == null || (tab = seg.table) == null) ? null : (HashEntry) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) & h)) 0) || initialCapacity MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; // Find power-of-two sizes best matching arguments int sshift = 0; int ssize = 1; while (ssize MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; int c = initialCapacity / ssize; if (c * ssize s0 = new Segment(loadFactor, (int)(cap * loadFactor), (HashEntry[])new HashEntry[cap]); Segment[] ss = (Segment[])new Segment[ssize]; UNSAFE.putOrderedObject(ss, SBASE, s0); // ordered write of segments[0] this.segments = ss; } /** * Creates a new, empty map with the specified initial capacity * and load factor and with the default concurrencyLevel (16). * * @param initialCapacity The implementation performs internal * sizing to accommodate this many elements. * @param loadFactor the load factor threshold, used to control resizing. * Resizing may be performed when the average number of elements per * bin exceeds this threshold. * @throws IllegalArgumentException if the initial capacity of * elements is negative or the load factor is nonpositive * * @since 1.6 */ public ConcurrentHashMap(int initialCapacity, float loadFactor) { this(initialCapacity, loadFactor, DEFAULT_CONCURRENCY_LEVEL); } /** * Creates a new, empty map with the specified initial capacity, * and with default load factor (0.75) and concurrencyLevel (16). * * @param initialCapacity the initial capacity. The implementation * performs internal sizing to accommodate this many elements. * @throws IllegalArgumentException if the initial capacity of * elements is negative. */ public ConcurrentHashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL); } /** * Creates a new, empty map with a default initial capacity (16), * load factor (0.75) and concurrencyLevel (16). */ public ConcurrentHashMap() { this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL); } /** * Creates a new map with the same mappings as the given map. * The map is created with a capacity of 1.5 times the number * of mappings in the given map or 16 (whichever is greater), * and a default load factor (0.75) and concurrencyLevel (16). * * @param m the map */ public ConcurrentHashMap(Map m) { this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL); putAll(m); } /** * Returns true if this map contains no key-value mappings. * * @return true if this map contains no key-value mappings */ public boolean isEmpty() { /* * Sum per-segment modCounts to avoid mis-reporting when * elements are concurrently added and removed in one segment * while checking another, in which case the table was never * actually empty at any point. (The sum ensures accuracy up * through at least 1[] segments = this.segments; for (int j = 0; j seg = segmentAt(segments, j); if (seg != null) { if (seg.count != 0) return false; sum += seg.modCount; } } if (sum != 0L) { // recheck unless no modifications for (int j = 0; j seg = segmentAt(segments, j); if (seg != null) { if (seg.count != 0) return false; sum -= seg.modCount; } } if (sum != 0L) return false; } return true; } /** * Returns the number of key-value mappings in this map. If the * map contains more than Integer.MAX_VALUE elements, returns * Integer.MAX_VALUE. * * @return the number of key-value mappings in this map */ public int size() { // Try a few times to get accurate count. On failure due to // continuous async changes in table, resort to locking. final Segment[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits long sum; // sum of modCounts long last = 0L; // previous sum int retries = -1; // first iteration isn't retry try { for (;;) { if (retries++ == RETRIES_BEFORE_LOCK) { for (int j = 0; j seg = segmentAt(segments, j); if (seg != null) { sum += seg.modCount; int c = seg.count; if (c RETRIES_BEFORE_LOCK) { for (int j = 0; j More formally, if this map contains a mapping from a key * {@code k} to a value {@code v} such that {@code key.equals(k)}, * then this method returns {@code v}; otherwise it returns * {@code null}. (There can be at most one such mapping.) * * @throws NullPointerException if the specified key is null */ public V get(Object key) { Segment s; // manually integrate access methods to reduce overhead HashEntry[] tab; int h = hash(key); long u = (((h >>> segmentShift) & segmentMask) )UNSAFE.getObjectVolatile(segments, u)) != null && (tab = s.table) != null) { for (HashEntry e = (HashEntry) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) & h)) true if and only if the specified object * is a key in this table, as determined by the * equals method; false otherwise. * @throws NullPointerException if the specified key is null */ @SuppressWarnings(\"unchecked\") public boolean containsKey(Object key) { Segment s; // same as get() except no need for volatile value read HashEntry[] tab; int h = hash(key); long u = (((h >>> segmentShift) & segmentMask) )UNSAFE.getObjectVolatile(segments, u)) != null && (tab = s.table) != null) { for (HashEntry e = (HashEntry) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) & h)) true if this map maps one or more keys to the * specified value. Note: This method requires a full internal * traversal of the hash table, and so is much slower than * method containsKey. * * @param value value whose presence in this map is to be tested * @return true if this map maps one or more keys to the * specified value * @throws NullPointerException if the specified value is null */ public boolean containsValue(Object value) { // Same idea as size() if (value == null) throw new NullPointerException(); final Segment[] segments = this.segments; boolean found = false; long last = 0; int retries = -1; try { outer: for (;;) { if (retries++ == RETRIES_BEFORE_LOCK) { for (int j = 0; j [] tab; Segment seg = segmentAt(segments, j); if (seg != null && (tab = seg.table) != null) { for (int i = 0 ; i e; for (e = entryAt(tab, i); e != null; e = e.next) { V v = e.value; if (v != null && value.equals(v)) { found = true; break outer; } } } sum += seg.modCount; } } if (retries > 0 && sum == last) break; last = sum; } } finally { if (retries > RETRIES_BEFORE_LOCK) { for (int j = 0; j true if and only if some key maps to the * value argument in this table as * determined by the equals method; * false otherwise * @throws NullPointerException if the specified value is null */ public boolean contains(Object value) { return containsValue(value); } /** * Maps the specified key to the specified value in this table. * Neither the key nor the value can be null. * * The value can be retrieved by calling the get method * with a key that is equal to the original key. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with key, or * null if there was no mapping for key * @throws NullPointerException if the specified key or value is null */ @SuppressWarnings(\"unchecked\") public V put(K key, V value) { Segment s; if (value == null) throw new NullPointerException(); int hash = hash(key); int j = (hash >>> segmentShift) & segmentMask; if ((s = (Segment)UNSAFE.getObject // nonvolatile; recheck (segments, (j null if there was no mapping for the key * @throws NullPointerException if the specified key or value is null */ @SuppressWarnings(\"unchecked\") public V putIfAbsent(K key, V value) { Segment s; if (value == null) throw new NullPointerException(); int hash = hash(key); int j = (hash >>> segmentShift) & segmentMask; if ((s = (Segment)UNSAFE.getObject (segments, (j m) { for (Map.Entry e : m.entrySet()) put(e.getKey(), e.getValue()); } /** * Removes the key (and its corresponding value) from this map. * This method does nothing if the key is not in the map. * * @param key the key that needs to be removed * @return the previous value associated with key, or * null if there was no mapping for key * @throws NullPointerException if the specified key is null */ public V remove(Object key) { int hash = hash(key); Segment s = segmentForHash(hash); return s == null ? null : s.remove(key, hash, null); } /** * {@inheritDoc} * * @throws NullPointerException if the specified key is null */ public boolean remove(Object key, Object value) { int hash = hash(key); Segment s; return value != null && (s = segmentForHash(hash)) != null && s.remove(key, hash, value) != null; } /** * {@inheritDoc} * * @throws NullPointerException if any of the arguments are null */ public boolean replace(K key, V oldValue, V newValue) { int hash = hash(key); if (oldValue == null || newValue == null) throw new NullPointerException(); Segment s = segmentForHash(hash); return s != null && s.replace(key, hash, oldValue, newValue); } /** * {@inheritDoc} * * @return the previous value associated with the specified key, * or null if there was no mapping for the key * @throws NullPointerException if the specified key or value is null */ public V replace(K key, V value) { int hash = hash(key); if (value == null) throw new NullPointerException(); Segment s = segmentForHash(hash); return s == null ? null : s.replace(key, hash, value); } /** * Removes all of the mappings from this map. */ public void clear() { final Segment[] segments = this.segments; for (int j = 0; j s = segmentAt(segments, j); if (s != null) s.clear(); } } /** * Returns a {@link Set} view of the keys contained in this map. * The set is backed by the map, so changes to the map are * reflected in the set, and vice-versa. The set supports element * removal, which removes the corresponding mapping from this map, * via the Iterator.remove, Set.remove, * removeAll, retainAll, and clear * operations. It does not support the add or * addAll operations. * * The view's iterator is a \"weakly consistent\" iterator * that will never throw {@link ConcurrentModificationException}, * and guarantees to traverse elements as they existed upon * construction of the iterator, and may (but is not guaranteed to) * reflect any modifications subsequent to construction. */ public Set keySet() { Set ks = keySet; return (ks != null) ? ks : (keySet = new KeySet()); } /** * Returns a {@link Collection} view of the values contained in this map. * The collection is backed by the map, so changes to the map are * reflected in the collection, and vice-versa. The collection * supports element removal, which removes the corresponding * mapping from this map, via the Iterator.remove, * Collection.remove, removeAll, * retainAll, and clear operations. It does not * support the add or addAll operations. * * The view's iterator is a \"weakly consistent\" iterator * that will never throw {@link ConcurrentModificationException}, * and guarantees to traverse elements as they existed upon * construction of the iterator, and may (but is not guaranteed to) * reflect any modifications subsequent to construction. */ public Collection values() { Collection vs = values; return (vs != null) ? vs : (values = new Values()); } /** * Returns a {@link Set} view of the mappings contained in this map. * The set is backed by the map, so changes to the map are * reflected in the set, and vice-versa. The set supports element * removal, which removes the corresponding mapping from the map, * via the Iterator.remove, Set.remove, * removeAll, retainAll, and clear * operations. It does not support the add or * addAll operations. * * The view's iterator is a \"weakly consistent\" iterator * that will never throw {@link ConcurrentModificationException}, * and guarantees to traverse elements as they existed upon * construction of the iterator, and may (but is not guaranteed to) * reflect any modifications subsequent to construction. */ public Set> entrySet() { Set> es = entrySet; return (es != null) ? es : (entrySet = new EntrySet()); } /** * Returns an enumeration of the keys in this table. * * @return an enumeration of the keys in this table * @see #keySet() */ public Enumeration keys() { return new KeyIterator(); } /** * Returns an enumeration of the values in this table. * * @return an enumeration of the values in this table * @see #values() */ public Enumeration elements() { return new ValueIterator(); } /* ---------------- Iterator Support -------------- */ abstract class HashIterator { int nextSegmentIndex; int nextTableIndex; HashEntry[] currentTable; HashEntry nextEntry; HashEntry lastReturned; HashIterator() { nextSegmentIndex = segments.length - 1; nextTableIndex = -1; advance(); } /** * Set nextEntry to first node of next non-empty table * (in backwards order, to simplify checks). */ final void advance() { for (;;) { if (nextTableIndex >= 0) { if ((nextEntry = entryAt(currentTable, nextTableIndex--)) != null) break; } else if (nextSegmentIndex >= 0) { Segment seg = segmentAt(segments, nextSegmentIndex--); if (seg != null && (currentTable = seg.table) != null) nextTableIndex = currentTable.length - 1; } else break; } } final HashEntry nextEntry() { HashEntry e = nextEntry; if (e == null) throw new NoSuchElementException(); lastReturned = e; // cannot assign until after null check if ((nextEntry = e.next) == null) advance(); return e; } public final boolean hasNext() { return nextEntry != null; } public final boolean hasMoreElements() { return nextEntry != null; } public final void remove() { if (lastReturned == null) throw new IllegalStateException(); ConcurrentHashMap.this.remove(lastReturned.key); lastReturned = null; } } final class KeyIterator extends HashIterator implements Iterator, Enumeration { public final K next() { return super.nextEntry().key; } public final K nextElement() { return super.nextEntry().key; } } final class ValueIterator extends HashIterator implements Iterator, Enumeration { public final V next() { return super.nextEntry().value; } public final V nextElement() { return super.nextEntry().value; } } /** * Custom Entry class used by EntryIterator.next(), that relays * setValue changes to the underlying map. */ final class WriteThroughEntry extends AbstractMap.SimpleEntry { WriteThroughEntry(K k, V v) { super(k,v); } /** * Set our entry's value and write through to the map. The * value to return is somewhat arbitrary here. Since a * WriteThroughEntry does not necessarily track asynchronous * changes, the most recent \"previous\" value could be * different from what we return (or could even have been * removed in which case the put will re-establish). We do not * and cannot guarantee more. */ public V setValue(V value) { if (value == null) throw new NullPointerException(); V v = super.setValue(value); ConcurrentHashMap.this.put(getKey(), value); return v; } } final class EntryIterator extends HashIterator implements Iterator> { public Map.Entry next() { HashEntry e = super.nextEntry(); return new WriteThroughEntry(e.key, e.value); } } final class KeySet extends AbstractSet { public Iterator iterator() { return new KeyIterator(); } public int size() { return ConcurrentHashMap.this.size(); } public boolean isEmpty() { return ConcurrentHashMap.this.isEmpty(); } public boolean contains(Object o) { return ConcurrentHashMap.this.containsKey(o); } public boolean remove(Object o) { return ConcurrentHashMap.this.remove(o) != null; } public void clear() { ConcurrentHashMap.this.clear(); } } final class Values extends AbstractCollection { public Iterator iterator() { return new ValueIterator(); } public int size() { return ConcurrentHashMap.this.size(); } public boolean isEmpty() { return ConcurrentHashMap.this.isEmpty(); } public boolean contains(Object o) { return ConcurrentHashMap.this.containsValue(o); } public void clear() { ConcurrentHashMap.this.clear(); } } final class EntrySet extends AbstractSet> { public Iterator> iterator() { return new EntryIterator(); } public boolean contains(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; V v = ConcurrentHashMap.this.get(e.getKey()); return v != null && v.equals(e.getValue()); } public boolean remove(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; return ConcurrentHashMap.this.remove(e.getKey(), e.getValue()); } public int size() { return ConcurrentHashMap.this.size(); } public boolean isEmpty() { return ConcurrentHashMap.this.isEmpty(); } public void clear() { ConcurrentHashMap.this.clear(); } } /* ---------------- Serialization Support -------------- */ /** * Save the state of the ConcurrentHashMap instance to a * stream (i.e., serialize it). * @param s the stream * @serialData * the key (Object) and value (Object) * for each key-value mapping, followed by a null pair. * The key-value mappings are emitted in no particular order. */ private void writeObject(java.io.ObjectOutputStream s) throws IOException { // force all segments for serialization compatibility for (int k = 0; k [] segments = this.segments; for (int k = 0; k seg = segmentAt(segments, k); seg.lock(); try { HashEntry[] tab = seg.table; for (int i = 0; i e; for (e = entryAt(tab, i); e != null; e = e.next) { s.writeObject(e.key); s.writeObject(e.value); } } } finally { seg.unlock(); } } s.writeObject(null); s.writeObject(null); } /** * Reconstitute the ConcurrentHashMap instance from a * stream (i.e., deserialize it). * @param s the stream */ @SuppressWarnings(\"unchecked\") private void readObject(java.io.ObjectInputStream s) throws IOException, ClassNotFoundException { // Don't call defaultReadObject() ObjectInputStream.GetField oisFields = s.readFields(); final Segment[] oisSegments = (Segment[])oisFields.get(\"segments\", null); final int ssize = oisSegments.length; if (ssize MAX_SEGMENTS || (ssize & (ssize-1)) != 0 ) // ssize not power of two throw new java.io.InvalidObjectException(\"Bad number of segments:\" + ssize); int sshift = 0, ssizeTmp = ssize; while (ssizeTmp > 1) { ++sshift; ssizeTmp >>>= 1; } UNSAFE.putIntVolatile(this, SEGSHIFT_OFFSET, 32 - sshift); UNSAFE.putIntVolatile(this, SEGMASK_OFFSET, ssize - 1); UNSAFE.putObjectVolatile(this, SEGMENTS_OFFSET, oisSegments); // set hashMask UNSAFE.putIntVolatile(this, HASHSEED_OFFSET, randomHashSeed(this)); // Re-initialize segments to be minimally sized, and let grow. int cap = MIN_SEGMENT_TABLE_CAPACITY; final Segment[] segments = this.segments; for (int k = 0; k seg = segments[k]; if (seg != null) { seg.threshold = (int)(cap * seg.loadFactor); seg.table = (HashEntry[]) new HashEntry[cap]; } } // Read the keys and values, and put the mappings in the table for (;;) { K key = (K) s.readObject(); V value = (V) s.readObject(); if (key == null) break; put(key, value); } } // Unsafe mechanics private static final sun.misc.Unsafe UNSAFE; private static final long SBASE; private static final int SSHIFT; private static final long TBASE; private static final int TSHIFT; private static final long HASHSEED_OFFSET; private static final long SEGSHIFT_OFFSET; private static final long SEGMASK_OFFSET; private static final long SEGMENTS_OFFSET; static { int ss, ts; try { UNSAFE = sun.misc.Unsafe.getUnsafe(); Class tc = HashEntry[].class; Class sc = Segment[].class; TBASE = UNSAFE.arrayBaseOffset(tc); SBASE = UNSAFE.arrayBaseOffset(sc); ts = UNSAFE.arrayIndexScale(tc); ss = UNSAFE.arrayIndexScale(sc); HASHSEED_OFFSET = UNSAFE.objectFieldOffset( ConcurrentHashMap.class.getDeclaredField(\"hashSeed\")); SEGSHIFT_OFFSET = UNSAFE.objectFieldOffset( ConcurrentHashMap.class.getDeclaredField(\"segmentShift\")); SEGMASK_OFFSET = UNSAFE.objectFieldOffset( ConcurrentHashMap.class.getDeclaredField(\"segmentMask\")); SEGMENTS_OFFSET = UNSAFE.objectFieldOffset( ConcurrentHashMap.class.getDeclaredField(\"segments\")); } catch (Exception e) { throw new Error(e); } if ((ss & (ss-1)) != 0 || (ts & (ts-1)) != 0) throw new Error(\"data type scale not a power of two\"); SSHIFT = 31 - Integer.numberOfLeadingZeros(ss); TSHIFT = 31 - Integer.numberOfLeadingZeros(ts); } } 2、 3、 "},"base/collection/hashmap源码/jdk1.7HashMap源码.html":{"url":"base/collection/hashmap源码/jdk1.7HashMap源码.html","title":"jdk1.7HashMap源码","keywords":"","body":" java8 /* * Copyright (c) 1997, 2010, Oracle and/or its affiliates. All rights reserved. * ORACLE PROPRIETARY/CONFIDENTIAL. Use is subject to license terms. * * * * * * * * * * * * * * * * * * * * */ package java.util; import java.io.*; /** * Hash table based implementation of the Map interface. This * implementation provides all of the optional map operations, and permits * null values and the null key. (The HashMap * class is roughly equivalent to Hashtable, except that it is * unsynchronized and permits nulls.) This class makes no guarantees as to * the order of the map; in particular, it does not guarantee that the order * will remain constant over time. * * This implementation provides constant-time performance for the basic * operations (get and put), assuming the hash function * disperses the elements properly among the buckets. Iteration over * collection views requires time proportional to the \"capacity\" of the * HashMap instance (the number of buckets) plus its size (the number * of key-value mappings). Thus, it's very important not to set the initial * capacity too high (or the load factor too low) if iteration performance is * important. * * An instance of HashMap has two parameters that affect its * performance: initial capacity and load factor. The * capacity is the number of buckets in the hash table, and the initial * capacity is simply the capacity at the time the hash table is created. The * load factor is a measure of how full the hash table is allowed to * get before its capacity is automatically increased. When the number of * entries in the hash table exceeds the product of the load factor and the * current capacity, the hash table is rehashed (that is, internal data * structures are rebuilt) so that the hash table has approximately twice the * number of buckets. * * As a general rule, the default load factor (.75) offers a good tradeoff * between time and space costs. Higher values decrease the space overhead * but increase the lookup cost (reflected in most of the operations of the * HashMap class, including get and put). The * expected number of entries in the map and its load factor should be taken * into account when setting its initial capacity, so as to minimize the * number of rehash operations. If the initial capacity is greater * than the maximum number of entries divided by the load factor, no * rehash operations will ever occur. * * If many mappings are to be stored in a HashMap instance, * creating it with a sufficiently large capacity will allow the mappings to * be stored more efficiently than letting it perform automatic rehashing as * needed to grow the table. * * Note that this implementation is not synchronized. * If multiple threads access a hash map concurrently, and at least one of * the threads modifies the map structurally, it must be * synchronized externally. (A structural modification is any operation * that adds or deletes one or more mappings; merely changing the value * associated with a key that an instance already contains is not a * structural modification.) This is typically accomplished by * synchronizing on some object that naturally encapsulates the map. * * If no such object exists, the map should be \"wrapped\" using the * {@link Collections#synchronizedMap Collections.synchronizedMap} * method. This is best done at creation time, to prevent accidental * unsynchronized access to the map: * Map m = Collections.synchronizedMap(new HashMap(...)); * * The iterators returned by all of this class's \"collection view methods\" * are fail-fast: if the map is structurally modified at any time after * the iterator is created, in any way except through the iterator's own * remove method, the iterator will throw a * {@link ConcurrentModificationException}. Thus, in the face of concurrent * modification, the iterator fails quickly and cleanly, rather than risking * arbitrary, non-deterministic behavior at an undetermined time in the * future. * * Note that the fail-fast behavior of an iterator cannot be guaranteed * as it is, generally speaking, impossible to make any hard guarantees in the * presence of unsynchronized concurrent modification. Fail-fast iterators * throw ConcurrentModificationException on a best-effort basis. * Therefore, it would be wrong to write a program that depended on this * exception for its correctness: the fail-fast behavior of iterators * should be used only to detect bugs. * * This class is a member of the * * Java Collections Framework. * * @param the type of keys maintained by this map * @param the type of mapped values * * @author Doug Lea * @author Josh Bloch * @author Arthur van Hoff * @author Neal Gafter * @see Object#hashCode() * @see Collection * @see Map * @see TreeMap * @see Hashtable * @since 1.2 */ public class HashMap extends AbstractMap implements Map, Cloneable, Serializable { /** * The default initial capacity - MUST be a power of two. */ static final int DEFAULT_INITIAL_CAPACITY = 1 [] EMPTY_TABLE = {}; /** * The table, resized as necessary. Length MUST Always be a power of two. */ transient Entry[] table = (Entry[]) EMPTY_TABLE; /** * The number of key-value mappings contained in this map. */ transient int size; /** * The next size value at which to resize (capacity * load factor). * @serial */ // If table == EMPTY_TABLE then this is the initial capacity at which the // table will be created when inflated. int threshold; /** * The load factor for the hash table. * * @serial */ final float loadFactor; /** * The number of times this HashMap has been structurally modified * Structural modifications are those that change the number of mappings in * the HashMap or otherwise modify its internal structure (e.g., * rehash). This field is used to make iterators on Collection-views of * the HashMap fail-fast. (See ConcurrentModificationException). */ transient int modCount; /** * The default threshold of map capacity above which alternative hashing is * used for String keys. Alternative hashing reduces the incidence of * collisions due to weak hash code calculation for String keys. * * This value may be overridden by defining the system property * {@code jdk.map.althashing.threshold}. A property value of {@code 1} * forces alternative hashing to be used at all times whereas * {@code -1} value ensures that alternative hashing is never used. */ static final int ALTERNATIVE_HASHING_THRESHOLD_DEFAULT = Integer.MAX_VALUE; /** * holds values which can't be initialized until after VM is booted. */ private static class Holder { /** * Table capacity above which to switch to use alternative hashing. */ static final int ALTERNATIVE_HASHING_THRESHOLD; static { String altThreshold = java.security.AccessController.doPrivileged( new sun.security.action.GetPropertyAction( \"jdk.map.althashing.threshold\")); int threshold; try { threshold = (null != altThreshold) ? Integer.parseInt(altThreshold) : ALTERNATIVE_HASHING_THRESHOLD_DEFAULT; // disable alternative hashing if -1 if (threshold == -1) { threshold = Integer.MAX_VALUE; } if (threshold HashMap with the specified initial * capacity and load factor. * * @param initialCapacity the initial capacity * @param loadFactor the load factor * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor HashMap with the specified initial * capacity and the default load factor (0.75). * * @param initialCapacity the initial capacity. * @throws IllegalArgumentException if the initial capacity is negative. */ public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } /** * Constructs an empty HashMap with the default initial capacity * (16) and the default load factor (0.75). */ public HashMap() { this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR); } /** * Constructs a new HashMap with the same mappings as the * specified Map. The HashMap is created with * default load factor (0.75) and an initial capacity sufficient to * hold the mappings in the specified Map. * * @param m the map whose mappings are to be placed in this map * @throws NullPointerException if the specified map is null */ public HashMap(Map m) { this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR); inflateTable(threshold); putAllForCreate(m); } private static int roundUpToPowerOf2(int number) { // assert number >= 0 : \"number must be non-negative\"; return number >= MAXIMUM_CAPACITY ? MAXIMUM_CAPACITY : (number > 1) ? Integer.highestOneBit((number - 1) = toSize int capacity = roundUpToPowerOf2(toSize); threshold = (int) Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1); table = new Entry[capacity]; initHashSeedAsNeeded(capacity); } // internal utilities /** * Initialization hook for subclasses. This method is called * in all constructors and pseudo-constructors (clone, readObject) * after HashMap has been initialized but before any entries have * been inserted. (In the absence of this method, readObject would * require explicit knowledge of subclasses.) */ void init() { } /** * Initialize the hashing mask value. We defer initialization until we * really need it. */ final boolean initHashSeedAsNeeded(int capacity) { boolean currentAltHashing = hashSeed != 0; boolean useAltHashing = sun.misc.VM.isBooted() && (capacity >= Holder.ALTERNATIVE_HASHING_THRESHOLD); boolean switching = currentAltHashing ^ useAltHashing; if (switching) { hashSeed = useAltHashing ? sun.misc.Hashing.randomHashSeed(this) : 0; } return switching; } /** * Retrieve object hash code and applies a supplemental hash function to the * result hash, which defends against poor quality hash functions. This is * critical because HashMap uses power-of-two length hash tables, that * otherwise encounter collisions for hashCodes that do not differ * in lower bits. Note: Null keys always map to hash 0, thus index 0. */ final int hash(Object k) { int h = hashSeed; if (0 != h && k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h >>> 20) ^ (h >>> 12); return h ^ (h >>> 7) ^ (h >>> 4); } /** * Returns index for hash code h. */ static int indexFor(int h, int length) { // assert Integer.bitCount(length) == 1 : \"length must be a non-zero power of 2\"; return h & (length-1); } /** * Returns the number of key-value mappings in this map. * * @return the number of key-value mappings in this map */ public int size() { return size; } /** * Returns true if this map contains no key-value mappings. * * @return true if this map contains no key-value mappings */ public boolean isEmpty() { return size == 0; } /** * Returns the value to which the specified key is mapped, * or {@code null} if this map contains no mapping for the key. * * More formally, if this map contains a mapping from a key * {@code k} to a value {@code v} such that {@code (key==null ? k==null : * key.equals(k))}, then this method returns {@code v}; otherwise * it returns {@code null}. (There can be at most one such mapping.) * * A return value of {@code null} does not necessarily * indicate that the map contains no mapping for the key; it's also * possible that the map explicitly maps the key to {@code null}. * The {@link #containsKey containsKey} operation may be used to * distinguish these two cases. * * @see #put(Object, Object) */ public V get(Object key) { if (key == null) return getForNullKey(); Entry entry = getEntry(key); return null == entry ? null : entry.getValue(); } /** * Offloaded version of get() to look up null keys. Null keys map * to index 0. This null case is split out into separate methods * for the sake of performance in the two most commonly used * operations (get and put), but incorporated with conditionals in * others. */ private V getForNullKey() { if (size == 0) { return null; } for (Entry e = table[0]; e != null; e = e.next) { if (e.key == null) return e.value; } return null; } /** * Returns true if this map contains a mapping for the * specified key. * * @param key The key whose presence in this map is to be tested * @return true if this map contains a mapping for the specified * key. */ public boolean containsKey(Object key) { return getEntry(key) != null; } /** * Returns the entry associated with the specified key in the * HashMap. Returns null if the HashMap contains no mapping * for the key. */ final Entry getEntry(Object key) { if (size == 0) { return null; } int hash = (key == null) ? 0 : hash(key); for (Entry e = table[indexFor(hash, table.length)]; e != null; e = e.next) { Object k; if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) return e; } return null; } /** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with key, or * null if there was no mapping for key. * (A null return can also indicate that the map * previously associated null with key.) */ public V put(K key, V value) { if (table == EMPTY_TABLE) { inflateTable(threshold); } if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash && ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(hash, key, value, i); return null; } /** * Offloaded version of put for null keys */ private V putForNullKey(V value) { for (Entry e = table[0]; e != null; e = e.next) { if (e.key == null) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(0, null, value, 0); return null; } /** * This method is used instead of put by constructors and * pseudoconstructors (clone, readObject). It does not resize the table, * check for comodification, etc. It calls createEntry rather than * addEntry. */ private void putForCreate(K key, V value) { int hash = null == key ? 0 : hash(key); int i = indexFor(hash, table.length); /** * Look for preexisting entry for key. This will never happen for * clone or deserialize. It will only happen for construction if the * input Map is a sorted map whose ordering is inconsistent w/ equals. */ for (Entry e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) { e.value = value; return; } } createEntry(hash, key, value, i); } private void putAllForCreate(Map m) { for (Map.Entry e : m.entrySet()) putForCreate(e.getKey(), e.getValue()); } /** * Rehashes the contents of this map into a new array with a * larger capacity. This method is called automatically when the * number of keys in this map reaches its threshold. * * If current capacity is MAXIMUM_CAPACITY, this method does not * resize the map, but sets threshold to Integer.MAX_VALUE. * This has the effect of preventing future calls. * * @param newCapacity the new capacity, MUST be a power of two; * must be greater than current capacity unless current * capacity is MAXIMUM_CAPACITY (in which case value * is irrelevant). */ void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity]; transfer(newTable, initHashSeedAsNeeded(newCapacity)); table = newTable; threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1); } /** * Transfers all entries from current table to newTable. */ void transfer(Entry[] newTable, boolean rehash) { int newCapacity = newTable.length; for (Entry e : table) { while(null != e) { Entry next = e.next; if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } } } /** * Copies all of the mappings from the specified map to this map. * These mappings will replace any mappings that this map had for * any of the keys currently in the specified map. * * @param m mappings to be stored in this map * @throws NullPointerException if the specified map is null */ public void putAll(Map m) { int numKeysToBeAdded = m.size(); if (numKeysToBeAdded == 0) return; if (table == EMPTY_TABLE) { inflateTable((int) Math.max(numKeysToBeAdded * loadFactor, threshold)); } /* * Expand the map if the map if the number of mappings to be added * is greater than or equal to threshold. This is conservative; the * obvious condition is (m.size() + size) >= threshold, but this * condition could result in a map with twice the appropriate capacity, * if the keys to be added overlap with the keys already in this map. * By using the conservative calculation, we subject ourself * to at most one extra resize. */ if (numKeysToBeAdded > threshold) { int targetCapacity = (int)(numKeysToBeAdded / loadFactor + 1); if (targetCapacity > MAXIMUM_CAPACITY) targetCapacity = MAXIMUM_CAPACITY; int newCapacity = table.length; while (newCapacity table.length) resize(newCapacity); } for (Map.Entry e : m.entrySet()) put(e.getKey(), e.getValue()); } /** * Removes the mapping for the specified key from this map if present. * * @param key key whose mapping is to be removed from the map * @return the previous value associated with key, or * null if there was no mapping for key. * (A null return can also indicate that the map * previously associated null with key.) */ public V remove(Object key) { Entry e = removeEntryForKey(key); return (e == null ? null : e.value); } /** * Removes and returns the entry associated with the specified key * in the HashMap. Returns null if the HashMap contains no mapping * for this key. */ final Entry removeEntryForKey(Object key) { if (size == 0) { return null; } int hash = (key == null) ? 0 : hash(key); int i = indexFor(hash, table.length); Entry prev = table[i]; Entry e = prev; while (e != null) { Entry next = e.next; Object k; if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) { modCount++; size--; if (prev == e) table[i] = next; else prev.next = next; e.recordRemoval(this); return e; } prev = e; e = next; } return e; } /** * Special version of remove for EntrySet using {@code Map.Entry.equals()} * for matching. */ final Entry removeMapping(Object o) { if (size == 0 || !(o instanceof Map.Entry)) return null; Map.Entry entry = (Map.Entry) o; Object key = entry.getKey(); int hash = (key == null) ? 0 : hash(key); int i = indexFor(hash, table.length); Entry prev = table[i]; Entry e = prev; while (e != null) { Entry next = e.next; if (e.hash == hash && e.equals(entry)) { modCount++; size--; if (prev == e) table[i] = next; else prev.next = next; e.recordRemoval(this); return e; } prev = e; e = next; } return e; } /** * Removes all of the mappings from this map. * The map will be empty after this call returns. */ public void clear() { modCount++; Arrays.fill(table, null); size = 0; } /** * Returns true if this map maps one or more keys to the * specified value. * * @param value value whose presence in this map is to be tested * @return true if this map maps one or more keys to the * specified value */ public boolean containsValue(Object value) { if (value == null) return containsNullValue(); Entry[] tab = table; for (int i = 0; i HashMap instance: the keys and * values themselves are not cloned. * * @return a shallow copy of this map */ public Object clone() { HashMap result = null; try { result = (HashMap)super.clone(); } catch (CloneNotSupportedException e) { // assert false; } if (result.table != EMPTY_TABLE) { result.inflateTable(Math.min( (int) Math.min( size * Math.min(1 / loadFactor, 4.0f), // we have limits... HashMap.MAXIMUM_CAPACITY), table.length)); } result.entrySet = null; result.modCount = 0; result.size = 0; result.init(); result.putAllForCreate(this); return result; } static class Entry implements Map.Entry { final K key; V value; Entry next; int hash; /** * Creates new entry. */ Entry(int h, K k, V v, Entry n) { value = v; next = n; key = k; hash = h; } public final K getKey() { return key; } public final V getValue() { return value; } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } public final boolean equals(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null && k1.equals(k2))) { Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null && v1.equals(v2))) return true; } return false; } public final int hashCode() { return Objects.hashCode(getKey()) ^ Objects.hashCode(getValue()); } public final String toString() { return getKey() + \"=\" + getValue(); } /** * This method is invoked whenever the value in an entry is * overwritten by an invocation of put(k,v) for a key k that's already * in the HashMap. */ void recordAccess(HashMap m) { } /** * This method is invoked whenever the entry is * removed from the table. */ void recordRemoval(HashMap m) { } } /** * Adds a new entry with the specified key, value and hash code to * the specified bucket. It is the responsibility of this * method to resize the table if appropriate. * * Subclass overrides this to alter the behavior of put method. */ void addEntry(int hash, K key, V value, int bucketIndex) { if ((size >= threshold) && (null != table[bucketIndex])) { resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); } createEntry(hash, key, value, bucketIndex); } /** * Like addEntry except that this version is used when creating entries * as part of Map construction or \"pseudo-construction\" (cloning, * deserialization). This version needn't worry about resizing the table. * * Subclass overrides this to alter the behavior of HashMap(Map), * clone, and readObject. */ void createEntry(int hash, K key, V value, int bucketIndex) { Entry e = table[bucketIndex]; table[bucketIndex] = new Entry<>(hash, key, value, e); size++; } private abstract class HashIterator implements Iterator { Entry next; // next entry to return int expectedModCount; // For fast-fail int index; // current slot Entry current; // current entry HashIterator() { expectedModCount = modCount; if (size > 0) { // advance to first entry Entry[] t = table; while (index nextEntry() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); Entry e = next; if (e == null) throw new NoSuchElementException(); if ((next = e.next) == null) { Entry[] t = table; while (index { public V next() { return nextEntry().value; } } private final class KeyIterator extends HashIterator { public K next() { return nextEntry().getKey(); } } private final class EntryIterator extends HashIterator> { public Map.Entry next() { return nextEntry(); } } // Subclass overrides these to alter behavior of views' iterator() method Iterator newKeyIterator() { return new KeyIterator(); } Iterator newValueIterator() { return new ValueIterator(); } Iterator> newEntryIterator() { return new EntryIterator(); } // Views private transient Set> entrySet = null; /** * Returns a {@link Set} view of the keys contained in this map. * The set is backed by the map, so changes to the map are * reflected in the set, and vice-versa. If the map is modified * while an iteration over the set is in progress (except through * the iterator's own remove operation), the results of * the iteration are undefined. The set supports element removal, * which removes the corresponding mapping from the map, via the * Iterator.remove, Set.remove, * removeAll, retainAll, and clear * operations. It does not support the add or addAll * operations. */ public Set keySet() { Set ks = keySet; return (ks != null ? ks : (keySet = new KeySet())); } private final class KeySet extends AbstractSet { public Iterator iterator() { return newKeyIterator(); } public int size() { return size; } public boolean contains(Object o) { return containsKey(o); } public boolean remove(Object o) { return HashMap.this.removeEntryForKey(o) != null; } public void clear() { HashMap.this.clear(); } } /** * Returns a {@link Collection} view of the values contained in this map. * The collection is backed by the map, so changes to the map are * reflected in the collection, and vice-versa. If the map is * modified while an iteration over the collection is in progress * (except through the iterator's own remove operation), * the results of the iteration are undefined. The collection * supports element removal, which removes the corresponding * mapping from the map, via the Iterator.remove, * Collection.remove, removeAll, * retainAll and clear operations. It does not * support the add or addAll operations. */ public Collection values() { Collection vs = values; return (vs != null ? vs : (values = new Values())); } private final class Values extends AbstractCollection { public Iterator iterator() { return newValueIterator(); } public int size() { return size; } public boolean contains(Object o) { return containsValue(o); } public void clear() { HashMap.this.clear(); } } /** * Returns a {@link Set} view of the mappings contained in this map. * The set is backed by the map, so changes to the map are * reflected in the set, and vice-versa. If the map is modified * while an iteration over the set is in progress (except through * the iterator's own remove operation, or through the * setValue operation on a map entry returned by the * iterator) the results of the iteration are undefined. The set * supports element removal, which removes the corresponding * mapping from the map, via the Iterator.remove, * Set.remove, removeAll, retainAll and * clear operations. It does not support the * add or addAll operations. * * @return a set view of the mappings contained in this map */ public Set> entrySet() { return entrySet0(); } private Set> entrySet0() { Set> es = entrySet; return es != null ? es : (entrySet = new EntrySet()); } private final class EntrySet extends AbstractSet> { public Iterator> iterator() { return newEntryIterator(); } public boolean contains(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry) o; Entry candidate = getEntry(e.getKey()); return candidate != null && candidate.equals(e); } public boolean remove(Object o) { return removeMapping(o) != null; } public int size() { return size; } public void clear() { HashMap.this.clear(); } } /** * Save the state of the HashMap instance to a stream (i.e., * serialize it). * * @serialData The capacity of the HashMap (the length of the * bucket array) is emitted (int), followed by the * size (an int, the number of key-value * mappings), followed by the key (Object) and value (Object) * for each key-value mapping. The key-value mappings are * emitted in no particular order. */ private void writeObject(java.io.ObjectOutputStream s) throws IOException { // Write out the threshold, loadfactor, and any hidden stuff s.defaultWriteObject(); // Write out number of buckets if (table==EMPTY_TABLE) { s.writeInt(roundUpToPowerOf2(threshold)); } else { s.writeInt(table.length); } // Write out size (number of Mappings) s.writeInt(size); // Write out keys and values (alternating) if (size > 0) { for(Map.Entry e : entrySet0()) { s.writeObject(e.getKey()); s.writeObject(e.getValue()); } } } private static final long serialVersionUID = 362498820763181265L; /** * Reconstitute the {@code HashMap} instance from a stream (i.e., * deserialize it). */ private void readObject(java.io.ObjectInputStream s) throws IOException, ClassNotFoundException { // Read in the threshold (ignored), loadfactor, and any hidden stuff s.defaultReadObject(); if (loadFactor []) EMPTY_TABLE; // Read in number of buckets s.readInt(); // ignored. // Read number of mappings int mappings = s.readInt(); if (mappings = 0.25) int capacity = (int) Math.min( mappings * Math.min(1 / loadFactor, 4.0f), // we have limits... HashMap.MAXIMUM_CAPACITY); // allocate the bucket array; if (mappings > 0) { inflateTable(capacity); } else { threshold = capacity; } init(); // Give subclass a chance to do its thing. // Read the keys and values, and put the mappings in the HashMap for (int i = 0; i "},"base/collection/hashmap源码/jdk1.8ConcurrentHashMap源码.html":{"url":"base/collection/hashmap源码/jdk1.8ConcurrentHashMap源码.html","title":"jdk1.8ConcurrentHashMap源码","keywords":"","body":" /* * ORACLE PROPRIETARY/CONFIDENTIAL. Use is subject to license terms. * * * * * * * * * * * * * * * * * * * * */ /* * * * * * * Written by Doug Lea with assistance from members of JCP JSR-166 * Expert Group and released to the public domain, as explained at * http://creativecommons.org/publicdomain/zero/1.0/ */ package java.util.concurrent; import java.io.ObjectStreamField; import java.io.Serializable; import java.lang.reflect.ParameterizedType; import java.lang.reflect.Type; import java.util.AbstractMap; import java.util.Arrays; import java.util.Collection; import java.util.Comparator; import java.util.Enumeration; import java.util.HashMap; import java.util.Hashtable; import java.util.Iterator; import java.util.Map; import java.util.NoSuchElementException; import java.util.Set; import java.util.Spliterator; import java.util.concurrent.ConcurrentMap; import java.util.concurrent.ForkJoinPool; import java.util.concurrent.atomic.AtomicReference; import java.util.concurrent.locks.LockSupport; import java.util.concurrent.locks.ReentrantLock; import java.util.function.BiConsumer; import java.util.function.BiFunction; import java.util.function.BinaryOperator; import java.util.function.Consumer; import java.util.function.DoubleBinaryOperator; import java.util.function.Function; import java.util.function.IntBinaryOperator; import java.util.function.LongBinaryOperator; import java.util.function.ToDoubleBiFunction; import java.util.function.ToDoubleFunction; import java.util.function.ToIntBiFunction; import java.util.function.ToIntFunction; import java.util.function.ToLongBiFunction; import java.util.function.ToLongFunction; import java.util.stream.Stream; /** * A hash table supporting full concurrency of retrievals and * high expected concurrency for updates. This class obeys the * same functional specification as {@link java.util.Hashtable}, and * includes versions of methods corresponding to each method of * {@code Hashtable}. However, even though all operations are * thread-safe, retrieval operations do not entail locking, * and there is not any support for locking the entire table * in a way that prevents all access. This class is fully * interoperable with {@code Hashtable} in programs that rely on its * thread safety but not on its synchronization details. * * Retrieval operations (including {@code get}) generally do not * block, so may overlap with update operations (including {@code put} * and {@code remove}). Retrievals reflect the results of the most * recently completed update operations holding upon their * onset. (More formally, an update operation for a given key bears a * happens-before relation with any (non-null) retrieval for * that key reporting the updated value.) For aggregate operations * such as {@code putAll} and {@code clear}, concurrent retrievals may * reflect insertion or removal of only some entries. Similarly, * Iterators, Spliterators and Enumerations return elements reflecting the * state of the hash table at some point at or since the creation of the * iterator/enumeration. They do not throw {@link * java.util.ConcurrentModificationException ConcurrentModificationException}. * However, iterators are designed to be used by only one thread at a time. * Bear in mind that the results of aggregate status methods including * {@code size}, {@code isEmpty}, and {@code containsValue} are typically * useful only when a map is not undergoing concurrent updates in other threads. * Otherwise the results of these methods reflect transient states * that may be adequate for monitoring or estimation purposes, but not * for program control. * * The table is dynamically expanded when there are too many * collisions (i.e., keys that have distinct hash codes but fall into * the same slot modulo the table size), with the expected average * effect of maintaining roughly two bins per mapping (corresponding * to a 0.75 load factor threshold for resizing). There may be much * variance around this average as mappings are added and removed, but * overall, this maintains a commonly accepted time/space tradeoff for * hash tables. However, resizing this or any other kind of hash * table may be a relatively slow operation. When possible, it is a * good idea to provide a size estimate as an optional {@code * initialCapacity} constructor argument. An additional optional * {@code loadFactor} constructor argument provides a further means of * customizing initial table capacity by specifying the table density * to be used in calculating the amount of space to allocate for the * given number of elements. Also, for compatibility with previous * versions of this class, constructors may optionally specify an * expected {@code concurrencyLevel} as an additional hint for * internal sizing. Note that using many keys with exactly the same * {@code hashCode()} is a sure way to slow down performance of any * hash table. To ameliorate impact, when keys are {@link Comparable}, * this class may use comparison order among keys to help break ties. * * A {@link Set} projection of a ConcurrentHashMap may be created * (using {@link #newKeySet()} or {@link #newKeySet(int)}), or viewed * (using {@link #keySet(Object)} when only keys are of interest, and the * mapped values are (perhaps transiently) not used or all take the * same mapping value. * * A ConcurrentHashMap can be used as scalable frequency map (a * form of histogram or multiset) by using {@link * java.util.concurrent.atomic.LongAdder} values and initializing via * {@link #computeIfAbsent computeIfAbsent}. For example, to add a count * to a {@code ConcurrentHashMap freqs}, you can use * {@code freqs.computeIfAbsent(k -> new LongAdder()).increment();} * * This class and its views and iterators implement all of the * optional methods of the {@link Map} and {@link Iterator} * interfaces. * * Like {@link Hashtable} but unlike {@link HashMap}, this class * does not allow {@code null} to be used as a key or value. * * ConcurrentHashMaps support a set of sequential and parallel bulk * operations that, unlike most {@link Stream} methods, are designed * to be safely, and often sensibly, applied even with maps that are * being concurrently updated by other threads; for example, when * computing a snapshot summary of the values in a shared registry. * There are three kinds of operation, each with four forms, accepting * functions with Keys, Values, Entries, and (Key, Value) arguments * and/or return values. Because the elements of a ConcurrentHashMap * are not ordered in any particular way, and may be processed in * different orders in different parallel executions, the correctness * of supplied functions should not depend on any ordering, or on any * other objects or values that may transiently change while * computation is in progress; and except for forEach actions, should * ideally be side-effect-free. Bulk operations on {@link java.util.Map.Entry} * objects do not support method {@code setValue}. * * * forEach: Perform a given action on each element. * A variant form applies a given transformation on each element * before performing the action. * * search: Return the first available non-null result of * applying a given function on each element; skipping further * search when a result is found. * * reduce: Accumulate each element. The supplied reduction * function cannot rely on ordering (more formally, it should be * both associative and commutative). There are five variants: * * * * Plain reductions. (There is not a form of this method for * (key, value) function arguments since there is no corresponding * return type.) * * Mapped reductions that accumulate the results of a given * function applied to each element. * * Reductions to scalar doubles, longs, and ints, using a * given basis value. * * * * * * These bulk operations accept a {@code parallelismThreshold} * argument. Methods proceed sequentially if the current map size is * estimated to be less than the given threshold. Using a value of * {@code Long.MAX_VALUE} suppresses all parallelism. Using a value * of {@code 1} results in maximal parallelism by partitioning into * enough subtasks to fully utilize the {@link * ForkJoinPool#commonPool()} that is used for all parallel * computations. Normally, you would initially choose one of these * extreme values, and then measure performance of using in-between * values that trade off overhead versus throughput. * * The concurrency properties of bulk operations follow * from those of ConcurrentHashMap: Any non-null result returned * from {@code get(key)} and related access methods bears a * happens-before relation with the associated insertion or * update. The result of any bulk operation reflects the * composition of these per-element relations (but is not * necessarily atomic with respect to the map as a whole unless it * is somehow known to be quiescent). Conversely, because keys * and values in the map are never null, null serves as a reliable * atomic indicator of the current lack of any result. To * maintain this property, null serves as an implicit basis for * all non-scalar reduction operations. For the double, long, and * int versions, the basis should be one that, when combined with * any other value, returns that other value (more formally, it * should be the identity element for the reduction). Most common * reductions have these properties; for example, computing a sum * with basis 0 or a minimum with basis MAX_VALUE. * * Search and transformation functions provided as arguments * should similarly return null to indicate the lack of any result * (in which case it is not used). In the case of mapped * reductions, this also enables transformations to serve as * filters, returning null (or, in the case of primitive * specializations, the identity basis) if the element should not * be combined. You can create compound transformations and * filterings by composing them yourself under this \"null means * there is nothing there now\" rule before using them in search or * reduce operations. * * Methods accepting and/or returning Entry arguments maintain * key-value associations. They may be useful for example when * finding the key for the greatest value. Note that \"plain\" Entry * arguments can be supplied using {@code new * AbstractMap.SimpleEntry(k,v)}. * * Bulk operations may complete abruptly, throwing an * exception encountered in the application of a supplied * function. Bear in mind when handling such exceptions that other * concurrently executing functions could also have thrown * exceptions, or would have done so if the first exception had * not occurred. * * Speedups for parallel compared to sequential forms are common * but not guaranteed. Parallel operations involving brief functions * on small maps may execute more slowly than sequential forms if the * underlying work to parallelize the computation is more expensive * than the computation itself. Similarly, parallelization may not * lead to much actual parallelism if all processors are busy * performing unrelated tasks. * * All arguments to all task methods must be non-null. * * This class is a member of the * * Java Collections Framework. * * @since 1.5 * @author Doug Lea * @param the type of keys maintained by this map * @param the type of mapped values */ public class ConcurrentHashMap extends AbstractMap implements ConcurrentMap, Serializable { private static final long serialVersionUID = 7249069246763182397L; /* * Overview: * * The primary design goal of this hash table is to maintain * concurrent readability (typically method get(), but also * iterators and related methods) while minimizing update * contention. Secondary goals are to keep space consumption about * the same or better than java.util.HashMap, and to support high * initial insertion rates on an empty table by many threads. * * This map usually acts as a binned (bucketed) hash table. Each * key-value mapping is held in a Node. Most nodes are instances * of the basic Node class with hash, key, value, and next * fields. However, various subclasses exist: TreeNodes are * arranged in balanced trees, not lists. TreeBins hold the roots * of sets of TreeNodes. ForwardingNodes are placed at the heads * of bins during resizing. ReservationNodes are used as * placeholders while establishing values in computeIfAbsent and * related methods. The types TreeBin, ForwardingNode, and * ReservationNode do not hold normal user keys, values, or * hashes, and are readily distinguishable during search etc * because they have negative hash fields and null key and value * fields. (These special nodes are either uncommon or transient, * so the impact of carrying around some unused fields is * insignificant.) * * The table is lazily initialized to a power-of-two size upon the * first insertion. Each bin in the table normally contains a * list of Nodes (most often, the list has only zero or one Node). * Table accesses require volatile/atomic reads, writes, and * CASes. Because there is no other way to arrange this without * adding further indirections, we use intrinsics * (sun.misc.Unsafe) operations. * * We use the top (sign) bit of Node hash fields for control * purposes -- it is available anyway because of addressing * constraints. Nodes with negative hash fields are specially * handled or ignored in map methods. * * Insertion (via put or its variants) of the first node in an * empty bin is performed by just CASing it to the bin. This is * by far the most common case for put operations under most * key/hash distributions. Other update operations (insert, * delete, and replace) require locks. We do not want to waste * the space required to associate a distinct lock object with * each bin, so instead use the first node of a bin list itself as * a lock. Locking support for these locks relies on builtin * \"synchronized\" monitors. * * Using the first node of a list as a lock does not by itself * suffice though: When a node is locked, any update must first * validate that it is still the first node after locking it, and * retry if not. Because new nodes are always appended to lists, * once a node is first in a bin, it remains first until deleted * or the bin becomes invalidated (upon resizing). * * The main disadvantage of per-bin locks is that other update * operations on other nodes in a bin list protected by the same * lock can stall, for example when user equals() or mapping * functions take a long time. However, statistically, under * random hash codes, this is not a common problem. Ideally, the * frequency of nodes in bins follows a Poisson distribution * (http://en.wikipedia.org/wiki/Poisson_distribution) with a * parameter of about 0.5 on average, given the resizing threshold * of 0.75, although with a large variance because of resizing * granularity. Ignoring variance, the expected occurrences of * list size k are (exp(-0.5) * pow(0.5, k) / factorial(k)). The * first values are: * * 0: 0.60653066 * 1: 0.30326533 * 2: 0.07581633 * 3: 0.01263606 * 4: 0.00157952 * 5: 0.00015795 * 6: 0.00001316 * 7: 0.00000094 * 8: 0.00000006 * more: less than 1 in ten million * * Lock contention probability for two threads accessing distinct * elements is roughly 1 / (8 * #elements) under random hashes. * * Actual hash code distributions encountered in practice * sometimes deviate significantly from uniform randomness. This * includes the case when N > (1>> 2)} for * the associated resizing threshold. */ private static final float LOAD_FACTOR = 0.75f; /** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2, and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */ static final int TREEIFY_THRESHOLD = 8; /** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */ static final int UNTREEIFY_THRESHOLD = 6; /** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * The value should be at least 4 * TREEIFY_THRESHOLD to avoid * conflicts between resizing and treeification thresholds. */ static final int MIN_TREEIFY_CAPACITY = 64; /** * Minimum number of rebinnings per transfer step. Ranges are * subdivided to allow multiple resizer threads. This value * serves as a lower bound to avoid resizers encountering * excessive memory contention. The value should be at least * DEFAULT_CAPACITY. */ private static final int MIN_TRANSFER_STRIDE = 16; /** * The number of bits used for generation stamp in sizeCtl. * Must be at least 6 for 32bit arrays. */ private static int RESIZE_STAMP_BITS = 16; /** * The maximum number of threads that can help resize. * Must fit in 32 - RESIZE_STAMP_BITS bits. */ private static final int MAX_RESIZERS = (1 implements Map.Entry { final int hash; final K key; volatile V val; volatile Node next; Node(int hash, K key, V val, Node next) { this.hash = hash; this.key = key; this.val = val; this.next = next; } public final K getKey() { return key; } public final V getValue() { return val; } public final int hashCode() { return key.hashCode() ^ val.hashCode(); } public final String toString(){ return key + \"=\" + val; } public final V setValue(V value) { throw new UnsupportedOperationException(); } public final boolean equals(Object o) { Object k, v, u; Map.Entry e; return ((o instanceof Map.Entry) && (k = (e = (Map.Entry)o).getKey()) != null && (v = e.getValue()) != null && (k == key || k.equals(key)) && (v == (u = val) || v.equals(u))); } /** * Virtualized support for map.get(); overridden in subclasses. */ Node find(int h, Object k) { Node e = this; if (k != null) { do { K ek; if (e.hash == h && ((ek = e.key) == k || (ek != null && k.equals(ek)))) return e; } while ((e = e.next) != null); } return null; } } /* ---------------- Static utilities -------------- */ /** * Spreads (XORs) higher bits of hash to lower and also forces top * bit to 0. Because the table uses power-of-two masking, sets of * hashes that vary only in bits above the current mask will * always collide. (Among known examples are sets of Float keys * holding consecutive whole numbers in small tables.) So we * apply a transform that spreads the impact of higher bits * downward. There is a tradeoff between speed, utility, and * quality of bit-spreading. Because many common sets of hashes * are already reasonably distributed (so don't benefit from * spreading), and because we use trees to handle large sets of * collisions in bins, we just XOR some shifted bits in the * cheapest possible way to reduce systematic lossage, as well as * to incorporate impact of the highest bits that would otherwise * never be used in index calculations because of table bounds. */ static final int spread(int h) { return (h ^ (h >>> 16)) & HASH_BITS; } /** * Returns a power of two table size for the given desired capacity. * See Hackers Delight, sec 3.2 */ private static final int tableSizeFor(int c) { int n = c - 1; n |= n >>> 1; n |= n >>> 2; n |= n >>> 4; n |= n >>> 8; n |= n >>> 16; return (n = MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } /** * Returns x's Class if it is of the form \"class C implements * Comparable\", else null. */ static Class comparableClassFor(Object x) { if (x instanceof Comparable) { Class c; Type[] ts, as; Type t; ParameterizedType p; if ((c = x.getClass()) == String.class) // bypass checks return c; if ((ts = c.getGenericInterfaces()) != null) { for (int i = 0; i kc, Object k, Object x) { return (x == null || x.getClass() != kc ? 0 : ((Comparable)k).compareTo(x)); } /* ---------------- Table element access -------------- */ /* * Volatile access methods are used for table elements as well as * elements of in-progress next table while resizing. All uses of * the tab arguments must be null checked by callers. All callers * also paranoically precheck that tab's length is not zero (or an * equivalent check), thus ensuring that any index argument taking * the form of a hash value anded with (length - 1) is a valid * index. Note that, to be correct wrt arbitrary concurrency * errors by users, these checks must operate on local variables, * which accounts for some odd-looking inline assignments below. * Note that calls to setTabAt always occur within locked regions, * and so in principle require only release ordering, not * full volatile semantics, but are currently coded as volatile * writes to be conservative. */ @SuppressWarnings(\"unchecked\") static final Node tabAt(Node[] tab, int i) { return (Node)U.getObjectVolatile(tab, ((long)i boolean casTabAt(Node[] tab, int i, Node c, Node v) { return U.compareAndSwapObject(tab, ((long)i void setTabAt(Node[] tab, int i, Node v) { U.putObjectVolatile(tab, ((long)i [] table; /** * The next table to use; non-null only while resizing. */ private transient volatile Node[] nextTable; /** * Base counter value, used mainly when there is no contention, * but also as a fallback during table initialization * races. Updated via CAS. */ private transient volatile long baseCount; /** * Table initialization and resizing control. When negative, the * table is being initialized or resized: -1 for initialization, * else -(1 + the number of active resizing threads). Otherwise, * when table is null, holds the initial table size to use upon * creation, or 0 for default. After initialization, holds the * next element count value upon which to resize the table. */ private transient volatile int sizeCtl; /** * The next table index (plus one) to split while resizing. */ private transient volatile int transferIndex; /** * Spinlock (locked via CAS) used when resizing and/or creating CounterCells. */ private transient volatile int cellsBusy; /** * Table of counter cells. When non-null, size is a power of 2. */ private transient volatile CounterCell[] counterCells; // views private transient KeySetView keySet; private transient ValuesView values; private transient EntrySetView entrySet; /* ---------------- Public operations -------------- */ /** * Creates a new, empty map with the default initial table size (16). */ public ConcurrentHashMap() { } /** * Creates a new, empty map with an initial table size * accommodating the specified number of elements without the need * to dynamically resize. * * @param initialCapacity The implementation performs internal * sizing to accommodate this many elements. * @throws IllegalArgumentException if the initial capacity of * elements is negative */ public ConcurrentHashMap(int initialCapacity) { if (initialCapacity = (MAXIMUM_CAPACITY >>> 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity >>> 1) + 1)); this.sizeCtl = cap; } /** * Creates a new map with the same mappings as the given map. * * @param m the map */ public ConcurrentHashMap(Map m) { this.sizeCtl = DEFAULT_CAPACITY; putAll(m); } /** * Creates a new, empty map with an initial table size based on * the given number of elements ({@code initialCapacity}) and * initial table density ({@code loadFactor}). * * @param initialCapacity the initial capacity. The implementation * performs internal sizing to accommodate this many elements, * given the specified load factor. * @param loadFactor the load factor (table density) for * establishing the initial table size * @throws IllegalArgumentException if the initial capacity of * elements is negative or the load factor is nonpositive * * @since 1.6 */ public ConcurrentHashMap(int initialCapacity, float loadFactor) { this(initialCapacity, loadFactor, 1); } /** * Creates a new, empty map with an initial table size based on * the given number of elements ({@code initialCapacity}), table * density ({@code loadFactor}), and number of concurrently * updating threads ({@code concurrencyLevel}). * * @param initialCapacity the initial capacity. The implementation * performs internal sizing to accommodate this many elements, * given the specified load factor. * @param loadFactor the load factor (table density) for * establishing the initial table size * @param concurrencyLevel the estimated number of concurrently * updating threads. The implementation may use this value as * a sizing hint. * @throws IllegalArgumentException if the initial capacity is * negative or the load factor or concurrencyLevel are * nonpositive */ public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) { if (!(loadFactor > 0.0f) || initialCapacity = (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); this.sizeCtl = cap; } // Original (since JDK1.2) Map methods /** * {@inheritDoc} */ public int size() { long n = sumCount(); return ((n (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); } /** * {@inheritDoc} */ public boolean isEmpty() { return sumCount() More formally, if this map contains a mapping from a key * {@code k} to a value {@code v} such that {@code key.equals(k)}, * then this method returns {@code v}; otherwise it returns * {@code null}. (There can be at most one such mapping.) * * @throws NullPointerException if the specified key is null */ public V get(Object key) { Node[] tab; Node e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null && (n = tab.length) > 0 && (e = tabAt(tab, (n - 1) & h)) != null) { if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null && key.equals(ek))) return e.val; } else if (eh [] t; if ((t = table) != null) { Traverser it = new Traverser(t, t.length, 0, t.length); for (Node p; (p = it.advance()) != null; ) { V v; if ((v = p.val) == value || (v != null && value.equals(v))) return true; } } return false; } /** * Maps the specified key to the specified value in this table. * Neither the key nor the value can be null. * * The value can be retrieved by calling the {@code get} method * with a key that is equal to the original key. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with {@code key}, or * {@code null} if there was no mapping for {@code key} * @throws NullPointerException if the specified key or value is null */ public V put(K key, V value) { return putVal(key, value, false); } /** Implementation for put and putIfAbsent */ final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node[] tab = table;;) { Node f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) { if (casTabAt(tab, i, null, new Node(hash, key, value, null))) break; // no lock when adding to empty bin } else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { V oldVal = null; synchronized (f) { if (tabAt(tab, i) == f) { if (fh >= 0) { binCount = 1; for (Node e = f;; ++binCount) { K ek; if (e.hash == hash && ((ek = e.key) == key || (ek != null && key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } Node pred = e; if ((e = e.next) == null) { pred.next = new Node(hash, key, value, null); break; } } } else if (f instanceof TreeBin) { Node p; binCount = 2; if ((p = ((TreeBin)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { if (binCount >= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } addCount(1L, binCount); return null; } /** * Copies all of the mappings from the specified map to this one. * These mappings replace any mappings that this map had for any of the * keys currently in the specified map. * * @param m mappings to be stored in this map */ public void putAll(Map m) { tryPresize(m.size()); for (Map.Entry e : m.entrySet()) putVal(e.getKey(), e.getValue(), false); } /** * Removes the key (and its corresponding value) from this map. * This method does nothing if the key is not in the map. * * @param key the key that needs to be removed * @return the previous value associated with {@code key}, or * {@code null} if there was no mapping for {@code key} * @throws NullPointerException if the specified key is null */ public V remove(Object key) { return replaceNode(key, null, null); } /** * Implementation for the four public remove/replace methods: * Replaces node value with v, conditional upon match of cv if * non-null. If resulting value is null, delete. */ final V replaceNode(Object key, V value, Object cv) { int hash = spread(key.hashCode()); for (Node[] tab = table;;) { Node f; int n, i, fh; if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) & hash)) == null) break; else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { V oldVal = null; boolean validated = false; synchronized (f) { if (tabAt(tab, i) == f) { if (fh >= 0) { validated = true; for (Node e = f, pred = null;;) { K ek; if (e.hash == hash && ((ek = e.key) == key || (ek != null && key.equals(ek)))) { V ev = e.val; if (cv == null || cv == ev || (ev != null && cv.equals(ev))) { oldVal = ev; if (value != null) e.val = value; else if (pred != null) pred.next = e.next; else setTabAt(tab, i, e.next); } break; } pred = e; if ((e = e.next) == null) break; } } else if (f instanceof TreeBin) { validated = true; TreeBin t = (TreeBin)f; TreeNode r, p; if ((r = t.root) != null && (p = r.findTreeNode(hash, key, null)) != null) { V pv = p.val; if (cv == null || cv == pv || (pv != null && cv.equals(pv))) { oldVal = pv; if (value != null) p.val = value; else if (t.removeTreeNode(p)) setTabAt(tab, i, untreeify(t.first)); } } } } } if (validated) { if (oldVal != null) { if (value == null) addCount(-1L, -1); return oldVal; } break; } } } return null; } /** * Removes all of the mappings from this map. */ public void clear() { long delta = 0L; // negative number of deletions int i = 0; Node[] tab = table; while (tab != null && i f = tabAt(tab, i); if (f == null) ++i; else if ((fh = f.hash) == MOVED) { tab = helpTransfer(tab, f); i = 0; // restart } else { synchronized (f) { if (tabAt(tab, i) == f) { Node p = (fh >= 0 ? f : (f instanceof TreeBin) ? ((TreeBin)f).first : null); while (p != null) { --delta; p = p.next; } setTabAt(tab, i++, null); } } } } if (delta != 0L) addCount(delta, -1); } /** * Returns a {@link Set} view of the keys contained in this map. * The set is backed by the map, so changes to the map are * reflected in the set, and vice-versa. The set supports element * removal, which removes the corresponding mapping from this map, * via the {@code Iterator.remove}, {@code Set.remove}, * {@code removeAll}, {@code retainAll}, and {@code clear} * operations. It does not support the {@code add} or * {@code addAll} operations. * * The view's iterators and spliterators are * weakly consistent. * * The view's {@code spliterator} reports {@link Spliterator#CONCURRENT}, * {@link Spliterator#DISTINCT}, and {@link Spliterator#NONNULL}. * * @return the set view */ public KeySetView keySet() { KeySetView ks; return (ks = keySet) != null ? ks : (keySet = new KeySetView(this, null)); } /** * Returns a {@link Collection} view of the values contained in this map. * The collection is backed by the map, so changes to the map are * reflected in the collection, and vice-versa. The collection * supports element removal, which removes the corresponding * mapping from this map, via the {@code Iterator.remove}, * {@code Collection.remove}, {@code removeAll}, * {@code retainAll}, and {@code clear} operations. It does not * support the {@code add} or {@code addAll} operations. * * The view's iterators and spliterators are * weakly consistent. * * The view's {@code spliterator} reports {@link Spliterator#CONCURRENT} * and {@link Spliterator#NONNULL}. * * @return the collection view */ public Collection values() { ValuesView vs; return (vs = values) != null ? vs : (values = new ValuesView(this)); } /** * Returns a {@link Set} view of the mappings contained in this map. * The set is backed by the map, so changes to the map are * reflected in the set, and vice-versa. The set supports element * removal, which removes the corresponding mapping from the map, * via the {@code Iterator.remove}, {@code Set.remove}, * {@code removeAll}, {@code retainAll}, and {@code clear} * operations. * * The view's iterators and spliterators are * weakly consistent. * * The view's {@code spliterator} reports {@link Spliterator#CONCURRENT}, * {@link Spliterator#DISTINCT}, and {@link Spliterator#NONNULL}. * * @return the set view */ public Set> entrySet() { EntrySetView es; return (es = entrySet) != null ? es : (entrySet = new EntrySetView(this)); } /** * Returns the hash code value for this {@link Map}, i.e., * the sum of, for each key-value pair in the map, * {@code key.hashCode() ^ value.hashCode()}. * * @return the hash code value for this map */ public int hashCode() { int h = 0; Node[] t; if ((t = table) != null) { Traverser it = new Traverser(t, t.length, 0, t.length); for (Node p; (p = it.advance()) != null; ) h += p.key.hashCode() ^ p.val.hashCode(); } return h; } /** * Returns a string representation of this map. The string * representation consists of a list of key-value mappings (in no * particular order) enclosed in braces (\"{@code {}}\"). Adjacent * mappings are separated by the characters {@code \", \"} (comma * and space). Each key-value mapping is rendered as the key * followed by an equals sign (\"{@code =}\") followed by the * associated value. * * @return a string representation of this map */ public String toString() { Node[] t; int f = (t = table) == null ? 0 : t.length; Traverser it = new Traverser(t, f, 0, f); StringBuilder sb = new StringBuilder(); sb.append('{'); Node p; if ((p = it.advance()) != null) { for (;;) { K k = p.key; V v = p.val; sb.append(k == this ? \"(this Map)\" : k); sb.append('='); sb.append(v == this ? \"(this Map)\" : v); if ((p = it.advance()) == null) break; sb.append(',').append(' '); } } return sb.append('}').toString(); } /** * Compares the specified object with this map for equality. * Returns {@code true} if the given object is a map with the same * mappings as this map. This operation may return misleading * results if either map is concurrently modified during execution * of this method. * * @param o object to be compared for equality with this map * @return {@code true} if the specified object is equal to this map */ public boolean equals(Object o) { if (o != this) { if (!(o instanceof Map)) return false; Map m = (Map) o; Node[] t; int f = (t = table) == null ? 0 : t.length; Traverser it = new Traverser(t, f, 0, f); for (Node p; (p = it.advance()) != null; ) { V val = p.val; Object v = m.get(p.key); if (v == null || (v != val && !v.equals(val))) return false; } for (Map.Entry e : m.entrySet()) { Object mk, mv, v; if ((mk = e.getKey()) == null || (mv = e.getValue()) == null || (v = get(mk)) == null || (mv != v && !mv.equals(v))) return false; } } return true; } /** * Stripped-down version of helper class used in previous version, * declared for the sake of serialization compatibility */ static class Segment extends ReentrantLock implements Serializable { private static final long serialVersionUID = 2249069246763182397L; final float loadFactor; Segment(float lf) { this.loadFactor = lf; } } /** * Saves the state of the {@code ConcurrentHashMap} instance to a * stream (i.e., serializes it). * @param s the stream * @throws java.io.IOException if an I/O error occurs * @serialData * the key (Object) and value (Object) * for each key-value mapping, followed by a null pair. * The key-value mappings are emitted in no particular order. */ private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException { // For serialization compatibility // Emulate segment calculation from previous version of this class int sshift = 0; int ssize = 1; while (ssize [] segments = (Segment[]) new Segment[DEFAULT_CONCURRENCY_LEVEL]; for (int i = 0; i (LOAD_FACTOR); s.putFields().put(\"segments\", segments); s.putFields().put(\"segmentShift\", segmentShift); s.putFields().put(\"segmentMask\", segmentMask); s.writeFields(); Node[] t; if ((t = table) != null) { Traverser it = new Traverser(t, t.length, 0, t.length); for (Node p; (p = it.advance()) != null; ) { s.writeObject(p.key); s.writeObject(p.val); } } s.writeObject(null); s.writeObject(null); segments = null; // throw away } /** * Reconstitutes the instance from a stream (that is, deserializes it). * @param s the stream * @throws ClassNotFoundException if the class of a serialized object * could not be found * @throws java.io.IOException if an I/O error occurs */ private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException { /* * To improve performance in typical cases, we create nodes * while reading, then place in table once size is known. * However, we must also validate uniqueness and deal with * overpopulated bins while doing so, which requires * specialized versions of putVal mechanics. */ sizeCtl = -1; // force exclusion for table construction s.defaultReadObject(); long size = 0L; Node p = null; for (;;) { @SuppressWarnings(\"unchecked\") K k = (K) s.readObject(); @SuppressWarnings(\"unchecked\") V v = (V) s.readObject(); if (k != null && v != null) { p = new Node(spread(k.hashCode()), k, v, p); ++size; } else break; } if (size == 0L) sizeCtl = 0; else { int n; if (size >= (long)(MAXIMUM_CAPACITY >>> 1)) n = MAXIMUM_CAPACITY; else { int sz = (int)size; n = tableSizeFor(sz + (sz >>> 1) + 1); } @SuppressWarnings(\"unchecked\") Node[] tab = (Node[])new Node[n]; int mask = n - 1; long added = 0L; while (p != null) { boolean insertAtFront; Node next = p.next, first; int h = p.hash, j = h & mask; if ((first = tabAt(tab, j)) == null) insertAtFront = true; else { K k = p.key; if (first.hash t = (TreeBin)first; if (t.putTreeVal(h, k, p.val) == null) ++added; insertAtFront = false; } else { int binCount = 0; insertAtFront = true; Node q; K qk; for (q = first; q != null; q = q.next) { if (q.hash == h && ((qk = q.key) == k || (qk != null && k.equals(qk)))) { insertAtFront = false; break; } ++binCount; } if (insertAtFront && binCount >= TREEIFY_THRESHOLD) { insertAtFront = false; ++added; p.next = first; TreeNode hd = null, tl = null; for (q = p; q != null; q = q.next) { TreeNode t = new TreeNode (q.hash, q.key, q.val, null, null); if ((t.prev = tl) == null) hd = t; else tl.next = t; tl = t; } setTabAt(tab, j, new TreeBin(hd)); } } } if (insertAtFront) { ++added; p.next = first; setTabAt(tab, j, p); } p = next; } table = tab; sizeCtl = n - (n >>> 2); baseCount = added; } } // ConcurrentMap methods /** * {@inheritDoc} * * @return the previous value associated with the specified key, * or {@code null} if there was no mapping for the key * @throws NullPointerException if the specified key or value is null */ public V putIfAbsent(K key, V value) { return putVal(key, value, true); } /** * {@inheritDoc} * * @throws NullPointerException if the specified key is null */ public boolean remove(Object key, Object value) { if (key == null) throw new NullPointerException(); return value != null && replaceNode(key, null, value) != null; } /** * {@inheritDoc} * * @throws NullPointerException if any of the arguments are null */ public boolean replace(K key, V oldValue, V newValue) { if (key == null || oldValue == null || newValue == null) throw new NullPointerException(); return replaceNode(key, newValue, oldValue) != null; } /** * {@inheritDoc} * * @return the previous value associated with the specified key, * or {@code null} if there was no mapping for the key * @throws NullPointerException if the specified key or value is null */ public V replace(K key, V value) { if (key == null || value == null) throw new NullPointerException(); return replaceNode(key, value, null); } // Overrides of JDK8+ Map extension method defaults /** * Returns the value to which the specified key is mapped, or the * given default value if this map contains no mapping for the * key. * * @param key the key whose associated value is to be returned * @param defaultValue the value to return if this map contains * no mapping for the given key * @return the mapping for the key, if present; else the default value * @throws NullPointerException if the specified key is null */ public V getOrDefault(Object key, V defaultValue) { V v; return (v = get(key)) == null ? defaultValue : v; } public void forEach(BiConsumer action) { if (action == null) throw new NullPointerException(); Node[] t; if ((t = table) != null) { Traverser it = new Traverser(t, t.length, 0, t.length); for (Node p; (p = it.advance()) != null; ) { action.accept(p.key, p.val); } } } public void replaceAll(BiFunction function) { if (function == null) throw new NullPointerException(); Node[] t; if ((t = table) != null) { Traverser it = new Traverser(t, t.length, 0, t.length); for (Node p; (p = it.advance()) != null; ) { V oldValue = p.val; for (K key = p.key;;) { V newValue = function.apply(key, oldValue); if (newValue == null) throw new NullPointerException(); if (replaceNode(key, newValue, oldValue) != null || (oldValue = get(key)) == null) break; } } } } /** * If the specified key is not already associated with a value, * attempts to compute its value using the given mapping function * and enters it into this map unless {@code null}. The entire * method invocation is performed atomically, so the function is * applied at most once per key. Some attempted update operations * on this map by other threads may be blocked while computation * is in progress, so the computation should be short and simple, * and must not attempt to update any other mappings of this map. * * @param key key with which the specified value is to be associated * @param mappingFunction the function to compute a value * @return the current (existing or computed) value associated with * the specified key, or null if the computed value is null * @throws NullPointerException if the specified key or mappingFunction * is null * @throws IllegalStateException if the computation detectably * attempts a recursive update to this map that would * otherwise never complete * @throws RuntimeException or Error if the mappingFunction does so, * in which case the mapping is left unestablished */ public V computeIfAbsent(K key, Function mappingFunction) { if (key == null || mappingFunction == null) throw new NullPointerException(); int h = spread(key.hashCode()); V val = null; int binCount = 0; for (Node[] tab = table;;) { Node f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) & h)) == null) { Node r = new ReservationNode(); synchronized (r) { if (casTabAt(tab, i, null, r)) { binCount = 1; Node node = null; try { if ((val = mappingFunction.apply(key)) != null) node = new Node(h, key, val, null); } finally { setTabAt(tab, i, node); } } } if (binCount != 0) break; } else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { boolean added = false; synchronized (f) { if (tabAt(tab, i) == f) { if (fh >= 0) { binCount = 1; for (Node e = f;; ++binCount) { K ek; V ev; if (e.hash == h && ((ek = e.key) == key || (ek != null && key.equals(ek)))) { val = e.val; break; } Node pred = e; if ((e = e.next) == null) { if ((val = mappingFunction.apply(key)) != null) { added = true; pred.next = new Node(h, key, val, null); } break; } } } else if (f instanceof TreeBin) { binCount = 2; TreeBin t = (TreeBin)f; TreeNode r, p; if ((r = t.root) != null && (p = r.findTreeNode(h, key, null)) != null) val = p.val; else if ((val = mappingFunction.apply(key)) != null) { added = true; t.putTreeVal(h, key, val); } } } } if (binCount != 0) { if (binCount >= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (!added) return val; break; } } } if (val != null) addCount(1L, binCount); return val; } /** * If the value for the specified key is present, attempts to * compute a new mapping given the key and its current mapped * value. The entire method invocation is performed atomically. * Some attempted update operations on this map by other threads * may be blocked while computation is in progress, so the * computation should be short and simple, and must not attempt to * update any other mappings of this map. * * @param key key with which a value may be associated * @param remappingFunction the function to compute a value * @return the new value associated with the specified key, or null if none * @throws NullPointerException if the specified key or remappingFunction * is null * @throws IllegalStateException if the computation detectably * attempts a recursive update to this map that would * otherwise never complete * @throws RuntimeException or Error if the remappingFunction does so, * in which case the mapping is unchanged */ public V computeIfPresent(K key, BiFunction remappingFunction) { if (key == null || remappingFunction == null) throw new NullPointerException(); int h = spread(key.hashCode()); V val = null; int delta = 0; int binCount = 0; for (Node[] tab = table;;) { Node f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) & h)) == null) break; else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { synchronized (f) { if (tabAt(tab, i) == f) { if (fh >= 0) { binCount = 1; for (Node e = f, pred = null;; ++binCount) { K ek; if (e.hash == h && ((ek = e.key) == key || (ek != null && key.equals(ek)))) { val = remappingFunction.apply(key, e.val); if (val != null) e.val = val; else { delta = -1; Node en = e.next; if (pred != null) pred.next = en; else setTabAt(tab, i, en); } break; } pred = e; if ((e = e.next) == null) break; } } else if (f instanceof TreeBin) { binCount = 2; TreeBin t = (TreeBin)f; TreeNode r, p; if ((r = t.root) != null && (p = r.findTreeNode(h, key, null)) != null) { val = remappingFunction.apply(key, p.val); if (val != null) p.val = val; else { delta = -1; if (t.removeTreeNode(p)) setTabAt(tab, i, untreeify(t.first)); } } } } } if (binCount != 0) break; } } if (delta != 0) addCount((long)delta, binCount); return val; } /** * Attempts to compute a mapping for the specified key and its * current mapped value (or {@code null} if there is no current * mapping). The entire method invocation is performed atomically. * Some attempted update operations on this map by other threads * may be blocked while computation is in progress, so the * computation should be short and simple, and must not attempt to * update any other mappings of this Map. * * @param key key with which the specified value is to be associated * @param remappingFunction the function to compute a value * @return the new value associated with the specified key, or null if none * @throws NullPointerException if the specified key or remappingFunction * is null * @throws IllegalStateException if the computation detectably * attempts a recursive update to this map that would * otherwise never complete * @throws RuntimeException or Error if the remappingFunction does so, * in which case the mapping is unchanged */ public V compute(K key, BiFunction remappingFunction) { if (key == null || remappingFunction == null) throw new NullPointerException(); int h = spread(key.hashCode()); V val = null; int delta = 0; int binCount = 0; for (Node[] tab = table;;) { Node f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) & h)) == null) { Node r = new ReservationNode(); synchronized (r) { if (casTabAt(tab, i, null, r)) { binCount = 1; Node node = null; try { if ((val = remappingFunction.apply(key, null)) != null) { delta = 1; node = new Node(h, key, val, null); } } finally { setTabAt(tab, i, node); } } } if (binCount != 0) break; } else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { synchronized (f) { if (tabAt(tab, i) == f) { if (fh >= 0) { binCount = 1; for (Node e = f, pred = null;; ++binCount) { K ek; if (e.hash == h && ((ek = e.key) == key || (ek != null && key.equals(ek)))) { val = remappingFunction.apply(key, e.val); if (val != null) e.val = val; else { delta = -1; Node en = e.next; if (pred != null) pred.next = en; else setTabAt(tab, i, en); } break; } pred = e; if ((e = e.next) == null) { val = remappingFunction.apply(key, null); if (val != null) { delta = 1; pred.next = new Node(h, key, val, null); } break; } } } else if (f instanceof TreeBin) { binCount = 1; TreeBin t = (TreeBin)f; TreeNode r, p; if ((r = t.root) != null) p = r.findTreeNode(h, key, null); else p = null; V pv = (p == null) ? null : p.val; val = remappingFunction.apply(key, pv); if (val != null) { if (p != null) p.val = val; else { delta = 1; t.putTreeVal(h, key, val); } } else if (p != null) { delta = -1; if (t.removeTreeNode(p)) setTabAt(tab, i, untreeify(t.first)); } } } } if (binCount != 0) { if (binCount >= TREEIFY_THRESHOLD) treeifyBin(tab, i); break; } } } if (delta != 0) addCount((long)delta, binCount); return val; } /** * If the specified key is not already associated with a * (non-null) value, associates it with the given value. * Otherwise, replaces the value with the results of the given * remapping function, or removes if {@code null}. The entire * method invocation is performed atomically. Some attempted * update operations on this map by other threads may be blocked * while computation is in progress, so the computation should be * short and simple, and must not attempt to update any other * mappings of this Map. * * @param key key with which the specified value is to be associated * @param value the value to use if absent * @param remappingFunction the function to recompute a value if present * @return the new value associated with the specified key, or null if none * @throws NullPointerException if the specified key or the * remappingFunction is null * @throws RuntimeException or Error if the remappingFunction does so, * in which case the mapping is unchanged */ public V merge(K key, V value, BiFunction remappingFunction) { if (key == null || value == null || remappingFunction == null) throw new NullPointerException(); int h = spread(key.hashCode()); V val = null; int delta = 0; int binCount = 0; for (Node[] tab = table;;) { Node f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) & h)) == null) { if (casTabAt(tab, i, null, new Node(h, key, value, null))) { delta = 1; val = value; break; } } else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { synchronized (f) { if (tabAt(tab, i) == f) { if (fh >= 0) { binCount = 1; for (Node e = f, pred = null;; ++binCount) { K ek; if (e.hash == h && ((ek = e.key) == key || (ek != null && key.equals(ek)))) { val = remappingFunction.apply(e.val, value); if (val != null) e.val = val; else { delta = -1; Node en = e.next; if (pred != null) pred.next = en; else setTabAt(tab, i, en); } break; } pred = e; if ((e = e.next) == null) { delta = 1; val = value; pred.next = new Node(h, key, val, null); break; } } } else if (f instanceof TreeBin) { binCount = 2; TreeBin t = (TreeBin)f; TreeNode r = t.root; TreeNode p = (r == null) ? null : r.findTreeNode(h, key, null); val = (p == null) ? value : remappingFunction.apply(p.val, value); if (val != null) { if (p != null) p.val = val; else { delta = 1; t.putTreeVal(h, key, val); } } else if (p != null) { delta = -1; if (t.removeTreeNode(p)) setTabAt(tab, i, untreeify(t.first)); } } } } if (binCount != 0) { if (binCount >= TREEIFY_THRESHOLD) treeifyBin(tab, i); break; } } } if (delta != 0) addCount((long)delta, binCount); return val; } // Hashtable legacy methods /** * Legacy method testing if some key maps into the specified value * in this table. This method is identical in functionality to * {@link #containsValue(Object)}, and exists solely to ensure * full compatibility with class {@link java.util.Hashtable}, * which supported this method prior to introduction of the * Java Collections framework. * * @param value a value to search for * @return {@code true} if and only if some key maps to the * {@code value} argument in this table as * determined by the {@code equals} method; * {@code false} otherwise * @throws NullPointerException if the specified value is null */ public boolean contains(Object value) { return containsValue(value); } /** * Returns an enumeration of the keys in this table. * * @return an enumeration of the keys in this table * @see #keySet() */ public Enumeration keys() { Node[] t; int f = (t = table) == null ? 0 : t.length; return new KeyIterator(t, f, 0, f, this); } /** * Returns an enumeration of the values in this table. * * @return an enumeration of the values in this table * @see #values() */ public Enumeration elements() { Node[] t; int f = (t = table) == null ? 0 : t.length; return new ValueIterator(t, f, 0, f, this); } // ConcurrentHashMap-only methods /** * Returns the number of mappings. This method should be used * instead of {@link #size} because a ConcurrentHashMap may * contain more mappings than can be represented as an int. The * value returned is an estimate; the actual count may differ if * there are concurrent insertions or removals. * * @return the number of mappings * @since 1.8 */ public long mappingCount() { long n = sumCount(); return (n the element type of the returned set * @return the new set * @since 1.8 */ public static KeySetView newKeySet() { return new KeySetView (new ConcurrentHashMap(), Boolean.TRUE); } /** * Creates a new {@link Set} backed by a ConcurrentHashMap * from the given type to {@code Boolean.TRUE}. * * @param initialCapacity The implementation performs internal * sizing to accommodate this many elements. * @param the element type of the returned set * @return the new set * @throws IllegalArgumentException if the initial capacity of * elements is negative * @since 1.8 */ public static KeySetView newKeySet(int initialCapacity) { return new KeySetView (new ConcurrentHashMap(initialCapacity), Boolean.TRUE); } /** * Returns a {@link Set} view of the keys in this map, using the * given common mapped value for any additions (i.e., {@link * Collection#add} and {@link Collection#addAll(Collection)}). * This is of course only appropriate if it is acceptable to use * the same value for all additions from this view. * * @param mappedValue the mapped value to use for any additions * @return the set view * @throws NullPointerException if the mappedValue is null */ public KeySetView keySet(V mappedValue) { if (mappedValue == null) throw new NullPointerException(); return new KeySetView(this, mappedValue); } /* ---------------- Special Nodes -------------- */ /** * A node inserted at head of bins during transfer operations. */ static final class ForwardingNode extends Node { final Node[] nextTable; ForwardingNode(Node[] tab) { super(MOVED, null, null, null); this.nextTable = tab; } Node find(int h, Object k) { // loop to avoid arbitrarily deep recursion on forwarding nodes outer: for (Node[] tab = nextTable;;) { Node e; int n; if (k == null || tab == null || (n = tab.length) == 0 || (e = tabAt(tab, (n - 1) & h)) == null) return null; for (;;) { int eh; K ek; if ((eh = e.hash) == h && ((ek = e.key) == k || (ek != null && k.equals(ek)))) return e; if (eh )e).nextTable; continue outer; } else return e.find(h, k); } if ((e = e.next) == null) return null; } } } } /** * A place-holder node used in computeIfAbsent and compute */ static final class ReservationNode extends Node { ReservationNode() { super(RESERVED, null, null, null); } Node find(int h, Object k) { return null; } } /* ---------------- Table Initialization and Resizing -------------- */ /** * Returns the stamp bits for resizing a table of size n. * Must be negative when shifted left by RESIZE_STAMP_SHIFT. */ static final int resizeStamp(int n) { return Integer.numberOfLeadingZeros(n) | (1 [] initTable() { Node[] tab; int sc; while ((tab = table) == null || tab.length == 0) { if ((sc = sizeCtl) 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(\"unchecked\") Node[] nt = (Node[])new Node[n]; table = tab = nt; sc = n - (n >>> 2); } } finally { sizeCtl = sc; } break; } } return tab; } /** * Adds to count, and if table is too small and not already * resizing, initiates transfer. If already resizing, helps * perform transfer if work is available. Rechecks occupancy * after a transfer to see if another resize is already needed * because resizings are lagging additions. * * @param x the count to add * @param check if = 0) { Node[] tab, nt; int n, sc; while (s >= (long)(sc = sizeCtl) && (tab = table) != null && (n = tab.length) >> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex [] helpTransfer(Node[] tab, Node f) { Node[] nextTab; int sc; if (tab != null && (f instanceof ForwardingNode) && (nextTab = ((ForwardingNode)f).nextTable) != null) { int rs = resizeStamp(tab.length); while (nextTab == nextTable && table == tab && (sc = sizeCtl) >> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex = (MAXIMUM_CAPACITY >>> 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size >>> 1) + 1); int sc; while ((sc = sizeCtl) >= 0) { Node[] tab = table; int n; if (tab == null || (n = tab.length) == 0) { n = (sc > c) ? sc : c; if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if (table == tab) { @SuppressWarnings(\"unchecked\") Node[] nt = (Node[])new Node[n]; table = nt; sc = n - (n >>> 2); } } finally { sizeCtl = sc; } } } else if (c = MAXIMUM_CAPACITY) break; else if (tab == table) { int rs = resizeStamp(n); if (sc [] nt; if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex [] tab, Node[] nextTab) { int n = tab.length, stride; if ((stride = (NCPU > 1) ? (n >>> 3) / NCPU : n) [] nt = (Node[])new Node[n fwd = new ForwardingNode(nextTab); boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab for (int i = 0, bound = 0;;) { Node f; int fh; while (advance) { int nextIndex, nextBound; if (--i >= bound || finishing) advance = false; else if ((nextIndex = transferIndex) stride ? nextIndex - stride : 0))) { bound = nextBound; i = nextIndex - 1; advance = false; } } if (i = n || i + n >= nextn) { int sc; if (finishing) { nextTable = null; table = nextTab; sizeCtl = (n >> 1); return; } if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { if ((sc - 2) != resizeStamp(n) ln, hn; if (fh >= 0) { int runBit = fh & n; Node lastRun = f; for (Node p = f.next; p != null; p = p.next) { int b = p.hash & n; if (b != runBit) { runBit = b; lastRun = p; } } if (runBit == 0) { ln = lastRun; hn = null; } else { hn = lastRun; ln = null; } for (Node p = f; p != lastRun; p = p.next) { int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph & n) == 0) ln = new Node(ph, pk, pv, ln); else hn = new Node(ph, pk, pv, hn); } setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; } else if (f instanceof TreeBin) { TreeBin t = (TreeBin)f; TreeNode lo = null, loTail = null; TreeNode hi = null, hiTail = null; int lc = 0, hc = 0; for (Node e = t.first; e != null; e = e.next) { int h = e.hash; TreeNode p = new TreeNode (h, e.key, e.val, null, null); if ((h & n) == 0) { if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; } else { if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; } } ln = (lc (lo) : t; hn = (hc (hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; } } } } } } /* ---------------- Counter support -------------- */ /** * A padded cell for distributing counts. Adapted from LongAdder * and Striped64. See their internal docs for explanation. */ @sun.misc.Contended static final class CounterCell { volatile long value; CounterCell(long x) { value = x; } } final long sumCount() { CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) { for (int i = 0; i 0) { if ((a = as[(n - 1) & h]) == null) { if (cellsBusy == 0) { // Try to attach new Cell CounterCell r = new CounterCell(x); // Optimistic create if (cellsBusy == 0 && U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) { boolean created = false; try { // Recheck under lock CounterCell[] rs; int m, j; if ((rs = counterCells) != null && (m = rs.length) > 0 && rs[j = (m - 1) & h] == null) { rs[j] = r; created = true; } } finally { cellsBusy = 0; } if (created) break; continue; // Slot is now non-empty } } collide = false; } else if (!wasUncontended) // CAS already known to fail wasUncontended = true; // Continue after rehash else if (U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x)) break; else if (counterCells != as || n >= NCPU) collide = false; // At max size or stale else if (!collide) collide = true; else if (cellsBusy == 0 && U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) { try { if (counterCells == as) {// Expand table unless stale CounterCell[] rs = new CounterCell[n [] tab, int index) { Node b; int n, sc; if (tab != null) { if ((n = tab.length) = 0) { synchronized (b) { if (tabAt(tab, index) == b) { TreeNode hd = null, tl = null; for (Node e = b; e != null; e = e.next) { TreeNode p = new TreeNode(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; } setTabAt(tab, index, new TreeBin(hd)); } } } } } /** * Returns a list on non-TreeNodes replacing those in given list. */ static Node untreeify(Node b) { Node hd = null, tl = null; for (Node q = b; q != null; q = q.next) { Node p = new Node(q.hash, q.key, q.val, null); if (tl == null) hd = p; else tl.next = p; tl = p; } return hd; } /* ---------------- TreeNodes -------------- */ /** * Nodes for use in TreeBins */ static final class TreeNode extends Node { TreeNode parent; // red-black tree links TreeNode left; TreeNode right; TreeNode prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node next, TreeNode parent) { super(hash, key, val, next); this.parent = parent; } Node find(int h, Object k) { return findTreeNode(h, k, null); } /** * Returns the TreeNode (or null if not found) for the given key * starting at given root. */ final TreeNode findTreeNode(int h, Object k, Class kc) { if (k != null) { TreeNode p = this; do { int ph, dir; K pk; TreeNode q; TreeNode pl = p.left, pr = p.right; if ((ph = p.hash) > h) p = pl; else if (ph extends Node { TreeNode root; volatile TreeNode first; volatile Thread waiter; volatile int lockState; // values for lockState static final int WRITER = 1; // set while holding write lock static final int WAITER = 2; // set when waiting for write lock static final int READER = 4; // increment value for setting read lock /** * Tie-breaking utility for ordering insertions when equal * hashCodes and non-comparable. We don't require a total * order, just a consistent insertion rule to maintain * equivalence across rebalancings. Tie-breaking further than * necessary simplifies testing a bit. */ static int tieBreakOrder(Object a, Object b) { int d; if (a == null || b == null || (d = a.getClass().getName(). compareTo(b.getClass().getName())) == 0) d = (System.identityHashCode(a) b) { super(TREEBIN, null, null, null); this.first = b; TreeNode r = null; for (TreeNode x = b, next; x != null; x = next) { next = (TreeNode)x.next; x.left = x.right = null; if (r == null) { x.parent = null; x.red = false; r = x; } else { K k = x.key; int h = x.hash; Class kc = null; for (TreeNode p = r;;) { int dir, ph; K pk = p.key; if ((ph = p.hash) > h) dir = -1; else if (ph xp = p; if ((p = (dir find(int h, Object k) { if (k != null) { for (Node e = first; e != null; ) { int s; K ek; if (((s = lockState) & (WAITER|WRITER)) != 0) { if (e.hash == h && ((ek = e.key) == k || (ek != null && k.equals(ek)))) return e; e = e.next; } else if (U.compareAndSwapInt(this, LOCKSTATE, s, s + READER)) { TreeNode r, p; try { p = ((r = root) == null ? null : r.findTreeNode(h, k, null)); } finally { Thread w; if (U.getAndAddInt(this, LOCKSTATE, -READER) == (READER|WAITER) && (w = waiter) != null) LockSupport.unpark(w); } return p; } } } return null; } /** * Finds or adds a node. * @return null if added */ final TreeNode putTreeVal(int h, K k, V v) { Class kc = null; boolean searched = false; for (TreeNode p = root;;) { int dir, ph; K pk; if (p == null) { first = root = new TreeNode(h, k, v, null, null); break; } else if ((ph = p.hash) > h) dir = -1; else if (ph q, ch; searched = true; if (((ch = p.left) != null && (q = ch.findTreeNode(h, k, kc)) != null) || ((ch = p.right) != null && (q = ch.findTreeNode(h, k, kc)) != null)) return q; } dir = tieBreakOrder(k, pk); } TreeNode xp = p; if ((p = (dir x, f = first; first = x = new TreeNode(h, k, v, f, xp); if (f != null) f.prev = x; if (dir p) { TreeNode next = (TreeNode)p.next; TreeNode pred = p.prev; // unlink traversal pointers TreeNode r, rl; if (pred == null) first = next; else pred.next = next; if (next != null) next.prev = pred; if (first == null) { root = null; return true; } if ((r = root) == null || r.right == null || // too small (rl = r.left) == null || rl.left == null) return true; lockRoot(); try { TreeNode replacement; TreeNode pl = p.left; TreeNode pr = p.right; if (pl != null && pr != null) { TreeNode s = pr, sl; while ((sl = s.left) != null) // find successor s = sl; boolean c = s.red; s.red = p.red; p.red = c; // swap colors TreeNode sr = s.right; TreeNode pp = p.parent; if (s == pr) { // p was s's direct parent p.parent = s; s.right = p; } else { TreeNode sp = s.parent; if ((p.parent = sp) != null) { if (s == sp.left) sp.left = p; else sp.right = p; } if ((s.right = pr) != null) pr.parent = s; } p.left = null; if ((p.right = sr) != null) sr.parent = p; if ((s.left = pl) != null) pl.parent = s; if ((s.parent = pp) == null) r = s; else if (p == pp.left) pp.left = s; else pp.right = s; if (sr != null) replacement = sr; else replacement = p; } else if (pl != null) replacement = pl; else if (pr != null) replacement = pr; else replacement = p; if (replacement != p) { TreeNode pp = replacement.parent = p.parent; if (pp == null) r = replacement; else if (p == pp.left) pp.left = replacement; else pp.right = replacement; p.left = p.right = p.parent = null; } root = (p.red) ? r : balanceDeletion(r, replacement); if (p == replacement) { // detach pointers TreeNode pp; if ((pp = p.parent) != null) { if (p == pp.left) pp.left = null; else if (p == pp.right) pp.right = null; p.parent = null; } } } finally { unlockRoot(); } assert checkInvariants(root); return false; } /* ------------------------------------------------------------ */ // Red-black tree methods, all adapted from CLR static TreeNode rotateLeft(TreeNode root, TreeNode p) { TreeNode r, pp, rl; if (p != null && (r = p.right) != null) { if ((rl = p.right = r.left) != null) rl.parent = p; if ((pp = r.parent = p.parent) == null) (root = r).red = false; else if (pp.left == p) pp.left = r; else pp.right = r; r.left = p; p.parent = r; } return root; } static TreeNode rotateRight(TreeNode root, TreeNode p) { TreeNode l, pp, lr; if (p != null && (l = p.left) != null) { if ((lr = p.left = l.right) != null) lr.parent = p; if ((pp = l.parent = p.parent) == null) (root = l).red = false; else if (pp.right == p) pp.right = l; else pp.left = l; l.right = p; p.parent = l; } return root; } static TreeNode balanceInsertion(TreeNode root, TreeNode x) { x.red = true; for (TreeNode xp, xpp, xppl, xppr;;) { if ((xp = x.parent) == null) { x.red = false; return x; } else if (!xp.red || (xpp = xp.parent) == null) return root; if (xp == (xppl = xpp.left)) { if ((xppr = xpp.right) != null && xppr.red) { xppr.red = false; xp.red = false; xpp.red = true; x = xpp; } else { if (x == xp.right) { root = rotateLeft(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; } if (xp != null) { xp.red = false; if (xpp != null) { xpp.red = true; root = rotateRight(root, xpp); } } } } else { if (xppl != null && xppl.red) { xppl.red = false; xp.red = false; xpp.red = true; x = xpp; } else { if (x == xp.left) { root = rotateRight(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; } if (xp != null) { xp.red = false; if (xpp != null) { xpp.red = true; root = rotateLeft(root, xpp); } } } } } } static TreeNode balanceDeletion(TreeNode root, TreeNode x) { for (TreeNode xp, xpl, xpr;;) { if (x == null || x == root) return root; else if ((xp = x.parent) == null) { x.red = false; return x; } else if (x.red) { x.red = false; return root; } else if ((xpl = xp.left) == x) { if ((xpr = xp.right) != null && xpr.red) { xpr.red = false; xp.red = true; root = rotateLeft(root, xp); xpr = (xp = x.parent) == null ? null : xp.right; } if (xpr == null) x = xp; else { TreeNode sl = xpr.left, sr = xpr.right; if ((sr == null || !sr.red) && (sl == null || !sl.red)) { xpr.red = true; x = xp; } else { if (sr == null || !sr.red) { if (sl != null) sl.red = false; xpr.red = true; root = rotateRight(root, xpr); xpr = (xp = x.parent) == null ? null : xp.right; } if (xpr != null) { xpr.red = (xp == null) ? false : xp.red; if ((sr = xpr.right) != null) sr.red = false; } if (xp != null) { xp.red = false; root = rotateLeft(root, xp); } x = root; } } } else { // symmetric if (xpl != null && xpl.red) { xpl.red = false; xp.red = true; root = rotateRight(root, xp); xpl = (xp = x.parent) == null ? null : xp.left; } if (xpl == null) x = xp; else { TreeNode sl = xpl.left, sr = xpl.right; if ((sl == null || !sl.red) && (sr == null || !sr.red)) { xpl.red = true; x = xp; } else { if (sl == null || !sl.red) { if (sr != null) sr.red = false; xpl.red = true; root = rotateLeft(root, xpl); xpl = (xp = x.parent) == null ? null : xp.left; } if (xpl != null) { xpl.red = (xp == null) ? false : xp.red; if ((sl = xpl.left) != null) sl.red = false; } if (xp != null) { xp.red = false; root = rotateRight(root, xp); } x = root; } } } } } /** * Recursive invariant check */ static boolean checkInvariants(TreeNode t) { TreeNode tp = t.parent, tl = t.left, tr = t.right, tb = t.prev, tn = (TreeNode)t.next; if (tb != null && tb.next != t) return false; if (tn != null && tn.prev != t) return false; if (tp != null && t != tp.left && t != tp.right) return false; if (tl != null && (tl.parent != t || tl.hash > t.hash)) return false; if (tr != null && (tr.parent != t || tr.hash k = TreeBin.class; LOCKSTATE = U.objectFieldOffset (k.getDeclaredField(\"lockState\")); } catch (Exception e) { throw new Error(e); } } } /* ----------------Table Traversal -------------- */ /** * Records the table, its length, and current traversal index for a * traverser that must process a region of a forwarded table before * proceeding with current table. */ static final class TableStack { int length; int index; Node[] tab; TableStack next; } /** * Encapsulates traversal for methods such as containsValue; also * serves as a base class for other iterators and spliterators. * * Method advance visits once each still-valid node that was * reachable upon iterator construction. It might miss some that * were added to a bin after the bin was visited, which is OK wrt * consistency guarantees. Maintaining this property in the face * of possible ongoing resizes requires a fair amount of * bookkeeping state that is difficult to optimize away amidst * volatile accesses. Even so, traversal maintains reasonable * throughput. * * Normally, iteration proceeds bin-by-bin traversing lists. * However, if the table has been resized, then all future steps * must traverse both the bin at the current index as well as at * (index + baseSize); and so on for further resizings. To * paranoically cope with potential sharing by users of iterators * across threads, iteration terminates if a bounds checks fails * for a table read. */ static class Traverser { Node[] tab; // current table; updated if resized Node next; // the next entry to use TableStack stack, spare; // to save/restore on ForwardingNodes int index; // index of bin to use next int baseIndex; // current index of initial table int baseLimit; // index bound for initial table final int baseSize; // initial table size Traverser(Node[] tab, int size, int index, int limit) { this.tab = tab; this.baseSize = size; this.baseIndex = this.index = index; this.baseLimit = limit; this.next = null; } /** * Advances if possible, returning next valid node, or null if none. */ final Node advance() { Node e; if ((e = next) != null) e = e.next; for (;;) { Node[] t; int i, n; // must use locals in checks if (e != null) return next = e; if (baseIndex >= baseLimit || (t = tab) == null || (n = t.length) )e).nextTable; e = null; pushState(t, i, n); continue; } else if (e instanceof TreeBin) e = ((TreeBin)e).first; else e = null; } if (stack != null) recoverState(n); else if ((index = i + baseSize) >= n) index = ++baseIndex; // visit upper slots if present } } /** * Saves traversal state upon encountering a forwarding node. */ private void pushState(Node[] t, int i, int n) { TableStack s = spare; // reuse if possible if (s != null) spare = s.next; else s = new TableStack(); s.tab = t; s.length = n; s.index = i; s.next = stack; stack = s; } /** * Possibly pops traversal state. * * @param n length of current table */ private void recoverState(int n) { TableStack s; int len; while ((s = stack) != null && (index += (len = s.length)) >= n) { n = len; index = s.index; tab = s.tab; s.tab = null; TableStack next = s.next; s.next = spare; // save for reuse stack = next; spare = s; } if (s == null && (index += baseSize) >= n) index = ++baseIndex; } } /** * Base of key, value, and entry Iterators. Adds fields to * Traverser to support iterator.remove. */ static class BaseIterator extends Traverser { final ConcurrentHashMap map; Node lastReturned; BaseIterator(Node[] tab, int size, int index, int limit, ConcurrentHashMap map) { super(tab, size, index, limit); this.map = map; advance(); } public final boolean hasNext() { return next != null; } public final boolean hasMoreElements() { return next != null; } public final void remove() { Node p; if ((p = lastReturned) == null) throw new IllegalStateException(); lastReturned = null; map.replaceNode(p.key, null, null); } } static final class KeyIterator extends BaseIterator implements Iterator, Enumeration { KeyIterator(Node[] tab, int index, int size, int limit, ConcurrentHashMap map) { super(tab, index, size, limit, map); } public final K next() { Node p; if ((p = next) == null) throw new NoSuchElementException(); K k = p.key; lastReturned = p; advance(); return k; } public final K nextElement() { return next(); } } static final class ValueIterator extends BaseIterator implements Iterator, Enumeration { ValueIterator(Node[] tab, int index, int size, int limit, ConcurrentHashMap map) { super(tab, index, size, limit, map); } public final V next() { Node p; if ((p = next) == null) throw new NoSuchElementException(); V v = p.val; lastReturned = p; advance(); return v; } public final V nextElement() { return next(); } } static final class EntryIterator extends BaseIterator implements Iterator> { EntryIterator(Node[] tab, int index, int size, int limit, ConcurrentHashMap map) { super(tab, index, size, limit, map); } public final Map.Entry next() { Node p; if ((p = next) == null) throw new NoSuchElementException(); K k = p.key; V v = p.val; lastReturned = p; advance(); return new MapEntry(k, v, map); } } /** * Exported Entry for EntryIterator */ static final class MapEntry implements Map.Entry { final K key; // non-null V val; // non-null final ConcurrentHashMap map; MapEntry(K key, V val, ConcurrentHashMap map) { this.key = key; this.val = val; this.map = map; } public K getKey() { return key; } public V getValue() { return val; } public int hashCode() { return key.hashCode() ^ val.hashCode(); } public String toString() { return key + \"=\" + val; } public boolean equals(Object o) { Object k, v; Map.Entry e; return ((o instanceof Map.Entry) && (k = (e = (Map.Entry)o).getKey()) != null && (v = e.getValue()) != null && (k == key || k.equals(key)) && (v == val || v.equals(val))); } /** * Sets our entry's value and writes through to the map. The * value to return is somewhat arbitrary here. Since we do not * necessarily track asynchronous changes, the most recent * \"previous\" value could be different from what we return (or * could even have been removed, in which case the put will * re-establish). We do not and cannot guarantee more. */ public V setValue(V value) { if (value == null) throw new NullPointerException(); V v = val; val = value; map.put(key, value); return v; } } static final class KeySpliterator extends Traverser implements Spliterator { long est; // size estimate KeySpliterator(Node[] tab, int size, int index, int limit, long est) { super(tab, size, index, limit); this.est = est; } public Spliterator trySplit() { int i, f, h; return (h = ((i = baseIndex) + (f = baseLimit)) >>> 1) (tab, baseSize, baseLimit = h, f, est >>>= 1); } public void forEachRemaining(Consumer action) { if (action == null) throw new NullPointerException(); for (Node p; (p = advance()) != null;) action.accept(p.key); } public boolean tryAdvance(Consumer action) { if (action == null) throw new NullPointerException(); Node p; if ((p = advance()) == null) return false; action.accept(p.key); return true; } public long estimateSize() { return est; } public int characteristics() { return Spliterator.DISTINCT | Spliterator.CONCURRENT | Spliterator.NONNULL; } } static final class ValueSpliterator extends Traverser implements Spliterator { long est; // size estimate ValueSpliterator(Node[] tab, int size, int index, int limit, long est) { super(tab, size, index, limit); this.est = est; } public Spliterator trySplit() { int i, f, h; return (h = ((i = baseIndex) + (f = baseLimit)) >>> 1) (tab, baseSize, baseLimit = h, f, est >>>= 1); } public void forEachRemaining(Consumer action) { if (action == null) throw new NullPointerException(); for (Node p; (p = advance()) != null;) action.accept(p.val); } public boolean tryAdvance(Consumer action) { if (action == null) throw new NullPointerException(); Node p; if ((p = advance()) == null) return false; action.accept(p.val); return true; } public long estimateSize() { return est; } public int characteristics() { return Spliterator.CONCURRENT | Spliterator.NONNULL; } } static final class EntrySpliterator extends Traverser implements Spliterator> { final ConcurrentHashMap map; // To export MapEntry long est; // size estimate EntrySpliterator(Node[] tab, int size, int index, int limit, long est, ConcurrentHashMap map) { super(tab, size, index, limit); this.map = map; this.est = est; } public Spliterator> trySplit() { int i, f, h; return (h = ((i = baseIndex) + (f = baseLimit)) >>> 1) (tab, baseSize, baseLimit = h, f, est >>>= 1, map); } public void forEachRemaining(Consumer> action) { if (action == null) throw new NullPointerException(); for (Node p; (p = advance()) != null; ) action.accept(new MapEntry(p.key, p.val, map)); } public boolean tryAdvance(Consumer> action) { if (action == null) throw new NullPointerException(); Node p; if ((p = advance()) == null) return false; action.accept(new MapEntry(p.key, p.val, map)); return true; } public long estimateSize() { return est; } public int characteristics() { return Spliterator.DISTINCT | Spliterator.CONCURRENT | Spliterator.NONNULL; } } // Parallel bulk operations /** * Computes initial batch value for bulk tasks. The returned value * is approximately exp2 of the number of times (minus one) to * split task by two before executing leaf action. This value is * faster to compute and more convenient to use as a guide to * splitting than is the depth, since it is used while dividing by * two anyway. */ final int batchFor(long b) { long n; if (b == Long.MAX_VALUE || (n = sumCount()) = sp) ? sp : (int)n; } /** * Performs the given action for each (key, value). * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param action the action * @since 1.8 */ public void forEach(long parallelismThreshold, BiConsumer action) { if (action == null) throw new NullPointerException(); new ForEachMappingTask (null, batchFor(parallelismThreshold), 0, 0, table, action).invoke(); } /** * Performs the given action for each non-null transformation * of each (key, value). * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element, or null if there is no transformation (in * which case the action is not applied) * @param action the action * @param the return type of the transformer * @since 1.8 */ public void forEach(long parallelismThreshold, BiFunction transformer, Consumer action) { if (transformer == null || action == null) throw new NullPointerException(); new ForEachTransformedMappingTask (null, batchFor(parallelismThreshold), 0, 0, table, transformer, action).invoke(); } /** * Returns a non-null result from applying the given search * function on each (key, value), or null if none. Upon * success, further element processing is suppressed and the * results of any other parallel invocations of the search * function are ignored. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param searchFunction a function returning a non-null * result on success, else null * @param the return type of the search function * @return a non-null result from applying the given search * function on each (key, value), or null if none * @since 1.8 */ public U search(long parallelismThreshold, BiFunction searchFunction) { if (searchFunction == null) throw new NullPointerException(); return new SearchMappingsTask (null, batchFor(parallelismThreshold), 0, 0, table, searchFunction, new AtomicReference()).invoke(); } /** * Returns the result of accumulating the given transformation * of all (key, value) pairs using the given reducer to * combine values, or null if none. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element, or null if there is no transformation (in * which case it is not combined) * @param reducer a commutative associative combining function * @param the return type of the transformer * @return the result of accumulating the given transformation * of all (key, value) pairs * @since 1.8 */ public U reduce(long parallelismThreshold, BiFunction transformer, BiFunction reducer) { if (transformer == null || reducer == null) throw new NullPointerException(); return new MapReduceMappingsTask (null, batchFor(parallelismThreshold), 0, 0, table, null, transformer, reducer).invoke(); } /** * Returns the result of accumulating the given transformation * of all (key, value) pairs using the given reducer to * combine values, and the given basis as an identity value. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element * @param basis the identity (initial default value) for the reduction * @param reducer a commutative associative combining function * @return the result of accumulating the given transformation * of all (key, value) pairs * @since 1.8 */ public double reduceToDouble(long parallelismThreshold, ToDoubleBiFunction transformer, double basis, DoubleBinaryOperator reducer) { if (transformer == null || reducer == null) throw new NullPointerException(); return new MapReduceMappingsToDoubleTask (null, batchFor(parallelismThreshold), 0, 0, table, null, transformer, basis, reducer).invoke(); } /** * Returns the result of accumulating the given transformation * of all (key, value) pairs using the given reducer to * combine values, and the given basis as an identity value. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element * @param basis the identity (initial default value) for the reduction * @param reducer a commutative associative combining function * @return the result of accumulating the given transformation * of all (key, value) pairs * @since 1.8 */ public long reduceToLong(long parallelismThreshold, ToLongBiFunction transformer, long basis, LongBinaryOperator reducer) { if (transformer == null || reducer == null) throw new NullPointerException(); return new MapReduceMappingsToLongTask (null, batchFor(parallelismThreshold), 0, 0, table, null, transformer, basis, reducer).invoke(); } /** * Returns the result of accumulating the given transformation * of all (key, value) pairs using the given reducer to * combine values, and the given basis as an identity value. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element * @param basis the identity (initial default value) for the reduction * @param reducer a commutative associative combining function * @return the result of accumulating the given transformation * of all (key, value) pairs * @since 1.8 */ public int reduceToInt(long parallelismThreshold, ToIntBiFunction transformer, int basis, IntBinaryOperator reducer) { if (transformer == null || reducer == null) throw new NullPointerException(); return new MapReduceMappingsToIntTask (null, batchFor(parallelismThreshold), 0, 0, table, null, transformer, basis, reducer).invoke(); } /** * Performs the given action for each key. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param action the action * @since 1.8 */ public void forEachKey(long parallelismThreshold, Consumer action) { if (action == null) throw new NullPointerException(); new ForEachKeyTask (null, batchFor(parallelismThreshold), 0, 0, table, action).invoke(); } /** * Performs the given action for each non-null transformation * of each key. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element, or null if there is no transformation (in * which case the action is not applied) * @param action the action * @param the return type of the transformer * @since 1.8 */ public void forEachKey(long parallelismThreshold, Function transformer, Consumer action) { if (transformer == null || action == null) throw new NullPointerException(); new ForEachTransformedKeyTask (null, batchFor(parallelismThreshold), 0, 0, table, transformer, action).invoke(); } /** * Returns a non-null result from applying the given search * function on each key, or null if none. Upon success, * further element processing is suppressed and the results of * any other parallel invocations of the search function are * ignored. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param searchFunction a function returning a non-null * result on success, else null * @param the return type of the search function * @return a non-null result from applying the given search * function on each key, or null if none * @since 1.8 */ public U searchKeys(long parallelismThreshold, Function searchFunction) { if (searchFunction == null) throw new NullPointerException(); return new SearchKeysTask (null, batchFor(parallelismThreshold), 0, 0, table, searchFunction, new AtomicReference()).invoke(); } /** * Returns the result of accumulating all keys using the given * reducer to combine values, or null if none. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param reducer a commutative associative combining function * @return the result of accumulating all keys using the given * reducer to combine values, or null if none * @since 1.8 */ public K reduceKeys(long parallelismThreshold, BiFunction reducer) { if (reducer == null) throw new NullPointerException(); return new ReduceKeysTask (null, batchFor(parallelismThreshold), 0, 0, table, null, reducer).invoke(); } /** * Returns the result of accumulating the given transformation * of all keys using the given reducer to combine values, or * null if none. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element, or null if there is no transformation (in * which case it is not combined) * @param reducer a commutative associative combining function * @param the return type of the transformer * @return the result of accumulating the given transformation * of all keys * @since 1.8 */ public U reduceKeys(long parallelismThreshold, Function transformer, BiFunction reducer) { if (transformer == null || reducer == null) throw new NullPointerException(); return new MapReduceKeysTask (null, batchFor(parallelismThreshold), 0, 0, table, null, transformer, reducer).invoke(); } /** * Returns the result of accumulating the given transformation * of all keys using the given reducer to combine values, and * the given basis as an identity value. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element * @param basis the identity (initial default value) for the reduction * @param reducer a commutative associative combining function * @return the result of accumulating the given transformation * of all keys * @since 1.8 */ public double reduceKeysToDouble(long parallelismThreshold, ToDoubleFunction transformer, double basis, DoubleBinaryOperator reducer) { if (transformer == null || reducer == null) throw new NullPointerException(); return new MapReduceKeysToDoubleTask (null, batchFor(parallelismThreshold), 0, 0, table, null, transformer, basis, reducer).invoke(); } /** * Returns the result of accumulating the given transformation * of all keys using the given reducer to combine values, and * the given basis as an identity value. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element * @param basis the identity (initial default value) for the reduction * @param reducer a commutative associative combining function * @return the result of accumulating the given transformation * of all keys * @since 1.8 */ public long reduceKeysToLong(long parallelismThreshold, ToLongFunction transformer, long basis, LongBinaryOperator reducer) { if (transformer == null || reducer == null) throw new NullPointerException(); return new MapReduceKeysToLongTask (null, batchFor(parallelismThreshold), 0, 0, table, null, transformer, basis, reducer).invoke(); } /** * Returns the result of accumulating the given transformation * of all keys using the given reducer to combine values, and * the given basis as an identity value. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element * @param basis the identity (initial default value) for the reduction * @param reducer a commutative associative combining function * @return the result of accumulating the given transformation * of all keys * @since 1.8 */ public int reduceKeysToInt(long parallelismThreshold, ToIntFunction transformer, int basis, IntBinaryOperator reducer) { if (transformer == null || reducer == null) throw new NullPointerException(); return new MapReduceKeysToIntTask (null, batchFor(parallelismThreshold), 0, 0, table, null, transformer, basis, reducer).invoke(); } /** * Performs the given action for each value. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param action the action * @since 1.8 */ public void forEachValue(long parallelismThreshold, Consumer action) { if (action == null) throw new NullPointerException(); new ForEachValueTask (null, batchFor(parallelismThreshold), 0, 0, table, action).invoke(); } /** * Performs the given action for each non-null transformation * of each value. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element, or null if there is no transformation (in * which case the action is not applied) * @param action the action * @param the return type of the transformer * @since 1.8 */ public void forEachValue(long parallelismThreshold, Function transformer, Consumer action) { if (transformer == null || action == null) throw new NullPointerException(); new ForEachTransformedValueTask (null, batchFor(parallelismThreshold), 0, 0, table, transformer, action).invoke(); } /** * Returns a non-null result from applying the given search * function on each value, or null if none. Upon success, * further element processing is suppressed and the results of * any other parallel invocations of the search function are * ignored. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param searchFunction a function returning a non-null * result on success, else null * @param the return type of the search function * @return a non-null result from applying the given search * function on each value, or null if none * @since 1.8 */ public U searchValues(long parallelismThreshold, Function searchFunction) { if (searchFunction == null) throw new NullPointerException(); return new SearchValuesTask (null, batchFor(parallelismThreshold), 0, 0, table, searchFunction, new AtomicReference()).invoke(); } /** * Returns the result of accumulating all values using the * given reducer to combine values, or null if none. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param reducer a commutative associative combining function * @return the result of accumulating all values * @since 1.8 */ public V reduceValues(long parallelismThreshold, BiFunction reducer) { if (reducer == null) throw new NullPointerException(); return new ReduceValuesTask (null, batchFor(parallelismThreshold), 0, 0, table, null, reducer).invoke(); } /** * Returns the result of accumulating the given transformation * of all values using the given reducer to combine values, or * null if none. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element, or null if there is no transformation (in * which case it is not combined) * @param reducer a commutative associative combining function * @param the return type of the transformer * @return the result of accumulating the given transformation * of all values * @since 1.8 */ public U reduceValues(long parallelismThreshold, Function transformer, BiFunction reducer) { if (transformer == null || reducer == null) throw new NullPointerException(); return new MapReduceValuesTask (null, batchFor(parallelismThreshold), 0, 0, table, null, transformer, reducer).invoke(); } /** * Returns the result of accumulating the given transformation * of all values using the given reducer to combine values, * and the given basis as an identity value. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element * @param basis the identity (initial default value) for the reduction * @param reducer a commutative associative combining function * @return the result of accumulating the given transformation * of all values * @since 1.8 */ public double reduceValuesToDouble(long parallelismThreshold, ToDoubleFunction transformer, double basis, DoubleBinaryOperator reducer) { if (transformer == null || reducer == null) throw new NullPointerException(); return new MapReduceValuesToDoubleTask (null, batchFor(parallelismThreshold), 0, 0, table, null, transformer, basis, reducer).invoke(); } /** * Returns the result of accumulating the given transformation * of all values using the given reducer to combine values, * and the given basis as an identity value. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element * @param basis the identity (initial default value) for the reduction * @param reducer a commutative associative combining function * @return the result of accumulating the given transformation * of all values * @since 1.8 */ public long reduceValuesToLong(long parallelismThreshold, ToLongFunction transformer, long basis, LongBinaryOperator reducer) { if (transformer == null || reducer == null) throw new NullPointerException(); return new MapReduceValuesToLongTask (null, batchFor(parallelismThreshold), 0, 0, table, null, transformer, basis, reducer).invoke(); } /** * Returns the result of accumulating the given transformation * of all values using the given reducer to combine values, * and the given basis as an identity value. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element * @param basis the identity (initial default value) for the reduction * @param reducer a commutative associative combining function * @return the result of accumulating the given transformation * of all values * @since 1.8 */ public int reduceValuesToInt(long parallelismThreshold, ToIntFunction transformer, int basis, IntBinaryOperator reducer) { if (transformer == null || reducer == null) throw new NullPointerException(); return new MapReduceValuesToIntTask (null, batchFor(parallelismThreshold), 0, 0, table, null, transformer, basis, reducer).invoke(); } /** * Performs the given action for each entry. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param action the action * @since 1.8 */ public void forEachEntry(long parallelismThreshold, Consumer> action) { if (action == null) throw new NullPointerException(); new ForEachEntryTask(null, batchFor(parallelismThreshold), 0, 0, table, action).invoke(); } /** * Performs the given action for each non-null transformation * of each entry. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element, or null if there is no transformation (in * which case the action is not applied) * @param action the action * @param the return type of the transformer * @since 1.8 */ public void forEachEntry(long parallelismThreshold, Function, ? extends U> transformer, Consumer action) { if (transformer == null || action == null) throw new NullPointerException(); new ForEachTransformedEntryTask (null, batchFor(parallelismThreshold), 0, 0, table, transformer, action).invoke(); } /** * Returns a non-null result from applying the given search * function on each entry, or null if none. Upon success, * further element processing is suppressed and the results of * any other parallel invocations of the search function are * ignored. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param searchFunction a function returning a non-null * result on success, else null * @param the return type of the search function * @return a non-null result from applying the given search * function on each entry, or null if none * @since 1.8 */ public U searchEntries(long parallelismThreshold, Function, ? extends U> searchFunction) { if (searchFunction == null) throw new NullPointerException(); return new SearchEntriesTask (null, batchFor(parallelismThreshold), 0, 0, table, searchFunction, new AtomicReference()).invoke(); } /** * Returns the result of accumulating all entries using the * given reducer to combine values, or null if none. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param reducer a commutative associative combining function * @return the result of accumulating all entries * @since 1.8 */ public Map.Entry reduceEntries(long parallelismThreshold, BiFunction, Map.Entry, ? extends Map.Entry> reducer) { if (reducer == null) throw new NullPointerException(); return new ReduceEntriesTask (null, batchFor(parallelismThreshold), 0, 0, table, null, reducer).invoke(); } /** * Returns the result of accumulating the given transformation * of all entries using the given reducer to combine values, * or null if none. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element, or null if there is no transformation (in * which case it is not combined) * @param reducer a commutative associative combining function * @param the return type of the transformer * @return the result of accumulating the given transformation * of all entries * @since 1.8 */ public U reduceEntries(long parallelismThreshold, Function, ? extends U> transformer, BiFunction reducer) { if (transformer == null || reducer == null) throw new NullPointerException(); return new MapReduceEntriesTask (null, batchFor(parallelismThreshold), 0, 0, table, null, transformer, reducer).invoke(); } /** * Returns the result of accumulating the given transformation * of all entries using the given reducer to combine values, * and the given basis as an identity value. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element * @param basis the identity (initial default value) for the reduction * @param reducer a commutative associative combining function * @return the result of accumulating the given transformation * of all entries * @since 1.8 */ public double reduceEntriesToDouble(long parallelismThreshold, ToDoubleFunction> transformer, double basis, DoubleBinaryOperator reducer) { if (transformer == null || reducer == null) throw new NullPointerException(); return new MapReduceEntriesToDoubleTask (null, batchFor(parallelismThreshold), 0, 0, table, null, transformer, basis, reducer).invoke(); } /** * Returns the result of accumulating the given transformation * of all entries using the given reducer to combine values, * and the given basis as an identity value. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element * @param basis the identity (initial default value) for the reduction * @param reducer a commutative associative combining function * @return the result of accumulating the given transformation * of all entries * @since 1.8 */ public long reduceEntriesToLong(long parallelismThreshold, ToLongFunction> transformer, long basis, LongBinaryOperator reducer) { if (transformer == null || reducer == null) throw new NullPointerException(); return new MapReduceEntriesToLongTask (null, batchFor(parallelismThreshold), 0, 0, table, null, transformer, basis, reducer).invoke(); } /** * Returns the result of accumulating the given transformation * of all entries using the given reducer to combine values, * and the given basis as an identity value. * * @param parallelismThreshold the (estimated) number of elements * needed for this operation to be executed in parallel * @param transformer a function returning the transformation * for an element * @param basis the identity (initial default value) for the reduction * @param reducer a commutative associative combining function * @return the result of accumulating the given transformation * of all entries * @since 1.8 */ public int reduceEntriesToInt(long parallelismThreshold, ToIntFunction> transformer, int basis, IntBinaryOperator reducer) { if (transformer == null || reducer == null) throw new NullPointerException(); return new MapReduceEntriesToIntTask (null, batchFor(parallelismThreshold), 0, 0, table, null, transformer, basis, reducer).invoke(); } /* ----------------Views -------------- */ /** * Base class for views. */ abstract static class CollectionView implements Collection, java.io.Serializable { private static final long serialVersionUID = 7249069246763182397L; final ConcurrentHashMap map; CollectionView(ConcurrentHashMap map) { this.map = map; } /** * Returns the map backing this view. * * @return the map backing this view */ public ConcurrentHashMap getMap() { return map; } /** * Removes all of the elements from this view, by removing all * the mappings from the map backing this view. */ public final void clear() { map.clear(); } public final int size() { return map.size(); } public final boolean isEmpty() { return map.isEmpty(); } // implementations below rely on concrete classes supplying these // abstract methods /** * Returns an iterator over the elements in this collection. * * The returned iterator is * weakly consistent. * * @return an iterator over the elements in this collection */ public abstract Iterator iterator(); public abstract boolean contains(Object o); public abstract boolean remove(Object o); private static final String oomeMsg = \"Required array size too large\"; public final Object[] toArray() { long sz = map.mappingCount(); if (sz > MAX_ARRAY_SIZE) throw new OutOfMemoryError(oomeMsg); int n = (int)sz; Object[] r = new Object[n]; int i = 0; for (E e : this) { if (i == n) { if (n >= MAX_ARRAY_SIZE) throw new OutOfMemoryError(oomeMsg); if (n >= MAX_ARRAY_SIZE - (MAX_ARRAY_SIZE >>> 1) - 1) n = MAX_ARRAY_SIZE; else n += (n >>> 1) + 1; r = Arrays.copyOf(r, n); } r[i++] = e; } return (i == n) ? r : Arrays.copyOf(r, i); } @SuppressWarnings(\"unchecked\") public final T[] toArray(T[] a) { long sz = map.mappingCount(); if (sz > MAX_ARRAY_SIZE) throw new OutOfMemoryError(oomeMsg); int m = (int)sz; T[] r = (a.length >= m) ? a : (T[])java.lang.reflect.Array .newInstance(a.getClass().getComponentType(), m); int n = r.length; int i = 0; for (E e : this) { if (i == n) { if (n >= MAX_ARRAY_SIZE) throw new OutOfMemoryError(oomeMsg); if (n >= MAX_ARRAY_SIZE - (MAX_ARRAY_SIZE >>> 1) - 1) n = MAX_ARRAY_SIZE; else n += (n >>> 1) + 1; r = Arrays.copyOf(r, n); } r[i++] = (T)e; } if (a == r && i it = iterator(); if (it.hasNext()) { for (;;) { Object e = it.next(); sb.append(e == this ? \"(this Collection)\" : e); if (!it.hasNext()) break; sb.append(',').append(' '); } } return sb.append(']').toString(); } public final boolean containsAll(Collection c) { if (c != this) { for (Object e : c) { if (e == null || !contains(e)) return false; } } return true; } public final boolean removeAll(Collection c) { if (c == null) throw new NullPointerException(); boolean modified = false; for (Iterator it = iterator(); it.hasNext();) { if (c.contains(it.next())) { it.remove(); modified = true; } } return modified; } public final boolean retainAll(Collection c) { if (c == null) throw new NullPointerException(); boolean modified = false; for (Iterator it = iterator(); it.hasNext();) { if (!c.contains(it.next())) { it.remove(); modified = true; } } return modified; } } /** * A view of a ConcurrentHashMap as a {@link Set} of keys, in * which additions may optionally be enabled by mapping to a * common value. This class cannot be directly instantiated. * See {@link #keySet() keySet()}, * {@link #keySet(Object) keySet(V)}, * {@link #newKeySet() newKeySet()}, * {@link #newKeySet(int) newKeySet(int)}. * * @since 1.8 */ public static class KeySetView extends CollectionView implements Set, java.io.Serializable { private static final long serialVersionUID = 7249069246763182397L; private final V value; KeySetView(ConcurrentHashMap map, V value) { // non-public super(map); this.value = value; } /** * Returns the default mapped value for additions, * or {@code null} if additions are not supported. * * @return the default mapped value for additions, or {@code null} * if not supported */ public V getMappedValue() { return value; } /** * {@inheritDoc} * @throws NullPointerException if the specified key is null */ public boolean contains(Object o) { return map.containsKey(o); } /** * Removes the key from this map view, by removing the key (and its * corresponding value) from the backing map. This method does * nothing if the key is not in the map. * * @param o the key to be removed from the backing map * @return {@code true} if the backing map contained the specified key * @throws NullPointerException if the specified key is null */ public boolean remove(Object o) { return map.remove(o) != null; } /** * @return an iterator over the keys of the backing map */ public Iterator iterator() { Node[] t; ConcurrentHashMap m = map; int f = (t = m.table) == null ? 0 : t.length; return new KeyIterator(t, f, 0, f, m); } /** * Adds the specified key to this set view by mapping the key to * the default mapped value in the backing map, if defined. * * @param e key to be added * @return {@code true} if this set changed as a result of the call * @throws NullPointerException if the specified key is null * @throws UnsupportedOperationException if no default mapped value * for additions was provided */ public boolean add(K e) { V v; if ((v = value) == null) throw new UnsupportedOperationException(); return map.putVal(e, v, true) == null; } /** * Adds all of the elements in the specified collection to this set, * as if by calling {@link #add} on each one. * * @param c the elements to be inserted into this set * @return {@code true} if this set changed as a result of the call * @throws NullPointerException if the collection or any of its * elements are {@code null} * @throws UnsupportedOperationException if no default mapped value * for additions was provided */ public boolean addAll(Collection c) { boolean added = false; V v; if ((v = value) == null) throw new UnsupportedOperationException(); for (K e : c) { if (map.putVal(e, v, true) == null) added = true; } return added; } public int hashCode() { int h = 0; for (K e : this) h += e.hashCode(); return h; } public boolean equals(Object o) { Set c; return ((o instanceof Set) && ((c = (Set)o) == this || (containsAll(c) && c.containsAll(this)))); } public Spliterator spliterator() { Node[] t; ConcurrentHashMap m = map; long n = m.sumCount(); int f = (t = m.table) == null ? 0 : t.length; return new KeySpliterator(t, f, 0, f, n action) { if (action == null) throw new NullPointerException(); Node[] t; if ((t = map.table) != null) { Traverser it = new Traverser(t, t.length, 0, t.length); for (Node p; (p = it.advance()) != null; ) action.accept(p.key); } } } /** * A view of a ConcurrentHashMap as a {@link Collection} of * values, in which additions are disabled. This class cannot be * directly instantiated. See {@link #values()}. */ static final class ValuesView extends CollectionView implements Collection, java.io.Serializable { private static final long serialVersionUID = 2249069246763182397L; ValuesView(ConcurrentHashMap map) { super(map); } public final boolean contains(Object o) { return map.containsValue(o); } public final boolean remove(Object o) { if (o != null) { for (Iterator it = iterator(); it.hasNext();) { if (o.equals(it.next())) { it.remove(); return true; } } } return false; } public final Iterator iterator() { ConcurrentHashMap m = map; Node[] t; int f = (t = m.table) == null ? 0 : t.length; return new ValueIterator(t, f, 0, f, m); } public final boolean add(V e) { throw new UnsupportedOperationException(); } public final boolean addAll(Collection c) { throw new UnsupportedOperationException(); } public Spliterator spliterator() { Node[] t; ConcurrentHashMap m = map; long n = m.sumCount(); int f = (t = m.table) == null ? 0 : t.length; return new ValueSpliterator(t, f, 0, f, n action) { if (action == null) throw new NullPointerException(); Node[] t; if ((t = map.table) != null) { Traverser it = new Traverser(t, t.length, 0, t.length); for (Node p; (p = it.advance()) != null; ) action.accept(p.val); } } } /** * A view of a ConcurrentHashMap as a {@link Set} of (key, value) * entries. This class cannot be directly instantiated. See * {@link #entrySet()}. */ static final class EntrySetView extends CollectionView> implements Set>, java.io.Serializable { private static final long serialVersionUID = 2249069246763182397L; EntrySetView(ConcurrentHashMap map) { super(map); } public boolean contains(Object o) { Object k, v, r; Map.Entry e; return ((o instanceof Map.Entry) && (k = (e = (Map.Entry)o).getKey()) != null && (r = map.get(k)) != null && (v = e.getValue()) != null && (v == r || v.equals(r))); } public boolean remove(Object o) { Object k, v; Map.Entry e; return ((o instanceof Map.Entry) && (k = (e = (Map.Entry)o).getKey()) != null && (v = e.getValue()) != null && map.remove(k, v)); } /** * @return an iterator over the entries of the backing map */ public Iterator> iterator() { ConcurrentHashMap m = map; Node[] t; int f = (t = m.table) == null ? 0 : t.length; return new EntryIterator(t, f, 0, f, m); } public boolean add(Entry e) { return map.putVal(e.getKey(), e.getValue(), false) == null; } public boolean addAll(Collection> c) { boolean added = false; for (Entry e : c) { if (add(e)) added = true; } return added; } public final int hashCode() { int h = 0; Node[] t; if ((t = map.table) != null) { Traverser it = new Traverser(t, t.length, 0, t.length); for (Node p; (p = it.advance()) != null; ) { h += p.hashCode(); } } return h; } public final boolean equals(Object o) { Set c; return ((o instanceof Set) && ((c = (Set)o) == this || (containsAll(c) && c.containsAll(this)))); } public Spliterator> spliterator() { Node[] t; ConcurrentHashMap m = map; long n = m.sumCount(); int f = (t = m.table) == null ? 0 : t.length; return new EntrySpliterator(t, f, 0, f, n > action) { if (action == null) throw new NullPointerException(); Node[] t; if ((t = map.table) != null) { Traverser it = new Traverser(t, t.length, 0, t.length); for (Node p; (p = it.advance()) != null; ) action.accept(new MapEntry(p.key, p.val, map)); } } } // ------------------------------------------------------- /** * Base class for bulk tasks. Repeats some fields and code from * class Traverser, because we need to subclass CountedCompleter. */ @SuppressWarnings(\"serial\") abstract static class BulkTask extends CountedCompleter { Node[] tab; // same as Traverser Node next; TableStack stack, spare; int index; int baseIndex; int baseLimit; final int baseSize; int batch; // split control BulkTask(BulkTask par, int b, int i, int f, Node[] t) { super(par); this.batch = b; this.index = this.baseIndex = i; if ((this.tab = t) == null) this.baseSize = this.baseLimit = 0; else if (par == null) this.baseSize = this.baseLimit = t.length; else { this.baseLimit = f; this.baseSize = par.baseSize; } } /** * Same as Traverser version */ final Node advance() { Node e; if ((e = next) != null) e = e.next; for (;;) { Node[] t; int i, n; if (e != null) return next = e; if (baseIndex >= baseLimit || (t = tab) == null || (n = t.length) )e).nextTable; e = null; pushState(t, i, n); continue; } else if (e instanceof TreeBin) e = ((TreeBin)e).first; else e = null; } if (stack != null) recoverState(n); else if ((index = i + baseSize) >= n) index = ++baseIndex; } } private void pushState(Node[] t, int i, int n) { TableStack s = spare; if (s != null) spare = s.next; else s = new TableStack(); s.tab = t; s.length = n; s.index = i; s.next = stack; stack = s; } private void recoverState(int n) { TableStack s; int len; while ((s = stack) != null && (index += (len = s.length)) >= n) { n = len; index = s.index; tab = s.tab; s.tab = null; TableStack next = s.next; s.next = spare; // save for reuse stack = next; spare = s; } if (s == null && (index += baseSize) >= n) index = ++baseIndex; } } /* * Task classes. Coded in a regular but ugly format/style to * simplify checks that each variant differs in the right way from * others. The null screenings exist because compilers cannot tell * that we've already null-checked task arguments, so we force * simplest hoisted bypass to help avoid convoluted traps. */ @SuppressWarnings(\"serial\") static final class ForEachKeyTask extends BulkTask { final Consumer action; ForEachKeyTask (BulkTask p, int b, int i, int f, Node[] t, Consumer action) { super(p, b, i, f, t); this.action = action; } public final void compute() { final Consumer action; if ((action = this.action) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); new ForEachKeyTask (this, batch >>>= 1, baseLimit = h, f, tab, action).fork(); } for (Node p; (p = advance()) != null;) action.accept(p.key); propagateCompletion(); } } } @SuppressWarnings(\"serial\") static final class ForEachValueTask extends BulkTask { final Consumer action; ForEachValueTask (BulkTask p, int b, int i, int f, Node[] t, Consumer action) { super(p, b, i, f, t); this.action = action; } public final void compute() { final Consumer action; if ((action = this.action) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); new ForEachValueTask (this, batch >>>= 1, baseLimit = h, f, tab, action).fork(); } for (Node p; (p = advance()) != null;) action.accept(p.val); propagateCompletion(); } } } @SuppressWarnings(\"serial\") static final class ForEachEntryTask extends BulkTask { final Consumer> action; ForEachEntryTask (BulkTask p, int b, int i, int f, Node[] t, Consumer> action) { super(p, b, i, f, t); this.action = action; } public final void compute() { final Consumer> action; if ((action = this.action) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); new ForEachEntryTask (this, batch >>>= 1, baseLimit = h, f, tab, action).fork(); } for (Node p; (p = advance()) != null; ) action.accept(p); propagateCompletion(); } } } @SuppressWarnings(\"serial\") static final class ForEachMappingTask extends BulkTask { final BiConsumer action; ForEachMappingTask (BulkTask p, int b, int i, int f, Node[] t, BiConsumer action) { super(p, b, i, f, t); this.action = action; } public final void compute() { final BiConsumer action; if ((action = this.action) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); new ForEachMappingTask (this, batch >>>= 1, baseLimit = h, f, tab, action).fork(); } for (Node p; (p = advance()) != null; ) action.accept(p.key, p.val); propagateCompletion(); } } } @SuppressWarnings(\"serial\") static final class ForEachTransformedKeyTask extends BulkTask { final Function transformer; final Consumer action; ForEachTransformedKeyTask (BulkTask p, int b, int i, int f, Node[] t, Function transformer, Consumer action) { super(p, b, i, f, t); this.transformer = transformer; this.action = action; } public final void compute() { final Function transformer; final Consumer action; if ((transformer = this.transformer) != null && (action = this.action) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); new ForEachTransformedKeyTask (this, batch >>>= 1, baseLimit = h, f, tab, transformer, action).fork(); } for (Node p; (p = advance()) != null; ) { U u; if ((u = transformer.apply(p.key)) != null) action.accept(u); } propagateCompletion(); } } } @SuppressWarnings(\"serial\") static final class ForEachTransformedValueTask extends BulkTask { final Function transformer; final Consumer action; ForEachTransformedValueTask (BulkTask p, int b, int i, int f, Node[] t, Function transformer, Consumer action) { super(p, b, i, f, t); this.transformer = transformer; this.action = action; } public final void compute() { final Function transformer; final Consumer action; if ((transformer = this.transformer) != null && (action = this.action) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); new ForEachTransformedValueTask (this, batch >>>= 1, baseLimit = h, f, tab, transformer, action).fork(); } for (Node p; (p = advance()) != null; ) { U u; if ((u = transformer.apply(p.val)) != null) action.accept(u); } propagateCompletion(); } } } @SuppressWarnings(\"serial\") static final class ForEachTransformedEntryTask extends BulkTask { final Function, ? extends U> transformer; final Consumer action; ForEachTransformedEntryTask (BulkTask p, int b, int i, int f, Node[] t, Function, ? extends U> transformer, Consumer action) { super(p, b, i, f, t); this.transformer = transformer; this.action = action; } public final void compute() { final Function, ? extends U> transformer; final Consumer action; if ((transformer = this.transformer) != null && (action = this.action) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); new ForEachTransformedEntryTask (this, batch >>>= 1, baseLimit = h, f, tab, transformer, action).fork(); } for (Node p; (p = advance()) != null; ) { U u; if ((u = transformer.apply(p)) != null) action.accept(u); } propagateCompletion(); } } } @SuppressWarnings(\"serial\") static final class ForEachTransformedMappingTask extends BulkTask { final BiFunction transformer; final Consumer action; ForEachTransformedMappingTask (BulkTask p, int b, int i, int f, Node[] t, BiFunction transformer, Consumer action) { super(p, b, i, f, t); this.transformer = transformer; this.action = action; } public final void compute() { final BiFunction transformer; final Consumer action; if ((transformer = this.transformer) != null && (action = this.action) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); new ForEachTransformedMappingTask (this, batch >>>= 1, baseLimit = h, f, tab, transformer, action).fork(); } for (Node p; (p = advance()) != null; ) { U u; if ((u = transformer.apply(p.key, p.val)) != null) action.accept(u); } propagateCompletion(); } } } @SuppressWarnings(\"serial\") static final class SearchKeysTask extends BulkTask { final Function searchFunction; final AtomicReference result; SearchKeysTask (BulkTask p, int b, int i, int f, Node[] t, Function searchFunction, AtomicReference result) { super(p, b, i, f, t); this.searchFunction = searchFunction; this.result = result; } public final U getRawResult() { return result.get(); } public final void compute() { final Function searchFunction; final AtomicReference result; if ((searchFunction = this.searchFunction) != null && (result = this.result) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { if (result.get() != null) return; addToPendingCount(1); new SearchKeysTask (this, batch >>>= 1, baseLimit = h, f, tab, searchFunction, result).fork(); } while (result.get() == null) { U u; Node p; if ((p = advance()) == null) { propagateCompletion(); break; } if ((u = searchFunction.apply(p.key)) != null) { if (result.compareAndSet(null, u)) quietlyCompleteRoot(); break; } } } } } @SuppressWarnings(\"serial\") static final class SearchValuesTask extends BulkTask { final Function searchFunction; final AtomicReference result; SearchValuesTask (BulkTask p, int b, int i, int f, Node[] t, Function searchFunction, AtomicReference result) { super(p, b, i, f, t); this.searchFunction = searchFunction; this.result = result; } public final U getRawResult() { return result.get(); } public final void compute() { final Function searchFunction; final AtomicReference result; if ((searchFunction = this.searchFunction) != null && (result = this.result) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { if (result.get() != null) return; addToPendingCount(1); new SearchValuesTask (this, batch >>>= 1, baseLimit = h, f, tab, searchFunction, result).fork(); } while (result.get() == null) { U u; Node p; if ((p = advance()) == null) { propagateCompletion(); break; } if ((u = searchFunction.apply(p.val)) != null) { if (result.compareAndSet(null, u)) quietlyCompleteRoot(); break; } } } } } @SuppressWarnings(\"serial\") static final class SearchEntriesTask extends BulkTask { final Function, ? extends U> searchFunction; final AtomicReference result; SearchEntriesTask (BulkTask p, int b, int i, int f, Node[] t, Function, ? extends U> searchFunction, AtomicReference result) { super(p, b, i, f, t); this.searchFunction = searchFunction; this.result = result; } public final U getRawResult() { return result.get(); } public final void compute() { final Function, ? extends U> searchFunction; final AtomicReference result; if ((searchFunction = this.searchFunction) != null && (result = this.result) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { if (result.get() != null) return; addToPendingCount(1); new SearchEntriesTask (this, batch >>>= 1, baseLimit = h, f, tab, searchFunction, result).fork(); } while (result.get() == null) { U u; Node p; if ((p = advance()) == null) { propagateCompletion(); break; } if ((u = searchFunction.apply(p)) != null) { if (result.compareAndSet(null, u)) quietlyCompleteRoot(); return; } } } } } @SuppressWarnings(\"serial\") static final class SearchMappingsTask extends BulkTask { final BiFunction searchFunction; final AtomicReference result; SearchMappingsTask (BulkTask p, int b, int i, int f, Node[] t, BiFunction searchFunction, AtomicReference result) { super(p, b, i, f, t); this.searchFunction = searchFunction; this.result = result; } public final U getRawResult() { return result.get(); } public final void compute() { final BiFunction searchFunction; final AtomicReference result; if ((searchFunction = this.searchFunction) != null && (result = this.result) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { if (result.get() != null) return; addToPendingCount(1); new SearchMappingsTask (this, batch >>>= 1, baseLimit = h, f, tab, searchFunction, result).fork(); } while (result.get() == null) { U u; Node p; if ((p = advance()) == null) { propagateCompletion(); break; } if ((u = searchFunction.apply(p.key, p.val)) != null) { if (result.compareAndSet(null, u)) quietlyCompleteRoot(); break; } } } } } @SuppressWarnings(\"serial\") static final class ReduceKeysTask extends BulkTask { final BiFunction reducer; K result; ReduceKeysTask rights, nextRight; ReduceKeysTask (BulkTask p, int b, int i, int f, Node[] t, ReduceKeysTask nextRight, BiFunction reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.reducer = reducer; } public final K getRawResult() { return result; } public final void compute() { final BiFunction reducer; if ((reducer = this.reducer) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new ReduceKeysTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, reducer)).fork(); } K r = null; for (Node p; (p = advance()) != null; ) { K u = p.key; r = (r == null) ? u : u == null ? r : reducer.apply(r, u); } result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") ReduceKeysTask t = (ReduceKeysTask)c, s = t.rights; while (s != null) { K tr, sr; if ((sr = s.result) != null) t.result = (((tr = t.result) == null) ? sr : reducer.apply(tr, sr)); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class ReduceValuesTask extends BulkTask { final BiFunction reducer; V result; ReduceValuesTask rights, nextRight; ReduceValuesTask (BulkTask p, int b, int i, int f, Node[] t, ReduceValuesTask nextRight, BiFunction reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.reducer = reducer; } public final V getRawResult() { return result; } public final void compute() { final BiFunction reducer; if ((reducer = this.reducer) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new ReduceValuesTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, reducer)).fork(); } V r = null; for (Node p; (p = advance()) != null; ) { V v = p.val; r = (r == null) ? v : reducer.apply(r, v); } result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") ReduceValuesTask t = (ReduceValuesTask)c, s = t.rights; while (s != null) { V tr, sr; if ((sr = s.result) != null) t.result = (((tr = t.result) == null) ? sr : reducer.apply(tr, sr)); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class ReduceEntriesTask extends BulkTask> { final BiFunction, Map.Entry, ? extends Map.Entry> reducer; Map.Entry result; ReduceEntriesTask rights, nextRight; ReduceEntriesTask (BulkTask p, int b, int i, int f, Node[] t, ReduceEntriesTask nextRight, BiFunction, Map.Entry, ? extends Map.Entry> reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.reducer = reducer; } public final Map.Entry getRawResult() { return result; } public final void compute() { final BiFunction, Map.Entry, ? extends Map.Entry> reducer; if ((reducer = this.reducer) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new ReduceEntriesTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, reducer)).fork(); } Map.Entry r = null; for (Node p; (p = advance()) != null; ) r = (r == null) ? p : reducer.apply(r, p); result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") ReduceEntriesTask t = (ReduceEntriesTask)c, s = t.rights; while (s != null) { Map.Entry tr, sr; if ((sr = s.result) != null) t.result = (((tr = t.result) == null) ? sr : reducer.apply(tr, sr)); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class MapReduceKeysTask extends BulkTask { final Function transformer; final BiFunction reducer; U result; MapReduceKeysTask rights, nextRight; MapReduceKeysTask (BulkTask p, int b, int i, int f, Node[] t, MapReduceKeysTask nextRight, Function transformer, BiFunction reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.transformer = transformer; this.reducer = reducer; } public final U getRawResult() { return result; } public final void compute() { final Function transformer; final BiFunction reducer; if ((transformer = this.transformer) != null && (reducer = this.reducer) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new MapReduceKeysTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, transformer, reducer)).fork(); } U r = null; for (Node p; (p = advance()) != null; ) { U u; if ((u = transformer.apply(p.key)) != null) r = (r == null) ? u : reducer.apply(r, u); } result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") MapReduceKeysTask t = (MapReduceKeysTask)c, s = t.rights; while (s != null) { U tr, sr; if ((sr = s.result) != null) t.result = (((tr = t.result) == null) ? sr : reducer.apply(tr, sr)); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class MapReduceValuesTask extends BulkTask { final Function transformer; final BiFunction reducer; U result; MapReduceValuesTask rights, nextRight; MapReduceValuesTask (BulkTask p, int b, int i, int f, Node[] t, MapReduceValuesTask nextRight, Function transformer, BiFunction reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.transformer = transformer; this.reducer = reducer; } public final U getRawResult() { return result; } public final void compute() { final Function transformer; final BiFunction reducer; if ((transformer = this.transformer) != null && (reducer = this.reducer) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new MapReduceValuesTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, transformer, reducer)).fork(); } U r = null; for (Node p; (p = advance()) != null; ) { U u; if ((u = transformer.apply(p.val)) != null) r = (r == null) ? u : reducer.apply(r, u); } result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") MapReduceValuesTask t = (MapReduceValuesTask)c, s = t.rights; while (s != null) { U tr, sr; if ((sr = s.result) != null) t.result = (((tr = t.result) == null) ? sr : reducer.apply(tr, sr)); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class MapReduceEntriesTask extends BulkTask { final Function, ? extends U> transformer; final BiFunction reducer; U result; MapReduceEntriesTask rights, nextRight; MapReduceEntriesTask (BulkTask p, int b, int i, int f, Node[] t, MapReduceEntriesTask nextRight, Function, ? extends U> transformer, BiFunction reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.transformer = transformer; this.reducer = reducer; } public final U getRawResult() { return result; } public final void compute() { final Function, ? extends U> transformer; final BiFunction reducer; if ((transformer = this.transformer) != null && (reducer = this.reducer) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new MapReduceEntriesTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, transformer, reducer)).fork(); } U r = null; for (Node p; (p = advance()) != null; ) { U u; if ((u = transformer.apply(p)) != null) r = (r == null) ? u : reducer.apply(r, u); } result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") MapReduceEntriesTask t = (MapReduceEntriesTask)c, s = t.rights; while (s != null) { U tr, sr; if ((sr = s.result) != null) t.result = (((tr = t.result) == null) ? sr : reducer.apply(tr, sr)); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class MapReduceMappingsTask extends BulkTask { final BiFunction transformer; final BiFunction reducer; U result; MapReduceMappingsTask rights, nextRight; MapReduceMappingsTask (BulkTask p, int b, int i, int f, Node[] t, MapReduceMappingsTask nextRight, BiFunction transformer, BiFunction reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.transformer = transformer; this.reducer = reducer; } public final U getRawResult() { return result; } public final void compute() { final BiFunction transformer; final BiFunction reducer; if ((transformer = this.transformer) != null && (reducer = this.reducer) != null) { for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new MapReduceMappingsTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, transformer, reducer)).fork(); } U r = null; for (Node p; (p = advance()) != null; ) { U u; if ((u = transformer.apply(p.key, p.val)) != null) r = (r == null) ? u : reducer.apply(r, u); } result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") MapReduceMappingsTask t = (MapReduceMappingsTask)c, s = t.rights; while (s != null) { U tr, sr; if ((sr = s.result) != null) t.result = (((tr = t.result) == null) ? sr : reducer.apply(tr, sr)); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class MapReduceKeysToDoubleTask extends BulkTask { final ToDoubleFunction transformer; final DoubleBinaryOperator reducer; final double basis; double result; MapReduceKeysToDoubleTask rights, nextRight; MapReduceKeysToDoubleTask (BulkTask p, int b, int i, int f, Node[] t, MapReduceKeysToDoubleTask nextRight, ToDoubleFunction transformer, double basis, DoubleBinaryOperator reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.transformer = transformer; this.basis = basis; this.reducer = reducer; } public final Double getRawResult() { return result; } public final void compute() { final ToDoubleFunction transformer; final DoubleBinaryOperator reducer; if ((transformer = this.transformer) != null && (reducer = this.reducer) != null) { double r = this.basis; for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new MapReduceKeysToDoubleTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, transformer, r, reducer)).fork(); } for (Node p; (p = advance()) != null; ) r = reducer.applyAsDouble(r, transformer.applyAsDouble(p.key)); result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") MapReduceKeysToDoubleTask t = (MapReduceKeysToDoubleTask)c, s = t.rights; while (s != null) { t.result = reducer.applyAsDouble(t.result, s.result); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class MapReduceValuesToDoubleTask extends BulkTask { final ToDoubleFunction transformer; final DoubleBinaryOperator reducer; final double basis; double result; MapReduceValuesToDoubleTask rights, nextRight; MapReduceValuesToDoubleTask (BulkTask p, int b, int i, int f, Node[] t, MapReduceValuesToDoubleTask nextRight, ToDoubleFunction transformer, double basis, DoubleBinaryOperator reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.transformer = transformer; this.basis = basis; this.reducer = reducer; } public final Double getRawResult() { return result; } public final void compute() { final ToDoubleFunction transformer; final DoubleBinaryOperator reducer; if ((transformer = this.transformer) != null && (reducer = this.reducer) != null) { double r = this.basis; for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new MapReduceValuesToDoubleTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, transformer, r, reducer)).fork(); } for (Node p; (p = advance()) != null; ) r = reducer.applyAsDouble(r, transformer.applyAsDouble(p.val)); result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") MapReduceValuesToDoubleTask t = (MapReduceValuesToDoubleTask)c, s = t.rights; while (s != null) { t.result = reducer.applyAsDouble(t.result, s.result); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class MapReduceEntriesToDoubleTask extends BulkTask { final ToDoubleFunction> transformer; final DoubleBinaryOperator reducer; final double basis; double result; MapReduceEntriesToDoubleTask rights, nextRight; MapReduceEntriesToDoubleTask (BulkTask p, int b, int i, int f, Node[] t, MapReduceEntriesToDoubleTask nextRight, ToDoubleFunction> transformer, double basis, DoubleBinaryOperator reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.transformer = transformer; this.basis = basis; this.reducer = reducer; } public final Double getRawResult() { return result; } public final void compute() { final ToDoubleFunction> transformer; final DoubleBinaryOperator reducer; if ((transformer = this.transformer) != null && (reducer = this.reducer) != null) { double r = this.basis; for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new MapReduceEntriesToDoubleTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, transformer, r, reducer)).fork(); } for (Node p; (p = advance()) != null; ) r = reducer.applyAsDouble(r, transformer.applyAsDouble(p)); result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") MapReduceEntriesToDoubleTask t = (MapReduceEntriesToDoubleTask)c, s = t.rights; while (s != null) { t.result = reducer.applyAsDouble(t.result, s.result); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class MapReduceMappingsToDoubleTask extends BulkTask { final ToDoubleBiFunction transformer; final DoubleBinaryOperator reducer; final double basis; double result; MapReduceMappingsToDoubleTask rights, nextRight; MapReduceMappingsToDoubleTask (BulkTask p, int b, int i, int f, Node[] t, MapReduceMappingsToDoubleTask nextRight, ToDoubleBiFunction transformer, double basis, DoubleBinaryOperator reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.transformer = transformer; this.basis = basis; this.reducer = reducer; } public final Double getRawResult() { return result; } public final void compute() { final ToDoubleBiFunction transformer; final DoubleBinaryOperator reducer; if ((transformer = this.transformer) != null && (reducer = this.reducer) != null) { double r = this.basis; for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new MapReduceMappingsToDoubleTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, transformer, r, reducer)).fork(); } for (Node p; (p = advance()) != null; ) r = reducer.applyAsDouble(r, transformer.applyAsDouble(p.key, p.val)); result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") MapReduceMappingsToDoubleTask t = (MapReduceMappingsToDoubleTask)c, s = t.rights; while (s != null) { t.result = reducer.applyAsDouble(t.result, s.result); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class MapReduceKeysToLongTask extends BulkTask { final ToLongFunction transformer; final LongBinaryOperator reducer; final long basis; long result; MapReduceKeysToLongTask rights, nextRight; MapReduceKeysToLongTask (BulkTask p, int b, int i, int f, Node[] t, MapReduceKeysToLongTask nextRight, ToLongFunction transformer, long basis, LongBinaryOperator reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.transformer = transformer; this.basis = basis; this.reducer = reducer; } public final Long getRawResult() { return result; } public final void compute() { final ToLongFunction transformer; final LongBinaryOperator reducer; if ((transformer = this.transformer) != null && (reducer = this.reducer) != null) { long r = this.basis; for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new MapReduceKeysToLongTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, transformer, r, reducer)).fork(); } for (Node p; (p = advance()) != null; ) r = reducer.applyAsLong(r, transformer.applyAsLong(p.key)); result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") MapReduceKeysToLongTask t = (MapReduceKeysToLongTask)c, s = t.rights; while (s != null) { t.result = reducer.applyAsLong(t.result, s.result); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class MapReduceValuesToLongTask extends BulkTask { final ToLongFunction transformer; final LongBinaryOperator reducer; final long basis; long result; MapReduceValuesToLongTask rights, nextRight; MapReduceValuesToLongTask (BulkTask p, int b, int i, int f, Node[] t, MapReduceValuesToLongTask nextRight, ToLongFunction transformer, long basis, LongBinaryOperator reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.transformer = transformer; this.basis = basis; this.reducer = reducer; } public final Long getRawResult() { return result; } public final void compute() { final ToLongFunction transformer; final LongBinaryOperator reducer; if ((transformer = this.transformer) != null && (reducer = this.reducer) != null) { long r = this.basis; for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new MapReduceValuesToLongTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, transformer, r, reducer)).fork(); } for (Node p; (p = advance()) != null; ) r = reducer.applyAsLong(r, transformer.applyAsLong(p.val)); result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") MapReduceValuesToLongTask t = (MapReduceValuesToLongTask)c, s = t.rights; while (s != null) { t.result = reducer.applyAsLong(t.result, s.result); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class MapReduceEntriesToLongTask extends BulkTask { final ToLongFunction> transformer; final LongBinaryOperator reducer; final long basis; long result; MapReduceEntriesToLongTask rights, nextRight; MapReduceEntriesToLongTask (BulkTask p, int b, int i, int f, Node[] t, MapReduceEntriesToLongTask nextRight, ToLongFunction> transformer, long basis, LongBinaryOperator reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.transformer = transformer; this.basis = basis; this.reducer = reducer; } public final Long getRawResult() { return result; } public final void compute() { final ToLongFunction> transformer; final LongBinaryOperator reducer; if ((transformer = this.transformer) != null && (reducer = this.reducer) != null) { long r = this.basis; for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new MapReduceEntriesToLongTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, transformer, r, reducer)).fork(); } for (Node p; (p = advance()) != null; ) r = reducer.applyAsLong(r, transformer.applyAsLong(p)); result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") MapReduceEntriesToLongTask t = (MapReduceEntriesToLongTask)c, s = t.rights; while (s != null) { t.result = reducer.applyAsLong(t.result, s.result); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class MapReduceMappingsToLongTask extends BulkTask { final ToLongBiFunction transformer; final LongBinaryOperator reducer; final long basis; long result; MapReduceMappingsToLongTask rights, nextRight; MapReduceMappingsToLongTask (BulkTask p, int b, int i, int f, Node[] t, MapReduceMappingsToLongTask nextRight, ToLongBiFunction transformer, long basis, LongBinaryOperator reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.transformer = transformer; this.basis = basis; this.reducer = reducer; } public final Long getRawResult() { return result; } public final void compute() { final ToLongBiFunction transformer; final LongBinaryOperator reducer; if ((transformer = this.transformer) != null && (reducer = this.reducer) != null) { long r = this.basis; for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new MapReduceMappingsToLongTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, transformer, r, reducer)).fork(); } for (Node p; (p = advance()) != null; ) r = reducer.applyAsLong(r, transformer.applyAsLong(p.key, p.val)); result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") MapReduceMappingsToLongTask t = (MapReduceMappingsToLongTask)c, s = t.rights; while (s != null) { t.result = reducer.applyAsLong(t.result, s.result); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class MapReduceKeysToIntTask extends BulkTask { final ToIntFunction transformer; final IntBinaryOperator reducer; final int basis; int result; MapReduceKeysToIntTask rights, nextRight; MapReduceKeysToIntTask (BulkTask p, int b, int i, int f, Node[] t, MapReduceKeysToIntTask nextRight, ToIntFunction transformer, int basis, IntBinaryOperator reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.transformer = transformer; this.basis = basis; this.reducer = reducer; } public final Integer getRawResult() { return result; } public final void compute() { final ToIntFunction transformer; final IntBinaryOperator reducer; if ((transformer = this.transformer) != null && (reducer = this.reducer) != null) { int r = this.basis; for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new MapReduceKeysToIntTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, transformer, r, reducer)).fork(); } for (Node p; (p = advance()) != null; ) r = reducer.applyAsInt(r, transformer.applyAsInt(p.key)); result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") MapReduceKeysToIntTask t = (MapReduceKeysToIntTask)c, s = t.rights; while (s != null) { t.result = reducer.applyAsInt(t.result, s.result); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class MapReduceValuesToIntTask extends BulkTask { final ToIntFunction transformer; final IntBinaryOperator reducer; final int basis; int result; MapReduceValuesToIntTask rights, nextRight; MapReduceValuesToIntTask (BulkTask p, int b, int i, int f, Node[] t, MapReduceValuesToIntTask nextRight, ToIntFunction transformer, int basis, IntBinaryOperator reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.transformer = transformer; this.basis = basis; this.reducer = reducer; } public final Integer getRawResult() { return result; } public final void compute() { final ToIntFunction transformer; final IntBinaryOperator reducer; if ((transformer = this.transformer) != null && (reducer = this.reducer) != null) { int r = this.basis; for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new MapReduceValuesToIntTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, transformer, r, reducer)).fork(); } for (Node p; (p = advance()) != null; ) r = reducer.applyAsInt(r, transformer.applyAsInt(p.val)); result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") MapReduceValuesToIntTask t = (MapReduceValuesToIntTask)c, s = t.rights; while (s != null) { t.result = reducer.applyAsInt(t.result, s.result); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class MapReduceEntriesToIntTask extends BulkTask { final ToIntFunction> transformer; final IntBinaryOperator reducer; final int basis; int result; MapReduceEntriesToIntTask rights, nextRight; MapReduceEntriesToIntTask (BulkTask p, int b, int i, int f, Node[] t, MapReduceEntriesToIntTask nextRight, ToIntFunction> transformer, int basis, IntBinaryOperator reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.transformer = transformer; this.basis = basis; this.reducer = reducer; } public final Integer getRawResult() { return result; } public final void compute() { final ToIntFunction> transformer; final IntBinaryOperator reducer; if ((transformer = this.transformer) != null && (reducer = this.reducer) != null) { int r = this.basis; for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new MapReduceEntriesToIntTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, transformer, r, reducer)).fork(); } for (Node p; (p = advance()) != null; ) r = reducer.applyAsInt(r, transformer.applyAsInt(p)); result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") MapReduceEntriesToIntTask t = (MapReduceEntriesToIntTask)c, s = t.rights; while (s != null) { t.result = reducer.applyAsInt(t.result, s.result); s = t.rights = s.nextRight; } } } } } @SuppressWarnings(\"serial\") static final class MapReduceMappingsToIntTask extends BulkTask { final ToIntBiFunction transformer; final IntBinaryOperator reducer; final int basis; int result; MapReduceMappingsToIntTask rights, nextRight; MapReduceMappingsToIntTask (BulkTask p, int b, int i, int f, Node[] t, MapReduceMappingsToIntTask nextRight, ToIntBiFunction transformer, int basis, IntBinaryOperator reducer) { super(p, b, i, f, t); this.nextRight = nextRight; this.transformer = transformer; this.basis = basis; this.reducer = reducer; } public final Integer getRawResult() { return result; } public final void compute() { final ToIntBiFunction transformer; final IntBinaryOperator reducer; if ((transformer = this.transformer) != null && (reducer = this.reducer) != null) { int r = this.basis; for (int i = baseIndex, f, h; batch > 0 && (h = ((f = baseLimit) + i) >>> 1) > i;) { addToPendingCount(1); (rights = new MapReduceMappingsToIntTask (this, batch >>>= 1, baseLimit = h, f, tab, rights, transformer, r, reducer)).fork(); } for (Node p; (p = advance()) != null; ) r = reducer.applyAsInt(r, transformer.applyAsInt(p.key, p.val)); result = r; CountedCompleter c; for (c = firstComplete(); c != null; c = c.nextComplete()) { @SuppressWarnings(\"unchecked\") MapReduceMappingsToIntTask t = (MapReduceMappingsToIntTask)c, s = t.rights; while (s != null) { t.result = reducer.applyAsInt(t.result, s.result); s = t.rights = s.nextRight; } } } } } // Unsafe mechanics private static final sun.misc.Unsafe U; private static final long SIZECTL; private static final long TRANSFERINDEX; private static final long BASECOUNT; private static final long CELLSBUSY; private static final long CELLVALUE; private static final long ABASE; private static final int ASHIFT; static { try { U = sun.misc.Unsafe.getUnsafe(); Class k = ConcurrentHashMap.class; SIZECTL = U.objectFieldOffset (k.getDeclaredField(\"sizeCtl\")); TRANSFERINDEX = U.objectFieldOffset (k.getDeclaredField(\"transferIndex\")); BASECOUNT = U.objectFieldOffset (k.getDeclaredField(\"baseCount\")); CELLSBUSY = U.objectFieldOffset (k.getDeclaredField(\"cellsBusy\")); Class ck = CounterCell.class; CELLVALUE = U.objectFieldOffset (ck.getDeclaredField(\"value\")); Class ak = Node[].class; ABASE = U.arrayBaseOffset(ak); int scale = U.arrayIndexScale(ak); if ((scale & (scale - 1)) != 0) throw new Error(\"data type scale not a power of two\"); ASHIFT = 31 - Integer.numberOfLeadingZeros(scale); } catch (Exception e) { throw new Error(e); } } } "},"base/collection/hashmap源码/jdk1.8HashMap源码.html":{"url":"base/collection/hashmap源码/jdk1.8HashMap源码.html","title":"jdk1.8HashMap源码","keywords":"","body":" java8 /* * Copyright (c) 1997, 2013, Oracle and/or its affiliates. All rights reserved. * ORACLE PROPRIETARY/CONFIDENTIAL. Use is subject to license terms. * * * * * * * * * * * * * * * * * * * * */ package java.util; import java.io.IOException; import java.io.InvalidObjectException; import java.io.Serializable; import java.lang.reflect.ParameterizedType; import java.lang.reflect.Type; import java.util.function.BiConsumer; import java.util.function.BiFunction; import java.util.function.Consumer; import java.util.function.Function; /** * Hash table based implementation of the Map interface. This * implementation provides all of the optional map operations, and permits * null values and the null key. (The HashMap * class is roughly equivalent to Hashtable, except that it is * unsynchronized and permits nulls.) This class makes no guarantees as to * the order of the map; in particular, it does not guarantee that the order * will remain constant over time. * * This implementation provides constant-time performance for the basic * operations (get and put), assuming the hash function * disperses the elements properly among the buckets. Iteration over * collection views requires time proportional to the \"capacity\" of the * HashMap instance (the number of buckets) plus its size (the number * of key-value mappings). Thus, it's very important not to set the initial * capacity too high (or the load factor too low) if iteration performance is * important. * * An instance of HashMap has two parameters that affect its * performance: initial capacity and load factor. The * capacity is the number of buckets in the hash table, and the initial * capacity is simply the capacity at the time the hash table is created. The * load factor is a measure of how full the hash table is allowed to * get before its capacity is automatically increased. When the number of * entries in the hash table exceeds the product of the load factor and the * current capacity, the hash table is rehashed (that is, internal data * structures are rebuilt) so that the hash table has approximately twice the * number of buckets. * * As a general rule, the default load factor (.75) offers a good * tradeoff between time and space costs. Higher values decrease the * space overhead but increase the lookup cost (reflected in most of * the operations of the HashMap class, including * get and put). The expected number of entries in * the map and its load factor should be taken into account when * setting its initial capacity, so as to minimize the number of * rehash operations. If the initial capacity is greater than the * maximum number of entries divided by the load factor, no rehash * operations will ever occur. * * If many mappings are to be stored in a HashMap * instance, creating it with a sufficiently large capacity will allow * the mappings to be stored more efficiently than letting it perform * automatic rehashing as needed to grow the table. Note that using * many keys with the same {@code hashCode()} is a sure way to slow * down performance of any hash table. To ameliorate impact, when keys * are {@link Comparable}, this class may use comparison order among * keys to help break ties. * * Note that this implementation is not synchronized. * If multiple threads access a hash map concurrently, and at least one of * the threads modifies the map structurally, it must be * synchronized externally. (A structural modification is any operation * that adds or deletes one or more mappings; merely changing the value * associated with a key that an instance already contains is not a * structural modification.) This is typically accomplished by * synchronizing on some object that naturally encapsulates the map. * * If no such object exists, the map should be \"wrapped\" using the * {@link Collections#synchronizedMap Collections.synchronizedMap} * method. This is best done at creation time, to prevent accidental * unsynchronized access to the map: * Map m = Collections.synchronizedMap(new HashMap(...)); * * The iterators returned by all of this class's \"collection view methods\" * are fail-fast: if the map is structurally modified at any time after * the iterator is created, in any way except through the iterator's own * remove method, the iterator will throw a * {@link ConcurrentModificationException}. Thus, in the face of concurrent * modification, the iterator fails quickly and cleanly, rather than risking * arbitrary, non-deterministic behavior at an undetermined time in the * future. * * Note that the fail-fast behavior of an iterator cannot be guaranteed * as it is, generally speaking, impossible to make any hard guarantees in the * presence of unsynchronized concurrent modification. Fail-fast iterators * throw ConcurrentModificationException on a best-effort basis. * Therefore, it would be wrong to write a program that depended on this * exception for its correctness: the fail-fast behavior of iterators * should be used only to detect bugs. * * This class is a member of the * * Java Collections Framework. * * @param the type of keys maintained by this map * @param the type of mapped values * * @author Doug Lea * @author Josh Bloch * @author Arthur van Hoff * @author Neal Gafter * @see Object#hashCode() * @see Collection * @see Map * @see TreeMap * @see Hashtable * @since 1.2 */ public class HashMap extends AbstractMap implements Map, Cloneable, Serializable { private static final long serialVersionUID = 362498820763181265L; /* * Implementation notes. * * This map usually acts as a binned (bucketed) hash table, but * when bins get too large, they are transformed into bins of * TreeNodes, each structured similarly to those in * java.util.TreeMap. Most methods try to use normal bins, but * relay to TreeNode methods when applicable (simply by checking * instanceof a node). Bins of TreeNodes may be traversed and * used like any others, but additionally support faster lookup * when overpopulated. However, since the vast majority of bins in * normal use are not overpopulated, checking for existence of * tree bins may be delayed in the course of table methods. * * Tree bins (i.e., bins whose elements are all TreeNodes) are * ordered primarily by hashCode, but in the case of ties, if two * elements are of the same \"class C implements Comparable\", * type then their compareTo method is used for ordering. (We * conservatively check generic types via reflection to validate * this -- see method comparableClassFor). The added complexity * of tree bins is worthwhile in providing worst-case O(log n) * operations when keys either have distinct hashes or are * orderable, Thus, performance degrades gracefully under * accidental or malicious usages in which hashCode() methods * return values that are poorly distributed, as well as those in * which many keys share a hashCode, so long as they are also * Comparable. (If neither of these apply, we may waste about a * factor of two in time and space compared to taking no * precautions. But the only known cases stem from poor user * programming practices that are already so slow that this makes * little difference.) * * Because TreeNodes are about twice the size of regular nodes, we * use them only when bins contain enough nodes to warrant use * (see TREEIFY_THRESHOLD). And when they become too small (due to * removal or resizing) they are converted back to plain bins. In * usages with well-distributed user hashCodes, tree bins are * rarely used. Ideally, under random hashCodes, the frequency of * nodes in bins follows a Poisson distribution * (http://en.wikipedia.org/wiki/Poisson_distribution) with a * parameter of about 0.5 on average for the default resizing * threshold of 0.75, although with a large variance because of * resizing granularity. Ignoring variance, the expected * occurrences of list size k are (exp(-0.5) * pow(0.5, k) / * factorial(k)). The first values are: * * 0: 0.60653066 * 1: 0.30326533 * 2: 0.07581633 * 3: 0.01263606 * 4: 0.00157952 * 5: 0.00015795 * 6: 0.00001316 * 7: 0.00000094 * 8: 0.00000006 * more: less than 1 in ten million * * The root of a tree bin is normally its first node. However, * sometimes (currently only upon Iterator.remove), the root might * be elsewhere, but can be recovered following parent links * (method TreeNode.root()). * * All applicable internal methods accept a hash code as an * argument (as normally supplied from a public method), allowing * them to call each other without recomputing user hashCodes. * Most internal methods also accept a \"tab\" argument, that is * normally the current table, but may be a new or old one when * resizing or converting. * * When bin lists are treeified, split, or untreeified, we keep * them in the same relative access/traversal order (i.e., field * Node.next) to better preserve locality, and to slightly * simplify handling of splits and traversals that invoke * iterator.remove. When using comparators on insertion, to keep a * total ordering (or as close as is required here) across * rebalancings, we compare classes and identityHashCodes as * tie-breakers. * * The use and transitions among plain vs tree modes is * complicated by the existence of subclass LinkedHashMap. See * below for hook methods defined to be invoked upon insertion, * removal and access that allow LinkedHashMap internals to * otherwise remain independent of these mechanics. (This also * requires that a map instance be passed to some utility methods * that may create new nodes.) * * The concurrent-programming-like SSA-based coding style helps * avoid aliasing errors amid all of the twisty pointer operations. */ /** * The default initial capacity - MUST be a power of two. */ static final int DEFAULT_INITIAL_CAPACITY = 1 implements Map.Entry { final int hash; final K key; V value; Node next; Node(int hash, K key, V value, Node next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + \"=\" + value; } public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry e = (Map.Entry)o; if (Objects.equals(key, e.getKey()) && Objects.equals(value, e.getValue())) return true; } return false; } } /* ---------------- Static utilities -------------- */ /** * Computes key.hashCode() and spreads (XORs) higher bits of hash * to lower. Because the table uses power-of-two masking, sets of * hashes that vary only in bits above the current mask will * always collide. (Among known examples are sets of Float keys * holding consecutive whole numbers in small tables.) So we * apply a transform that spreads the impact of higher bits * downward. There is a tradeoff between speed, utility, and * quality of bit-spreading. Because many common sets of hashes * are already reasonably distributed (so don't benefit from * spreading), and because we use trees to handle large sets of * collisions in bins, we just XOR some shifted bits in the * cheapest possible way to reduce systematic lossage, as well as * to incorporate impact of the highest bits that would otherwise * never be used in index calculations because of table bounds. */ static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); } /** * Returns x's Class if it is of the form \"class C implements * Comparable\", else null. */ static Class comparableClassFor(Object x) { if (x instanceof Comparable) { Class c; Type[] ts, as; Type t; ParameterizedType p; if ((c = x.getClass()) == String.class) // bypass checks return c; if ((ts = c.getGenericInterfaces()) != null) { for (int i = 0; i kc, Object k, Object x) { return (x == null || x.getClass() != kc ? 0 : ((Comparable)k).compareTo(x)); } /** * Returns a power of two size for the given target capacity. */ static final int tableSizeFor(int cap) { int n = cap - 1; n |= n >>> 1; n |= n >>> 2; n |= n >>> 4; n |= n >>> 8; n |= n >>> 16; return (n = MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } /* ---------------- Fields -------------- */ /** * The table, initialized on first use, and resized as * necessary. When allocated, length is always a power of two. * (We also tolerate length zero in some operations to allow * bootstrapping mechanics that are currently not needed.) */ transient Node[] table; /** * Holds cached entrySet(). Note that AbstractMap fields are used * for keySet() and values(). */ transient Set> entrySet; /** * The number of key-value mappings contained in this map. */ transient int size; /** * The number of times this HashMap has been structurally modified * Structural modifications are those that change the number of mappings in * the HashMap or otherwise modify its internal structure (e.g., * rehash). This field is used to make iterators on Collection-views of * the HashMap fail-fast. (See ConcurrentModificationException). */ transient int modCount; /** * The next size value at which to resize (capacity * load factor). * * @serial */ // (The javadoc description is true upon serialization. // Additionally, if the table array has not been allocated, this // field holds the initial array capacity, or zero signifying // DEFAULT_INITIAL_CAPACITY.) int threshold; /** * The load factor for the hash table. * * @serial */ final float loadFactor; /* ---------------- Public operations -------------- */ /** * Constructs an empty HashMap with the specified initial * capacity and load factor. * * @param initialCapacity the initial capacity * @param loadFactor the load factor * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor HashMap with the specified initial * capacity and the default load factor (0.75). * * @param initialCapacity the initial capacity. * @throws IllegalArgumentException if the initial capacity is negative. */ public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } /** * Constructs an empty HashMap with the default initial capacity * (16) and the default load factor (0.75). */ public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted } /** * Constructs a new HashMap with the same mappings as the * specified Map. The HashMap is created with * default load factor (0.75) and an initial capacity sufficient to * hold the mappings in the specified Map. * * @param m the map whose mappings are to be placed in this map * @throws NullPointerException if the specified map is null */ public HashMap(Map m) { this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); } /** * Implements Map.putAll and Map constructor * * @param m the map * @param evict false when initially constructing this map, else * true (relayed to method afterNodeInsertion). */ final void putMapEntries(Map m, boolean evict) { int s = m.size(); if (s > 0) { if (table == null) { // pre-size float ft = ((float)s / loadFactor) + 1.0F; int t = ((ft threshold) threshold = tableSizeFor(t); } else if (s > threshold) resize(); for (Map.Entry e : m.entrySet()) { K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); } } } /** * Returns the number of key-value mappings in this map. * * @return the number of key-value mappings in this map */ public int size() { return size; } /** * Returns true if this map contains no key-value mappings. * * @return true if this map contains no key-value mappings */ public boolean isEmpty() { return size == 0; } /** * Returns the value to which the specified key is mapped, * or {@code null} if this map contains no mapping for the key. * * More formally, if this map contains a mapping from a key * {@code k} to a value {@code v} such that {@code (key==null ? k==null : * key.equals(k))}, then this method returns {@code v}; otherwise * it returns {@code null}. (There can be at most one such mapping.) * * A return value of {@code null} does not necessarily * indicate that the map contains no mapping for the key; it's also * possible that the map explicitly maps the key to {@code null}. * The {@link #containsKey containsKey} operation may be used to * distinguish these two cases. * * @see #put(Object, Object) */ public V get(Object key) { Node e; return (e = getNode(hash(key), key)) == null ? null : e.value; } /** * Implements Map.get and related methods * * @param hash hash for key * @param key the key * @return the node, or null if none */ final Node getNode(int hash, Object key) { Node[] tab; Node first, e; int n; K k; if ((tab = table) != null && (n = tab.length) > 0 && (first = tab[(n - 1) & hash]) != null) { if (first.hash == hash && // always check first node ((k = first.key) == key || (key != null && key.equals(k)))) return first; if ((e = first.next) != null) { if (first instanceof TreeNode) return ((TreeNode)first).getTreeNode(hash, key); do { if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } /** * Returns true if this map contains a mapping for the * specified key. * * @param key The key whose presence in this map is to be tested * @return true if this map contains a mapping for the specified * key. */ public boolean containsKey(Object key) { return getNode(hash(key), key) != null; } /** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with key, or * null if there was no mapping for key. * (A null return can also indicate that the map * previously associated null with key.) */ public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } /** * Implements Map.put and related methods * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don't change existing value * @param evict if false, the table is in creation mode. * @return previous value, or null if none */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node[] tab; Node p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) & hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node e; K k; if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode)p).putTreeVal(this, tab, hash, key, value); else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size > threshold) resize(); afterNodeInsertion(evict); return null; } /** * Initializes or doubles table size. If null, allocates in * accord with initial capacity target held in field threshold. * Otherwise, because we are using power-of-two expansion, the * elements from each bin must either stay at same index, or move * with a power of two offset in the new table. * * @return the table */ final Node[] resize() { Node[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap > 0) { if (oldCap >= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } else if ((newCap = oldCap = DEFAULT_INITIAL_CAPACITY) newThr = oldThr 0) // initial capacity was placed in threshold newCap = oldThr; else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap [] newTab = (Node[])new Node[newCap]; table = newTab; if (oldTab != null) { for (int j = 0; j e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash & (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode)e).split(this, newTab, j, oldCap); else { // preserve order Node loHead = null, loTail = null; Node hiHead = null, hiTail = null; Node next; do { next = e.next; if ((e.hash & oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); if (loTail != null) { loTail.next = null; newTab[j] = loHead; } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } /** * Replaces all linked nodes in bin at index for given hash unless * table is too small, in which case resizes instead. */ final void treeifyBin(Node[] tab, int hash) { int n, index; Node e; if (tab == null || (n = tab.length) hd = null, tl = null; do { TreeNode p = replacementTreeNode(e, null); if (tl == null) hd = p; else { p.prev = tl; tl.next = p; } tl = p; } while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); } } /** * Copies all of the mappings from the specified map to this map. * These mappings will replace any mappings that this map had for * any of the keys currently in the specified map. * * @param m mappings to be stored in this map * @throws NullPointerException if the specified map is null */ public void putAll(Map m) { putMapEntries(m, true); } /** * Removes the mapping for the specified key from this map if present. * * @param key key whose mapping is to be removed from the map * @return the previous value associated with key, or * null if there was no mapping for key. * (A null return can also indicate that the map * previously associated null with key.) */ public V remove(Object key) { Node e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value; } /** * Implements Map.remove and related methods * * @param hash hash for key * @param key the key * @param value the value to match if matchValue, else ignored * @param matchValue if true only remove if value is equal * @param movable if false do not move other nodes while removing * @return the node, or null if none */ final Node removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) { Node[] tab; Node p; int n, index; if ((tab = table) != null && (n = tab.length) > 0 && (p = tab[index = (n - 1) & hash]) != null) { Node node = null, e; K k; V v; if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k)))) node = p; else if ((e = p.next) != null) { if (p instanceof TreeNode) node = ((TreeNode)p).getTreeNode(hash, key); else { do { if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) { node = e; break; } p = e; } while ((e = e.next) != null); } } if (node != null && (!matchValue || (v = node.value) == value || (value != null && value.equals(v)))) { if (node instanceof TreeNode) ((TreeNode)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; } } return null; } /** * Removes all of the mappings from this map. * The map will be empty after this call returns. */ public void clear() { Node[] tab; modCount++; if ((tab = table) != null && size > 0) { size = 0; for (int i = 0; i true if this map maps one or more keys to the * specified value. * * @param value value whose presence in this map is to be tested * @return true if this map maps one or more keys to the * specified value */ public boolean containsValue(Object value) { Node[] tab; V v; if ((tab = table) != null && size > 0) { for (int i = 0; i e = tab[i]; e != null; e = e.next) { if ((v = e.value) == value || (value != null && value.equals(v))) return true; } } } return false; } /** * Returns a {@link Set} view of the keys contained in this map. * The set is backed by the map, so changes to the map are * reflected in the set, and vice-versa. If the map is modified * while an iteration over the set is in progress (except through * the iterator's own remove operation), the results of * the iteration are undefined. The set supports element removal, * which removes the corresponding mapping from the map, via the * Iterator.remove, Set.remove, * removeAll, retainAll, and clear * operations. It does not support the add or addAll * operations. * * @return a set view of the keys contained in this map */ public Set keySet() { Set ks; return (ks = keySet) == null ? (keySet = new KeySet()) : ks; } final class KeySet extends AbstractSet { public final int size() { return size; } public final void clear() { HashMap.this.clear(); } public final Iterator iterator() { return new KeyIterator(); } public final boolean contains(Object o) { return containsKey(o); } public final boolean remove(Object key) { return removeNode(hash(key), key, null, false, true) != null; } public final Spliterator spliterator() { return new KeySpliterator<>(HashMap.this, 0, -1, 0, 0); } public final void forEach(Consumer action) { Node[] tab; if (action == null) throw new NullPointerException(); if (size > 0 && (tab = table) != null) { int mc = modCount; for (int i = 0; i e = tab[i]; e != null; e = e.next) action.accept(e.key); } if (modCount != mc) throw new ConcurrentModificationException(); } } } /** * Returns a {@link Collection} view of the values contained in this map. * The collection is backed by the map, so changes to the map are * reflected in the collection, and vice-versa. If the map is * modified while an iteration over the collection is in progress * (except through the iterator's own remove operation), * the results of the iteration are undefined. The collection * supports element removal, which removes the corresponding * mapping from the map, via the Iterator.remove, * Collection.remove, removeAll, * retainAll and clear operations. It does not * support the add or addAll operations. * * @return a view of the values contained in this map */ public Collection values() { Collection vs; return (vs = values) == null ? (values = new Values()) : vs; } final class Values extends AbstractCollection { public final int size() { return size; } public final void clear() { HashMap.this.clear(); } public final Iterator iterator() { return new ValueIterator(); } public final boolean contains(Object o) { return containsValue(o); } public final Spliterator spliterator() { return new ValueSpliterator<>(HashMap.this, 0, -1, 0, 0); } public final void forEach(Consumer action) { Node[] tab; if (action == null) throw new NullPointerException(); if (size > 0 && (tab = table) != null) { int mc = modCount; for (int i = 0; i e = tab[i]; e != null; e = e.next) action.accept(e.value); } if (modCount != mc) throw new ConcurrentModificationException(); } } } /** * Returns a {@link Set} view of the mappings contained in this map. * The set is backed by the map, so changes to the map are * reflected in the set, and vice-versa. If the map is modified * while an iteration over the set is in progress (except through * the iterator's own remove operation, or through the * setValue operation on a map entry returned by the * iterator) the results of the iteration are undefined. The set * supports element removal, which removes the corresponding * mapping from the map, via the Iterator.remove, * Set.remove, removeAll, retainAll and * clear operations. It does not support the * add or addAll operations. * * @return a set view of the mappings contained in this map */ public Set> entrySet() { Set> es; return (es = entrySet) == null ? (entrySet = new EntrySet()) : es; } final class EntrySet extends AbstractSet> { public final int size() { return size; } public final void clear() { HashMap.this.clear(); } public final Iterator> iterator() { return new EntryIterator(); } public final boolean contains(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry) o; Object key = e.getKey(); Node candidate = getNode(hash(key), key); return candidate != null && candidate.equals(e); } public final boolean remove(Object o) { if (o instanceof Map.Entry) { Map.Entry e = (Map.Entry) o; Object key = e.getKey(); Object value = e.getValue(); return removeNode(hash(key), key, value, true, true) != null; } return false; } public final Spliterator> spliterator() { return new EntrySpliterator<>(HashMap.this, 0, -1, 0, 0); } public final void forEach(Consumer> action) { Node[] tab; if (action == null) throw new NullPointerException(); if (size > 0 && (tab = table) != null) { int mc = modCount; for (int i = 0; i e = tab[i]; e != null; e = e.next) action.accept(e); } if (modCount != mc) throw new ConcurrentModificationException(); } } } // Overrides of JDK8 Map extension methods @Override public V getOrDefault(Object key, V defaultValue) { Node e; return (e = getNode(hash(key), key)) == null ? defaultValue : e.value; } @Override public V putIfAbsent(K key, V value) { return putVal(hash(key), key, value, true, true); } @Override public boolean remove(Object key, Object value) { return removeNode(hash(key), key, value, true, true) != null; } @Override public boolean replace(K key, V oldValue, V newValue) { Node e; V v; if ((e = getNode(hash(key), key)) != null && ((v = e.value) == oldValue || (v != null && v.equals(oldValue)))) { e.value = newValue; afterNodeAccess(e); return true; } return false; } @Override public V replace(K key, V value) { Node e; if ((e = getNode(hash(key), key)) != null) { V oldValue = e.value; e.value = value; afterNodeAccess(e); return oldValue; } return null; } @Override public V computeIfAbsent(K key, Function mappingFunction) { if (mappingFunction == null) throw new NullPointerException(); int hash = hash(key); Node[] tab; Node first; int n, i; int binCount = 0; TreeNode t = null; Node old = null; if (size > threshold || (tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((first = tab[i = (n - 1) & hash]) != null) { if (first instanceof TreeNode) old = (t = (TreeNode)first).getTreeNode(hash, key); else { Node e = first; K k; do { if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) { old = e; break; } ++binCount; } while ((e = e.next) != null); } V oldValue; if (old != null && (oldValue = old.value) != null) { afterNodeAccess(old); return oldValue; } } V v = mappingFunction.apply(key); if (v == null) { return null; } else if (old != null) { old.value = v; afterNodeAccess(old); return v; } else if (t != null) t.putTreeVal(this, tab, hash, key, v); else { tab[i] = newNode(hash, key, v, first); if (binCount >= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); } ++modCount; ++size; afterNodeInsertion(true); return v; } public V computeIfPresent(K key, BiFunction remappingFunction) { if (remappingFunction == null) throw new NullPointerException(); Node e; V oldValue; int hash = hash(key); if ((e = getNode(hash, key)) != null && (oldValue = e.value) != null) { V v = remappingFunction.apply(key, oldValue); if (v != null) { e.value = v; afterNodeAccess(e); return v; } else removeNode(hash, key, null, false, true); } return null; } @Override public V compute(K key, BiFunction remappingFunction) { if (remappingFunction == null) throw new NullPointerException(); int hash = hash(key); Node[] tab; Node first; int n, i; int binCount = 0; TreeNode t = null; Node old = null; if (size > threshold || (tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((first = tab[i = (n - 1) & hash]) != null) { if (first instanceof TreeNode) old = (t = (TreeNode)first).getTreeNode(hash, key); else { Node e = first; K k; do { if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) { old = e; break; } ++binCount; } while ((e = e.next) != null); } } V oldValue = (old == null) ? null : old.value; V v = remappingFunction.apply(key, oldValue); if (old != null) { if (v != null) { old.value = v; afterNodeAccess(old); } else removeNode(hash, key, null, false, true); } else if (v != null) { if (t != null) t.putTreeVal(this, tab, hash, key, v); else { tab[i] = newNode(hash, key, v, first); if (binCount >= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); } ++modCount; ++size; afterNodeInsertion(true); } return v; } @Override public V merge(K key, V value, BiFunction remappingFunction) { if (value == null) throw new NullPointerException(); if (remappingFunction == null) throw new NullPointerException(); int hash = hash(key); Node[] tab; Node first; int n, i; int binCount = 0; TreeNode t = null; Node old = null; if (size > threshold || (tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((first = tab[i = (n - 1) & hash]) != null) { if (first instanceof TreeNode) old = (t = (TreeNode)first).getTreeNode(hash, key); else { Node e = first; K k; do { if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) { old = e; break; } ++binCount; } while ((e = e.next) != null); } } if (old != null) { V v; if (old.value != null) v = remappingFunction.apply(old.value, value); else v = value; if (v != null) { old.value = v; afterNodeAccess(old); } else removeNode(hash, key, null, false, true); return v; } if (value != null) { if (t != null) t.putTreeVal(this, tab, hash, key, value); else { tab[i] = newNode(hash, key, value, first); if (binCount >= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); } ++modCount; ++size; afterNodeInsertion(true); } return value; } @Override public void forEach(BiConsumer action) { Node[] tab; if (action == null) throw new NullPointerException(); if (size > 0 && (tab = table) != null) { int mc = modCount; for (int i = 0; i e = tab[i]; e != null; e = e.next) action.accept(e.key, e.value); } if (modCount != mc) throw new ConcurrentModificationException(); } } @Override public void replaceAll(BiFunction function) { Node[] tab; if (function == null) throw new NullPointerException(); if (size > 0 && (tab = table) != null) { int mc = modCount; for (int i = 0; i e = tab[i]; e != null; e = e.next) { e.value = function.apply(e.key, e.value); } } if (modCount != mc) throw new ConcurrentModificationException(); } } /* ------------------------------------------------------------ */ // Cloning and serialization /** * Returns a shallow copy of this HashMap instance: the keys and * values themselves are not cloned. * * @return a shallow copy of this map */ @SuppressWarnings(\"unchecked\") @Override public Object clone() { HashMap result; try { result = (HashMap)super.clone(); } catch (CloneNotSupportedException e) { // this shouldn't happen, since we are Cloneable throw new InternalError(e); } result.reinitialize(); result.putMapEntries(this, false); return result; } // These methods are also used when serializing HashSets final float loadFactor() { return loadFactor; } final int capacity() { return (table != null) ? table.length : (threshold > 0) ? threshold : DEFAULT_INITIAL_CAPACITY; } /** * Save the state of the HashMap instance to a stream (i.e., * serialize it). * * @serialData The capacity of the HashMap (the length of the * bucket array) is emitted (int), followed by the * size (an int, the number of key-value * mappings), followed by the key (Object) and value (Object) * for each key-value mapping. The key-value mappings are * emitted in no particular order. */ private void writeObject(java.io.ObjectOutputStream s) throws IOException { int buckets = capacity(); // Write out the threshold, loadfactor, and any hidden stuff s.defaultWriteObject(); s.writeInt(buckets); s.writeInt(size); internalWriteEntries(s); } /** * Reconstitute the {@code HashMap} instance from a stream (i.e., * deserialize it). */ private void readObject(java.io.ObjectInputStream s) throws IOException, ClassNotFoundException { // Read in the threshold (ignored), loadfactor, and any hidden stuff s.defaultReadObject(); reinitialize(); if (loadFactor 0) { // (if zero, use defaults) // Size the table using given load factor only if within // range of 0.25...4.0 float lf = Math.min(Math.max(0.25f, loadFactor), 4.0f); float fc = (float)mappings / lf + 1.0f; int cap = ((fc = MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)fc)); float ft = (float)cap * lf; threshold = ((cap [] tab = (Node[])new Node[cap]; table = tab; // Read the keys and values, and put the mappings in the HashMap for (int i = 0; i next; // next entry to return Node current; // current entry int expectedModCount; // for fast-fail int index; // current slot HashIterator() { expectedModCount = modCount; Node[] t = table; current = next = null; index = 0; if (t != null && size > 0) { // advance to first entry do {} while (index nextNode() { Node[] t; Node e = next; if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (e == null) throw new NoSuchElementException(); if ((next = (current = e).next) == null && (t = table) != null) { do {} while (index p = current; if (p == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); current = null; K key = p.key; removeNode(hash(key), key, null, false, false); expectedModCount = modCount; } } final class KeyIterator extends HashIterator implements Iterator { public final K next() { return nextNode().key; } } final class ValueIterator extends HashIterator implements Iterator { public final V next() { return nextNode().value; } } final class EntryIterator extends HashIterator implements Iterator> { public final Map.Entry next() { return nextNode(); } } /* ------------------------------------------------------------ */ // spliterators static class HashMapSpliterator { final HashMap map; Node current; // current node int index; // current index, modified on advance/split int fence; // one past last index int est; // size estimate int expectedModCount; // for comodification checks HashMapSpliterator(HashMap m, int origin, int fence, int est, int expectedModCount) { this.map = m; this.index = origin; this.fence = fence; this.est = est; this.expectedModCount = expectedModCount; } final int getFence() { // initialize fence and size on first use int hi; if ((hi = fence) m = map; est = m.size; expectedModCount = m.modCount; Node[] tab = m.table; hi = fence = (tab == null) ? 0 : tab.length; } return hi; } public final long estimateSize() { getFence(); // force init return (long) est; } } static final class KeySpliterator extends HashMapSpliterator implements Spliterator { KeySpliterator(HashMap m, int origin, int fence, int est, int expectedModCount) { super(m, origin, fence, est, expectedModCount); } public KeySpliterator trySplit() { int hi = getFence(), lo = index, mid = (lo + hi) >>> 1; return (lo >= mid || current != null) ? null : new KeySpliterator<>(map, lo, index = mid, est >>>= 1, expectedModCount); } public void forEachRemaining(Consumer action) { int i, hi, mc; if (action == null) throw new NullPointerException(); HashMap m = map; Node[] tab = m.table; if ((hi = fence) = hi && (i = index) >= 0 && (i p = current; current = null; do { if (p == null) p = tab[i++]; else { action.accept(p.key); p = p.next; } } while (p != null || i action) { int hi; if (action == null) throw new NullPointerException(); Node[] tab = map.table; if (tab != null && tab.length >= (hi = getFence()) && index >= 0) { while (current != null || index extends HashMapSpliterator implements Spliterator { ValueSpliterator(HashMap m, int origin, int fence, int est, int expectedModCount) { super(m, origin, fence, est, expectedModCount); } public ValueSpliterator trySplit() { int hi = getFence(), lo = index, mid = (lo + hi) >>> 1; return (lo >= mid || current != null) ? null : new ValueSpliterator<>(map, lo, index = mid, est >>>= 1, expectedModCount); } public void forEachRemaining(Consumer action) { int i, hi, mc; if (action == null) throw new NullPointerException(); HashMap m = map; Node[] tab = m.table; if ((hi = fence) = hi && (i = index) >= 0 && (i p = current; current = null; do { if (p == null) p = tab[i++]; else { action.accept(p.value); p = p.next; } } while (p != null || i action) { int hi; if (action == null) throw new NullPointerException(); Node[] tab = map.table; if (tab != null && tab.length >= (hi = getFence()) && index >= 0) { while (current != null || index extends HashMapSpliterator implements Spliterator> { EntrySpliterator(HashMap m, int origin, int fence, int est, int expectedModCount) { super(m, origin, fence, est, expectedModCount); } public EntrySpliterator trySplit() { int hi = getFence(), lo = index, mid = (lo + hi) >>> 1; return (lo >= mid || current != null) ? null : new EntrySpliterator<>(map, lo, index = mid, est >>>= 1, expectedModCount); } public void forEachRemaining(Consumer> action) { int i, hi, mc; if (action == null) throw new NullPointerException(); HashMap m = map; Node[] tab = m.table; if ((hi = fence) = hi && (i = index) >= 0 && (i p = current; current = null; do { if (p == null) p = tab[i++]; else { action.accept(p); p = p.next; } } while (p != null || i > action) { int hi; if (action == null) throw new NullPointerException(); Node[] tab = map.table; if (tab != null && tab.length >= (hi = getFence()) && index >= 0) { while (current != null || index e = current; current = current.next; action.accept(e); if (map.modCount != expectedModCount) throw new ConcurrentModificationException(); return true; } } } return false; } public int characteristics() { return (fence newNode(int hash, K key, V value, Node next) { return new Node<>(hash, key, value, next); } // For conversion from TreeNodes to plain nodes Node replacementNode(Node p, Node next) { return new Node<>(p.hash, p.key, p.value, next); } // Create a tree bin node TreeNode newTreeNode(int hash, K key, V value, Node next) { return new TreeNode<>(hash, key, value, next); } // For treeifyBin TreeNode replacementTreeNode(Node p, Node next) { return new TreeNode<>(p.hash, p.key, p.value, next); } /** * Reset to initial default state. Called by clone and readObject. */ void reinitialize() { table = null; entrySet = null; keySet = null; values = null; modCount = 0; threshold = 0; size = 0; } // Callbacks to allow LinkedHashMap post-actions void afterNodeAccess(Node p) { } void afterNodeInsertion(boolean evict) { } void afterNodeRemoval(Node p) { } // Called only from writeObject, to ensure compatible ordering. void internalWriteEntries(java.io.ObjectOutputStream s) throws IOException { Node[] tab; if (size > 0 && (tab = table) != null) { for (int i = 0; i e = tab[i]; e != null; e = e.next) { s.writeObject(e.key); s.writeObject(e.value); } } } } /* ------------------------------------------------------------ */ // Tree bins /** * Entry for Tree bins. Extends LinkedHashMap.Entry (which in turn * extends Node) so can be used as extension of either regular or * linked node. */ static final class TreeNode extends LinkedHashMap.Entry { TreeNode parent; // red-black tree links TreeNode left; TreeNode right; TreeNode prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node next) { super(hash, key, val, next); } /** * Returns root of tree containing this node. */ final TreeNode root() { for (TreeNode r = this, p;;) { if ((p = r.parent) == null) return r; r = p; } } /** * Ensures that the given root is the first node of its bin. */ static void moveRootToFront(Node[] tab, TreeNode root) { int n; if (root != null && tab != null && (n = tab.length) > 0) { int index = (n - 1) & root.hash; TreeNode first = (TreeNode)tab[index]; if (root != first) { Node rn; tab[index] = root; TreeNode rp = root.prev; if ((rn = root.next) != null) ((TreeNode)rn).prev = rp; if (rp != null) rp.next = rn; if (first != null) first.prev = root; root.next = first; root.prev = null; } assert checkInvariants(root); } } /** * Finds the node starting at root p with the given hash and key. * The kc argument caches comparableClassFor(key) upon first use * comparing keys. */ final TreeNode find(int h, Object k, Class kc) { TreeNode p = this; do { int ph, dir; K pk; TreeNode pl = p.left, pr = p.right, q; if ((ph = p.hash) > h) p = pl; else if (ph getTreeNode(int h, Object k) { return ((parent != null) ? root() : this).find(h, k, null); } /** * Tie-breaking utility for ordering insertions when equal * hashCodes and non-comparable. We don't require a total * order, just a consistent insertion rule to maintain * equivalence across rebalancings. Tie-breaking further than * necessary simplifies testing a bit. */ static int tieBreakOrder(Object a, Object b) { int d; if (a == null || b == null || (d = a.getClass().getName(). compareTo(b.getClass().getName())) == 0) d = (System.identityHashCode(a) [] tab) { TreeNode root = null; for (TreeNode x = this, next; x != null; x = next) { next = (TreeNode)x.next; x.left = x.right = null; if (root == null) { x.parent = null; x.red = false; root = x; } else { K k = x.key; int h = x.hash; Class kc = null; for (TreeNode p = root;;) { int dir, ph; K pk = p.key; if ((ph = p.hash) > h) dir = -1; else if (ph xp = p; if ((p = (dir untreeify(HashMap map) { Node hd = null, tl = null; for (Node q = this; q != null; q = q.next) { Node p = map.replacementNode(q, null); if (tl == null) hd = p; else tl.next = p; tl = p; } return hd; } /** * Tree version of putVal. */ final TreeNode putTreeVal(HashMap map, Node[] tab, int h, K k, V v) { Class kc = null; boolean searched = false; TreeNode root = (parent != null) ? root() : this; for (TreeNode p = root;;) { int dir, ph; K pk; if ((ph = p.hash) > h) dir = -1; else if (ph q, ch; searched = true; if (((ch = p.left) != null && (q = ch.find(h, k, kc)) != null) || ((ch = p.right) != null && (q = ch.find(h, k, kc)) != null)) return q; } dir = tieBreakOrder(k, pk); } TreeNode xp = p; if ((p = (dir xpn = xp.next; TreeNode x = map.newTreeNode(h, k, v, xpn); if (dir )xpn).prev = x; moveRootToFront(tab, balanceInsertion(root, x)); return null; } } } /** * Removes the given node, that must be present before this call. * This is messier than typical red-black deletion code because we * cannot swap the contents of an interior node with a leaf * successor that is pinned by \"next\" pointers that are accessible * independently during traversal. So instead we swap the tree * linkages. If the current tree appears to have too few nodes, * the bin is converted back to a plain bin. (The test triggers * somewhere between 2 and 6 nodes, depending on tree structure). */ final void removeTreeNode(HashMap map, Node[] tab, boolean movable) { int n; if (tab == null || (n = tab.length) == 0) return; int index = (n - 1) & hash; TreeNode first = (TreeNode)tab[index], root = first, rl; TreeNode succ = (TreeNode)next, pred = prev; if (pred == null) tab[index] = first = succ; else pred.next = succ; if (succ != null) succ.prev = pred; if (first == null) return; if (root.parent != null) root = root.root(); if (root == null || root.right == null || (rl = root.left) == null || rl.left == null) { tab[index] = first.untreeify(map); // too small return; } TreeNode p = this, pl = left, pr = right, replacement; if (pl != null && pr != null) { TreeNode s = pr, sl; while ((sl = s.left) != null) // find successor s = sl; boolean c = s.red; s.red = p.red; p.red = c; // swap colors TreeNode sr = s.right; TreeNode pp = p.parent; if (s == pr) { // p was s's direct parent p.parent = s; s.right = p; } else { TreeNode sp = s.parent; if ((p.parent = sp) != null) { if (s == sp.left) sp.left = p; else sp.right = p; } if ((s.right = pr) != null) pr.parent = s; } p.left = null; if ((p.right = sr) != null) sr.parent = p; if ((s.left = pl) != null) pl.parent = s; if ((s.parent = pp) == null) root = s; else if (p == pp.left) pp.left = s; else pp.right = s; if (sr != null) replacement = sr; else replacement = p; } else if (pl != null) replacement = pl; else if (pr != null) replacement = pr; else replacement = p; if (replacement != p) { TreeNode pp = replacement.parent = p.parent; if (pp == null) root = replacement; else if (p == pp.left) pp.left = replacement; else pp.right = replacement; p.left = p.right = p.parent = null; } TreeNode r = p.red ? root : balanceDeletion(root, replacement); if (replacement == p) { // detach TreeNode pp = p.parent; p.parent = null; if (pp != null) { if (p == pp.left) pp.left = null; else if (p == pp.right) pp.right = null; } } if (movable) moveRootToFront(tab, r); } /** * Splits nodes in a tree bin into lower and upper tree bins, * or untreeifies if now too small. Called only from resize; * see above discussion about split bits and indices. * * @param map the map * @param tab the table for recording bin heads * @param index the index of the table being split * @param bit the bit of hash to split on */ final void split(HashMap map, Node[] tab, int index, int bit) { TreeNode b = this; // Relink into lo and hi lists, preserving order TreeNode loHead = null, loTail = null; TreeNode hiHead = null, hiTail = null; int lc = 0, hc = 0; for (TreeNode e = b, next; e != null; e = next) { next = (TreeNode)e.next; e.next = null; if ((e.hash & bit) == 0) { if ((e.prev = loTail) == null) loHead = e; else loTail.next = e; loTail = e; ++lc; } else { if ((e.prev = hiTail) == null) hiHead = e; else hiTail.next = e; hiTail = e; ++hc; } } if (loHead != null) { if (lc TreeNode rotateLeft(TreeNode root, TreeNode p) { TreeNode r, pp, rl; if (p != null && (r = p.right) != null) { if ((rl = p.right = r.left) != null) rl.parent = p; if ((pp = r.parent = p.parent) == null) (root = r).red = false; else if (pp.left == p) pp.left = r; else pp.right = r; r.left = p; p.parent = r; } return root; } static TreeNode rotateRight(TreeNode root, TreeNode p) { TreeNode l, pp, lr; if (p != null && (l = p.left) != null) { if ((lr = p.left = l.right) != null) lr.parent = p; if ((pp = l.parent = p.parent) == null) (root = l).red = false; else if (pp.right == p) pp.right = l; else pp.left = l; l.right = p; p.parent = l; } return root; } static TreeNode balanceInsertion(TreeNode root, TreeNode x) { x.red = true; for (TreeNode xp, xpp, xppl, xppr;;) { if ((xp = x.parent) == null) { x.red = false; return x; } else if (!xp.red || (xpp = xp.parent) == null) return root; if (xp == (xppl = xpp.left)) { if ((xppr = xpp.right) != null && xppr.red) { xppr.red = false; xp.red = false; xpp.red = true; x = xpp; } else { if (x == xp.right) { root = rotateLeft(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; } if (xp != null) { xp.red = false; if (xpp != null) { xpp.red = true; root = rotateRight(root, xpp); } } } } else { if (xppl != null && xppl.red) { xppl.red = false; xp.red = false; xpp.red = true; x = xpp; } else { if (x == xp.left) { root = rotateRight(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; } if (xp != null) { xp.red = false; if (xpp != null) { xpp.red = true; root = rotateLeft(root, xpp); } } } } } } static TreeNode balanceDeletion(TreeNode root, TreeNode x) { for (TreeNode xp, xpl, xpr;;) { if (x == null || x == root) return root; else if ((xp = x.parent) == null) { x.red = false; return x; } else if (x.red) { x.red = false; return root; } else if ((xpl = xp.left) == x) { if ((xpr = xp.right) != null && xpr.red) { xpr.red = false; xp.red = true; root = rotateLeft(root, xp); xpr = (xp = x.parent) == null ? null : xp.right; } if (xpr == null) x = xp; else { TreeNode sl = xpr.left, sr = xpr.right; if ((sr == null || !sr.red) && (sl == null || !sl.red)) { xpr.red = true; x = xp; } else { if (sr == null || !sr.red) { if (sl != null) sl.red = false; xpr.red = true; root = rotateRight(root, xpr); xpr = (xp = x.parent) == null ? null : xp.right; } if (xpr != null) { xpr.red = (xp == null) ? false : xp.red; if ((sr = xpr.right) != null) sr.red = false; } if (xp != null) { xp.red = false; root = rotateLeft(root, xp); } x = root; } } } else { // symmetric if (xpl != null && xpl.red) { xpl.red = false; xp.red = true; root = rotateRight(root, xp); xpl = (xp = x.parent) == null ? null : xp.left; } if (xpl == null) x = xp; else { TreeNode sl = xpl.left, sr = xpl.right; if ((sl == null || !sl.red) && (sr == null || !sr.red)) { xpl.red = true; x = xp; } else { if (sl == null || !sl.red) { if (sr != null) sr.red = false; xpl.red = true; root = rotateLeft(root, xpl); xpl = (xp = x.parent) == null ? null : xp.left; } if (xpl != null) { xpl.red = (xp == null) ? false : xp.red; if ((sl = xpl.left) != null) sl.red = false; } if (xp != null) { xp.red = false; root = rotateRight(root, xp); } x = root; } } } } } /** * Recursive invariant check */ static boolean checkInvariants(TreeNode t) { TreeNode tp = t.parent, tl = t.left, tr = t.right, tb = t.prev, tn = (TreeNode)t.next; if (tb != null && tb.next != t) return false; if (tn != null && tn.prev != t) return false; if (tp != null && t != tp.left && t != tp.right) return false; if (tl != null && (tl.parent != t || tl.hash > t.hash)) return false; if (tr != null && (tr.parent != t || tr.hash "},"base/collection/hashset/hashset源码学习.html":{"url":"base/collection/hashset/hashset源码学习.html","title":"hashset源码学习","keywords":"","body":" HashSet 一道经典面试题 HashSet 一道经典面试题 注： HashSet是Set接口的实现类，所以也是存储无序的、不可重复的数据 。 说白了，HashSet就是限制了功能的HashMap，所以了解HashMap的实现原理，这个HashSet自然就通 对于HashSet中保存的对象，主要要正确重写equals方法和hashCode方法，以保证放入Set对象的唯一性 虽说是Set是对于重复的元素不放入，倒不如直接说是底层的Map直接把原值替代了（这个Set的put方法的返回值真有意思） HashSet没有提供get()方法，愿意是同HashMap一样，Set内部是无序的，只能通过迭代的方式获得 "},"base/collection/集合类学习.html":{"url":"base/collection/集合类学习.html","title":"集合类学习","keywords":"","body":" |----Collection接口：单列集合，用来存储一个一个的对象 |----List接口：存储有序的、可重复的数据。 |----ArrayList：作为List接口的主要实现类；线程不安全的，效率高；底层使用Object[] elementData存储 |----LinkedList：对于频繁的插入、删除操作，使用此类效率比ArrayList高；底层使用双向链表存储 |----Vector：作为List接口的古老实现类；线程安全的，效率低；底层使用Object[] elementData存储 |----Set接口：存储无序的、不可重复的数据 |----HashSet：作为Set接口的主要实现类；线程不安全的；可以存储null值 |----LinkedHashSet：作为HashSet的子类；遍历其内部数据时，可以按照添加的顺序遍历在添加数据的同时， 每个数据还维护了两个引用，记录此数据前一个数据和后一个数据。对于频繁的遍历操作，LinkedHashSet效率高于HashSet. |----TreeSet：可以照添加对象的指定属性，进行排序。 "},"base/string/StringBuffer和StringBuilder.html":{"url":"base/string/StringBuffer和StringBuilder.html","title":"集合StringBuffer和StringBuilder","keywords":"","body":" JAVA面试题 StringBuffer和StringBuilder的区别，从源码角度分析 StringBuffer和StringBuilder共同点 共同继承AbstractStringBuilder 都是可变类，通过append来拼接字符串，AbstractStringBuilder定义的value字符数组是可变， 而String不可变，String中的value是final类型的,另外String还定义了个hash变量，代表哈希值 toString方法都是用new String来实现的 初始化容量相同，都是16 AbstractStringBuilder定义了2个变量,注意字符数组value前面不是final类型的，说明是可变的，而String的value是final类型的，不可变的 char[] value; int count; public int length() { return count; } /** * This no-arg constructor is necessary for serialization of subclasses. */ AbstractStringBuilder() { } /** * Creates an AbstractStringBuilder of the specified capacity. */ AbstractStringBuilder(int capacity) { value = new char[capacity]; } public StringBuffer() { super(16); } public StringBuilder() { super(16); } String /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 StringBuffer 有toString缓存，只要对象改变，该缓存都被清空，而调用toString时则赋值 是同步的，线程安全的,线程安全通过在方法前加上synchronized,适用于多线程 因为有同步，所以效率低 /** * A cache of the last value returned by toString. Cleared * whenever the StringBuffer is modified. */ private transient char[] toStringCache; @Override public synchronized String toString() { if (toStringCache == null) { toStringCache = Arrays.copyOfRange(value, 0, count); } return new String(toStringCache, true); } @Override public synchronized StringBuffer append(String str) { toStringCache = null; super.append(str); return this; } @Override public synchronized int length() { return count; } StringBuilder 没有缓存 不同步，线程不安全，适用于单线程 效率高 @Override public String toString() { // Create a copy, don't share the array return new String(value, 0, count); } 参考：https://www.cnblogs.com/yfafa/p/7455879.html 扩容 扩容为原来2倍+2 /** * This implements the expansion semantics of ensureCapacity with no * size check or synchronization. */ void expandCapacity(int minimumCapacity) { int newCapacity = value.length * 2 + 2; if (newCapacity - minimumCapacity 什么是同步，什么是异步 同步：A发过去一条请求，需要收到接收方B的回复，A才可以进行下一步操作，否则一直等待 异步：A发过去一条请求，不论有没有收到B的回复，A都会继续做下面的事情，而不做等待。 STringBuffer为什么是同步的 假设你在火车上，只有一个厕所。ABC都在排队。 简单来说，线程同步就是 A进去了，门锁上。BC只能在外面干等着。这是线程安全的，不会产生问题。 线程不同步就是 A进去的同时，没关门，B也进去了。这是线程不安全。 关于使用StringBuffer内存溢出问题 StringBuffer在数据内容增大时，会为StringBuffer对象追加申请内存，申请数量为当前内存量的一倍，即StringBuffer总数为原内存量的2倍。 问题就在这里：现有数据为1024KB，内存为1536KB；在数据修改数据量为1025KB后，那么内存总量将会变为3072KB，只因为1KB的内存数据增长，而申请1536KB内存，有可能导致内存溢出。 StringBuffer的内存溢出实例 https://blog.csdn.net/iteye_5555/article/details/81959383 用jmap查看了一下内存的情况 把heap的dump信息拿出来,通过 jmap -dump:format=b,file=heap.bin 把当时的这个进程的dump拿了下来 https://www.iteye.com/blog/ahuaxuan-662629 StringBuffer内容清空效率比较 "},"base/string/string源码.html":{"url":"base/string/string源码.html","title":"string源码","keywords":"","body":" /* * Copyright (c) 1994, 2013, Oracle and/or its affiliates. All rights reserved. * ORACLE PROPRIETARY/CONFIDENTIAL. Use is subject to license terms. * * * * * * * * * * * * * * * * * * * * */ package java.lang; import java.io.ObjectStreamField; import java.io.UnsupportedEncodingException; import java.nio.charset.Charset; import java.util.ArrayList; import java.util.Arrays; import java.util.Comparator; import java.util.Formatter; import java.util.Locale; import java.util.Objects; import java.util.StringJoiner; import java.util.regex.Matcher; import java.util.regex.Pattern; import java.util.regex.PatternSyntaxException; /** * The {@code String} class represents character strings. All * string literals in Java programs, such as {@code \"abc\"}, are * implemented as instances of this class. * * Strings are constant; their values cannot be changed after they * are created. String buffers support mutable strings. * Because String objects are immutable they can be shared. For example: * * String str = \"abc\"; * * is equivalent to: * * char data[] = {'a', 'b', 'c'}; * String str = new String(data); * * Here are some more examples of how strings can be used: * * System.out.println(\"abc\"); * String cde = \"cde\"; * System.out.println(\"abc\" + cde); * String c = \"abc\".substring(2,3); * String d = cde.substring(1, 2); * * * The class {@code String} includes methods for examining * individual characters of the sequence, for comparing strings, for * searching strings, for extracting substrings, and for creating a * copy of a string with all characters translated to uppercase or to * lowercase. Case mapping is based on the Unicode Standard version * specified by the {@link java.lang.Character Character} class. * * The Java language provides special support for the string * concatenation operator (&nbsp;+&nbsp;), and for conversion of * other objects to strings. String concatenation is implemented * through the {@code StringBuilder}(or {@code StringBuffer}) * class and its {@code append} method. * String conversions are implemented through the method * {@code toString}, defined by {@code Object} and * inherited by all classes in Java. For additional information on * string concatenation and conversion, see Gosling, Joy, and Steele, * The Java Language Specification. * * Unless otherwise noted, passing a null argument to a constructor * or method in this class will cause a {@link NullPointerException} to be * thrown. * * A {@code String} represents a string in the UTF-16 format * in which supplementary characters are represented by surrogate * pairs (see the section Unicode * Character Representations in the {@code Character} class for * more information). * Index values refer to {@code char} code units, so a supplementary * character uses two positions in a {@code String}. * The {@code String} class provides methods for dealing with * Unicode code points (i.e., characters), in addition to those for * dealing with Unicode code units (i.e., {@code char} values). * * @author Lee Boynton * @author Arthur van Hoff * @author Martin Buchholz * @author Ulf Zibis * @see java.lang.Object#toString() * @see java.lang.StringBuffer * @see java.lang.StringBuilder * @see java.nio.charset.Charset * @since JDK1.0 */ public final class String implements java.io.Serializable, Comparable, CharSequence { /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 /** use serialVersionUID from JDK 1.0.2 for interoperability */ private static final long serialVersionUID = -6849794470754667710L; /** * Class String is special cased within the Serialization Stream Protocol. * * A String instance is written into an ObjectOutputStream according to * * Object Serialization Specification, Section 6.2, \"Stream Elements\" */ private static final ObjectStreamField[] serialPersistentFields = new ObjectStreamField[0]; /** * Initializes a newly created {@code String} object so that it represents * an empty character sequence. Note that use of this constructor is * unnecessary since Strings are immutable. */ public String() { this.value = \"\".value; } /** * Initializes a newly created {@code String} object so that it represents * the same sequence of characters as the argument; in other words, the * newly created string is a copy of the argument string. Unless an * explicit copy of {@code original} is needed, use of this constructor is * unnecessary since Strings are immutable. * * @param original * A {@code String} */ public String(String original) { this.value = original.value; this.hash = original.hash; } /** * Allocates a new {@code String} so that it represents the sequence of * characters currently contained in the character array argument. The * contents of the character array are copied; subsequent modification of * the character array does not affect the newly created string. * * @param value * The initial value of the string */ public String(char value[]) { this.value = Arrays.copyOf(value, value.length); } /** * Allocates a new {@code String} that contains characters from a subarray * of the character array argument. The {@code offset} argument is the * index of the first character of the subarray and the {@code count} * argument specifies the length of the subarray. The contents of the * subarray are copied; subsequent modification of the character array does * not affect the newly created string. * * @param value * Array that is the source of characters * * @param offset * The initial offset * * @param count * The length * * @throws IndexOutOfBoundsException * If the {@code offset} and {@code count} arguments index * characters outside the bounds of the {@code value} array */ public String(char value[], int offset, int count) { if (offset >>1. if (offset > value.length - count) { throw new StringIndexOutOfBoundsException(offset + count); } this.value = Arrays.copyOfRange(value, offset, offset+count); } /** * Allocates a new {@code String} that contains characters from a subarray * of the Unicode code point array * argument. The {@code offset} argument is the index of the first code * point of the subarray and the {@code count} argument specifies the * length of the subarray. The contents of the subarray are converted to * {@code char}s; subsequent modification of the {@code int} array does not * affect the newly created string. * * @param codePoints * Array that is the source of Unicode code points * * @param offset * The initial offset * * @param count * The length * * @throws IllegalArgumentException * If any invalid Unicode code point is found in {@code * codePoints} * * @throws IndexOutOfBoundsException * If the {@code offset} and {@code count} arguments index * characters outside the bounds of the {@code codePoints} array * * @since 1.5 */ public String(int[] codePoints, int offset, int count) { if (offset >>1. if (offset > codePoints.length - count) { throw new StringIndexOutOfBoundsException(offset + count); } final int end = offset + count; // Pass 1: Compute precise size of char[] int n = count; for (int i = offset; i The {@code offset} argument is the index of the first byte of the * subarray, and the {@code count} argument specifies the length of the * subarray. * * Each {@code byte} in the subarray is converted to a {@code char} as * specified in the method above. * * @deprecated This method does not properly convert bytes into characters. * As of JDK&nbsp;1.1, the preferred way to do this is via the * {@code String} constructors that take a {@link * java.nio.charset.Charset}, charset name, or that use the platform's * default charset. * * @param ascii * The bytes to be converted to characters * * @param hibyte * The top 8 bits of each 16-bit Unicode code unit * * @param offset * The initial offset * @param count * The length * * @throws IndexOutOfBoundsException * If the {@code offset} or {@code count} argument is invalid * * @see #String(byte[], int) * @see #String(byte[], int, int, java.lang.String) * @see #String(byte[], int, int, java.nio.charset.Charset) * @see #String(byte[], int, int) * @see #String(byte[], java.lang.String) * @see #String(byte[], java.nio.charset.Charset) * @see #String(byte[]) */ @Deprecated public String(byte ascii[], int hibyte, int offset, int count) { checkBounds(ascii, offset, count); char value[] = new char[count]; if (hibyte == 0) { for (int i = count; i-- > 0;) { value[i] = (char)(ascii[i + offset] & 0xff); } } else { hibyte 0;) { value[i] = (char)(hibyte | (ascii[i + offset] & 0xff)); } } this.value = value; } /** * Allocates a new {@code String} containing characters constructed from * an array of 8-bit integer values. Each character cin the * resulting string is constructed from the corresponding component * b in the byte array such that: * * * c == (char)(((hibyte &amp; 0xff) &lt;&lt; 8) * | (b &amp; 0xff)) * * * @deprecated This method does not properly convert bytes into * characters. As of JDK&nbsp;1.1, the preferred way to do this is via the * {@code String} constructors that take a {@link * java.nio.charset.Charset}, charset name, or that use the platform's * default charset. * * @param ascii * The bytes to be converted to characters * * @param hibyte * The top 8 bits of each 16-bit Unicode code unit * * @see #String(byte[], int, int, java.lang.String) * @see #String(byte[], int, int, java.nio.charset.Charset) * @see #String(byte[], int, int) * @see #String(byte[], java.lang.String) * @see #String(byte[], java.nio.charset.Charset) * @see #String(byte[]) */ @Deprecated public String(byte ascii[], int hibyte) { this(ascii, hibyte, 0, ascii.length); } /* Common private utility method used to bounds check the byte array * and requested offset & length values used by the String(byte[],..) * constructors. */ private static void checkBounds(byte[] bytes, int offset, int length) { if (length bytes.length - length) throw new StringIndexOutOfBoundsException(offset + length); } /** * Constructs a new {@code String} by decoding the specified subarray of * bytes using the specified charset. The length of the new {@code String} * is a function of the charset, and hence may not be equal to the length * of the subarray. * * The behavior of this constructor when the given bytes are not valid * in the given charset is unspecified. The {@link * java.nio.charset.CharsetDecoder} class should be used when more control * over the decoding process is required. * * @param bytes * The bytes to be decoded into characters * * @param offset * The index of the first byte to decode * * @param length * The number of bytes to decode * @param charsetName * The name of a supported {@linkplain java.nio.charset.Charset * charset} * * @throws UnsupportedEncodingException * If the named charset is not supported * * @throws IndexOutOfBoundsException * If the {@code offset} and {@code length} arguments index * characters outside the bounds of the {@code bytes} array * * @since JDK1.1 */ public String(byte bytes[], int offset, int length, String charsetName) throws UnsupportedEncodingException { if (charsetName == null) throw new NullPointerException(\"charsetName\"); checkBounds(bytes, offset, length); this.value = StringCoding.decode(charsetName, bytes, offset, length); } /** * Constructs a new {@code String} by decoding the specified subarray of * bytes using the specified {@linkplain java.nio.charset.Charset charset}. * The length of the new {@code String} is a function of the charset, and * hence may not be equal to the length of the subarray. * * This method always replaces malformed-input and unmappable-character * sequences with this charset's default replacement string. The {@link * java.nio.charset.CharsetDecoder} class should be used when more control * over the decoding process is required. * * @param bytes * The bytes to be decoded into characters * * @param offset * The index of the first byte to decode * * @param length * The number of bytes to decode * * @param charset * The {@linkplain java.nio.charset.Charset charset} to be used to * decode the {@code bytes} * * @throws IndexOutOfBoundsException * If the {@code offset} and {@code length} arguments index * characters outside the bounds of the {@code bytes} array * * @since 1.6 */ public String(byte bytes[], int offset, int length, Charset charset) { if (charset == null) throw new NullPointerException(\"charset\"); checkBounds(bytes, offset, length); this.value = StringCoding.decode(charset, bytes, offset, length); } /** * Constructs a new {@code String} by decoding the specified array of bytes * using the specified {@linkplain java.nio.charset.Charset charset}. The * length of the new {@code String} is a function of the charset, and hence * may not be equal to the length of the byte array. * * The behavior of this constructor when the given bytes are not valid * in the given charset is unspecified. The {@link * java.nio.charset.CharsetDecoder} class should be used when more control * over the decoding process is required. * * @param bytes * The bytes to be decoded into characters * * @param charsetName * The name of a supported {@linkplain java.nio.charset.Charset * charset} * * @throws UnsupportedEncodingException * If the named charset is not supported * * @since JDK1.1 */ public String(byte bytes[], String charsetName) throws UnsupportedEncodingException { this(bytes, 0, bytes.length, charsetName); } /** * Constructs a new {@code String} by decoding the specified array of * bytes using the specified {@linkplain java.nio.charset.Charset charset}. * The length of the new {@code String} is a function of the charset, and * hence may not be equal to the length of the byte array. * * This method always replaces malformed-input and unmappable-character * sequences with this charset's default replacement string. The {@link * java.nio.charset.CharsetDecoder} class should be used when more control * over the decoding process is required. * * @param bytes * The bytes to be decoded into characters * * @param charset * The {@linkplain java.nio.charset.Charset charset} to be used to * decode the {@code bytes} * * @since 1.6 */ public String(byte bytes[], Charset charset) { this(bytes, 0, bytes.length, charset); } /** * Constructs a new {@code String} by decoding the specified subarray of * bytes using the platform's default charset. The length of the new * {@code String} is a function of the charset, and hence may not be equal * to the length of the subarray. * * The behavior of this constructor when the given bytes are not valid * in the default charset is unspecified. The {@link * java.nio.charset.CharsetDecoder} class should be used when more control * over the decoding process is required. * * @param bytes * The bytes to be decoded into characters * * @param offset * The index of the first byte to decode * * @param length * The number of bytes to decode * * @throws IndexOutOfBoundsException * If the {@code offset} and the {@code length} arguments index * characters outside the bounds of the {@code bytes} array * * @since JDK1.1 */ public String(byte bytes[], int offset, int length) { checkBounds(bytes, offset, length); this.value = StringCoding.decode(bytes, offset, length); } /** * Constructs a new {@code String} by decoding the specified array of bytes * using the platform's default charset. The length of the new {@code * String} is a function of the charset, and hence may not be equal to the * length of the byte array. * * The behavior of this constructor when the given bytes are not valid * in the default charset is unspecified. The {@link * java.nio.charset.CharsetDecoder} class should be used when more control * over the decoding process is required. * * @param bytes * The bytes to be decoded into characters * * @since JDK1.1 */ public String(byte bytes[]) { this(bytes, 0, bytes.length); } /** * Allocates a new string that contains the sequence of characters * currently contained in the string buffer argument. The contents of the * string buffer are copied; subsequent modification of the string buffer * does not affect the newly created string. * * @param buffer * A {@code StringBuffer} */ public String(StringBuffer buffer) { synchronized(buffer) { this.value = Arrays.copyOf(buffer.getValue(), buffer.length()); } } /** * Allocates a new string that contains the sequence of characters * currently contained in the string builder argument. The contents of the * string builder are copied; subsequent modification of the string builder * does not affect the newly created string. * * This constructor is provided to ease migration to {@code * StringBuilder}. Obtaining a string from a string builder via the {@code * toString} method is likely to run faster and is generally preferred. * * @param builder * A {@code StringBuilder} * * @since 1.5 */ public String(StringBuilder builder) { this.value = Arrays.copyOf(builder.getValue(), builder.length()); } /* * Package private constructor which shares value array for speed. * this constructor is always expected to be called with share==true. * a separate constructor is needed because we already have a public * String(char[]) constructor that makes a copy of the given char[]. */ String(char[] value, boolean share) { // assert share : \"unshared not supported\"; this.value = value; } /** * Returns the length of this string. * The length is equal to the number of Unicode * code units in the string. * * @return the length of the sequence of characters represented by this * object. */ public int length() { return value.length; } /** * Returns {@code true} if, and only if, {@link #length()} is {@code 0}. * * @return {@code true} if {@link #length()} is {@code 0}, otherwise * {@code false} * * @since 1.6 */ public boolean isEmpty() { return value.length == 0; } /** * Returns the {@code char} value at the * specified index. An index ranges from {@code 0} to * {@code length() - 1}. The first {@code char} value of the sequence * is at index {@code 0}, the next at index {@code 1}, * and so on, as for array indexing. * * If the {@code char} value specified by the index is a * surrogate, the surrogate * value is returned. * * @param index the index of the {@code char} value. * @return the {@code char} value at the specified index of this string. * The first {@code char} value is at index {@code 0}. * @exception IndexOutOfBoundsException if the {@code index} * argument is negative or not less than the length of this * string. */ public char charAt(int index) { if ((index = value.length)) { throw new StringIndexOutOfBoundsException(index); } return value[index]; } /** * Returns the character (Unicode code point) at the specified * index. The index refers to {@code char} values * (Unicode code units) and ranges from {@code 0} to * {@link #length()}{@code - 1}. * * If the {@code char} value specified at the given index * is in the high-surrogate range, the following index is less * than the length of this {@code String}, and the * {@code char} value at the following index is in the * low-surrogate range, then the supplementary code point * corresponding to this surrogate pair is returned. Otherwise, * the {@code char} value at the given index is returned. * * @param index the index to the {@code char} values * @return the code point value of the character at the * {@code index} * @exception IndexOutOfBoundsException if the {@code index} * argument is negative or not less than the length of this * string. * @since 1.5 */ public int codePointAt(int index) { if ((index = value.length)) { throw new StringIndexOutOfBoundsException(index); } return Character.codePointAtImpl(value, index, value.length); } /** * Returns the character (Unicode code point) before the specified * index. The index refers to {@code char} values * (Unicode code units) and ranges from {@code 1} to {@link * CharSequence#length() length}. * * If the {@code char} value at {@code (index - 1)} * is in the low-surrogate range, {@code (index - 2)} is not * negative, and the {@code char} value at {@code (index - * 2)} is in the high-surrogate range, then the * supplementary code point value of the surrogate pair is * returned. If the {@code char} value at {@code index - * 1} is an unpaired low-surrogate or a high-surrogate, the * surrogate value is returned. * * @param index the index following the code point that should be returned * @return the Unicode code point value before the given index. * @exception IndexOutOfBoundsException if the {@code index} * argument is less than 1 or greater than the length * of this string. * @since 1.5 */ public int codePointBefore(int index) { int i = index - 1; if ((i = value.length)) { throw new StringIndexOutOfBoundsException(index); } return Character.codePointBeforeImpl(value, index, 0); } /** * Returns the number of Unicode code points in the specified text * range of this {@code String}. The text range begins at the * specified {@code beginIndex} and extends to the * {@code char} at index {@code endIndex - 1}. Thus the * length (in {@code char}s) of the text range is * {@code endIndex-beginIndex}. Unpaired surrogates within * the text range count as one code point each. * * @param beginIndex the index to the first {@code char} of * the text range. * @param endIndex the index after the last {@code char} of * the text range. * @return the number of Unicode code points in the specified text * range * @exception IndexOutOfBoundsException if the * {@code beginIndex} is negative, or {@code endIndex} * is larger than the length of this {@code String}, or * {@code beginIndex} is larger than {@code endIndex}. * @since 1.5 */ public int codePointCount(int beginIndex, int endIndex) { if (beginIndex value.length || beginIndex > endIndex) { throw new IndexOutOfBoundsException(); } return Character.codePointCountImpl(value, beginIndex, endIndex - beginIndex); } /** * Returns the index within this {@code String} that is * offset from the given {@code index} by * {@code codePointOffset} code points. Unpaired surrogates * within the text range given by {@code index} and * {@code codePointOffset} count as one code point each. * * @param index the index to be offset * @param codePointOffset the offset in code points * @return the index within this {@code String} * @exception IndexOutOfBoundsException if {@code index} * is negative or larger then the length of this * {@code String}, or if {@code codePointOffset} is positive * and the substring starting with {@code index} has fewer * than {@code codePointOffset} code points, * or if {@code codePointOffset} is negative and the substring * before {@code index} has fewer than the absolute value * of {@code codePointOffset} code points. * @since 1.5 */ public int offsetByCodePoints(int index, int codePointOffset) { if (index value.length) { throw new IndexOutOfBoundsException(); } return Character.offsetByCodePointsImpl(value, 0, value.length, index, codePointOffset); } /** * Copy characters from this string into dst starting at dstBegin. * This method doesn't perform any range checking. */ void getChars(char dst[], int dstBegin) { System.arraycopy(value, 0, dst, dstBegin, value.length); } /** * Copies characters from this string into the destination character * array. * * The first character to be copied is at index {@code srcBegin}; * the last character to be copied is at index {@code srcEnd-1} * (thus the total number of characters to be copied is * {@code srcEnd-srcBegin}). The characters are copied into the * subarray of {@code dst} starting at index {@code dstBegin} * and ending at index: * * dstBegin + (srcEnd-srcBegin) - 1 * * * @param srcBegin index of the first character in the string * to copy. * @param srcEnd index after the last character in the string * to copy. * @param dst the destination array. * @param dstBegin the start offset in the destination array. * @exception IndexOutOfBoundsException If any of the following * is true: * {@code srcBegin} is negative. * {@code srcBegin} is greater than {@code srcEnd} * {@code srcEnd} is greater than the length of this * string * {@code dstBegin} is negative * {@code dstBegin+(srcEnd-srcBegin)} is larger than * {@code dst.length} */ public void getChars(int srcBegin, int srcEnd, char dst[], int dstBegin) { if (srcBegin value.length) { throw new StringIndexOutOfBoundsException(srcEnd); } if (srcBegin > srcEnd) { throw new StringIndexOutOfBoundsException(srcEnd - srcBegin); } System.arraycopy(value, srcBegin, dst, dstBegin, srcEnd - srcBegin); } /** * Copies characters from this string into the destination byte array. Each * byte receives the 8 low-order bits of the corresponding character. The * eight high-order bits of each character are not copied and do not * participate in the transfer in any way. * * The first character to be copied is at index {@code srcBegin}; the * last character to be copied is at index {@code srcEnd-1}. The total * number of characters to be copied is {@code srcEnd-srcBegin}. The * characters, converted to bytes, are copied into the subarray of {@code * dst} starting at index {@code dstBegin} and ending at index: * * * dstBegin + (srcEnd-srcBegin) - 1 * * * @deprecated This method does not properly convert characters into * bytes. As of JDK&nbsp;1.1, the preferred way to do this is via the * {@link #getBytes()} method, which uses the platform's default charset. * * @param srcBegin * Index of the first character in the string to copy * * @param srcEnd * Index after the last character in the string to copy * * @param dst * The destination array * * @param dstBegin * The start offset in the destination array * * @throws IndexOutOfBoundsException * If any of the following is true: * * {@code srcBegin} is negative * {@code srcBegin} is greater than {@code srcEnd} * {@code srcEnd} is greater than the length of this String * {@code dstBegin} is negative * {@code dstBegin+(srcEnd-srcBegin)} is larger than {@code * dst.length} * */ @Deprecated public void getBytes(int srcBegin, int srcEnd, byte dst[], int dstBegin) { if (srcBegin value.length) { throw new StringIndexOutOfBoundsException(srcEnd); } if (srcBegin > srcEnd) { throw new StringIndexOutOfBoundsException(srcEnd - srcBegin); } Objects.requireNonNull(dst); int j = dstBegin; int n = srcEnd; int i = srcBegin; char[] val = value; /* avoid getfield opcode */ while (i The behavior of this method when this string cannot be encoded in * the given charset is unspecified. The {@link * java.nio.charset.CharsetEncoder} class should be used when more control * over the encoding process is required. * * @param charsetName * The name of a supported {@linkplain java.nio.charset.Charset * charset} * * @return The resultant byte array * * @throws UnsupportedEncodingException * If the named charset is not supported * * @since JDK1.1 */ public byte[] getBytes(String charsetName) throws UnsupportedEncodingException { if (charsetName == null) throw new NullPointerException(); return StringCoding.encode(charsetName, value, 0, value.length); } /** * Encodes this {@code String} into a sequence of bytes using the given * {@linkplain java.nio.charset.Charset charset}, storing the result into a * new byte array. * * This method always replaces malformed-input and unmappable-character * sequences with this charset's default replacement byte array. The * {@link java.nio.charset.CharsetEncoder} class should be used when more * control over the encoding process is required. * * @param charset * The {@linkplain java.nio.charset.Charset} to be used to encode * the {@code String} * * @return The resultant byte array * * @since 1.6 */ public byte[] getBytes(Charset charset) { if (charset == null) throw new NullPointerException(); return StringCoding.encode(charset, value, 0, value.length); } /** * Encodes this {@code String} into a sequence of bytes using the * platform's default charset, storing the result into a new byte array. * * The behavior of this method when this string cannot be encoded in * the default charset is unspecified. The {@link * java.nio.charset.CharsetEncoder} class should be used when more control * over the encoding process is required. * * @return The resultant byte array * * @since JDK1.1 */ public byte[] getBytes() { return StringCoding.encode(value, 0, value.length); } /** * Compares this string to the specified object. The result is {@code * true} if and only if the argument is not {@code null} and is a {@code * String} object that represents the same sequence of characters as this * object. * * @param anObject * The object to compare this {@code String} against * * @return {@code true} if the given object represents a {@code String} * equivalent to this string, {@code false} otherwise * * @see #compareTo(String) * @see #equalsIgnoreCase(String) */ public boolean equals(Object anObject) { if (this == anObject) { return true; } if (anObject instanceof String) { String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) { char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- != 0) { if (v1[i] != v2[i]) return false; i++; } return true; } } return false; } /** * Compares this string to the specified {@code StringBuffer}. The result * is {@code true} if and only if this {@code String} represents the same * sequence of characters as the specified {@code StringBuffer}. This method * synchronizes on the {@code StringBuffer}. * * @param sb * The {@code StringBuffer} to compare this {@code String} against * * @return {@code true} if this {@code String} represents the same * sequence of characters as the specified {@code StringBuffer}, * {@code false} otherwise * * @since 1.4 */ public boolean contentEquals(StringBuffer sb) { return contentEquals((CharSequence)sb); } private boolean nonSyncContentEquals(AbstractStringBuilder sb) { char v1[] = value; char v2[] = sb.getValue(); int n = v1.length; if (n != sb.length()) { return false; } for (int i = 0; i Two characters {@code c1} and {@code c2} are considered the same * ignoring case if at least one of the following is true: * * The two characters are the same (as compared by the * {@code ==} operator) * Applying the method {@link * java.lang.Character#toUpperCase(char)} to each character * produces the same result * Applying the method {@link * java.lang.Character#toLowerCase(char)} to each character * produces the same result * * * @param anotherString * The {@code String} to compare this {@code String} against * * @return {@code true} if the argument is not {@code null} and it * represents an equivalent {@code String} ignoring case; {@code * false} otherwise * * @see #equals(Object) */ public boolean equalsIgnoreCase(String anotherString) { return (this == anotherString) ? true : (anotherString != null) && (anotherString.value.length == value.length) && regionMatches(true, 0, anotherString, 0, value.length); } /** * Compares two strings lexicographically. * The comparison is based on the Unicode value of each character in * the strings. The character sequence represented by this * {@code String} object is compared lexicographically to the * character sequence represented by the argument string. The result is * a negative integer if this {@code String} object * lexicographically precedes the argument string. The result is a * positive integer if this {@code String} object lexicographically * follows the argument string. The result is zero if the strings * are equal; {@code compareTo} returns {@code 0} exactly when * the {@link #equals(Object)} method would return {@code true}. * * This is the definition of lexicographic ordering. If two strings are * different, then either they have different characters at some index * that is a valid index for both strings, or their lengths are different, * or both. If they have different characters at one or more index * positions, let k be the smallest such index; then the string * whose character at position k has the smaller value, as * determined by using the &lt; operator, lexicographically precedes the * other string. In this case, {@code compareTo} returns the * difference of the two character values at position {@code k} in * the two string -- that is, the value: * * this.charAt(k)-anotherString.charAt(k) * * If there is no index position at which they differ, then the shorter * string lexicographically precedes the longer string. In this case, * {@code compareTo} returns the difference of the lengths of the * strings -- that is, the value: * * this.length()-anotherString.length() * * * @param anotherString the {@code String} to be compared. * @return the value {@code 0} if the argument string is equal to * this string; a value less than {@code 0} if this string * is lexicographically less than the string argument; and a * value greater than {@code 0} if this string is * lexicographically greater than the string argument. */ public int compareTo(String anotherString) { int len1 = value.length; int len2 = anotherString.value.length; int lim = Math.min(len1, len2); char v1[] = value; char v2[] = anotherString.value; int k = 0; while (k * Note that this Comparator does not take locale into account, * and will result in an unsatisfactory ordering for certain locales. * The java.text package provides Collators to allow * locale-sensitive ordering. * * @see java.text.Collator#compare(String, String) * @since 1.2 */ public static final Comparator CASE_INSENSITIVE_ORDER = new CaseInsensitiveComparator(); private static class CaseInsensitiveComparator implements Comparator, java.io.Serializable { // use serialVersionUID from JDK 1.2.2 for interoperability private static final long serialVersionUID = 8575799808933029326L; public int compare(String s1, String s2) { int n1 = s1.length(); int n2 = s2.length(); int min = Math.min(n1, n2); for (int i = 0; i * Note that this method does not take locale into account, * and will result in an unsatisfactory ordering for certain locales. * The java.text package provides collators to allow * locale-sensitive ordering. * * @param str the {@code String} to be compared. * @return a negative integer, zero, or a positive integer as the * specified String is greater than, equal to, or less * than this String, ignoring case considerations. * @see java.text.Collator#compare(String, String) * @since 1.2 */ public int compareToIgnoreCase(String str) { return CASE_INSENSITIVE_ORDER.compare(this, str); } /** * Tests if two string regions are equal. * * A substring of this {@code String} object is compared to a substring * of the argument other. The result is true if these substrings * represent identical character sequences. The substring of this * {@code String} object to be compared begins at index {@code toffset} * and has length {@code len}. The substring of other to be compared * begins at index {@code ooffset} and has length {@code len}. The * result is {@code false} if and only if at least one of the following * is true: * {@code toffset} is negative. * {@code ooffset} is negative. * {@code toffset+len} is greater than the length of this * {@code String} object. * {@code ooffset+len} is greater than the length of the other * argument. * There is some nonnegative integer k less than {@code len} * such that: * {@code this.charAt(toffset + }k{@code ) != other.charAt(ooffset + } * k{@code )} * * * @param toffset the starting offset of the subregion in this string. * @param other the string argument. * @param ooffset the starting offset of the subregion in the string * argument. * @param len the number of characters to compare. * @return {@code true} if the specified subregion of this string * exactly matches the specified subregion of the string argument; * {@code false} otherwise. */ public boolean regionMatches(int toffset, String other, int ooffset, int len) { char ta[] = value; int to = toffset; char pa[] = other.value; int po = ooffset; // Note: toffset, ooffset, or len might be near -1>>>1. if ((ooffset (long)value.length - len) || (ooffset > (long)other.value.length - len)) { return false; } while (len-- > 0) { if (ta[to++] != pa[po++]) { return false; } } return true; } /** * Tests if two string regions are equal. * * A substring of this {@code String} object is compared to a substring * of the argument {@code other}. The result is {@code true} if these * substrings represent character sequences that are the same, ignoring * case if and only if {@code ignoreCase} is true. The substring of * this {@code String} object to be compared begins at index * {@code toffset} and has length {@code len}. The substring of * {@code other} to be compared begins at index {@code ooffset} and * has length {@code len}. The result is {@code false} if and only if * at least one of the following is true: * {@code toffset} is negative. * {@code ooffset} is negative. * {@code toffset+len} is greater than the length of this * {@code String} object. * {@code ooffset+len} is greater than the length of the other * argument. * {@code ignoreCase} is {@code false} and there is some nonnegative * integer k less than {@code len} such that: * * this.charAt(toffset+k) != other.charAt(ooffset+k) * * {@code ignoreCase} is {@code true} and there is some nonnegative * integer k less than {@code len} such that: * * Character.toLowerCase(this.charAt(toffset+k)) != Character.toLowerCase(other.charAt(ooffset+k)) * * and: * * Character.toUpperCase(this.charAt(toffset+k)) != * Character.toUpperCase(other.charAt(ooffset+k)) * * * * @param ignoreCase if {@code true}, ignore case when comparing * characters. * @param toffset the starting offset of the subregion in this * string. * @param other the string argument. * @param ooffset the starting offset of the subregion in the string * argument. * @param len the number of characters to compare. * @return {@code true} if the specified subregion of this string * matches the specified subregion of the string argument; * {@code false} otherwise. Whether the matching is exact * or case insensitive depends on the {@code ignoreCase} * argument. */ public boolean regionMatches(boolean ignoreCase, int toffset, String other, int ooffset, int len) { char ta[] = value; int to = toffset; char pa[] = other.value; int po = ooffset; // Note: toffset, ooffset, or len might be near -1>>>1. if ((ooffset (long)value.length - len) || (ooffset > (long)other.value.length - len)) { return false; } while (len-- > 0) { char c1 = ta[to++]; char c2 = pa[po++]; if (c1 == c2) { continue; } if (ignoreCase) { // If characters don't match but case may be ignored, // try converting both characters to uppercase. // If the results match, then the comparison scan should // continue. char u1 = Character.toUpperCase(c1); char u2 = Character.toUpperCase(c2); if (u1 == u2) { continue; } // Unfortunately, conversion to uppercase does not work properly // for the Georgian alphabet, which has strange rules about case // conversion. So we need to make one last check before // exiting. if (Character.toLowerCase(u1) == Character.toLowerCase(u2)) { continue; } } return false; } return true; } /** * Tests if the substring of this string beginning at the * specified index starts with the specified prefix. * * @param prefix the prefix. * @param toffset where to begin looking in this string. * @return {@code true} if the character sequence represented by the * argument is a prefix of the substring of this object starting * at index {@code toffset}; {@code false} otherwise. * The result is {@code false} if {@code toffset} is * negative or greater than the length of this * {@code String} object; otherwise the result is the same * as the result of the expression * * this.substring(toffset).startsWith(prefix) * */ public boolean startsWith(String prefix, int toffset) { char ta[] = value; int to = toffset; char pa[] = prefix.value; int po = 0; int pc = prefix.value.length; // Note: toffset might be near -1>>>1. if ((toffset value.length - pc)) { return false; } while (--pc >= 0) { if (ta[to++] != pa[po++]) { return false; } } return true; } /** * Tests if this string starts with the specified prefix. * * @param prefix the prefix. * @return {@code true} if the character sequence represented by the * argument is a prefix of the character sequence represented by * this string; {@code false} otherwise. * Note also that {@code true} will be returned if the * argument is an empty string or is equal to this * {@code String} object as determined by the * {@link #equals(Object)} method. * @since 1. 0 */ public boolean startsWith(String prefix) { return startsWith(prefix, 0); } /** * Tests if this string ends with the specified suffix. * * @param suffix the suffix. * @return {@code true} if the character sequence represented by the * argument is a suffix of the character sequence represented by * this object; {@code false} otherwise. Note that the * result will be {@code true} if the argument is the * empty string or is equal to this {@code String} object * as determined by the {@link #equals(Object)} method. */ public boolean endsWith(String suffix) { return startsWith(suffix, value.length - suffix.value.length); } /** * Returns a hash code for this string. The hash code for a * {@code String} object is computed as * * s[0]*31^(n-1) + s[1]*31^(n-2) + ... + s[n-1] * * using {@code int} arithmetic, where {@code s[i]} is the * ith character of the string, {@code n} is the length of * the string, and {@code ^} indicates exponentiation. * (The hash value of the empty string is zero.) * * @return a hash code value for this object. */ public int hashCode() { int h = hash; if (h == 0 && value.length > 0) { char val[] = value; for (int i = 0; i k such that: * * this.charAt(k) == ch * * is true. For other values of {@code ch}, it is the * smallest value k such that: * * this.codePointAt(k) == ch * * is true. In either case, if no such character occurs in this * string, then {@code -1} is returned. * * @param ch a character (Unicode code point). * @return the index of the first occurrence of the character in the * character sequence represented by this object, or * {@code -1} if the character does not occur. */ public int indexOf(int ch) { return indexOf(ch, 0); } /** * Returns the index within this string of the first occurrence of the * specified character, starting the search at the specified index. * * If a character with value {@code ch} occurs in the * character sequence represented by this {@code String} * object at an index no smaller than {@code fromIndex}, then * the index of the first such occurrence is returned. For values * of {@code ch} in the range from 0 to 0xFFFF (inclusive), * this is the smallest value k such that: * * (this.charAt(k) == ch) {@code &&} (k &gt;= fromIndex) * * is true. For other values of {@code ch}, it is the * smallest value k such that: * * (this.codePointAt(k) == ch) {@code &&} (k &gt;= fromIndex) * * is true. In either case, if no such character occurs in this * string at or after position {@code fromIndex}, then * {@code -1} is returned. * * * There is no restriction on the value of {@code fromIndex}. If it * is negative, it has the same effect as if it were zero: this entire * string may be searched. If it is greater than the length of this * string, it has the same effect as if it were equal to the length of * this string: {@code -1} is returned. * * All indices are specified in {@code char} values * (Unicode code units). * * @param ch a character (Unicode code point). * @param fromIndex the index to start the search from. * @return the index of the first occurrence of the character in the * character sequence represented by this object that is greater * than or equal to {@code fromIndex}, or {@code -1} * if the character does not occur. */ public int indexOf(int ch, int fromIndex) { final int max = value.length; if (fromIndex = max) { // Note: fromIndex might be near -1>>>1. return -1; } if (ch k such that: * * this.charAt(k) == ch * * is true. For other values of {@code ch}, it is the * largest value k such that: * * this.codePointAt(k) == ch * * is true. In either case, if no such character occurs in this * string, then {@code -1} is returned. The * {@code String} is searched backwards starting at the last * character. * * @param ch a character (Unicode code point). * @return the index of the last occurrence of the character in the * character sequence represented by this object, or * {@code -1} if the character does not occur. */ public int lastIndexOf(int ch) { return lastIndexOf(ch, value.length - 1); } /** * Returns the index within this string of the last occurrence of * the specified character, searching backward starting at the * specified index. For values of {@code ch} in the range * from 0 to 0xFFFF (inclusive), the index returned is the largest * value k such that: * * (this.charAt(k) == ch) {@code &&} (k &lt;= fromIndex) * * is true. For other values of {@code ch}, it is the * largest value k such that: * * (this.codePointAt(k) == ch) {@code &&} (k &lt;= fromIndex) * * is true. In either case, if no such character occurs in this * string at or before position {@code fromIndex}, then * {@code -1} is returned. * * All indices are specified in {@code char} values * (Unicode code units). * * @param ch a character (Unicode code point). * @param fromIndex the index to start the search from. There is no * restriction on the value of {@code fromIndex}. If it is * greater than or equal to the length of this string, it has * the same effect as if it were equal to one less than the * length of this string: this entire string may be searched. * If it is negative, it has the same effect as if it were -1: * -1 is returned. * @return the index of the last occurrence of the character in the * character sequence represented by this object that is less * than or equal to {@code fromIndex}, or {@code -1} * if the character does not occur before that point. */ public int lastIndexOf(int ch, int fromIndex) { if (ch = 0; i--) { if (value[i] == ch) { return i; } } return -1; } else { return lastIndexOfSupplementary(ch, fromIndex); } } /** * Handles (rare) calls of lastIndexOf with a supplementary character. */ private int lastIndexOfSupplementary(int ch, int fromIndex) { if (Character.isValidCodePoint(ch)) { final char[] value = this.value; char hi = Character.highSurrogate(ch); char lo = Character.lowSurrogate(ch); int i = Math.min(fromIndex, value.length - 2); for (; i >= 0; i--) { if (value[i] == hi && value[i + 1] == lo) { return i; } } } return -1; } /** * Returns the index within this string of the first occurrence of the * specified substring. * * The returned index is the smallest value k for which: * * this.startsWith(str, k) * * If no such value of k exists, then {@code -1} is returned. * * @param str the substring to search for. * @return the index of the first occurrence of the specified substring, * or {@code -1} if there is no such occurrence. */ public int indexOf(String str) { return indexOf(str, 0); } /** * Returns the index within this string of the first occurrence of the * specified substring, starting at the specified index. * * The returned index is the smallest value k for which: * * k &gt;= fromIndex {@code &&} this.startsWith(str, k) * * If no such value of k exists, then {@code -1} is returned. * * @param str the substring to search for. * @param fromIndex the index from which to start the search. * @return the index of the first occurrence of the specified substring, * starting at the specified index, * or {@code -1} if there is no such occurrence. */ public int indexOf(String str, int fromIndex) { return indexOf(value, 0, value.length, str.value, 0, str.value.length, fromIndex); } /** * Code shared by String and AbstractStringBuilder to do searches. The * source is the character array being searched, and the target * is the string being searched for. * * @param source the characters being searched. * @param sourceOffset offset of the source string. * @param sourceCount count of the source string. * @param target the characters being searched for. * @param fromIndex the index to begin searching from. */ static int indexOf(char[] source, int sourceOffset, int sourceCount, String target, int fromIndex) { return indexOf(source, sourceOffset, sourceCount, target.value, 0, target.value.length, fromIndex); } /** * Code shared by String and StringBuffer to do searches. The * source is the character array being searched, and the target * is the string being searched for. * * @param source the characters being searched. * @param sourceOffset offset of the source string. * @param sourceCount count of the source string. * @param target the characters being searched for. * @param targetOffset offset of the target string. * @param targetCount count of the target string. * @param fromIndex the index to begin searching from. */ static int indexOf(char[] source, int sourceOffset, int sourceCount, char[] target, int targetOffset, int targetCount, int fromIndex) { if (fromIndex >= sourceCount) { return (targetCount == 0 ? sourceCount : -1); } if (fromIndex The returned index is the largest value k for which: * * this.startsWith(str, k) * * If no such value of k exists, then {@code -1} is returned. * * @param str the substring to search for. * @return the index of the last occurrence of the specified substring, * or {@code -1} if there is no such occurrence. */ public int lastIndexOf(String str) { return lastIndexOf(str, value.length); } /** * Returns the index within this string of the last occurrence of the * specified substring, searching backward starting at the specified index. * * The returned index is the largest value k for which: * * k {@code k) * * If no such value of k exists, then {@code -1} is returned. * * @param str the substring to search for. * @param fromIndex the index to start the search from. * @return the index of the last occurrence of the specified substring, * searching backward from the specified index, * or {@code -1} if there is no such occurrence. */ public int lastIndexOf(String str, int fromIndex) { return lastIndexOf(value, 0, value.length, str.value, 0, str.value.length, fromIndex); } /** * Code shared by String and AbstractStringBuilder to do searches. The * source is the character array being searched, and the target * is the string being searched for. * * @param source the characters being searched. * @param sourceOffset offset of the source string. * @param sourceCount count of the source string. * @param target the characters being searched for. * @param fromIndex the index to begin searching from. */ static int lastIndexOf(char[] source, int sourceOffset, int sourceCount, String target, int fromIndex) { return lastIndexOf(source, sourceOffset, sourceCount, target.value, 0, target.value.length, fromIndex); } /** * Code shared by String and StringBuffer to do searches. The * source is the character array being searched, and the target * is the string being searched for. * * @param source the characters being searched. * @param sourceOffset offset of the source string. * @param sourceCount count of the source string. * @param target the characters being searched for. * @param targetOffset offset of the target string. * @param targetCount count of the target string. * @param fromIndex the index to begin searching from. */ static int lastIndexOf(char[] source, int sourceOffset, int sourceCount, char[] target, int targetOffset, int targetCount, int fromIndex) { /* * Check arguments; return immediately where possible. For * consistency, don't check for null str. */ int rightIndex = sourceCount - targetCount; if (fromIndex rightIndex) { fromIndex = rightIndex; } /* Empty string always matches. */ if (targetCount == 0) { return fromIndex; } int strLastIndex = targetOffset + targetCount - 1; char strLastChar = target[strLastIndex]; int min = sourceOffset + targetCount - 1; int i = min + fromIndex; startSearchForLastChar: while (true) { while (i >= min && source[i] != strLastChar) { i--; } if (i start) { if (source[j--] != target[k--]) { i--; continue startSearchForLastChar; } } return start - sourceOffset + 1; } } /** * Returns a string that is a substring of this string. The * substring begins with the character at the specified index and * extends to the end of this string. * Examples: * * \"unhappy\".substring(2) returns \"happy\" * \"Harbison\".substring(3) returns \"bison\" * \"emptiness\".substring(9) returns \"\" (an empty string) * * * @param beginIndex the beginning index, inclusive. * @return the specified substring. * @exception IndexOutOfBoundsException if * {@code beginIndex} is negative or larger than the * length of this {@code String} object. */ public String substring(int beginIndex) { if (beginIndex * Examples: * * \"hamburger\".substring(4, 8) returns \"urge\" * \"smiles\".substring(1, 5) returns \"mile\" * * * @param beginIndex the beginning index, inclusive. * @param endIndex the ending index, exclusive. * @return the specified substring. * @exception IndexOutOfBoundsException if the * {@code beginIndex} is negative, or * {@code endIndex} is larger than the length of * this {@code String} object, or * {@code beginIndex} is larger than * {@code endIndex}. */ public String substring(int beginIndex, int endIndex) { if (beginIndex value.length) { throw new StringIndexOutOfBoundsException(endIndex); } int subLen = endIndex - beginIndex; if (subLen An invocation of this method of the form * * * str.subSequence(begin,&nbsp;end) * * behaves in exactly the same way as the invocation * * * str.substring(begin,&nbsp;end) * * @apiNote * This method is defined so that the {@code String} class can implement * the {@link CharSequence} interface. * * @param beginIndex the begin index, inclusive. * @param endIndex the end index, exclusive. * @return the specified subsequence. * * @throws IndexOutOfBoundsException * if {@code beginIndex} or {@code endIndex} is negative, * if {@code endIndex} is greater than {@code length()}, * or if {@code beginIndex} is greater than {@code endIndex} * * @since 1.4 * @spec JSR-51 */ public CharSequence subSequence(int beginIndex, int endIndex) { return this.substring(beginIndex, endIndex); } /** * Concatenates the specified string to the end of this string. * * If the length of the argument string is {@code 0}, then this * {@code String} object is returned. Otherwise, a * {@code String} object is returned that represents a character * sequence that is the concatenation of the character sequence * represented by this {@code String} object and the character * sequence represented by the argument string. * Examples: * * \"cares\".concat(\"s\") returns \"caress\" * \"to\".concat(\"get\").concat(\"her\") returns \"together\" * * * @param str the {@code String} that is concatenated to the end * of this {@code String}. * @return a string that represents the concatenation of this object's * characters followed by the string argument's characters. */ public String concat(String str) { int otherLen = str.length(); if (otherLen == 0) { return this; } int len = value.length; char buf[] = Arrays.copyOf(value, len + otherLen); str.getChars(buf, len); return new String(buf, true); } /** * Returns a string resulting from replacing all occurrences of * {@code oldChar} in this string with {@code newChar}. * * If the character {@code oldChar} does not occur in the * character sequence represented by this {@code String} object, * then a reference to this {@code String} object is returned. * Otherwise, a {@code String} object is returned that * represents a character sequence identical to the character sequence * represented by this {@code String} object, except that every * occurrence of {@code oldChar} is replaced by an occurrence * of {@code newChar}. * * Examples: * * \"mesquite in your cellar\".replace('e', 'o') * returns \"mosquito in your collar\" * \"the war of baronets\".replace('r', 'y') * returns \"the way of bayonets\" * \"sparring with a purple porpoise\".replace('p', 't') * returns \"starring with a turtle tortoise\" * \"JonL\".replace('q', 'x') returns \"JonL\" (no change) * * * @param oldChar the old character. * @param newChar the new character. * @return a string derived from this string by replacing every * occurrence of {@code oldChar} with {@code newChar}. */ public String replace(char oldChar, char newChar) { if (oldChar != newChar) { int len = value.length; int i = -1; char[] val = value; /* avoid getfield opcode */ while (++i regular expression. * * An invocation of this method of the form * str{@code .matches(}regex{@code )} yields exactly the * same result as the expression * * * {@link java.util.regex.Pattern}.{@link java.util.regex.Pattern#matches(String,CharSequence) * matches(regex, str)} * * * @param regex * the regular expression to which this string is to be matched * * @return {@code true} if, and only if, this string matches the * given regular expression * * @throws PatternSyntaxException * if the regular expression's syntax is invalid * * @see java.util.regex.Pattern * * @since 1.4 * @spec JSR-51 */ public boolean matches(String regex) { return Pattern.matches(regex, this); } /** * Returns true if and only if this string contains the specified * sequence of char values. * * @param s the sequence to search for * @return true if this string contains {@code s}, false otherwise * @since 1.5 */ public boolean contains(CharSequence s) { return indexOf(s.toString()) > -1; } /** * Replaces the first substring of this string that matches the given regular expression with the * given replacement. * * An invocation of this method of the form * str{@code .replaceFirst(}regex{@code ,} repl{@code )} * yields exactly the same result as the expression * * * * {@link java.util.regex.Pattern}.{@link * java.util.regex.Pattern#compile compile}(regex).{@link * java.util.regex.Pattern#matcher(java.lang.CharSequence) matcher}(str).{@link * java.util.regex.Matcher#replaceFirst replaceFirst}(repl) * * * * * Note that backslashes ({@code \\}) and dollar signs ({@code $}) in the * replacement string may cause the results to be different than if it were * being treated as a literal replacement string; see * {@link java.util.regex.Matcher#replaceFirst}. * Use {@link java.util.regex.Matcher#quoteReplacement} to suppress the special * meaning of these characters, if desired. * * @param regex * the regular expression to which this string is to be matched * @param replacement * the string to be substituted for the first match * * @return The resulting {@code String} * * @throws PatternSyntaxException * if the regular expression's syntax is invalid * * @see java.util.regex.Pattern * * @since 1.4 * @spec JSR-51 */ public String replaceFirst(String regex, String replacement) { return Pattern.compile(regex).matcher(this).replaceFirst(replacement); } /** * Replaces each substring of this string that matches the given regular expression with the * given replacement. * * An invocation of this method of the form * str{@code .replaceAll(}regex{@code ,} repl{@code )} * yields exactly the same result as the expression * * * * {@link java.util.regex.Pattern}.{@link * java.util.regex.Pattern#compile compile}(regex).{@link * java.util.regex.Pattern#matcher(java.lang.CharSequence) matcher}(str).{@link * java.util.regex.Matcher#replaceAll replaceAll}(repl) * * * * * Note that backslashes ({@code \\}) and dollar signs ({@code $}) in the * replacement string may cause the results to be different than if it were * being treated as a literal replacement string; see * {@link java.util.regex.Matcher#replaceAll Matcher.replaceAll}. * Use {@link java.util.regex.Matcher#quoteReplacement} to suppress the special * meaning of these characters, if desired. * * @param regex * the regular expression to which this string is to be matched * @param replacement * the string to be substituted for each match * * @return The resulting {@code String} * * @throws PatternSyntaxException * if the regular expression's syntax is invalid * * @see java.util.regex.Pattern * * @since 1.4 * @spec JSR-51 */ public String replaceAll(String regex, String replacement) { return Pattern.compile(regex).matcher(this).replaceAll(replacement); } /** * Replaces each substring of this string that matches the literal target * sequence with the specified literal replacement sequence. The * replacement proceeds from the beginning of the string to the end, for * example, replacing \"aa\" with \"b\" in the string \"aaa\" will result in * \"ba\" rather than \"ab\". * * @param target The sequence of char values to be replaced * @param replacement The replacement sequence of char values * @return The resulting string * @since 1.5 */ public String replace(CharSequence target, CharSequence replacement) { return Pattern.compile(target.toString(), Pattern.LITERAL).matcher( this).replaceAll(Matcher.quoteReplacement(replacement.toString())); } /** * Splits this string around matches of the given * regular expression. * * The array returned by this method contains each substring of this * string that is terminated by another substring that matches the given * expression or is terminated by the end of the string. The substrings in * the array are in the order in which they occur in this string. If the * expression does not match any part of the input then the resulting array * has just one element, namely this string. * * When there is a positive-width match at the beginning of this * string then an empty leading substring is included at the beginning * of the resulting array. A zero-width match at the beginning however * never produces such empty leading substring. * * The {@code limit} parameter controls the number of times the * pattern is applied and therefore affects the length of the resulting * array. If the limit n is greater than zero then the pattern * will be applied at most n&nbsp;-&nbsp;1 times, the array's * length will be no greater than n, and the array's last entry * will contain all input beyond the last matched delimiter. If n * is non-positive then the pattern will be applied as many times as * possible and the array can have any length. If n is zero then * the pattern will be applied as many times as possible, the array can * have any length, and trailing empty strings will be discarded. * * The string {@code \"boo:and:foo\"}, for example, yields the * following results with these parameters: * * * * Regex * Limit * Result * * : * 2 * {@code { \"boo\", \"and:foo\" }} * : * 5 * {@code { \"boo\", \"and\", \"foo\" }} * : * -2 * {@code { \"boo\", \"and\", \"foo\" }} * o * 5 * {@code { \"b\", \"\", \":and:f\", \"\", \"\" }} * o * -2 * {@code { \"b\", \"\", \":and:f\", \"\", \"\" }} * o * 0 * {@code { \"b\", \"\", \":and:f\" }} * * * An invocation of this method of the form * str.{@code split(}regex{@code ,}&nbsp;n{@code )} * yields the same result as the expression * * * * {@link java.util.regex.Pattern}.{@link * java.util.regex.Pattern#compile compile}(regex).{@link * java.util.regex.Pattern#split(java.lang.CharSequence,int) split}(str,&nbsp;n) * * * * * @param regex * the delimiting regular expression * * @param limit * the result threshold, as described above * * @return the array of strings computed by splitting this string * around matches of the given regular expression * * @throws PatternSyntaxException * if the regular expression's syntax is invalid * * @see java.util.regex.Pattern * * @since 1.4 * @spec JSR-51 */ public String[] split(String regex, int limit) { /* fastpath if the regex is a (1)one-char String and this character is not one of the RegEx's meta characters \".$|()[{^?*+\\\\\", or (2)two-char String and the first char is the backslash and the second is not the ascii digit or ascii letter. */ char ch = 0; if (((regex.value.length == 1 && \".$|()[{^?*+\\\\\".indexOf(ch = regex.charAt(0)) == -1) || (regex.length() == 2 && regex.charAt(0) == '\\\\' && (((ch = regex.charAt(1))-'0')|('9'-ch)) Character.MAX_LOW_SURROGATE)) { int off = 0; int next = 0; boolean limited = limit > 0; ArrayList list = new ArrayList<>(); while ((next = indexOf(ch, off)) != -1) { if (!limited || list.size() 0 && list.get(resultSize - 1).length() == 0) { resultSize--; } } String[] result = new String[resultSize]; return list.subList(0, resultSize).toArray(result); } return Pattern.compile(regex).split(this, limit); } /** * Splits this string around matches of the given regular expression. * * This method works as if by invoking the two-argument {@link * #split(String, int) split} method with the given expression and a limit * argument of zero. Trailing empty strings are therefore not included in * the resulting array. * * The string {@code \"boo:and:foo\"}, for example, yields the following * results with these expressions: * * * * Regex * Result * * : * {@code { \"boo\", \"and\", \"foo\" }} * o * {@code { \"b\", \"\", \":and:f\" }} * * * * @param regex * the delimiting regular expression * * @return the array of strings computed by splitting this string * around matches of the given regular expression * * @throws PatternSyntaxException * if the regular expression's syntax is invalid * * @see java.util.regex.Pattern * * @since 1.4 * @spec JSR-51 */ public String[] split(String regex) { return split(regex, 0); } /** * Returns a new String composed of copies of the * {@code CharSequence elements} joined together with a copy of * the specified {@code delimiter}. * * For example, * {@code * String message = String.join(\"-\", \"Java\", \"is\", \"cool\"); * // message returned is: \"Java-is-cool\" * } * * Note that if an element is null, then {@code \"null\"} is added. * * @param delimiter the delimiter that separates each element * @param elements the elements to join together. * * @return a new {@code String} that is composed of the {@code elements} * separated by the {@code delimiter} * * @throws NullPointerException If {@code delimiter} or {@code elements} * is {@code null} * * @see java.util.StringJoiner * @since 1.8 */ public static String join(CharSequence delimiter, CharSequence... elements) { Objects.requireNonNull(delimiter); Objects.requireNonNull(elements); // Number of elements not likely worth Arrays.stream overhead. StringJoiner joiner = new StringJoiner(delimiter); for (CharSequence cs: elements) { joiner.add(cs); } return joiner.toString(); } /** * Returns a new {@code String} composed of copies of the * {@code CharSequence elements} joined together with a copy of the * specified {@code delimiter}. * * For example, * {@code * List strings = new LinkedList<>(); * strings.add(\"Java\");strings.add(\"is\"); * strings.add(\"cool\"); * String message = String.join(\" \", strings); * //message returned is: \"Java is cool\" * * Set strings = new LinkedHashSet<>(); * strings.add(\"Java\"); strings.add(\"is\"); * strings.add(\"very\"); strings.add(\"cool\"); * String message = String.join(\"-\", strings); * //message returned is: \"Java-is-very-cool\" * } * * Note that if an individual element is {@code null}, then {@code \"null\"} is added. * * @param delimiter a sequence of characters that is used to separate each * of the {@code elements} in the resulting {@code String} * @param elements an {@code Iterable} that will have its {@code elements} * joined together. * * @return a new {@code String} that is composed from the {@code elements} * argument * * @throws NullPointerException If {@code delimiter} or {@code elements} * is {@code null} * * @see #join(CharSequence,CharSequence...) * @see java.util.StringJoiner * @since 1.8 */ public static String join(CharSequence delimiter, Iterable elements) { Objects.requireNonNull(delimiter); Objects.requireNonNull(elements); StringJoiner joiner = new StringJoiner(delimiter); for (CharSequence cs: elements) { joiner.add(cs); } return joiner.toString(); } /** * Converts all of the characters in this {@code String} to lower * case using the rules of the given {@code Locale}. Case mapping is based * on the Unicode Standard version specified by the {@link java.lang.Character Character} * class. Since case mappings are not always 1:1 char mappings, the resulting * {@code String} may be a different length than the original {@code String}. * * Examples of lowercase mappings are in the following table: * * * Language Code of Locale * Upper Case * Lower Case * Description * * * tr (Turkish) * &#92;u0130 * &#92;u0069 * capital letter I with dot above -&gt; small letter i * * * tr (Turkish) * &#92;u0049 * &#92;u0131 * capital letter I -&gt; small letter dotless i * * * (all) * French Fries * french fries * lowercased all chars in String * * * (all) * * * * * * * lowercased all chars in String * * * * @param locale use the case transformation rules for this locale * @return the {@code String}, converted to lowercase. * @see java.lang.String#toLowerCase() * @see java.lang.String#toUpperCase() * @see java.lang.String#toUpperCase(Locale) * @since 1.1 */ public String toLowerCase(Locale locale) { if (locale == null) { throw new NullPointerException(); } int firstUpper; final int len = value.length; /* Now check if there are any characters that need to be changed. */ scan: { for (firstUpper = 0 ; firstUpper = Character.MIN_HIGH_SURROGATE) && (c = Character.MIN_HIGH_SURROGATE && (char)srcChar = Character.MIN_SUPPLEMENTARY_CODE_POINT)) { if (lowerChar == Character.ERROR) { lowerCharArray = ConditionalSpecialCasing.toLowerCaseCharArray(this, i, locale); } else if (srcCount == 2) { resultOffset += Character.toChars(lowerChar, result, i + resultOffset) - srcCount; continue; } else { lowerCharArray = Character.toChars(lowerChar); } /* Grow result if needed */ int mapLen = lowerCharArray.length; if (mapLen > srcCount) { char[] result2 = new char[result.length + mapLen - srcCount]; System.arraycopy(result, 0, result2, 0, i + resultOffset); result = result2; } for (int x = 0; x * Note: This method is locale sensitive, and may produce unexpected * results if used for strings that are intended to be interpreted locale * independently. * Examples are programming language identifiers, protocol keys, and HTML * tags. * For instance, {@code \"TITLE\".toLowerCase()} in a Turkish locale * returns {@code \"t\\u005Cu0131tle\"}, where '\\u005Cu0131' is the * LATIN SMALL LETTER DOTLESS I character. * To obtain correct results for locale insensitive strings, use * {@code toLowerCase(Locale.ROOT)}. * * @return the {@code String}, converted to lowercase. * @see java.lang.String#toLowerCase(Locale) */ public String toLowerCase() { return toLowerCase(Locale.getDefault()); } /** * Converts all of the characters in this {@code String} to upper * case using the rules of the given {@code Locale}. Case mapping is based * on the Unicode Standard version specified by the {@link java.lang.Character Character} * class. Since case mappings are not always 1:1 char mappings, the resulting * {@code String} may be a different length than the original {@code String}. * * Examples of locale-sensitive and 1:M case mappings are in the following table. * * * * Language Code of Locale * Lower Case * Upper Case * Description * * * tr (Turkish) * &#92;u0069 * &#92;u0130 * small letter i -&gt; capital letter I with dot above * * * tr (Turkish) * &#92;u0131 * &#92;u0049 * small letter dotless i -&gt; capital letter I * * * (all) * &#92;u00df * &#92;u0053 &#92;u0053 * small letter sharp s -&gt; two letters: SS * * * (all) * Fahrvergn&uuml;gen * FAHRVERGN&Uuml;GEN * * * * @param locale use the case transformation rules for this locale * @return the {@code String}, converted to uppercase. * @see java.lang.String#toUpperCase() * @see java.lang.String#toLowerCase() * @see java.lang.String#toLowerCase(Locale) * @since 1.1 */ public String toUpperCase(Locale locale) { if (locale == null) { throw new NullPointerException(); } int firstLower; final int len = value.length; /* Now check if there are any characters that need to be changed. */ scan: { for (firstLower = 0 ; firstLower = Character.MIN_HIGH_SURROGATE) && (c = Character.MIN_HIGH_SURROGATE && (char)srcChar = Character.MIN_SUPPLEMENTARY_CODE_POINT)) { if (upperChar == Character.ERROR) { if (localeDependent) { upperCharArray = ConditionalSpecialCasing.toUpperCaseCharArray(this, i, locale); } else { upperCharArray = Character.toUpperCaseCharArray(srcChar); } } else if (srcCount == 2) { resultOffset += Character.toChars(upperChar, result, i + resultOffset) - srcCount; continue; } else { upperCharArray = Character.toChars(upperChar); } /* Grow result if needed */ int mapLen = upperCharArray.length; if (mapLen > srcCount) { char[] result2 = new char[result.length + mapLen - srcCount]; System.arraycopy(result, 0, result2, 0, i + resultOffset); result = result2; } for (int x = 0; x * Note: This method is locale sensitive, and may produce unexpected * results if used for strings that are intended to be interpreted locale * independently. * Examples are programming language identifiers, protocol keys, and HTML * tags. * For instance, {@code \"title\".toUpperCase()} in a Turkish locale * returns {@code \"T\\u005Cu0130TLE\"}, where '\\u005Cu0130' is the * LATIN CAPITAL LETTER I WITH DOT ABOVE character. * To obtain correct results for locale insensitive strings, use * {@code toUpperCase(Locale.ROOT)}. * * @return the {@code String}, converted to uppercase. * @see java.lang.String#toUpperCase(Locale) */ public String toUpperCase() { return toUpperCase(Locale.getDefault()); } /** * Returns a string whose value is this string, with any leading and trailing * whitespace removed. * * If this {@code String} object represents an empty character * sequence, or the first and last characters of character sequence * represented by this {@code String} object both have codes * greater than {@code '\\u005Cu0020'} (the space character), then a * reference to this {@code String} object is returned. * * Otherwise, if there is no character with a code greater than * {@code '\\u005Cu0020'} in the string, then a * {@code String} object representing an empty string is * returned. * * Otherwise, let k be the index of the first character in the * string whose code is greater than {@code '\\u005Cu0020'}, and let * m be the index of the last character in the string whose code * is greater than {@code '\\u005Cu0020'}. A {@code String} * object is returned, representing the substring of this string that * begins with the character at index k and ends with the * character at index m-that is, the result of * {@code this.substring(k, m + 1)}. * * This method may be used to trim whitespace (as defined above) from * the beginning and end of a string. * * @return A string whose value is this string, with any leading and trailing white * space removed, or this string if it has no leading or * trailing white space. */ public String trim() { int len = value.length; int st = 0; char[] val = value; /* avoid getfield opcode */ while ((st 0) || (len The locale always used is the one returned by {@link * java.util.Locale#getDefault() Locale.getDefault()}. * * @param format * A format string * * @param args * Arguments referenced by the format specifiers in the format * string. If there are more arguments than format specifiers, the * extra arguments are ignored. The number of arguments is * variable and may be zero. The maximum number of arguments is * limited by the maximum dimension of a Java array as defined by * The Java&trade; Virtual Machine Specification. * The behaviour on a * {@code null} argument depends on the conversion. * * @throws java.util.IllegalFormatException * If a format string contains an illegal syntax, a format * specifier that is incompatible with the given arguments, * insufficient arguments given the format string, or other * illegal conditions. For specification of all possible * formatting errors, see the Details section of the * formatter class specification. * * @return A formatted string * * @see java.util.Formatter * @since 1.5 */ public static String format(String format, Object... args) { return new Formatter().format(format, args).toString(); } /** * Returns a formatted string using the specified locale, format string, * and arguments. * * @param l * The {@linkplain java.util.Locale locale} to apply during * formatting. If {@code l} is {@code null} then no localization * is applied. * * @param format * A format string * * @param args * Arguments referenced by the format specifiers in the format * string. If there are more arguments than format specifiers, the * extra arguments are ignored. The number of arguments is * variable and may be zero. The maximum number of arguments is * limited by the maximum dimension of a Java array as defined by * The Java&trade; Virtual Machine Specification. * The behaviour on a * {@code null} argument depends on the * conversion. * * @throws java.util.IllegalFormatException * If a format string contains an illegal syntax, a format * specifier that is incompatible with the given arguments, * insufficient arguments given the format string, or other * illegal conditions. For specification of all possible * formatting errors, see the Details section of the * formatter class specification * * @return A formatted string * * @see java.util.Formatter * @since 1.5 */ public static String format(Locale l, String format, Object... args) { return new Formatter(l).format(format, args).toString(); } /** * Returns the string representation of the {@code Object} argument. * * @param obj an {@code Object}. * @return if the argument is {@code null}, then a string equal to * {@code \"null\"}; otherwise, the value of * {@code obj.toString()} is returned. * @see java.lang.Object#toString() */ public static String valueOf(Object obj) { return (obj == null) ? \"null\" : obj.toString(); } /** * Returns the string representation of the {@code char} array * argument. The contents of the character array are copied; subsequent * modification of the character array does not affect the returned * string. * * @param data the character array. * @return a {@code String} that contains the characters of the * character array. */ public static String valueOf(char data[]) { return new String(data); } /** * Returns the string representation of a specific subarray of the * {@code char} array argument. * * The {@code offset} argument is the index of the first * character of the subarray. The {@code count} argument * specifies the length of the subarray. The contents of the subarray * are copied; subsequent modification of the character array does not * affect the returned string. * * @param data the character array. * @param offset initial offset of the subarray. * @param count length of the subarray. * @return a {@code String} that contains the characters of the * specified subarray of the character array. * @exception IndexOutOfBoundsException if {@code offset} is * negative, or {@code count} is negative, or * {@code offset+count} is larger than * {@code data.length}. */ public static String valueOf(char data[], int offset, int count) { return new String(data, offset, count); } /** * Equivalent to {@link #valueOf(char[], int, int)}. * * @param data the character array. * @param offset initial offset of the subarray. * @param count length of the subarray. * @return a {@code String} that contains the characters of the * specified subarray of the character array. * @exception IndexOutOfBoundsException if {@code offset} is * negative, or {@code count} is negative, or * {@code offset+count} is larger than * {@code data.length}. */ public static String copyValueOf(char data[], int offset, int count) { return new String(data, offset, count); } /** * Equivalent to {@link #valueOf(char[])}. * * @param data the character array. * @return a {@code String} that contains the characters of the * character array. */ public static String copyValueOf(char data[]) { return new String(data); } /** * Returns the string representation of the {@code boolean} argument. * * @param b a {@code boolean}. * @return if the argument is {@code true}, a string equal to * {@code \"true\"} is returned; otherwise, a string equal to * {@code \"false\"} is returned. */ public static String valueOf(boolean b) { return b ? \"true\" : \"false\"; } /** * Returns the string representation of the {@code char} * argument. * * @param c a {@code char}. * @return a string of length {@code 1} containing * as its single character the argument {@code c}. */ public static String valueOf(char c) { char data[] = {c}; return new String(data, true); } /** * Returns the string representation of the {@code int} argument. * * The representation is exactly the one returned by the * {@code Integer.toString} method of one argument. * * @param i an {@code int}. * @return a string representation of the {@code int} argument. * @see java.lang.Integer#toString(int, int) */ public static String valueOf(int i) { return Integer.toString(i); } /** * Returns the string representation of the {@code long} argument. * * The representation is exactly the one returned by the * {@code Long.toString} method of one argument. * * @param l a {@code long}. * @return a string representation of the {@code long} argument. * @see java.lang.Long#toString(long) */ public static String valueOf(long l) { return Long.toString(l); } /** * Returns the string representation of the {@code float} argument. * * The representation is exactly the one returned by the * {@code Float.toString} method of one argument. * * @param f a {@code float}. * @return a string representation of the {@code float} argument. * @see java.lang.Float#toString(float) */ public static String valueOf(float f) { return Float.toString(f); } /** * Returns the string representation of the {@code double} argument. * * The representation is exactly the one returned by the * {@code Double.toString} method of one argument. * * @param d a {@code double}. * @return a string representation of the {@code double} argument. * @see java.lang.Double#toString(double) */ public static String valueOf(double d) { return Double.toString(d); } /** * Returns a canonical representation for the string object. * * A pool of strings, initially empty, is maintained privately by the * class {@code String}. * * When the intern method is invoked, if the pool already contains a * string equal to this {@code String} object as determined by * the {@link #equals(Object)} method, then the string from the pool is * returned. Otherwise, this {@code String} object is added to the * pool and a reference to this {@code String} object is returned. * * It follows that for any two strings {@code s} and {@code t}, * {@code s.intern() == t.intern()} is {@code true} * if and only if {@code s.equals(t)} is {@code true}. * * All literal strings and string-valued constant expressions are * interned. String literals are defined in section 3.10.5 of the * The Java&trade; Language Specification. * * @return a string that has the same contents as this string, but is * guaranteed to be from a pool of unique strings. */ public native String intern(); } "},"base/string/string源码学习.html":{"url":"base/string/string源码学习.html","title":"string源码学习","keywords":"","body":" intern 调用intern方法时，若字符串常量池存在和调用对象相等的字符串，则返回常量池对应字符串的应用，若常量池没有，则先创建再返回对应的的引用 看下面源码解析就可以看出 /** * Returns a canonical representation for the string object. * * A pool of strings, initially empty, is maintained privately by the * class {@code String}. * * When the intern method is invoked, if the pool already contains a * string equal to this {@code String} object as determined by * the {@link #equals(Object)} method, then the string from the pool is * returned. Otherwise, this {@code String} object is added to the * pool and a reference to this {@code String} object is returned. * * It follows that for any two strings {@code s} and {@code t}, * {@code s.intern() == t.intern()} is {@code true} * if and only if {@code s.equals(t)} is {@code true}. * * All literal strings and string-valued constant expressions are * interned. String literals are defined in section 3.10.5 of the * The Java&trade; Language Specification. * * @return a string that has the same contents as this string, but is * guaranteed to be from a pool of unique strings. */ public native String intern(); hashCode public int hashCode() { int h = hash; if (h == 0 && value.length > 0) { char val[] = value; for (int i = 0; i 在String类中有个私有实例字段hash表示该串的哈希值，在第一次调用hashCode方法时，字符串的哈希值被计算并且赋值给hash字段，之后再调用hashCode方法便可以直接取hash字段返回。 String类中的hashCode计算方法还是比较简单的，就是以31为权，每一位为字符的ASCII值进行运算，用自然溢出来等效取模。哈希计算公式可以计为s[0]31^(n-1) + s[1]31^(n-2) + ... + s[n-1] 字符串哈希可以做很多事情，通常是类似于字符串判等，判回文之类的。 但是仅仅依赖于哈希值来判断其实是不严谨的，除非能够保证不会有哈希冲突，通常这一点很难做到。 就拿jdk中String类的哈希方法来举例，字符串\"gdejicbegh\"与字符串\"hgebcijedg\"具有相同的hashCode()返回值-801038016，并且它们具有reverse的关系。这个例子说明了用jdk中默认的hashCode方法判断字符串相等或者字符串回文，都存在反例。 https://www.cnblogs.com/bsjl/p/8626581.html 字符串对象的哈希码根据以下公式计算： s[0]*31^(n-1) + s[1]*31^(n-2) + ... + s[n-1] 使用 int 算法，这里 s[i] 是字符串的第 i 个字符，n 是字符串的长度，^ 表示求幂。空字符串的哈希值为 0。 "},"base/string/字符串string学习.html":{"url":"base/string/字符串string学习.html","title":"字符串string学习","keywords":"","body":" public class Main { public static void main(String[] args) { String s1 = \"hello\"; String s2 = \"hello\"; String s3 = \"he\" + \"llo\"; String s4 = \"hel\" + new String(\"lo\"); String s5 = new String(\"hello\"); String s6 = s5.intern(); String s7 = \"h\"; String s8 = \"ello\"; String s9 = s7 + s8; System.out.println(s1==s2);//true System.out.println(s1==s3);//true System.out.println(s1==s4);//false System.out.println(s1==s9);//false System.out.println(s4==s5);//false System.out.println(s1==s6);//true } } String类的final修饰的，以字面量的形式创建String变量时，jvm会在编译期间就把该字面量hello放到字符串常量池中，由Java程序启动的时候就已经加载到内存中了。这个字符串常量池的特点就是有且只有一份相同的字面量，如果有其它相同的字面量，jvm则返回这个字面量的引用，如果没有相同的字面量，则在字符串常量池创建这个字面量并返回它的引用。 由于s2指向的字面量hello在常量池中已经存在了（s1先于s2），于是jvm就返回这个字面量绑定的引用，所以s1==s2。 s3中字面量的拼接其实就是hello，jvm在编译期间就已经对它进行优化，所以s1和s3也是相等的。 s4中的new String(\"lo\")生成了两个对象，lo，new String(\"lo\")，lo存在字符串常量池，new String(\"lo\")存在堆中，String s4 = \"hel\" + new String(\"lo\")实质上是两个对象的相加，编译器不会进行优化，相加的结果存在堆中，而s1存在字符串常量池中，当然不相等。s1==s9的原理一样。 s4==s5两个相加的结果都在堆中，不用说，肯定不相等。 s1==s6中，s5.intern()方法能使一个位于堆中的字符串在运行期间动态地加入到字符串常量池中（字符串常量池的内容是程序启动的时候就已经加载好了），如果字符串常量池中有该对象对应的字面量，则返回该字面量在字符串常量池中的引用，否则，创建复制一份该字面量到字符串常量池并返回它的引用。因此s1==s6输出true。 参考： jdk9后String源码由char数组改为byte数组存储 "},"base/string/字符串string学习2-lg.html":{"url":"base/string/字符串string学习2-lg.html","title":"字符串string学习2","keywords":"","body":" String 是如何实现的？它有哪些重要的方法？ 存储结构 以主流的 JDK 版本 1.8 来说，String 内部实际存储结构为 char 数组，源码如下： public final class String implements java.io.Serializable, Comparable, CharSequence { // 用于存储字符串的值 private final char value[]; // 缓存字符串的 hash code private int hash; // Default to 0 // ......其他内容 } String 源码中包含下面几个重要的方法。 1. 多构造方法 String 字符串有以下 4 个重要的构造方法： // String 为参数的构造方法 public String(String original) { this.value = original.value; this.hash = original.hash; } // char[] 为参数构造方法 public String(char value[]) { this.value = Arrays.copyOf(value, value.length); } // StringBuffer 为参数的构造方法 public String(StringBuffer buffer) { synchronized(buffer) { this.value = Arrays.copyOf(buffer.getValue(), buffer.length()); } } // StringBuilder 为参数的构造方法 public String(StringBuilder builder) { this.value = Arrays.copyOf(builder.getValue(), builder.length()); } 其中，比较容易被我们忽略的是以 StringBuffer 和 StringBuilder 为参数的构造函数，因为这三种数据类型，我们通常都是单独使用的，所以这个小细节我们需要特别留意一下。 2. equals() 比较两个字符串是否相等 public boolean equals(Object anObject) { // 对象引用相同直接返回 true if (this == anObject) { return true; } // 判断需要对比的值是否为 String 类型，如果不是则直接返回 false if (anObject instanceof String) { String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) { // 把两个字符串都转换为 char 数组对比 char v1[] = value; char v2[] = anotherString.value; int i = 0; // 循环比对两个字符串的每一个字符 while (n-- != 0) { // 如果其中有一个字符不相等就 true false，否则继续对比 if (v1[i] != v2[i]) return false; i++; } return true; } } return false; } String 类型重写了 Object 中的 equals() 方法，equals() 方法需要传递一个 Object 类型的参数值，在比较时会先通过 instanceof 判断是否为 String 类型，如果不是则会直接返回 false，instanceof 的使用如下： Object oString = \"123\"; Object oInt = 123; System.out.println(oString instanceof String); // 返回 true System.out.println(oInt instanceof String); // 返回 false 当判断参数为 String 类型之后，会循环对比两个字符串中的每一个字符，当所有字符都相等时返回 true，否则则返回 false。 还有一个和 equals() 比较类似的方法 equalsIgnoreCase()，它是用于忽略字符串的大小写之后进行字符串对比。 3. compareTo() 比较两个字符串 compareTo() 方法用于比较两个字符串，返回的结果为 int 类型的值，源码如下： public int compareTo(String anotherString) { int len1 = value.length; int len2 = anotherString.value.length; // 获取到两个字符串长度最短的那个 int 值 int lim = Math.min(len1, len2); char v1[] = value; char v2[] = anotherString.value; int k = 0; // 对比每一个字符 while (k 从源码中可以看出，compareTo() 方法会循环对比所有的字符，当两个字符串中有任意一个字符不相同时，则 return char1-char2。比如，两个字符串分别存储的是 1 和 2，返回的值是 -1；如果存储的是 1 和 1，则返回的值是 0 ，如果存储的是 2 和 1，则返回的值是 1。 还有一个和 compareTo() 比较类似的方法 compareToIgnoreCase()，用于忽略大小写后比较两个字符串。 可以看出 compareTo() 方法和 equals() 方法都是用于比较两个字符串的，但它们有两点不同： equals() 可以接收一个 Object 类型的参数，而 compareTo() 只能接收一个 String 类型的参数； equals() 返回值为 Boolean，而 compareTo() 的返回值则为 int。 它们都可以用于两个字符串的比较，当 equals() 方法返回 true 时，或者是 compareTo() 方法返回 0 时，则表示两个字符串完全相同。 4. 其他重要方法 indexOf()：查询字符串首次出现的下标位置 lastIndexOf()：查询字符串最后出现的下标位置 contains()：查询字符串中是否包含另一个字符串 toLowerCase()：把字符串全部转换成小写 toUpperCase()：把字符串全部转换成大写length()：查询字符串的长度trim()：去掉字符串首尾空格replace()：替换字符串中的某些字符split()：把字符串分割并返回字符串数组join()：把字符串数组转为字符串 考点分析 这道题目考察的重点是，你对 Java 源码的理解，这也从侧面反应了你是否热爱和喜欢专研程序，而这正是一个优秀程序员所必备的特质。 String 源码属于所有源码中最基础、最简单的一个，对 String 源码的理解也反应了你的 Java 基础功底。 String 问题如果再延伸一下，会问到一些更多的知识细节，这也是大厂一贯使用的面试策略，从一个知识点入手然后扩充更多的知识细节，对于 String 也不例外，通常还会关联的询问以下问题： 为什么 String 类型要用 final 修饰？== 和 equals 的区别是什么？String 和 StringBuilder、StringBuffer 有什么区别？String 的 intern() 方法有什么含义？String 类型在 JVM（Java 虚拟机）中是如何存储的？编译器对 String 做了哪些优化？ 接下来我们一起来看这些问题的答案。 知识扩展 1. == 和 equals 的区别 == 对于基本数据类型来说，是用于比较 “值”是否相等的；而对于引用类型来说，是用于比较引用地址是否相同的。 查看源码我们可以知道 Object 中也有 equals() 方法，源码如下： public boolean equals(Object obj) { return (this == obj); } 可以看出，Object 中的 equals() 方法其实就是 ==，而 String 重写了 equals() 方法把它修改成比较两个字符串的值是否相等。 源码如下： public boolean equals(Object anObject) { // 对象引用相同直接返回 true if (this == anObject) { return true; } // 判断需要对比的值是否为 String 类型，如果不是则直接返回 false if (anObject instanceof String) { String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) { // 把两个字符串都转换为 char 数组对比 char v1[] = value; char v2[] = anotherString.value; int i = 0; // 循环比对两个字符串的每一个字符 while (n-- != 0) { // 如果其中有一个字符不相等就 true false，否则继续对比 if (v1[i] != v2[i]) return false; i++; } return true; } } return false; } 2. final 修饰的好处 从 String 类的源码我们可以看出 String 是被 final 修饰的不可继承类，源码如下： public final class String implements java.io.Serializable, Comparable, CharSequence { //...... } 那这样设计有什么好处呢？ Java 语言之父 James Gosling 的回答是，他会更倾向于使用 final，因为它能够缓存结果，当你在传参时不需要考虑谁会修改它的值；如果是可变类的话，则有可能需要重新拷贝出来一个新值进行传参，这样在性能上就会有一定的损失。 James Gosling 还说迫使 String 类设计成不可变的另一个原因是安全，当你在调用其他方法时，比如调用一些系统级操作指令之前，可能会有一系列校验，如果是可变类的话，可能在你校验过后，它的内部的值又被改变了，这样有可能会引起严重的系统崩溃问题，这是迫使 String 类设计成不可变类的一个重要原因。 总结来说，使用 final 修饰的第一个好处是安全；第二个好处是高效，以 JVM 中的字符串常量池来举例，如下两个变量： String s1 = \"java\"; String s2 = \"java\"; 只有字符串是不可变时，我们才能实现字符串常量池，字符串常量池可以为我们缓存字符串，提高程序的运行效率，如下图所示： 3. String 和 StringBuilder、StringBuffer 的区别 因为 String 类型是不可变的，所以在字符串拼接的时候如果使用 String 的话性能会很低，因此我们就需要使用另一个数据类型 StringBuffer，它提供了 append 和 insert 方法可用于字符串的拼接，它使用 synchronized 来保证线程安全，如下源码所示： @Override public synchronized StringBuffer append(Object obj) { toStringCache = null; super.append(String.valueOf(obj)); return this; } @Override public synchronized StringBuffer append(String str) { toStringCache = null; super.append(str); return this; } 因为它使用了 synchronized 来保证线程安全，所以性能不是很高，于是在 JDK 1.5 就有了 StringBuilder，它同样提供了 append 和 insert 的拼接方法，但它没有使用 synchronized 来修饰，因此在性能上要优于 StringBuffer，所以在非并发操作的环境下可使用 StringBuilder 来进行字符串拼接。 4. String 和 JVM String 常见的创建方式有两种，new String() 的方式和直接赋值的方式，直接赋值的方式会先去字符串常量池中查找是否已经有此值，如果有则把引用地址直接指向此值，否则会先在常量池中创建，然后再把引用指向此值；而 new String() 的方式一定会先在堆上创建一个字符串对象，然后再去常量池中查询此字符串的值是否已经存在，如果不存在会先在常量池中创建此字符串，然后把引用的值指向此字符串，如下代码所示： String s1 = new String(\"Java\"); String s2 = s1.intern(); String s3 = \"Java\"; System.out.println(s1 == s2); // false System.out.println(s2 == s3); // true 它们在 JVM 存储的位置，如下图所示： 小贴士：JDK 1.7 之后把永生代换成的元空间，把字符串常量池从方法区移到了 Java 堆上。 除此之外编译器还会对 String 字符串做一些优化，例如以下代码： String s1 = \"Ja\" + \"va\"; String s2 = \"Java\"; System.out.println(s1 == s2); 虽然 s1 拼接了多个字符串，但对比的结果却是 true，我们使用反编译工具，看到的结果如下： Compiled from \"StringExample.java\" public class com.lagou.interview.StringExample { public com.lagou.interview.StringExample(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.\"\":()V 4: return LineNumberTable: line 3: 0 public static void main(java.lang.String[]); Code: 0: ldc #2 // String Java 2: astore_1 3: ldc #2 // String Java 5: astore_2 6: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 9: aload_1 10: aload_2 11: if_acmpne 18 14: iconst_1 15: goto 19 18: iconst_0 19: invokevirtual #4 // Method java/io/PrintStream.println:(Z)V 22: return LineNumberTable: line 5: 0 line 6: 3 line 7: 6 line 8: 22 } 从编译代码 #2 可以看出，代码 \"Ja\"+\"va\" 被直接编译成了 \"Java\" ，因此 s1==s2 的结果才是 true，这就是编译器对字符串优化的结果。 小结 本课时从 String 的源码入手，重点讲了 String 的构造方法、equals() 方法和 compareTo() 方法，其中 equals() 重写了 Object 的 equals() 方法，把引用对比改成了字符串值对比，也介绍了 final 修饰 String 的好处，可以提高效率和增强安全性，同时我们还介绍了 String 和 JVM 的一些执行细节。 "},"base/线程/ThreadPoolExecutor.html":{"url":"base/线程/ThreadPoolExecutor.html","title":"ThreadPoolExecutor详解 ThreadPoolExecutor 的参数含义及源码执行流程","keywords":"","body":" 线程池是为了避免线程频繁的创建和销毁带来的性能消耗，而建立的一种池化技术，它是把已创建的线程放入“池”中，当有任务来临时就可以重用已有的线程，无需等待创建的过程，这样就可以有效提高程序的响应速度。但如果要说线程池的话一定离不开 ThreadPoolExecutor ，在阿里巴巴的《Java 开发手册》中是这样规定线程池的： 线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的读者更加明确线程池的运行规则，规避资源耗尽的风险。 说明：Executors 返回的线程池对象的弊端如下： FixedThreadPool 和 SingleThreadPool：允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。 CachedThreadPool 和 ScheduledThreadPool：允许的创建线程数量为 Integer.MAX_VALUE，可能会创建大量的线程，从而导致 OOM。 "},"base/线程/线程的状态-lg.html":{"url":"base/线程/线程的状态-lg.html","title":"线程的状态","keywords":"","body":" 线程的状态有哪些？它是如何工作的？ 线程的状态在 JDK 1.5 之后以枚举的方式被定义在 Thread 的源码中，它总共包含以下 6 个状态： NEW，新建状态，线程被创建出来，但尚未启动时的线程状态； RUNNABLE，就绪状态，表示可以运行的线程状态，它可能正在运行，或者是在排队等待操作系统给它分配 CPU 资源； BLOCKED，阻塞等待锁的线程状态，表示处于阻塞状态的线程正在等待监视器锁，比如等待执行 synchronized 代码块或者使用 synchronized 标记的方法； WAITING，等待状态，一个处于等待状态的线程正在等待另一个线程执行某个特定的动作，比如，一个线程调用了 Object.wait() 方法，那它就在等待另一个线程调用 Object.notify() 或 Object.notifyAll() 方法； TIMED_WAITING，计时等待状态，和等待状态（WAITING）类似，它只是多了超时时间，比如调用了有超时时间设置的方法 Object.wait(long timeout) 和 Thread.join(long timeout) 等这些方法时，它才会进入此状态； TERMINATED，终止状态，表示线程已经执行完成。 线程状态的源代码如下： public enum State { /** * 新建状态，线程被创建出来，但尚未启动时的线程状态 */ NEW, /** * 就绪状态，表示可以运行的线程状态，但它在排队等待来自操作系统的 CPU 资源 */ RUNNABLE, /** * 阻塞等待锁的线程状态，表示正在处于阻塞状态的线程 * 正在等待监视器锁，比如等待执行 synchronized 代码块或者 * 使用 synchronized 标记的方法 */ BLOCKED, /** * 等待状态，一个处于等待状态的线程正在等待另一个线程执行某个特定的动作。 * 例如，一个线程调用了 Object.wait() 它在等待另一个线程调用 * Object.notify() 或 Object.notifyAll() */ WAITING, /** * 计时等待状态，和等待状态 (WAITING) 类似，只是多了超时时间，比如 * 调用了有超时时间设置的方法 Object.wait(long timeout) 和 * Thread.join(long timeout) 就会进入此状态 */ TIMED_WAITING, /** * 终止状态，表示线程已经执行完成 */ } 线程的工作模式是，首先先要创建线程并指定线程需要执行的业务方法，然后再调用线程的 start() 方法，此时线程就从 NEW（新建）状态变成了 RUNNABLE（就绪）状态，此时线程会判断要执行的方法中有没有 synchronized 同步代码块，如果有并且其他线程也在使用此锁，那么线程就会变为 BLOCKED（阻塞等待）状态，当其他线程使用完此锁之后，线程会继续执行剩余的方法。 当遇到 Object.wait() 或 Thread.join() 方法时，线程会变为 WAITING（等待状态）状态，如果是带了超时时间的等待方法，那么线程会进入 TIMED_WAITING（计时等待）状态，当有其他线程执行了 notify() 或 notifyAll() 方法之后，线程被唤醒继续执行剩余的业务方法，直到方法执行完成为止，此时整个线程的流程就执行完了，执行流程如下图所示： 考点分析 线程一般会作为并发编程的起始问题，用于引出更多的关于并发编程的面试问题。当然对于线程的掌握程度也决定了你对并发编程的掌握程度，通常面试官还会问： BLOCKED（阻塞等待）和 WAITING（等待）有什么区别？- start() 方法和 run() 方法有什么区别？ - 线程的优先级有什么用？该如何设置？ - 线程的常用方法有哪些？ - 知识扩展 1.BLOCKED 和 WAITING 的区别 虽然 BLOCKED 和 WAITING 都有等待的含义，但二者有着本质的区别，首先它们状态形成的调用方法不同，其次 BLOCKED 可以理解为当前线程还处于活跃状态，只是在阻塞等待其他线程使用完某个锁资源；而 WAITING 则是因为自身调用了 Object.wait() 或着是 Thread.join() 又或者是 LockSupport.park() 而进入等待状态，只能等待其他线程执行某个特定的动作才能被继续唤醒，比如当线程因为调用了 Object.wait() 而进入 WAITING 状态之后，则需要等待另一个线程执行 Object.notify() 或 Object.notifyAll() 才能被唤醒。 2.start() 和 run() 的区别 首先从 Thread 源码来看，start() 方法属于 Thread 自身的方法，并且使用了 synchronized 来保证线程安全，源码如下： public synchronized void start() { // 状态验证，不等于 NEW 的状态会抛出异常 if (threadStatus != 0) throw new IllegalThreadStateException(); // 通知线程组，此线程即将启动 group.add(this); boolean started = false; try { start0(); started = true; } finally { try { if (!started) { group.threadStartFailed(this); } } catch (Throwable ignore) { // 不处理任何异常，如果 start0 抛出异常，则它将被传递到调用堆栈上 } } } run() 方法为 Runnable 的抽象方法，必须由调用类重写此方法，重写的 run() 方法其实就是此线程要执行的业务方法，源码如下： public class Thread implements Runnable { // 忽略其他方法...... private Runnable target; @Override public void run() { if (target != null) { target.run(); } } } @FunctionalInterface public interface Runnable { public abstract void run(); } 从执行的效果来说，start() 方法可以开启多线程，让线程从 NEW 状态转换成 RUNNABLE 状态，而 run() 方法只是一个普通的方法。 其次，它们可调用的次数不同，start() 方法不能被多次调用，否则会抛出 java.lang.IllegalStateException；而 run() 方法可以进行多次调用，因为它只是一个普通的方法而已。 测试start方法多次调用 package com.view.Thread; /** * */ public class ThreadTest2 { public static void main(String args[]) { Thread1 thread1 = new Thread1(); thread1.start(); thread1.start();//多次调用抛出java.lang.IllegalThreadStateException } static class Thread1 extends Thread { @Override public void run() { System.out.println(\"执行run方法\"); } } } 结果 \"D:\\Program Files\\Java8\\jdk1.8.0_77\\bin\\java.exe\" \"-javaagent:D:\\Program Files\\JetBrains\\IntelliJ IDEA 2019.3.4\\lib\\idea_rt.jar=9573:D:\\Program Files\\JetBrains\\IntelliJ IDEA 2019.3.4\\bin\" -Dfile.encoding=UTF-8 -classpath \"D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\charsets.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\deploy.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\ext\\access-bridge-64.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\ext\\cldrdata.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\ext\\dnsns.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\ext\\jaccess.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\ext\\jfxrt.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\ext\\localedata.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\ext\\nashorn.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\ext\\sunec.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\ext\\sunjce_provider.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\ext\\sunmscapi.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\ext\\sunpkcs11.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\ext\\zipfs.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\javaws.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\jce.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\jfr.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\jfxswt.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\jsse.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\management-agent.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\plugin.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\resources.jar;D:\\Program Files\\Java8\\jdk1.8.0_77\\jre\\lib\\rt.jar;D:\\idea-workspace3\\lqx-project-demo-github\\basic-view\\target\\classes;D:\\idea-workspace3\\lqx-project-demo-github\\basic-server\\target\\classes;D:\\idea-workspace3\\lqx-project-demo-github\\demo\\search\\elasticsearch-starter\\target\\classes;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot-configuration-processor\\2.1.6.RELEASE\\spring-boot-configuration-processor-2.1.6.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot-starter-json\\2.1.6.RELEASE\\spring-boot-starter-json-2.1.6.RELEASE.jar;D:\\maven\\repository\\com\\fasterxml\\jackson\\datatype\\jackson-datatype-jdk8\\2.9.9\\jackson-datatype-jdk8-2.9.9.jar;D:\\maven\\repository\\com\\fasterxml\\jackson\\datatype\\jackson-datatype-jsr310\\2.9.9\\jackson-datatype-jsr310-2.9.9.jar;D:\\maven\\repository\\com\\fasterxml\\jackson\\module\\jackson-module-parameter-names\\2.9.9\\jackson-module-parameter-names-2.9.9.jar;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot-starter-data-jpa\\2.1.6.RELEASE\\spring-boot-starter-data-jpa-2.1.6.RELEASE.jar;D:\\maven\\repository\\javax\\transaction\\javax.transaction-api\\1.3\\javax.transaction-api-1.3.jar;D:\\maven\\repository\\org\\hibernate\\hibernate-core\\5.3.10.Final\\hibernate-core-5.3.10.Final.jar;D:\\maven\\repository\\antlr\\antlr\\2.7.7\\antlr-2.7.7.jar;D:\\maven\\repository\\org\\jboss\\jandex\\2.0.5.Final\\jandex-2.0.5.Final.jar;D:\\maven\\repository\\org\\dom4j\\dom4j\\2.1.1\\dom4j-2.1.1.jar;D:\\maven\\repository\\org\\hibernate\\common\\hibernate-commons-annotations\\5.0.4.Final\\hibernate-commons-annotations-5.0.4.Final.jar;D:\\maven\\repository\\org\\springframework\\data\\spring-data-jpa\\2.1.9.RELEASE\\spring-data-jpa-2.1.9.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\spring-orm\\5.1.8.RELEASE\\spring-orm-5.1.8.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\spring-aspects\\5.1.8.RELEASE\\spring-aspects-5.1.8.RELEASE.jar;D:\\maven\\repository\\commons-codec\\commons-codec\\1.12\\commons-codec-1.12.jar;D:\\maven\\repository\\io\\springfox\\springfox-swagger2\\2.7.0\\springfox-swagger2-2.7.0.jar;D:\\maven\\repository\\io\\swagger\\swagger-annotations\\1.5.13\\swagger-annotations-1.5.13.jar;D:\\maven\\repository\\io\\swagger\\swagger-models\\1.5.13\\swagger-models-1.5.13.jar;D:\\maven\\repository\\io\\springfox\\springfox-spi\\2.7.0\\springfox-spi-2.7.0.jar;D:\\maven\\repository\\io\\springfox\\springfox-core\\2.7.0\\springfox-core-2.7.0.jar;D:\\maven\\repository\\io\\springfox\\springfox-schema\\2.7.0\\springfox-schema-2.7.0.jar;D:\\maven\\repository\\io\\springfox\\springfox-swagger-common\\2.7.0\\springfox-swagger-common-2.7.0.jar;D:\\maven\\repository\\io\\springfox\\springfox-spring-web\\2.7.0\\springfox-spring-web-2.7.0.jar;D:\\maven\\repository\\org\\reflections\\reflections\\0.9.11\\reflections-0.9.11.jar;D:\\maven\\repository\\com\\google\\guava\\guava\\18.0\\guava-18.0.jar;D:\\maven\\repository\\com\\fasterxml\\classmate\\1.4.0\\classmate-1.4.0.jar;D:\\maven\\repository\\org\\springframework\\plugin\\spring-plugin-core\\1.2.0.RELEASE\\spring-plugin-core-1.2.0.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\plugin\\spring-plugin-metadata\\1.2.0.RELEASE\\spring-plugin-metadata-1.2.0.RELEASE.jar;D:\\maven\\repository\\org\\mapstruct\\mapstruct\\1.1.0.Final\\mapstruct-1.1.0.Final.jar;D:\\maven\\repository\\io\\springfox\\springfox-swagger-ui\\2.7.0\\springfox-swagger-ui-2.7.0.jar;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot-starter-log4j\\1.3.5.RELEASE\\spring-boot-starter-log4j-1.3.5.RELEASE.jar;D:\\maven\\repository\\log4j\\log4j\\1.2.17\\log4j-1.2.17.jar;D:\\maven\\repository\\com\\alibaba\\fastjson\\1.2.44\\fastjson-1.2.44.jar;D:\\maven\\repository\\org\\apache\\httpcomponents\\httpclient\\4.4.1\\httpclient-4.4.1.jar;D:\\maven\\repository\\org\\apache\\httpcomponents\\httpcore\\4.4.11\\httpcore-4.4.11.jar;D:\\maven\\repository\\com\\google\\code\\gson\\gson\\2.8.5\\gson-2.8.5.jar;D:\\maven\\repository\\org\\elasticsearch\\elasticsearch\\6.2.4\\elasticsearch-6.2.4.jar;D:\\maven\\repository\\org\\elasticsearch\\elasticsearch-core\\6.2.4\\elasticsearch-core-6.2.4.jar;D:\\maven\\repository\\org\\apache\\lucene\\lucene-core\\7.2.1\\lucene-core-7.2.1.jar;D:\\maven\\repository\\org\\apache\\lucene\\lucene-analyzers-common\\7.2.1\\lucene-analyzers-common-7.2.1.jar;D:\\maven\\repository\\org\\apache\\lucene\\lucene-backward-codecs\\7.2.1\\lucene-backward-codecs-7.2.1.jar;D:\\maven\\repository\\org\\apache\\lucene\\lucene-grouping\\7.2.1\\lucene-grouping-7.2.1.jar;D:\\maven\\repository\\org\\apache\\lucene\\lucene-highlighter\\7.2.1\\lucene-highlighter-7.2.1.jar;D:\\maven\\repository\\org\\apache\\lucene\\lucene-join\\7.2.1\\lucene-join-7.2.1.jar;D:\\maven\\repository\\org\\apache\\lucene\\lucene-memory\\7.2.1\\lucene-memory-7.2.1.jar;D:\\maven\\repository\\org\\apache\\lucene\\lucene-misc\\7.2.1\\lucene-misc-7.2.1.jar;D:\\maven\\repository\\org\\apache\\lucene\\lucene-queries\\7.2.1\\lucene-queries-7.2.1.jar;D:\\maven\\repository\\org\\apache\\lucene\\lucene-queryparser\\7.2.1\\lucene-queryparser-7.2.1.jar;D:\\maven\\repository\\org\\apache\\lucene\\lucene-sandbox\\7.2.1\\lucene-sandbox-7.2.1.jar;D:\\maven\\repository\\org\\apache\\lucene\\lucene-spatial\\7.2.1\\lucene-spatial-7.2.1.jar;D:\\maven\\repository\\org\\apache\\lucene\\lucene-spatial-extras\\7.2.1\\lucene-spatial-extras-7.2.1.jar;D:\\maven\\repository\\org\\apache\\lucene\\lucene-spatial3d\\7.2.1\\lucene-spatial3d-7.2.1.jar;D:\\maven\\repository\\org\\apache\\lucene\\lucene-suggest\\7.2.1\\lucene-suggest-7.2.1.jar;D:\\maven\\repository\\org\\elasticsearch\\securesm\\1.2\\securesm-1.2.jar;D:\\maven\\repository\\org\\elasticsearch\\elasticsearch-cli\\6.2.4\\elasticsearch-cli-6.2.4.jar;D:\\maven\\repository\\net\\sf\\jopt-simple\\jopt-simple\\5.0.2\\jopt-simple-5.0.2.jar;D:\\maven\\repository\\com\\carrotsearch\\hppc\\0.7.1\\hppc-0.7.1.jar;D:\\maven\\repository\\joda-time\\joda-time\\2.10.2\\joda-time-2.10.2.jar;D:\\maven\\repository\\com\\fasterxml\\jackson\\dataformat\\jackson-dataformat-smile\\2.9.9\\jackson-dataformat-smile-2.9.9.jar;D:\\maven\\repository\\com\\fasterxml\\jackson\\dataformat\\jackson-dataformat-cbor\\2.9.9\\jackson-dataformat-cbor-2.9.9.jar;D:\\maven\\repository\\com\\tdunning\\t-digest\\3.0\\t-digest-3.0.jar;D:\\maven\\repository\\org\\hdrhistogram\\HdrHistogram\\2.1.9\\HdrHistogram-2.1.9.jar;D:\\maven\\repository\\org\\apache\\logging\\log4j\\log4j-api\\2.11.2\\log4j-api-2.11.2.jar;D:\\maven\\repository\\org\\elasticsearch\\jna\\4.5.1\\jna-4.5.1.jar;D:\\maven\\repository\\org\\elasticsearch\\client\\rest\\6.0.0-alpha1\\rest-6.0.0-alpha1.jar;D:\\maven\\repository\\org\\apache\\httpcomponents\\httpasyncclient\\4.1.4\\httpasyncclient-4.1.4.jar;D:\\maven\\repository\\org\\apache\\httpcomponents\\httpcore-nio\\4.4.11\\httpcore-nio-4.4.11.jar;D:\\maven\\repository\\org\\elasticsearch\\client\\elasticsearch-rest-high-level-client\\6.2.4\\elasticsearch-rest-high-level-client-6.2.4.jar;D:\\maven\\repository\\org\\elasticsearch\\client\\elasticsearch-rest-client\\6.2.4\\elasticsearch-rest-client-6.2.4.jar;D:\\maven\\repository\\org\\elasticsearch\\plugin\\parent-join-client\\6.2.4\\parent-join-client-6.2.4.jar;D:\\maven\\repository\\org\\elasticsearch\\plugin\\aggs-matrix-stats-client\\6.2.4\\aggs-matrix-stats-client-6.2.4.jar;D:\\maven\\repository\\org\\elasticsearch\\plugin\\rank-eval-client\\6.2.4\\rank-eval-client-6.2.4.jar;D:\\maven\\repository\\org\\elasticsearch\\client\\transport\\6.2.4\\transport-6.2.4.jar;D:\\maven\\repository\\org\\elasticsearch\\plugin\\reindex-client\\6.2.4\\reindex-client-6.2.4.jar;D:\\maven\\repository\\org\\elasticsearch\\plugin\\lang-mustache-client\\6.2.4\\lang-mustache-client-6.2.4.jar;D:\\maven\\repository\\com\\github\\spullara\\mustache\\java\\compiler\\0.9.3\\compiler-0.9.3.jar;D:\\maven\\repository\\org\\elasticsearch\\plugin\\percolator-client\\6.2.4\\percolator-client-6.2.4.jar;D:\\maven\\repository\\org\\elasticsearch\\plugin\\transport-netty4-client\\6.2.4\\transport-netty4-client-6.2.4.jar;D:\\maven\\repository\\io\\netty\\netty-codec-http\\4.1.36.Final\\netty-codec-http-4.1.36.Final.jar;D:\\maven\\repository\\org\\locationtech\\spatial4j\\spatial4j\\0.6\\spatial4j-0.6.jar;D:\\maven\\repository\\com\\vividsolutions\\jts\\1.13\\jts-1.13.jar;D:\\maven\\repository\\org\\apache\\logging\\log4j\\log4j-core\\2.11.2\\log4j-core-2.11.2.jar;D:\\maven\\repository\\org\\ansj\\ansj_seg\\5.1.1\\ansj_seg-5.1.1.jar;D:\\maven\\repository\\org\\nlpcn\\nlp-lang\\1.7.2\\nlp-lang-1.7.2.jar;D:\\maven\\repository\\org\\apache\\httpcomponents\\httpmime\\4.5.6\\httpmime-4.5.6.jar;D:\\maven\\repository\\org\\jsoup\\jsoup\\1.11.2\\jsoup-1.11.2.jar;D:\\maven\\repository\\javax\\servlet\\javax.servlet-api\\4.0.1\\javax.servlet-api-4.0.1.jar;D:\\maven\\repository\\javax\\persistence\\javax.persistence-api\\2.2\\javax.persistence-api-2.2.jar;D:\\maven\\repository\\org\\postgresql\\postgresql\\42.2.2\\postgresql-42.2.2.jar;D:\\maven\\repository\\org\\apache\\shiro\\shiro-core\\1.4.0\\shiro-core-1.4.0.jar;D:\\maven\\repository\\org\\apache\\shiro\\shiro-lang\\1.4.0\\shiro-lang-1.4.0.jar;D:\\maven\\repository\\org\\apache\\shiro\\shiro-cache\\1.4.0\\shiro-cache-1.4.0.jar;D:\\maven\\repository\\org\\apache\\shiro\\shiro-crypto-hash\\1.4.0\\shiro-crypto-hash-1.4.0.jar;D:\\maven\\repository\\org\\apache\\shiro\\shiro-crypto-core\\1.4.0\\shiro-crypto-core-1.4.0.jar;D:\\maven\\repository\\org\\apache\\shiro\\shiro-crypto-cipher\\1.4.0\\shiro-crypto-cipher-1.4.0.jar;D:\\maven\\repository\\org\\apache\\shiro\\shiro-config-core\\1.4.0\\shiro-config-core-1.4.0.jar;D:\\maven\\repository\\org\\apache\\shiro\\shiro-config-ogdl\\1.4.0\\shiro-config-ogdl-1.4.0.jar;D:\\maven\\repository\\commons-beanutils\\commons-beanutils\\1.9.3\\commons-beanutils-1.9.3.jar;D:\\maven\\repository\\org\\apache\\shiro\\shiro-event\\1.4.0\\shiro-event-1.4.0.jar;D:\\maven\\repository\\org\\apache\\shiro\\shiro-web\\1.4.0\\shiro-web-1.4.0.jar;D:\\maven\\repository\\org\\apache\\shiro\\shiro-spring\\1.4.0\\shiro-spring-1.4.0.jar;D:\\maven\\repository\\org\\apache\\shiro\\shiro-ehcache\\1.4.0\\shiro-ehcache-1.4.0.jar;D:\\maven\\repository\\net\\sf\\ehcache\\ehcache-core\\2.6.11\\ehcache-core-2.6.11.jar;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot-starter-amqp\\2.1.6.RELEASE\\spring-boot-starter-amqp-2.1.6.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\spring-messaging\\5.1.8.RELEASE\\spring-messaging-5.1.8.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\spring-beans\\5.1.8.RELEASE\\spring-beans-5.1.8.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\amqp\\spring-rabbit\\2.1.7.RELEASE\\spring-rabbit-2.1.7.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\amqp\\spring-amqp\\2.1.7.RELEASE\\spring-amqp-2.1.7.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\retry\\spring-retry\\1.2.4.RELEASE\\spring-retry-1.2.4.RELEASE.jar;D:\\maven\\repository\\com\\rabbitmq\\amqp-client\\5.4.3\\amqp-client-5.4.3.jar;D:\\maven\\repository\\org\\apache\\poi\\poi\\4.0.0\\poi-4.0.0.jar;D:\\maven\\repository\\org\\apache\\poi\\poi-ooxml\\4.0.0\\poi-ooxml-4.0.0.jar;D:\\maven\\repository\\org\\apache\\poi\\poi-ooxml-schemas\\4.0.0\\poi-ooxml-schemas-4.0.0.jar;D:\\maven\\repository\\org\\apache\\xmlbeans\\xmlbeans\\3.0.1\\xmlbeans-3.0.1.jar;D:\\maven\\repository\\org\\apache\\commons\\commons-compress\\1.18\\commons-compress-1.18.jar;D:\\maven\\repository\\com\\github\\virtuald\\curvesapi\\1.04\\curvesapi-1.04.jar;D:\\maven\\repository\\com\\xuxueli\\xxl-job-core\\1.8.2\\xxl-job-core-1.8.2.jar;D:\\maven\\repository\\javax\\servlet\\jsp\\jsp-api\\2.2\\jsp-api-2.2.jar;D:\\maven\\repository\\org\\eclipse\\jetty\\jetty-server\\9.4.19.v20190610\\jetty-server-9.4.19.v20190610.jar;D:\\maven\\repository\\org\\eclipse\\jetty\\jetty-http\\9.4.19.v20190610\\jetty-http-9.4.19.v20190610.jar;D:\\maven\\repository\\org\\eclipse\\jetty\\jetty-util\\9.4.19.v20190610\\jetty-util-9.4.19.v20190610.jar;D:\\maven\\repository\\org\\eclipse\\jetty\\jetty-io\\9.4.19.v20190610\\jetty-io-9.4.19.v20190610.jar;D:\\maven\\repository\\com\\caucho\\hessian\\4.0.38\\hessian-4.0.38.jar;D:\\maven\\repository\\org\\codehaus\\jackson\\jackson-mapper-asl\\1.9.13\\jackson-mapper-asl-1.9.13.jar;D:\\maven\\repository\\org\\codehaus\\jackson\\jackson-core-asl\\1.9.13\\jackson-core-asl-1.9.13.jar;D:\\maven\\repository\\org\\springframework\\spring-context\\5.1.8.RELEASE\\spring-context-5.1.8.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\spring-expression\\5.1.8.RELEASE\\spring-expression-5.1.8.RELEASE.jar;D:\\maven\\repository\\org\\codehaus\\groovy\\groovy-all\\2.4.5\\groovy-all-2.4.5.jar;D:\\maven\\repository\\org\\apache\\commons\\commons-exec\\1.3\\commons-exec-1.3.jar;D:\\maven\\repository\\com\\belerweb\\pinyin4j\\2.5.0\\pinyin4j-2.5.0.jar;D:\\maven\\repository\\org\\redisson\\redisson\\3.11.2\\redisson-3.11.2.jar;D:\\maven\\repository\\io\\netty\\netty-common\\4.1.36.Final\\netty-common-4.1.36.Final.jar;D:\\maven\\repository\\io\\netty\\netty-codec\\4.1.36.Final\\netty-codec-4.1.36.Final.jar;D:\\maven\\repository\\io\\netty\\netty-buffer\\4.1.36.Final\\netty-buffer-4.1.36.Final.jar;D:\\maven\\repository\\io\\netty\\netty-transport\\4.1.36.Final\\netty-transport-4.1.36.Final.jar;D:\\maven\\repository\\io\\netty\\netty-resolver\\4.1.36.Final\\netty-resolver-4.1.36.Final.jar;D:\\maven\\repository\\io\\netty\\netty-resolver-dns\\4.1.36.Final\\netty-resolver-dns-4.1.36.Final.jar;D:\\maven\\repository\\io\\netty\\netty-codec-dns\\4.1.36.Final\\netty-codec-dns-4.1.36.Final.jar;D:\\maven\\repository\\io\\netty\\netty-handler\\4.1.36.Final\\netty-handler-4.1.36.Final.jar;D:\\maven\\repository\\javax\\cache\\cache-api\\1.1.1\\cache-api-1.1.1.jar;D:\\maven\\repository\\io\\projectreactor\\reactor-core\\3.2.10.RELEASE\\reactor-core-3.2.10.RELEASE.jar;D:\\maven\\repository\\org\\reactivestreams\\reactive-streams\\1.0.2\\reactive-streams-1.0.2.jar;D:\\maven\\repository\\io\\reactivex\\rxjava2\\rxjava\\2.2.9\\rxjava-2.2.9.jar;D:\\maven\\repository\\de\\ruedigermoeller\\fst\\2.57\\fst-2.57.jar;D:\\maven\\repository\\org\\javassist\\javassist\\3.21.0-GA\\javassist-3.21.0-GA.jar;D:\\maven\\repository\\org\\objenesis\\objenesis\\2.5.1\\objenesis-2.5.1.jar;D:\\maven\\repository\\com\\fasterxml\\jackson\\dataformat\\jackson-dataformat-yaml\\2.9.9\\jackson-dataformat-yaml-2.9.9.jar;D:\\maven\\repository\\com\\fasterxml\\jackson\\core\\jackson-core\\2.9.9\\jackson-core-2.9.9.jar;D:\\maven\\repository\\com\\fasterxml\\jackson\\core\\jackson-databind\\2.9.9\\jackson-databind-2.9.9.jar;D:\\maven\\repository\\com\\fasterxml\\jackson\\core\\jackson-annotations\\2.9.0\\jackson-annotations-2.9.0.jar;D:\\maven\\repository\\net\\bytebuddy\\byte-buddy\\1.9.13\\byte-buddy-1.9.13.jar;D:\\maven\\repository\\org\\jodd\\jodd-bean\\5.0.10\\jodd-bean-5.0.10.jar;D:\\maven\\repository\\org\\jodd\\jodd-core\\5.0.10\\jodd-core-5.0.10.jar;D:\\maven\\repository\\org\\redisson\\redisson-spring-boot-starter\\3.11.2\\redisson-spring-boot-starter-3.11.2.jar;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot-starter-actuator\\2.1.6.RELEASE\\spring-boot-starter-actuator-2.1.6.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot-actuator-autoconfigure\\2.1.6.RELEASE\\spring-boot-actuator-autoconfigure-2.1.6.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot-actuator\\2.1.6.RELEASE\\spring-boot-actuator-2.1.6.RELEASE.jar;D:\\maven\\repository\\io\\micrometer\\micrometer-core\\1.1.5\\micrometer-core-1.1.5.jar;D:\\maven\\repository\\org\\latencyutils\\LatencyUtils\\2.0.3\\LatencyUtils-2.0.3.jar;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot-starter-web\\2.1.6.RELEASE\\spring-boot-starter-web-2.1.6.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot-starter-tomcat\\2.1.6.RELEASE\\spring-boot-starter-tomcat-2.1.6.RELEASE.jar;D:\\maven\\repository\\org\\apache\\tomcat\\embed\\tomcat-embed-core\\9.0.21\\tomcat-embed-core-9.0.21.jar;D:\\maven\\repository\\org\\apache\\tomcat\\embed\\tomcat-embed-el\\9.0.21\\tomcat-embed-el-9.0.21.jar;D:\\maven\\repository\\org\\apache\\tomcat\\embed\\tomcat-embed-websocket\\9.0.21\\tomcat-embed-websocket-9.0.21.jar;D:\\maven\\repository\\org\\hibernate\\validator\\hibernate-validator\\6.0.17.Final\\hibernate-validator-6.0.17.Final.jar;D:\\maven\\repository\\javax\\validation\\validation-api\\2.0.1.Final\\validation-api-2.0.1.Final.jar;D:\\maven\\repository\\org\\jboss\\logging\\jboss-logging\\3.3.2.Final\\jboss-logging-3.3.2.Final.jar;D:\\maven\\repository\\org\\springframework\\spring-web\\5.1.8.RELEASE\\spring-web-5.1.8.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\spring-webmvc\\5.1.8.RELEASE\\spring-webmvc-5.1.8.RELEASE.jar;D:\\maven\\repository\\org\\redisson\\redisson-spring-data-21\\3.11.2\\redisson-spring-data-21-3.11.2.jar;D:\\maven\\repository\\redis\\clients\\jedis\\3.0.1\\jedis-3.0.1.jar;D:\\maven\\repository\\org\\apache\\commons\\commons-pool2\\2.6.2\\commons-pool2-2.6.2.jar;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot-starter-data-redis\\2.1.6.RELEASE\\spring-boot-starter-data-redis-2.1.6.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\data\\spring-data-redis\\2.1.9.RELEASE\\spring-data-redis-2.1.9.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\data\\spring-data-keyvalue\\2.1.9.RELEASE\\spring-data-keyvalue-2.1.9.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\data\\spring-data-commons\\2.1.9.RELEASE\\spring-data-commons-2.1.9.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\spring-tx\\5.1.8.RELEASE\\spring-tx-5.1.8.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\spring-oxm\\5.1.8.RELEASE\\spring-oxm-5.1.8.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\spring-aop\\5.1.8.RELEASE\\spring-aop-5.1.8.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\spring-context-support\\5.1.8.RELEASE\\spring-context-support-5.1.8.RELEASE.jar;D:\\maven\\repository\\io\\lettuce\\lettuce-core\\5.1.7.RELEASE\\lettuce-core-5.1.7.RELEASE.jar;D:\\maven\\repository\\com\\microsoft\\sqlserver\\mssql-jdbc\\6.1.0.jre8\\mssql-jdbc-6.1.0.jre8.jar;D:\\maven\\repository\\com\\microsoft\\azure\\azure-keyvault\\0.9.3\\azure-keyvault-0.9.3.jar;D:\\maven\\repository\\com\\microsoft\\azure\\azure-core\\0.9.3\\azure-core-0.9.3.jar;D:\\maven\\repository\\commons-lang\\commons-lang\\2.6\\commons-lang-2.6.jar;D:\\maven\\repository\\javax\\mail\\mail\\1.4.5\\mail-1.4.5.jar;D:\\maven\\repository\\javax\\activation\\activation\\1.1\\activation-1.1.jar;D:\\maven\\repository\\com\\sun\\jersey\\jersey-client\\1.13\\jersey-client-1.13.jar;D:\\maven\\repository\\com\\sun\\jersey\\jersey-core\\1.13\\jersey-core-1.13.jar;D:\\maven\\repository\\com\\sun\\jersey\\jersey-json\\1.13\\jersey-json-1.13.jar;D:\\maven\\repository\\com\\sun\\xml\\bind\\jaxb-impl\\2.2.3-1\\jaxb-impl-2.2.3-1.jar;D:\\maven\\repository\\org\\codehaus\\jackson\\jackson-jaxrs\\1.9.2\\jackson-jaxrs-1.9.2.jar;D:\\maven\\repository\\org\\codehaus\\jackson\\jackson-xc\\1.9.2\\jackson-xc-1.9.2.jar;D:\\maven\\repository\\javax\\inject\\javax.inject\\1\\javax.inject-1.jar;D:\\maven\\repository\\com\\microsoft\\azure\\adal4j\\1.0.0\\adal4j-1.0.0.jar;D:\\maven\\repository\\com\\nimbusds\\oauth2-oidc-sdk\\4.5\\oauth2-oidc-sdk-4.5.jar;D:\\maven\\repository\\net\\jcip\\jcip-annotations\\1.0\\jcip-annotations-1.0.jar;D:\\maven\\repository\\com\\nimbusds\\lang-tag\\1.4\\lang-tag-1.4.jar;D:\\maven\\repository\\com\\nimbusds\\nimbus-jose-jwt\\3.1.2\\nimbus-jose-jwt-3.1.2.jar;D:\\maven\\repository\\org\\bouncycastle\\bcprov-jdk15on\\1.51\\bcprov-jdk15on-1.51.jar;D:\\maven\\repository\\mysql\\mysql-connector-java\\5.1.41\\mysql-connector-java-5.1.41.jar;D:\\maven\\repository\\org\\slf4j\\slf4j-api\\1.7.26\\slf4j-api-1.7.26.jar;D:\\maven\\repository\\org\\slf4j\\jcl-over-slf4j\\1.7.26\\jcl-over-slf4j-1.7.26.jar;D:\\maven\\repository\\org\\slf4j\\jul-to-slf4j\\1.7.26\\jul-to-slf4j-1.7.26.jar;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot-starter\\2.1.6.RELEASE\\spring-boot-starter-2.1.6.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot\\2.1.6.RELEASE\\spring-boot-2.1.6.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot-autoconfigure\\2.1.6.RELEASE\\spring-boot-autoconfigure-2.1.6.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot-starter-logging\\2.1.6.RELEASE\\spring-boot-starter-logging-2.1.6.RELEASE.jar;D:\\maven\\repository\\ch\\qos\\logback\\logback-classic\\1.2.3\\logback-classic-1.2.3.jar;D:\\maven\\repository\\ch\\qos\\logback\\logback-core\\1.2.3\\logback-core-1.2.3.jar;D:\\maven\\repository\\org\\apache\\logging\\log4j\\log4j-to-slf4j\\2.11.2\\log4j-to-slf4j-2.11.2.jar;D:\\maven\\repository\\javax\\annotation\\javax.annotation-api\\1.3.2\\javax.annotation-api-1.3.2.jar;D:\\maven\\repository\\org\\springframework\\spring-core\\5.1.8.RELEASE\\spring-core-5.1.8.RELEASE.jar;D:\\maven\\repository\\org\\springframework\\spring-jcl\\5.1.8.RELEASE\\spring-jcl-5.1.8.RELEASE.jar;D:\\maven\\repository\\org\\yaml\\snakeyaml\\1.23\\snakeyaml-1.23.jar;D:\\maven\\repository\\com\\baomidou\\mybatis-plus-boot-starter\\3.1.2\\mybatis-plus-boot-starter-3.1.2.jar;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot-starter-jdbc\\2.1.6.RELEASE\\spring-boot-starter-jdbc-2.1.6.RELEASE.jar;D:\\maven\\repository\\com\\zaxxer\\HikariCP\\3.2.0\\HikariCP-3.2.0.jar;D:\\maven\\repository\\org\\springframework\\spring-jdbc\\5.1.8.RELEASE\\spring-jdbc-5.1.8.RELEASE.jar;D:\\maven\\repository\\com\\baomidou\\mybatis-plus\\3.1.2\\mybatis-plus-3.1.2.jar;D:\\maven\\repository\\com\\baomidou\\mybatis-plus-generator\\3.1.2\\mybatis-plus-generator-3.1.2.jar;D:\\maven\\repository\\com\\alibaba\\druid\\1.1.18\\druid-1.1.18.jar;D:\\maven\\repository\\com\\alibaba\\druid-spring-boot-starter\\1.1.18\\druid-spring-boot-starter-1.1.18.jar;D:\\maven\\repository\\org\\json\\json\\20180130\\json-20180130.jar;D:\\maven\\repository\\org\\apache\\oltu\\oauth2\\org.apache.oltu.oauth2.client\\0.31\\org.apache.oltu.oauth2.client-0.31.jar;D:\\maven\\repository\\org\\apache\\oltu\\oauth2\\org.apache.oltu.oauth2.common\\0.31\\org.apache.oltu.oauth2.common-0.31.jar;D:\\maven\\repository\\org\\codehaus\\jettison\\jettison\\1.2\\jettison-1.2.jar;D:\\maven\\repository\\com\\squareup\\okhttp3\\okhttp\\3.8.1\\okhttp-3.8.1.jar;D:\\maven\\repository\\com\\squareup\\okio\\okio\\1.13.0\\okio-1.13.0.jar;D:\\maven\\repository\\org\\apache\\commons\\commons-collections4\\4.4\\commons-collections4-4.4.jar;D:\\maven\\repository\\org\\apache\\commons\\commons-vfs2\\2.1\\commons-vfs2-2.1.jar;D:\\maven\\repository\\commons-logging\\commons-logging\\1.2\\commons-logging-1.2.jar;D:\\maven\\repository\\org\\apache\\commons\\commons-lang3\\3.4\\commons-lang3-3.4.jar;D:\\maven\\repository\\commons-collections\\commons-collections\\3.1\\commons-collections-3.1.jar;D:\\maven\\repository\\commons-fileupload\\commons-fileupload\\1.3.1\\commons-fileupload-1.3.1.jar;D:\\maven\\repository\\commons-io\\commons-io\\2.2\\commons-io-2.2.jar;D:\\maven\\repository\\com\\baomidou\\mybatis-plus-extension\\3.1.2\\mybatis-plus-extension-3.1.2.jar;D:\\maven\\repository\\com\\baomidou\\mybatis-plus-core\\3.1.2\\mybatis-plus-core-3.1.2.jar;D:\\maven\\repository\\com\\baomidou\\mybatis-plus-annotation\\3.1.2\\mybatis-plus-annotation-3.1.2.jar;D:\\maven\\repository\\com\\github\\jsqlparser\\jsqlparser\\1.2\\jsqlparser-1.2.jar;D:\\maven\\repository\\org\\mybatis\\mybatis\\3.5.1\\mybatis-3.5.1.jar;D:\\maven\\repository\\org\\mybatis\\mybatis-spring\\2.0.1\\mybatis-spring-2.0.1.jar;D:\\maven\\repository\\com\\baomidou\\dynamic-datasource-spring-boot-starter\\2.5.5\\dynamic-datasource-spring-boot-starter-2.5.5.jar;D:\\maven\\repository\\org\\springframework\\boot\\spring-boot-starter-aop\\2.1.6.RELEASE\\spring-boot-starter-aop-2.1.6.RELEASE.jar;D:\\maven\\repository\\org\\aspectj\\aspectjweaver\\1.9.4\\aspectjweaver-1.9.4.jar;D:\\maven\\repository\\org\\projectlombok\\lombok\\1.16.10\\lombok-1.16.10.jar;D:\\maven\\repository\\net\\minidev\\json-smart\\2.3\\json-smart-2.3.jar;D:\\maven\\repository\\net\\minidev\\accessors-smart\\1.2\\accessors-smart-1.2.jar;D:\\maven\\repository\\org\\ow2\\asm\\asm\\5.0.4\\asm-5.0.4.jar;D:\\maven\\repository\\javax\\xml\\bind\\jaxb-api\\2.3.1\\jaxb-api-2.3.1.jar;D:\\maven\\repository\\javax\\activation\\javax.activation-api\\1.2.0\\javax.activation-api-1.2.0.jar\" com.view.Thread.ThreadTest2 Exception in thread \"main\" java.lang.IllegalThreadStateException at java.lang.Thread.start(Thread.java:705) at com.view.Thread.ThreadTest2.main(ThreadTest2.java:12) 执行run方法 Process finished with exit code 1 3.线程优先级 在 Thread 源码中和线程优先级相关的属性有 3 个： // 线程可以拥有的最小优先级 public final static int MIN_PRIORITY = 1; // 线程默认优先级 public final static int NORM_PRIORITY = 5; // 线程可以拥有的最大优先级 public final static int MAX_PRIORITY = 10 线程的优先级可以理解为线程抢占 CPU 时间片的概率，优先级越高的线程优先执行的概率就越大，但并不能保证优先级高的线程一定先执行。 在程序中我们可以通过 Thread.setPriority() 来设置优先级，setPriority() 源码如下： public final void setPriority(int newPriority) { ThreadGroup g; checkAccess(); // 先验证优先级的合理性 if (newPriority > MAX_PRIORITY || newPriority g.getMaxPriority()) { newPriority = g.getMaxPriority(); } setPriority0(priority = newPriority); } } 4.线程的常用方法 线程的常用方法有以下几个。 （1）join() 在一个线程中调用 other.join() ，这时候当前线程会让出执行权给 other 线程，直到 other 线程执行完或者过了超时时间之后再继续执行当前线程，join() 源码如下： public final synchronized void join(long millis) throws InterruptedException { long base = System.currentTimeMillis(); long now = 0; // 超时时间不能小于 0 if (millis 从源码中可以看出 join() 方法底层还是通过 wait() 方法来实现的。 例如，在未使用 join() 时，代码如下： public class ThreadExample { public static void main(String[] args) throws InterruptedException { Thread thread = new Thread(() -> { for (int i = 1; i 程序执行结果为： 主线程睡眠：1秒。 子线程睡眠：1秒。 主线程睡眠：2秒。 子线程睡眠：2秒。 主线程睡眠：3秒。 子线程睡眠：3秒。 子线程睡眠：4秒。 子线程睡眠：5秒。 从结果可以看出，在未使用 join() 时主子线程会交替执行。 然后我们再把 join() 方法加入到代码中，代码如下： public class ThreadExample { public static void main(String[] args) throws InterruptedException { Thread thread = new Thread(() -> { for (int i = 1; i 程序执行结果为： 子线程睡眠：1秒。 子线程睡眠：2秒。 主线程睡眠：1秒。 // thread.join(2000); 等待 2 秒之后，主线程和子线程再交替执行 子线程睡眠：3秒。 主线程睡眠：2秒。 子线程睡眠：4秒。 子线程睡眠：5秒。 主线程睡眠：3秒。 从执行结果可以看出，添加 join() 方法之后，主线程会先等子线程执行 2 秒之后才继续执行。 （2）yield() 看 Thread 的源码可以知道 yield() 为本地方法，也就是说 yield() 是由 C 或 C++ 实现的，源码如下： public static native void yield(); yield() 方法表示给线程调度器一个当前线程愿意出让 CPU 使用权的暗示，但是线程调度器可能会忽略这个暗示。 比如我们执行这段包含了 yield() 方法的代码，如下所示： public static void main(String[] args) throws InterruptedException { Runnable runnable = new Runnable() { @Override public void run() { for (int i = 0; i 当我们把这段代码执行多次之后会发现，每次执行的结果都不相同，这是因为 yield() 执行非常不稳定，线程调度器不一定会采纳 yield() 出让 CPU 使用权的建议，从而导致了这样的结果。 小结 本课时我们介绍了线程的 6 种状态以及线程的执行流程，还介绍了 BLOCKED（阻塞等待）和 WAITING（等待）的区别，start() 方法和 run() 方法的区别，以及 join() 方法和 yield() 方法的作用，但我们不能死记硬背，要多动手实践才能真正的理解这些知识点。 "},"base/网络/http和tcp.html":{"url":"base/网络/http和tcp.html","title":"http和tcp","keywords":"","body":" "},"base/test.html":{"url":"base/test.html","title":"Test","keywords":"","body":" "},"base/基本类型装箱拆箱.html":{"url":"base/基本类型装箱拆箱.html","title":"集合基本类型装箱拆箱","keywords":"","body":" Integer 测试 //理论：整型8个字节，2的八次方共256（-128到127），这区间有整型缓存，就是Java的装箱拆箱 Integer a1 = 128; Integer a2 = 128; System.out.println(\"a1=128;a2=128;a1==a2：\" + (a1 == a2));//false 说明128之后是没有有缓存的 Integer b1 = 127; Integer b2 = 127; System.out.println(\"b1=127;b2=127;b1==b2：\" + (b1 == b2));//false 说明127之内有缓存(注意比较的是地址) int c1 = 130; Integer c2 = 130; System.out.println(\"int c1=129;c2=129;c1==c2：\" + (c1 == c2));//true 比较的是值，而不是地址 System.out.println(\"\"); Integer a11 = -128; Integer a22 = -128; System.out.println(\"a11=-128;a22=-128;a11==a22：\" + (a11 == a22));//true 说明-128之内有缓存(注意比较的是地址) Integer b11 = -127; Integer b22 = -127; System.out.println(\"b11=-127;b22=-127;b11==b22：\" + (b11 == b22));//true 说明-127之内有缓存(注意比较的是地址) Integer c11 = -130; Integer c22 = -130; System.out.println(\"c11=-130;c22=-130;c11==c22：\" + (c11 == c22));//false 比较的是值，而不是地址 System.out.println(Integer.SIZE); 装箱 * This method will always cache values in the range -128 to 127, * inclusive, and may cache other values outside of this range. * * @param i an {@code int} value. * @return an {@code Integer} instance representing {@code i}. * @since 1.5 */ public static Integer valueOf(int i) { if (i >= IntegerCache.low && i Integer a1 = 128;直接定义变量为某值，会调用valueOf进行装箱，也就是会返回IntegerCache.cache里的对象（在-128到127内则返回的对象相同） 或者new Integer对象Integer a2 = 128;判断a1==a2是对地址的比较,也就是2个对象的地址,在区间-128到127返回的对象相同，所以地址相同，在范围外则不同 问：a1存在哪里，128存在哪里 问：java中一个integer对象的内存占用是多少?可以通过java方法输出吗?. Integer类型所占内存是4个字节，也就是4B。 System.out.println(Integer.SIZE);//32 Integer源码 @Native public static final int SIZE = 32;//表示多少位,4字节共32位 public static final int BYTES = SIZE / Byte.SIZE;//Byte.SIZE=8,所以BYTES=4 Long 问：如下程序运行结果是什么？ Long l1 = 128L; Long l2 = 128L; System.out.print(l1 == l2); //1 System.out.print(l1 == 128L); //2 Long l3 = 127L; Long l4 = 127L; System.out.print(l3 == l4); //3 System.out.print(l3 == 127L); //4 答：对于注释 1 的语句，Long 包装类型常量 cache 为 -128 到 127 之间，所以 l1 和 l2 变量是两个对象，== 比较的是对象的地址，所以打印为 false。 对于注释 2 的语句，由于包装类型在表达式中且表达式中至少有一个不是包装类型，所以 Long l1 == 128L 中 l1 自动拆箱退化为基本类型比较，所以数值比较为 true。 对于注释 3 的语句，Long 包装类型 -128 到 127 之间的值维护在一个常量池中，所以 l3 和 l4 引用同一个对象，故打印 true。 对于注释 4 的语句类似注释 2 语句，所以打印为 true。 参考：https://mp.weixin.qq.com/s/akaEG8SPwlyaF2eXY_-AUA 问：下面程序的运行结果是什么？ Integer i1 = new Integer(127); Integer i2 = new Integer(127); System.out.println(i1 == i2); //false System.out.println(i1.equals(i2)); //true Integer i3 = new Integer(128); Integer i4 = new Integer(128); System.out.println(i3 == i4); //false System.out.println(i3.equals(i4)); //true Integer i5 = 128; Integer i6 = 128; System.out.println(i5 == i6); //false System.out.println(i5.equals(i6)); //true Integer i7 = 127; Integer i8 = 127; System.out.println(i7 == i8); //true System.out.println(i7.equals(i8)); //true System.out.println(i1 == i7); //false i1是new的，new出来的肯定是新的地址，而i7是Integer.cache缓存里的 答：答案如上注释所述，通过查看 Integer 的源码可以发现，针对 -128 到 127 之间的数据做了一个数据缓冲池，如果数据是该范围内的，每次并不创建新的对象，所以就有了上面的结果。 问：java 语句 Integer i = 1; i += 1; 做了哪些事情？ 答：首先 Integer i = 1; 做了自动装箱（使用 valueOf() 方法将 int 装箱为 Integer 类型），接着 i += 1; 先将 Integer 类型的 i 自动拆箱成 int（使用 intValue() 方法将 Integer 拆箱为 int），完成加法运行之后的 i 再装箱成 Integer 类型。 Integer i1 = 100; Integer i2 = 100; Integer i3 = 200; Integer i4 = 200; System.out.println(i1 == i2); //true System.out.println(i3 == i4); //false Double d1 = 100.0; Double d2 = 100.0; Double d3 = 200.0; Double d4 = 200.0; System.out.println(d1 == d2); //false System.out.println(d3 == d4); //false Boolean b1 = false; Boolean b2 = false; Boolean b3 = true; Boolean b4 = true; System.out.println(b1 == b2); //true System.out.println(b3 == b4); //true Integer a = 1; Integer b = 2; Integer c = 3; Integer d = 3; Integer e = 321; Integer f = 321; Long g = 3L; Long h = 2L; System.out.println(c == d); //true System.out.println(e == f); //false System.out.println(c == (a + b)); //true System.out.println(c.equals(a + b)); //true System.out.println(g == (a + b)); //true System.out.println(g.equals(a + b)); //false System.out.println(g.equals(a + h)); //true Integer a = 444; int b = 444; System.out.println(a==b); //true System.out.println(a.equals(b)); //true 答： 答案如上注释部分，核心考察点就是上道题的答案，即 java 1.5 开始的自动装箱拆箱机制其实是编译器自动完成的替换，装箱阶段自动替换为了 valueOf 方法，拆箱阶段自动替换为了 xxxValue 方法。 对于 Integer 类型的 valueOf 方法参数如果是 -128~127 之间的值会直接返回内部缓存池中已经存在对象的引用，参数是其他范围值则返回新建对象； 而 Double 类型与 Integer 类型一样会调用到 Double 的 valueOf 方法，但是 Double 的区别在于不管传入的参数值是多少都会 new 一个对象来表达该数值（因为在指定范围内浮点型数据个数是不确定的，整型等个数是确定的，所以可以 Cache）。 注意：Integer、Short、Byte、Character、Long 的 valueOf 方法实现类似，而 Double 和 Float 比较特殊，每次返回新包装对象。 对于两边都是包装类型的比较 == 比较的是引用，equals 比较的是值， 对于两边有一边是表达式（包含算数运算）则 == 比较的是数值（自动触发拆箱过程），对于包装类型 equals 方法不会进行类型转换。 参考：https://blog.csdn.net/weixin_42346767/article/details/85067398 "},"base/基本类型装箱拆箱2.html":{"url":"base/基本类型装箱拆箱2.html","title":"基本类型装箱拆箱2","keywords":"","body":" Integer源码 public boolean equals(Object obj) { if (obj instanceof Integer) { return value == ((Integer)obj).intValue(); } return false; } Long /** * Compares this object to the specified object. The result is * {@code true} if and only if the argument is not * {@code null} and is a {@code Long} object that * contains the same {@code long} value as this object. * * @param obj the object to compare with. * @return {@code true} if the objects are the same; * {@code false} otherwise. */ public boolean equals(Object obj) { if (obj instanceof Long) { return value == ((Long)obj).longValue(); } return false; } "},"designPatterns/spring/spring享元模式.html":{"url":"designPatterns/spring/spring享元模式.html","title":"spring享元模式","keywords":"","body":" 1、 享元模式的主要优点如下： 可以极大减少内存中对象的数量，使得相同或相似对象在内存中只保存一份，从而可以节约系统资源，提高系统性能。 享元模式的外部状态相对独立，而且不会影响其内部状态，从而使得享元对象可以在不同的环境中被共享。 享元模式的主要缺点如下： 享元模式使得系统变得复杂，需要分离出内部状态和外部状态，这使得程序的逻辑复杂化。 为了使对象可以共享，享元模式需要将享元对象的部分状态外部化，而读取外部状态将使得运行时间变长。 2、 角色 Flyweight（抽象享元类）：通常是一个接口或抽象类，在抽象享元类中声明了具体享元类公共的方法，这些方法可以向外界提供享元对象的内部数据（内部状态），同时也可以通过这些方法来设置外部数据（外部状态）。 ConcreteFlyweight（具体享元类）：它实现了抽象享元类，其实例称为享元对象；在具体享元类中为内部状态提供了存储空间。通常我们可以结合单例模式来设计具体享元类，为每一个具体享元类提供唯一的享元对象。 UnsharedConcreteFlyweight（非共享具体享元类）：并不是所有的抽象享元类的子类都需要被共享，不能被共享的子类可设计为非共享具体享元类；当需要一个非共享具体享元类的对象时可以直接通过实例化创建。 FlyweightFactory（享元工厂类）：享元工厂类用于创建并管理享元对象，它针对抽象享元类编程，将各种类型的具体享元对象存储在一个享元池中，享元池一般设计为一个存储“键值对”的集合（也可以是其他类型的集合），可以结合工厂模式进行设计；当用户请求一个具体享元对象时，享元工厂提供一个存储在享元池中已创建的实例或者创建一个新的实例（如果不存在的话），返回新创建的实例并将其存储在享元池中。 设计模式 | 享元模式及典型应用 3、 "},"designPatterns/spring/spring代理模式.html":{"url":"designPatterns/spring/spring代理模式.html","title":"spring代理模式","keywords":"","body":" 输出生成的代理类，参考：https://www.cnblogs.com/jhxxb/p/10557738.html https://yq.aliyun.com/articles/659721?spm=a2c4e.11155435.0.0.54dd5d5chL5eMQ "},"designPatterns/spring/spring策略模式.html":{"url":"designPatterns/spring/spring策略模式.html","title":"spring策略模式","keywords":"","body":" 定义 定义一组算法，将每个算法都封装起来，并且使它们之间可以互换。策略模式使这些算法在客户端调用它们的时候能够互不影响地变化。 返回值是固定的，中间逻辑不一样。例如从家出发去旅游，可以开车，做公交车，坐火车等。但是目的地是固定的。 阎宏博士的《JAVA与模式》一书中这样描述：中策略模式属于对象的行为模式。其用意是针对一组算法，将每一个算法封装到具有共同接口的独立的类中，从而使得它们可以相互替换。策略模式使得算法可以在不影响到客户端的情况下发生变化。 如何到达目的地就是算法，开车、坐公交、坐火车等相当于对算法的封装，他们同属于一个范畴叫做出行方式，而出行人是算法的调用者，并且可以根据需求选择算法，这就是将算法和职责的分离。 image 这个模式涉及三个角色： 环境角色（Context）：持有一个strategy的引用 抽象策略角色（Strategy）：所有策略类的顶级父类或是接口，定义策略中的方法 具体策略角色（ConcreteStrategy）：实现抽象策略中的方法，具体算法的实现者 List numbers = new ArrayList(); // todo 想list中添加内容 Collections.sort(numbers, new Comparator() { //返回值是固定的 //0 、-1 、1 //0 、 >0 、上面的例子中，Comparator就是策略的抽象，这里用了一个匿名内部类实现，而Collections.sort()方法测试集合工具类的一个职责——排序，而排序的逻辑由具体的策略来实现。 理解： 不变的,共同的地方：抽象到父类或者接口中， 会变的,具体的实现(具体策略)则放到子类或实现类中 参考 https://blog.csdn.net/u011659172/article/details/79140561 https://blog.csdn.net/hsj1213522415/article/details/81291741 https://blog.csdn.net/weisg81/article/details/59057661 https://www.jianshu.com/p/3ea48ecd7178 https://blog.csdn.net/weixin_34416754/article/details/89589194 http://laijianfeng.org/2018/10/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F%E5%8F%8A%E5%85%B8%E5%9E%8B%E5%BA%94%E7%94%A8/ "},"jvm/jvm内存模型.html":{"url":"jvm/jvm内存模型.html","title":"jvm内存模型","keywords":"","body":" https://www.jianshu.com/p/e86f83284a99 suvivor为什么任意时间中from和to中必须要有一个是空的 那为什么要使Suvivor From与Suvivor To交换角色？为了保证俩块Suvivor中永远有一块是空的，这样就会始终保证内存的连续性，不会产生内存碎片 老年代采用的是标记整理算法 "},"jvm/jvm参数.html":{"url":"jvm/jvm参数.html","title":"jvm参数","keywords":"","body":" https://blog.csdn.net/qq_43040688/article/details/105013526 Xms 起始内存 Xmx 最大内存 Xmn 新生代内存 Xss 栈大小。 就是创建线程后，分配给每一个线程的内存大小 -XX:NewRatio=n:设置年轻代和年老代的比值。如:为3，表示年轻代与年老代比值为1：3，年轻代占整个年轻代年老代和的1/4 -XX:SurvivorRatio=n:年轻代中Eden区与两个Survivor区的比值。注意Survivor区有两个。如：3，表示Eden：Survivor=3：2，一个Survivor区占整个年轻代的1/5 -XX:MaxPermSize=n:设置持久代大小 收集器设置 -XX:+UseSerialGC:设置串行收集器 -XX:+UseParallelGC:设置并行收集器 -XX:+UseParalledlOldGC:设置并行年老代收集器 -XX:+UseConcMarkSweepGC:设置并发收集器 垃圾回收统计信息 -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:filename 并行收集器设置 -XX:ParallelGCThreads=n:设置并行收集器收集时使用的CPU数。并行收集线程数。 -XX:MaxGCPauseMillis=n:设置并行收集最大暂停时间 -XX:GCTimeRatio=n:设置垃圾回收时间占程序运行时间的百分比。公式为1/(1+n) 并发收集器设置 -XX:+CMSIncrementalMode:设置为增量模式。适用于单CPU情况。 -XX:ParallelGCThreads=n:设置并发收集器年轻代收集方式为并行收集时，使用的CPU数。并行收集线程数 https://blog.csdn.net/qq_43040688/article/details/105013526 "},"mybatis/mybatis分页插件/mybatis插件引入.html":{"url":"mybatis/mybatis分页插件/mybatis插件引入.html","title":"mybatis插件引入","keywords":"","body":" "},"mybatis/mybatis分页插件/Page.html":{"url":"mybatis/mybatis分页插件/Page.html","title":"Page","keywords":"","body":"package com.hoby.pageutil; import java.util.Collections; import java.util.List; public class Page implements java.io.Serializable { private int pageShow = 10; private int totalPage; private int totalCount; private int start; private int nowPage; private List result = Collections.emptyList(); public Page() { } public Page(int pageShow, int nowPage) { this.pageShow = pageShow; this.nowPage = nowPage; } public int getStart() { start = (getNowPage() - 1) * getPageShow(); if (start getResult() { return result; } public void setResult(List result) { this.result = result; } public void setTotalPage(int totalPage) { this.totalPage = totalPage; } public void setNowPage(int nowPage) { this.nowPage = nowPage; } public int getTotalPage() { return (int) Math.ceil(totalCount * 1.0 / pageShow); } public int getNowPage() { if (nowPage "},"mybatis/mybatis分页插件/PagePlugin.html":{"url":"mybatis/mybatis分页插件/PagePlugin.html","title":"Page Plugin","keywords":"","body":"package com.hoby.pageutil; import java.lang.reflect.Field; import java.sql.Connection; import java.sql.PreparedStatement; import java.sql.ResultSet; import java.sql.SQLException; import java.util.List; import java.util.Map; import java.util.Properties; import org.apache.ibatis.executor.ErrorContext; import org.apache.ibatis.executor.ExecutorException; import org.apache.ibatis.executor.statement.BaseStatementHandler; import org.apache.ibatis.executor.statement.RoutingStatementHandler; import org.apache.ibatis.executor.statement.StatementHandler; import org.apache.ibatis.mapping.BoundSql; import org.apache.ibatis.mapping.MappedStatement; import org.apache.ibatis.mapping.ParameterMapping; import org.apache.ibatis.mapping.ParameterMode; import org.apache.ibatis.plugin.Interceptor; import org.apache.ibatis.plugin.Intercepts; import org.apache.ibatis.plugin.Invocation; import org.apache.ibatis.plugin.Plugin; import org.apache.ibatis.plugin.Signature; import org.apache.ibatis.reflection.MetaObject; import org.apache.ibatis.reflection.property.PropertyTokenizer; import org.apache.ibatis.scripting.xmltags.ForEachSqlNode; import org.apache.ibatis.session.Configuration; import org.apache.ibatis.type.TypeHandler; import org.apache.ibatis.type.TypeHandlerRegistry; @Intercepts({ @Signature(type = StatementHandler.class, method = \"prepare\", args = { Connection.class }) }) public class PagePlugin implements Interceptor { private static String dialect = \"\"; // 数据库方言 private static String pageSqlId = \"\"; // mapper.xml中需要拦截的ID(正则匹配) public Object intercept(Invocation ivk) throws Throwable { if (ivk.getTarget() instanceof RoutingStatementHandler) { RoutingStatementHandler statementHandler = (RoutingStatementHandler) ivk.getTarget(); BaseStatementHandler delegate = (BaseStatementHandler) ReflectHelper.getValueByFieldName(statementHandler, \"delegate\"); MappedStatement mappedStatement = (MappedStatement) ReflectHelper.getValueByFieldName(delegate, \"mappedStatement\"); if (mappedStatement.getId().matches(pageSqlId)) { // 拦截需要分页的SQL BoundSql boundSql = delegate.getBoundSql(); Object parameterObject = boundSql.getParameterObject();// 分页SQL中parameterType属性对应的实体参数，即Mapper接口中执行分页方法的参数,该参数不得为空 if (parameterObject == null) { throw new NullPointerException(\"parameterObject尚未实例化！\"); } else { Connection connection = (Connection) ivk.getArgs()[0]; String sql = boundSql.getSql(); String countSql = getCountSQL(sql); // 记录统计 PreparedStatement countStmt = connection.prepareStatement(countSql); BoundSql countBS = new BoundSql(mappedStatement.getConfiguration(), countSql, boundSql.getParameterMappings(), parameterObject); setParameters(countStmt, mappedStatement, countBS, parameterObject); ResultSet rs = countStmt.executeQuery(); int count = 0; if (rs.next()) { count = rs.getInt(1); } rs.close(); countStmt.close(); Page page = null; if (parameterObject instanceof Page) { // 参数就是Page实体 page = (Page) parameterObject; page.setTotalCount(count); page.setTotalPage((int) Math.ceil(count * 1.0 / page.getPageShow())); } else if (parameterObject instanceof Map) { Map paraMap = (Map) parameterObject; page = (Page) paraMap.get(\"page\"); if (page == null) page = new Page(); page.setTotalCount(count); page.setTotalPage((int) Math.ceil(count * 1.0 / page.getPageShow())); paraMap.put(\"page\", page); } else { // 参数为某个实体，该实体拥有Page属性 Field pageField = ReflectHelper.getFieldByFieldName(parameterObject, \"page\"); if (pageField != null) { page = (Page) ReflectHelper.getValueByFieldName(parameterObject, \"page\"); if (page == null) page = new Page(); page.setTotalCount(count); page.setTotalPage((int) Math.ceil(count * 1.0 / page.getPageShow())); ReflectHelper.setValueByFieldName(parameterObject, \"page\", page); // 通过反射，对实体对象设置分页对象 } else { throw new NoSuchFieldException(parameterObject.getClass().getName() + \"不存在 page 属性！\"); } } String pageSql = generatePageSql(sql, page); ReflectHelper.setValueByFieldName(boundSql, \"sql\", pageSql); // 将分页sql语句反射回BoundSql. } } } return ivk.proceed(); } // public static void main(String[] args) { // String sql = \" select p.*,di.\\\"name\\\" district, q.\\\"name\\\" // scene_name,(SELECT user_name from fw_user WHERE fw_user_id=u.parent_id) // parent_phone,\\n \" // + \" split_part(p.\\\"location\\\",',',1) // province,split_part(p.\\\"location\\\",',',2) city\\n \" // + \" from jd_patient p left join fw_user u on u.fw_user_id=p.fw_user_id\\n // \" // + \" left join jd_district di on p.jd_district_id=di.jd_district_id\\n \" // + \" left join jd_qrcode q on p.jd_qrcode_id = q.jd_qrcode_id\\n \" // + \" where 1 = 1\\n \" // + \" and p.age = '12'\\n\" // + \"ORDER BY p.create_date DESC,p.location\"; // System.out.println(getCountSQL(sql)); // } private static String getCountSQL(String sql) { String lowerCaseSQL = sql.toLowerCase().replace(\"\\n\", \" \").replace(\"\\t\", \" \"); int index = lowerCaseSQL.lastIndexOf(\"order by\"); if (index != -1) { sql = sql.substring(0, index); } return \"select count(0) from (\" + sql + \") as tmp_count\"; } /** * 对SQL参数(?)设值,参考org.apache.ibatis.executor.parameter. * DefaultParameterHandler * * @param ps * @param mappedStatement * @param boundSql * @param parameterObject * @throws SQLException */ private void setParameters(PreparedStatement ps, MappedStatement mappedStatement, BoundSql boundSql, Object parameterObject) throws SQLException { ErrorContext.instance().activity(\"setting parameters\").object(mappedStatement.getParameterMap().getId()); List parameterMappings = boundSql.getParameterMappings(); if (parameterMappings != null) { Configuration configuration = mappedStatement.getConfiguration(); TypeHandlerRegistry typeHandlerRegistry = configuration.getTypeHandlerRegistry(); MetaObject metaObject = parameterObject == null ? null : configuration.newMetaObject(parameterObject); for (int i = 0; i 0) { StringBuffer pageSql = new StringBuffer(); if (\"MySQL\".equals(dialect)) { pageSql.append(sql); pageSql.append(\" limit \" + page.getStart() + \",\" + page.getPageShow()); } else if (\"Oracle\".equals(dialect)) { pageSql.append(\"select * from (select tmp_tb.*,ROWNUM row_id from (\"); pageSql.append(sql); pageSql.append(\") as tmp_tb where ROWNUM\"); pageSql.append(page.getStart()); } else if (\"PostgreSQL\".equals(dialect)) { pageSql.append(sql); pageSql.append(\" limit \" + page.getPageShow() + \" offset \" + page.getStart()); } return pageSql.toString(); } else { return sql; } } public Object plugin(Object arg0) { // TODO Auto-generated method stub return Plugin.wrap(arg0, this); } public void setProperties(Properties p) { dialect = p.getProperty(\"dialect\"); // if (dialect!=null && dialect.trim().length()>0) { // System.out.println(\"dialect property is not found!\"); // } pageSqlId = p.getProperty(\"pageSqlId\"); // if (pageSqlId!=null && pageSqlId.trim().length()>0) { // System.out.println(\"pageSqlId property is not found!\"); // } } } "},"mybatis/mybatis分页插件/ReflectHelper.html":{"url":"mybatis/mybatis分页插件/ReflectHelper.html","title":"Reflect Helper","keywords":"","body":"package com.hoby.pageutil; import java.lang.reflect.Field; /** * @author Administrator * 反射工具 */ public class ReflectHelper { /** * 获取obj对象fieldName的Field * @param obj * @param fieldName * @return */ public static Field getFieldByFieldName(Object obj, String fieldName) { for (Class superClass = obj.getClass(); superClass != Object.class; superClass = superClass .getSuperclass()) { try { return superClass.getDeclaredField(fieldName); } catch (NoSuchFieldException e) { } } return null; } /** * 获取obj对象fieldName的属性值 * @param obj * @param fieldName * @return * @throws SecurityException * @throws NoSuchFieldException * @throws IllegalArgumentException * @throws IllegalAccessException */ public static Object getValueByFieldName(Object obj, String fieldName) throws SecurityException, NoSuchFieldException, IllegalArgumentException, IllegalAccessException { Field field = getFieldByFieldName(obj, fieldName); Object value = null; if(field!=null){ if (field.isAccessible()) { value = field.get(obj); } else { field.setAccessible(true); value = field.get(obj); field.setAccessible(false); } } return value; } /** * 设置obj对象fieldName的属性值 * @param obj * @param fieldName * @param value * @throws SecurityException * @throws NoSuchFieldException * @throws IllegalArgumentException * @throws IllegalAccessException */ public static void setValueByFieldName(Object obj, String fieldName, Object value) throws SecurityException, NoSuchFieldException, IllegalArgumentException, IllegalAccessException { try{ Field field = obj.getClass().getDeclaredField(fieldName); if (field.isAccessible()) { field.set(obj, value); } else { field.setAccessible(true); field.set(obj, value); field.setAccessible(false); } }catch(Exception err){ // } } } "},"mybatis/mybatis分页插件2/mybatis插件引入.html":{"url":"mybatis/mybatis分页插件2/mybatis插件引入.html","title":"mybatis插件引入","keywords":"","body":" "},"mybatis/mybatis分页插件2/PageInterceptor.html":{"url":"mybatis/mybatis分页插件2/PageInterceptor.html","title":"Page Interceptor","keywords":"","body":"/** * */ package com.fantastrip.framework.persistence.mybatis.plugin; import java.lang.reflect.Field; import java.util.Map; import java.util.Properties; import org.apache.ibatis.executor.Executor; import org.apache.ibatis.mapping.BoundSql; import org.apache.ibatis.mapping.MappedStatement; import org.apache.ibatis.mapping.SqlSource; import org.apache.ibatis.plugin.Interceptor; import org.apache.ibatis.plugin.Intercepts; import org.apache.ibatis.plugin.Invocation; import org.apache.ibatis.plugin.Plugin; import org.apache.ibatis.plugin.Signature; import org.apache.ibatis.session.ResultHandler; import org.apache.ibatis.session.RowBounds; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import com.fantastrip.framework.persistence.mybatis.MybatisDaoUtils; /** * @author xiai_fei * * @date 2014-3-21 下午12:14:04 */ @Intercepts({ @Signature(method = \"query\", type = Executor.class, args = { MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class }) }) public class PageInterceptor implements Interceptor { private static final Logger log = LoggerFactory .getLogger(PageInterceptor.class); private String SQL_PATTERN = \"\"; @SuppressWarnings(\"rawtypes\") @Override public Object intercept(Invocation invocation) throws Throwable { if (log.isDebugEnabled()) log.debug(\"come inro page interceptor intercept >>> \"); final MappedStatement mappedStatement = (MappedStatement) invocation .getArgs()[0]; if (mappedStatement.getId().matches(SQL_PATTERN)) { Object parameter = invocation.getArgs()[1]; log.debug(\" match pagination sql_pattern >>> \"); Pagination page = null; if (parameter instanceof Map) { Map map = (Map) parameter; for (Object o : map.values()) { if (o instanceof Pagination) { page = (Pagination) o; log.debug(\" find pagination in map parameter >>> \"); break; } } } if (parameter instanceof Pagination) page = (Pagination) parameter; else{ page = MybatisDaoUtils.getParamterPageField(parameter); } if (page != null) { log.debug(\" pageination parameter exist , do pagination intercepter >>> \"); BoundSql boundSql = (BoundSql) mappedStatement .getBoundSql(parameter); // 获取分页Sql语句 String pageSql = this .getPageSql(page, boundSql.getSql().trim()); log.debug(\" intercepter pagesql is : \" + pageSql); // 利用反射设置当前BoundSql对应的sql属性为我们建立好的分页Sql语句 ReflectUtil.setFieldValue(boundSql, \"sql\", pageSql); invocation.getArgs()[0] = copyFromMappedStatement( mappedStatement, new DefinedSqlSource(boundSql)); ; invocation.getArgs()[2] = new RowBounds( RowBounds.NO_ROW_OFFSET, RowBounds.NO_ROW_LIMIT); } } return invocation.proceed(); } @Override public Object plugin(Object target) { return Plugin.wrap(target, this); } @Override public void setProperties(Properties properties) { SQL_PATTERN = properties.getProperty(\"sql_pattern\"); } /** * 根据page对象获取对应的分页查询Sql语句， * * * @param page * 分页对象 * @param sql * 原sql语句 * @return */ private String getPageSql(Pagination page, String sql) { StringBuffer sqlBuffer = new StringBuffer(sql); // 计算第一条记录的位置，Mysql中记录的位置是从0开始的。 int offset = (page.getPageNo() - 1) * page.getPageSize(); sqlBuffer.append(\" limit \").append(offset).append(\",\").append(page.getPageSize()); int illegalCharIndex = sqlBuffer.indexOf(\";\"); if (illegalCharIndex >= 0)sqlBuffer.deleteCharAt(illegalCharIndex); return sqlBuffer.toString(); } private MappedStatement copyFromMappedStatement(MappedStatement ms, SqlSource newSqlSource) { MappedStatement.Builder builder = new MappedStatement.Builder( ms.getConfiguration(), ms.getId(), newSqlSource, ms.getSqlCommandType()); builder.resource(ms.getResource()); builder.fetchSize(ms.getFetchSize()); builder.statementType(ms.getStatementType()); builder.keyGenerator(ms.getKeyGenerator()); if (ms.getKeyProperties() != null) { for (String keyProperty : ms.getKeyProperties()) { builder.keyProperty(keyProperty); } } builder.timeout(ms.getTimeout()); builder.parameterMap(ms.getParameterMap()); builder.resultMaps(ms.getResultMaps()); builder.cache(ms.getCache()); return builder.build(); } private class DefinedSqlSource implements SqlSource { BoundSql boundSql; public DefinedSqlSource(BoundSql boundSql) { this.boundSql = boundSql; } @Override public BoundSql getBoundSql(Object parameterObject) { return boundSql; } } /** * * 利用反射进行操作的一个工具类 * */ private static class ReflectUtil { /** * 利用反射获取指定对象的指定属性 * * @param obj * 目标对象 * @param fieldName * 目标属性 * @return 目标属性的值 */ @SuppressWarnings(\"unused\") public static Object getFieldValue(Object obj, String fieldName) { Object result = null; Field field = ReflectUtil.getField(obj, fieldName); if (field != null) { field.setAccessible(true); try { result = field.get(obj); } catch (Exception e) { log.error(\"occur error on getFiledValue by reflect \", e); } } return result; } /** * 利用反射获取指定对象里面的指定属性 * * @param obj * 目标对象 * @param fieldName * 目标属性 * @return 目标字段 */ private static Field getField(Object obj, String fieldName) { Field field = null; for (Class clazz = obj.getClass(); clazz != Object.class; clazz = clazz .getSuperclass()) { try { field = clazz.getDeclaredField(fieldName); break; } catch (NoSuchFieldException e) { // 这里不用做处理，子类没有该字段可能对应的父类有，都没有就返回null。 } } return field; } /** * 利用反射设置指定对象的指定属性为指定的值 * * @param obj * 目标对象 * @param fieldName * 目标属性 * @param fieldValue * 目标值 */ public static void setFieldValue(Object obj, String fieldName, String fieldValue) { Field field = ReflectUtil.getField(obj, fieldName); if (field != null) { try { field.setAccessible(true); field.set(obj, fieldValue); } catch (Exception e) { log.error(\"occur error on setFieldValue by reflect \", e); } } } } } "},"mybatis/mybatis分页插件2/Pagination.html":{"url":"mybatis/mybatis分页插件2/Pagination.html","title":"Pagination","keywords":"","body":"/** * */ package com.fantastrip.framework.persistence.mybatis.plugin; /** * * MYBATIS 分页参数 * * @author xiai_fei * * @date 2014-3-21 下午2:55:17 */ public class Pagination { public static final int MAX_PAGE_SIZE = 100; private int pageSize ; private int pageNo ; public Pagination(){ } public Pagination(int pageSize , int pageNo){ this.pageNo = pageNo"},"mybatis/mybatis-view1.html":{"url":"mybatis/mybatis-view1.html","title":"Mybatis View 1","keywords":"","body":" 1、什么是 MyBatis？ 答： MyBatis 是一个可以自定义 SQL、存储过程和高级映射的持久层框架。 2、讲下 MyBatis 的缓存 答： MyBatis 的缓存分为一级缓存和二级缓存,一级缓存放在 session 里面,默认就有,二级缓 存放在它的命名空间里,默认是不打开的,使用二级缓存属性类需要实现 Serializable 序列化 接口(可用来保存对象的状态),可在它的映射文件中配置 3、 Mybatis 是如何进行分页的？分页插件的原理是什么？ 答： 1） Mybatis 使用 RowBounds 对象进行分页，也可以直接编写 sql 实现分页，也可以使用 Mybatis 的分页插件。 2）分页插件的原理：实现 Mybatis 提供的接口，实现自定义插件，在插件的拦截方法内拦 截待执行的 sql，然后重写 sql。 举例： select from student，拦截 sql 后重写为： select t. from （ select * from student） t limit 0， 10 4、简述 Mybatis 的插件运行原理，以及如何编写一个插件？ 答： 1） Mybatis 仅可以编写针对 ParameterHandler、 ResultSetHandler、 StatementHandler、 Executor 这 4 种接口的插件， Mybatis 通过动态代理，为需要拦截的接口生成代理对象以实 现接口方法拦截功能，每当执行这 4 种接口对象的方法时，就会进入拦截方法，具体就是 InvocationHandler 的 invoke()方法，当然，只会拦截那些你指定需要拦截的方法。 2）实现 Mybatis 的 Interceptor 接口并复写 intercept()方法，然后在给插件编写注解，指定 要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。 5、 Mybatis 动态 sql 是做什么的？都有哪些动态 sql？能简述一下动态 sql 的执行原理不？ 答： 1） Mybatis 动态 sql 可以让我们在 Xml 映射文件内，以标签的形式编写动态 sql，完成逻辑 判断和动态拼接 sql 的功能。 2） Mybatis 提供了 9 种动态 sql 标签： trim|where|set|foreach|if|choose|when|otherwise|bind。 3）其执行原理为，使用 OGNL 从 sql 参数对象中计算表达式的值，根据表达式的值动态拼 接 sql，以此来完成动态 sql 的功能。 6、 #{}和${}的区别是什么？ 答： 1） #{}是预编译处理， ${}是字符串替换。 2） Mybatis 在处理#{}时，会将 sql 中的#{}替换为?号，调用 PreparedStatement 的 set 方法 来赋值； 3） Mybatis 在处理${}时，就是把${}替换成变量的值。 4）使用#{}可以有效的防止 SQL 注入，提高系统安全性。 7、为什么说 Mybatis 是半自动 ORM 映射工具？它与全自动的区别在哪里？ 答： Hibernate 属于全自动 ORM 映射工具，使用 Hibernate 查询关联对象或者关联集合对象 时，可以根据对象关系模型直接获取，所以它是全自动的。而 Mybatis 在查询关联对象或 关联集合对象时，需要手动编写 sql 来完成，所以，称之为半自动 ORM 映射工具。 8、 Mybatis 是否支持延迟加载？如果支持，它的实现原理是什么？ 答： 1） Mybatis 仅支持 association 关联对象和 collection 关联集合对象的延迟加载， association 指的就是一对一， collection 指的就是一对多查询。在 Mybatis 配置文件中，可以配置是否 启用延迟加载 lazyLoadingEnabled=true|false。 2）它的原理是，使用 CGLIB 创建目标对象的代理对象，当调用目标方法时，进入拦截器方 法，比如调用 a.getB().getName()，拦截器 invoke()方法发现 a.getB()是 null 值，那么就会单 独发送事先保存好的查询关联 B 对象的 sql，把 B 查询上来，然后调用 a.setB(b)，于是 a 的 对象 b 属性就有值了，接着完成 a.getB().getName()方法的调用。这就是延迟加载的基本原 理。 9、 MyBatis 与 Hibernate 有哪些不同？ 答： 1） Mybatis 和 hibernate 不同，它不完全是一个 ORM 框架，因为 MyBatis 需要程序员自己 编写 Sql 语句，不过 mybatis 可以通过 XML 或注解方式灵活配置要运行的 sql 语句，并将 java 对象和 sql 语句映射生成最终执行的 sql，最后将 sql 执行的结果再映射生成 java 对 象。 2） Mybatis 学习门槛低，简单易学，程序员直接编写原生态 sql，可严格控制 sql 执行性 能，灵活度高，非常适合对关系数据模型要求不高的软件开发，例如互联网软件、企业运 营类软件等，因为这类软件需求变化频繁，一但需求变化要求成果输出迅速。但是灵活的 前提是 mybatis 无法做到数据库无关性，如果需要实现支持多种数据库的软件则需要自定 义多套 sql 映射文件，工作量大。 3） Hibernate 对象/关系映射能力强，数据库无关性好，对于关系模型要求高的软件（例如 需求固定的定制化软件）如果用 hibernate 开发可以节省很多代码，提高效率。但是 Hibernate 的缺点是学习门槛高，要精通门槛更高，而且怎么设计 O/R 映射，在性能和对象 模型之间如何权衡，以及怎样用好 Hibernate 需要具有很强的经验和能力才行。 总之，按照用户的需求在有限的资源环境下只要能做出维护性、扩展性良好的软件架构都 是好架构，所以框架只有适合才是最好。 10、 MyBatis 的好处是什么？ 答： 1） MyBatis 把 sql 语句从 Java 源程序中独立出来，放在单独的 XML 文件中编写，给程序的 维护带来了很大便利。 2） MyBatis 封装了底层 JDBC API 的调用细节，并能自动将结果集转换成 Java Bean 对象， 大大简化了 Java 数据库编程的重复工作。 3）因为 MyBatis 需要程序员自己去编写 sql 语句，程序员可以结合数据库自身的特点灵活 控制 sql 语句，因此能够实现比 Hibernate 等全自动 orm 框架更高的查询效率，能够完成复 杂查询。 11、简述 Mybatis 的 Xml 映射文件和 Mybatis 内部数据结构之间的映射关系？ 答： Mybatis 将所有 Xml 配置信息都封装到 All-In-One 重量级对象 Configuration 内部。在 Xml 映射文件中， 标签会被解析为 ParameterMap 对象，其每个子元素会 被解析为 ParameterMapping 对象。 标签会被解析为 ResultMap 对象，其每个子 元素会被解析为 ResultMapping 对象。每一个、 、 、 标签 均会被解析为 MappedStatement 对象，标签内的 sql 会被解析为 BoundSql 对象。 12、什么是 MyBatis 的接口绑定,有什么好处？ 答：接口映射就是在 MyBatis 中任意定义接口,然后把接口里面的方法和 SQL 语句绑定,我们 直接调用接口方法就可以,这样比起原来了 SqlSession 提供的方法我们可以有更加灵活的选 择和设置. 13、接口绑定有几种实现方式,分别是怎么实现的? 答：接口绑定有两种实现方式,一种是通过注解绑定,就是在接口的方法上面加上 @Select@Update 等注解里面包含 Sql 语句来绑定,另外一种就是通过 xml 里面写 SQL 来绑 定,在这种情况下,要指定 xml 映射文件里面的 namespace 必须为接口的全路径名. 14、什么情况下用注解绑定,什么情况下用 xml 绑定？ 答：当 Sql 语句比较简单时候,用注解绑定；当 SQL 语句比较复杂时候,用 xml 绑定,一般用 xml 绑定的比较多 15、 MyBatis 实现一对一有几种方式?具体怎么操作的？ 答：有联合查询和嵌套查询,联合查询是几个表联合查询,只查询一次,通过在 resultMap 里面 配置 association 节点配置一对一的类就可以完成;嵌套查询是先查一个表,根据这个表里面 的结果的外键 id,去再另外一个表里面查询数据,也是通过 association 配置,但另外一个表的 查询通过 select 属性配置。 16、 Mybatis 能执行一对一、一对多的关联查询吗？都有哪些实现方式，以及它们之间的区 别？ 答：能， Mybatis 不仅可以执行一对一、一对多的关联查询，还可以执行多对一，多对多的 关联查询，多对一查询，其实就是一对一查询，只需要把 selectOne()修改为 selectList()即 可；多对多查询，其实就是一对多查询，只需要把 selectOne()修改为 selectList()即可。 关联对象查询，有两种实现方式，一种是单独发送一个 sql 去查询关联对象，赋给主对 象，然后返回主对象。另一种是使用嵌套查询，嵌套查询的含义为使用 join 查询，一部分 列是 A 对象的属性值，另外一部分列是关联对象 B 的属性值，好处是只发一个 sql 查询， 就可以把主对象和其关联对象查出来。 17、 MyBatis 里面的动态 Sql 是怎么设定的?用什么语法? 答： MyBatis 里面的动态 Sql 一般是通过 if 节点来实现,通过 OGNL 语法来实现,但是如果要 写的完整,必须配合 where,trim 节点,where 节点是判断包含节点有内容就插入 where,否则不 插入,trim 节点是用来判断如果动态语句是以 and 或 or 开始,那么会自动把这个 and 或者 or 取掉。 18、 Mybatis 是如何将 sql 执行结果封装为目标对象并返回的？都有哪些映射形式？ 答： 第一种是使用标签，逐一定义列名和对象属性名之间的映射关系。 第二种是使用 sql 列的别名功能，将列别名书写为对象属性名，比如 T_NAME AS NAME，对 象属性名一般是 name，小写，但是列名不区分大小写， Mybatis 会忽略列名大小写，智能 找到与之对应对象属性名，你甚至可以写成 T_NAME AS NaMe， Mybatis 一样可以正常工 作。 有了列名与属性名的映射关系后， Mybatis 通过反射创建对象，同时使用反射给对象的属性 逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。 19、 Xml 映射文件中，除了常见的 select|insert|updae|delete 标签之外，还有哪些标签？ 答：还有很多其他的标签， 、 、 、 、 ，加上动态 sql 的 9 个标签， trim|where|set|foreach|if|choose|when|otherwise|bind 等，其中为 sql 片段标签，通 过标签引入 sql 片段， 为不支持自增的主键生成策略标签。 20、当实体类中的属性名和表中的字段名不一样，如果将查询的结果封装到指定 pojo？ 答： 1）通过在查询的 sql 语句中定义字段名的别名。 2）通过来映射字段名和实体类属性名的一一对应的关系。 21、模糊查询 like 语句该怎么写 答： 1）在 java 中拼接通配符，通过#{}赋值 2）在 Sql 语句中拼接通配符 （不安全 会引起 Sql 注入） 22、通常一个 Xml 映射文件，都会写一个 Dao 接口与之对应, Dao 的工作原理，是否可以重 载？ 答：不能重载，因为通过 Dao 寻找 Xml 对应的 sql 的时候全限名+方法名的保存和寻找策 略。接口工作原理为 jdk 动态代理原理，运行时会为 dao 生成 proxy，代理对象会拦截接口 方法，去执行对应的 sql 返回数据。 23、 Mybatis 映射文件中，如果 A 标签通过 include 引用了 B 标签的内容，请问， B 标签能 否定义在 A 标签的后面，还是说必须定义在 A 标签的前面？ 答：虽然 Mybatis 解析 Xml 映射文件是按照顺序解析的，但是，被引用的 B 标签依然可以 定义在任何地方， Mybatis 都可以正确识别。原理是， Mybatis 解析 A 标签，发现 A 标签引 用了 B 标签，但是 B 标签尚未解析到，尚不存在，此时， Mybatis 会将 A 标签标记为未解 析状态，然后继续解析余下的标签，包含 B 标签，待所有标签解析完毕， Mybatis 会重新 解析那些被标记为未解析的标签，此时再解析 A 标签时， B 标签已经存在， A 标签也就可 以正常解析完成了。 24、 Mybatis 的 Xml 映射文件中，不同的 Xml 映射文件， id 是否可以重复？ 答：不同的 Xml 映射文件，如果配置了 namespace，那么 id 可以重复；如果没有配置 namespace，那么 id 不能重复；毕竟 namespace 不是必须的，只是最佳实践而已。原因就 是 namespace+id 是作为 Map的 key 使用的，如果没有 namespace，就剩下 id，那么， id 重复会导致数据互相覆盖。有了 namespace，自然 id 就 可以重复， namespace 不同， namespace+id 自然也就不同。 25、 Mybatis 中如何执行批处理？ 答：使用 BatchExecutor 完成批处理。 26、 Mybatis 都有哪些 Executor 执行器？它们之间的区别是什么？ 答： Mybatis 有三种基本的 Executor 执行器， SimpleExecutor、 ReuseExecutor、 BatchExecutor。 1） SimpleExecutor：每执行一次 update 或 select，就开启一个 Statement 对 象，用完立刻关闭 Statement 对象。 2） ReuseExecutor：执行 update 或 select，以 sql 作为 key 查找 Statement 对象，存在就使用，不存在就创建，用完后，不关闭 Statement 对象， 而是放置于 Map3） BatchExecutor：完成批处理。 27、 Mybatis 中如何指定使用哪一种 Executor 执行器？ 答：在 Mybatis 配置文件中，可以指定默认的 ExecutorType 执行器类型，也可以手动给 DefaultSqlSessionFactory 的创建 SqlSession 的方法传递 ExecutorType 类型参数。 28、 Mybatis 执行批量插入，能返回数据库主键列表吗？ 答：能， JDBC 都能， Mybatis 当然也能。 29、 Mybatis 是否可以映射 Enum 枚举类？ 答： Mybatis 可以映射枚举类，不单可以映射枚举类， Mybatis 可以映射任何对象到表的一 列上。映射方式为自定义一个 TypeHandler，实现 TypeHandler 的 setParameter()和 getResult()接口方法。 TypeHandler 有两个作用，一是完成从 javaType 至 jdbcType 的转换， 二是完成 jdbcType 至 javaType 的转换，体现为 setParameter()和 getResult()两个方法，分别 代表设置 sql 问号占位符参数和获取列查询结果。 30、如何获取自动生成的(主)键值？ 答：配置文件设置 usegeneratedkeys 为 true 31、在 mapper 中如何传递多个参数？ 答： 1）直接在方法中传递参数， xml 文件用#{0} #{1}来获取 2）使用 @param 注解:这样可以直接在 xml 文件中通过#{name}来获取 32、 resultType resultMap 的区别？ 答： 1）类的名字和数据库相同时，可以直接设置 resultType 参数为 Pojo 类 2）若不同，需要设置 resultMap 将结果名字和 Pojo 名字进行转换 33、使用 MyBatis 的 mapper 接口调用时有哪些要求？ 答： 1） Mapper 接口方法名和 mapper.xml 中定义的每个 sql 的 id 相同 2） Mapper 接口方法的输入参数类型和 mapper.xml 中定义的每个 sql 的 parameterType 的 类型相同 3） Mapper 接口方法的输出参数类型和 mapper.xml 中定义的每个 sql 的 resultType 的类型 相同 4） Mapper.xml 文件中的 namespace 即是 mapper 接口的类路径。 34、 Mybatis 比 IBatis 比较大的几个改进是什么？ 答： 1）有接口绑定,包括注解绑定 sql 和 xml 绑定 Sql 2）动态 sql 由原来的节点配置变成 OGNL 表达式 3） 在一对一,一对多的时候引进了 association,在一对多的时候引入了 collection 节点,不过都是在 resultMap 里面配置 35、 IBatis 和 MyBatis 在核心处理类分别叫什么？ 答： IBatis 里面的核心处理类交 SqlMapClient,MyBatis 里面的核心处理类叫做 SqlSession。 36、 IBatis 和 MyBatis 在细节上的不同有哪些？ 答： 1）在 sql 里面变量命名有原来的#变量# 变成了#{变量} 2）原来的$变量$变成了${变量} 3）原来在 sql 节点里面的 class 都换名字交 type 4）原来的 queryForObject queryForList 变成了 selectOne selectList5）原来的别名设置在映 射文件里面放在了核心配置文件里 "},"mybatis/mybatis学习.html":{"url":"mybatis/mybatis学习.html","title":"mybatis学习","keywords":"","body":" MyBatis-Spring中间件逻辑分析(怎么把Mapper接口注册到Spring中) Mybatis常见面试题总结Mybatis源码分析（七）自定义缓存、分页的实现Mybatis源码分析（六）插件的创建代理过程mybatis-spring事务处理机制分析MyBatis源码分析 插件实现原理 java面试题:Spring中BeanFactory与FactoryBean的区别阿里面试题：Mybatis中的Dao接口和XML文件里的SQL是如何建立关系的？ mybatis-plus源码解析(二)----基于@MapperScan注解扫描加载Mappermybatis-plus源码解析(三)----Mapper接口动态代理调用过程 MyBatis之一级缓存原理以及失效情况 "},"mybatis/mybatis源码.html":{"url":"mybatis/mybatis源码.html","title":"mybatis源码","keywords":"","body":" MapperScan @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Documented @Import(MapperScannerRegistrar.class) @Repeatable(MapperScans.class) public @interface MapperScan { MapperScans @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Documented @Import(MapperScannerRegistrar.RepeatingRegistrar.class) public @interface MapperScans { MapperScan[] value(); } MapperScannerRegistrar public class MapperScannerRegistrar implements ImportBeanDefinitionRegistrar, ResourceLoaderAware { private ResourceLoader resourceLoader; @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { AnnotationAttributes mapperScanAttrs = AnnotationAttributes .fromMap(importingClassMetadata.getAnnotationAttributes(MapperScan.class.getName())); if (mapperScanAttrs != null) { registerBeanDefinitions(mapperScanAttrs, registry); } } void registerBeanDefinitions(AnnotationAttributes annoAttrs, BeanDefinitionRegistry registry) { ClassPathMapperScanner scanner = new ClassPathMapperScanner(registry); // this check is needed in Spring 3.1 Optional.ofNullable(resourceLoader).ifPresent(scanner::setResourceLoader); Class annotationClass = annoAttrs.getClass(\"annotationClass\"); if (!Annotation.class.equals(annotationClass)) { scanner.setAnnotationClass(annotationClass); } Class markerInterface = annoAttrs.getClass(\"markerInterface\"); if (!Class.class.equals(markerInterface)) { scanner.setMarkerInterface(markerInterface); } Class generatorClass = annoAttrs.getClass(\"nameGenerator\"); if (!BeanNameGenerator.class.equals(generatorClass)) { scanner.setBeanNameGenerator(BeanUtils.instantiateClass(generatorClass)); } Class mapperFactoryBeanClass = annoAttrs.getClass(\"factoryBean\"); if (!MapperFactoryBean.class.equals(mapperFactoryBeanClass)) { scanner.setMapperFactoryBeanClass(mapperFactoryBeanClass); } scanner.setSqlSessionTemplateBeanName(annoAttrs.getString(\"sqlSessionTemplateRef\")); scanner.setSqlSessionFactoryBeanName(annoAttrs.getString(\"sqlSessionFactoryRef\")); List basePackages = new ArrayList<>(); basePackages.addAll( Arrays.stream(annoAttrs.getStringArray(\"value\")) .filter(StringUtils::hasText) .collect(Collectors.toList())); basePackages.addAll( Arrays.stream(annoAttrs.getStringArray(\"basePackages\")) .filter(StringUtils::hasText) .collect(Collectors.toList())); basePackages.addAll( Arrays.stream(annoAttrs.getClassArray(\"basePackageClasses\")) .map(ClassUtils::getPackageName) .collect(Collectors.toList())); scanner.registerFilters(); scanner.doScan(StringUtils.toStringArray(basePackages)); } ClassPathMapperScanner private void processBeanDefinitions(Set beanDefinitions) { GenericBeanDefinition definition; for (BeanDefinitionHolder holder : beanDefinitions) { definition = (GenericBeanDefinition) holder.getBeanDefinition(); String beanClassName = definition.getBeanClassName(); LOGGER.debug(() -> \"Creating MapperFactoryBean with name '\" + holder.getBeanName() + \"' and '\" + beanClassName + \"' mapperInterface\"); // the mapper interface is the original class of the bean // but, the actual class of the bean is MapperFactoryBean definition.getConstructorArgumentValues().addGenericArgumentValue(beanClassName); // issue #59 definition.setBeanClass(this.mapperFactoryBeanClass); definition.getPropertyValues().add(\"addToConfig\", this.addToConfig); boolean explicitFactoryUsed = false; if (StringUtils.hasText(this.sqlSessionFactoryBeanName)) { definition.getPropertyValues().add(\"sqlSessionFactory\", new RuntimeBeanReference(this.sqlSessionFactoryBeanName)); explicitFactoryUsed = true; } else if (this.sqlSessionFactory != null) { definition.getPropertyValues().add(\"sqlSessionFactory\", this.sqlSessionFactory); explicitFactoryUsed = true; } if (StringUtils.hasText(this.sqlSessionTemplateBeanName)) { if (explicitFactoryUsed) { LOGGER.warn(() -> \"Cannot use both: sqlSessionTemplate and sqlSessionFactory together. sqlSessionFactory is ignored.\"); } definition.getPropertyValues().add(\"sqlSessionTemplate\", new RuntimeBeanReference(this.sqlSessionTemplateBeanName)); explicitFactoryUsed = true; } else if (this.sqlSessionTemplate != null) { if (explicitFactoryUsed) { LOGGER.warn(() -> \"Cannot use both: sqlSessionTemplate and sqlSessionFactory together. sqlSessionFactory is ignored.\"); } definition.getPropertyValues().add(\"sqlSessionTemplate\", this.sqlSessionTemplate); explicitFactoryUsed = true; } if (!explicitFactoryUsed) { LOGGER.debug(() -> \"Enabling autowire by type for MapperFactoryBean with name '\" + holder.getBeanName() + \"'.\"); definition.setAutowireMode(AbstractBeanDefinition.AUTOWIRE_BY_TYPE); } } } MapperFactoryBean public class MapperFactoryBean extends SqlSessionDaoSupport implements FactoryBean { private Class mapperInterface; private boolean addToConfig = true; public MapperFactoryBean() { //intentionally empty } public MapperFactoryBean(Class mapperInterface) { this.mapperInterface = mapperInterface; } /** * {@inheritDoc} */ @Override protected void checkDaoConfig() { super.checkDaoConfig(); notNull(this.mapperInterface, \"Property 'mapperInterface' is required\"); Configuration configuration = getSqlSession().getConfiguration(); if (this.addToConfig && !configuration.hasMapper(this.mapperInterface)) { try { configuration.addMapper(this.mapperInterface); } catch (Exception e) { logger.error(\"Error while adding the mapper '\" + this.mapperInterface + \"' to configuration.\", e); throw new IllegalArgumentException(e); } finally { ErrorContext.instance().reset(); } } } /** * {@inheritDoc} */ @Override public T getObject() throws Exception { return getSqlSession().getMapper(this.mapperInterface); } /** * {@inheritDoc} */ @Override public Class getObjectType() { return this.mapperInterface; } MapperScannerConfigurer public class MapperScannerConfigurer implements BeanDefinitionRegistryPostProcessor, InitializingBean, ApplicationContextAware, BeanNameAware { @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) { if (this.processPropertyPlaceHolders) { processPropertyPlaceHolders(); } ClassPathMapperScanner scanner = new ClassPathMapperScanner(registry); scanner.setAddToConfig(this.addToConfig); scanner.setAnnotationClass(this.annotationClass); scanner.setMarkerInterface(this.markerInterface); scanner.setSqlSessionFactory(this.sqlSessionFactory); scanner.setSqlSessionTemplate(this.sqlSessionTemplate); scanner.setSqlSessionFactoryBeanName(this.sqlSessionFactoryBeanName); scanner.setSqlSessionTemplateBeanName(this.sqlSessionTemplateBeanName); scanner.setResourceLoader(this.applicationContext); scanner.setBeanNameGenerator(this.nameGenerator); scanner.setMapperFactoryBeanClass(this.mapperFactoryBeanClass); scanner.registerFilters(); scanner.scan(StringUtils.tokenizeToStringArray(this.basePackage, ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS)); } SqlSessionDaoSupport public abstract class SqlSessionDaoSupport extends DaoSupport { private SqlSessionTemplate sqlSessionTemplate; public abstract class DaoSupport implements InitializingBean { @Override public final void afterPropertiesSet() throws IllegalArgumentException, BeanInitializationException { // Let abstract subclasses check their configuration. checkDaoConfig(); // Let concrete implementations initialize themselves. try { initDao(); } catch (Exception ex) { throw new BeanInitializationException(\"Initialization of DAO failed\", ex); } } protected abstract void checkDaoConfig() throws IllegalArgumentException; protected void initDao() throws Exception { } } SqlSessionTemplate public class SqlSessionTemplate implements SqlSession, DisposableBean { private final SqlSessionFactory sqlSessionFactory; private final SqlSession sqlSessionProxy; public SqlSessionTemplate(SqlSessionFactory sqlSessionFactory, ExecutorType executorType, PersistenceExceptionTranslator exceptionTranslator) { notNull(sqlSessionFactory, \"Property 'sqlSessionFactory' is required\"); notNull(executorType, \"Property 'executorType' is required\"); this.sqlSessionFactory = sqlSessionFactory; this.executorType = executorType; this.exceptionTranslator = exceptionTranslator; this.sqlSessionProxy = (SqlSession) newProxyInstance( SqlSessionFactory.class.getClassLoader(), new Class[] { SqlSession.class }, new SqlSessionInterceptor()); } @Override public T getMapper(Class type) { return getConfiguration().getMapper(type, this); } @Override public Configuration getConfiguration() { return this.sqlSessionFactory.getConfiguration(); } SqlSessionInterceptor private class SqlSessionInterceptor implements InvocationHandler { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { SqlSession sqlSession = getSqlSession( SqlSessionTemplate.this.sqlSessionFactory, SqlSessionTemplate.this.executorType, SqlSessionTemplate.this.exceptionTranslator); try { Object result = method.invoke(sqlSession, args); if (!isSqlSessionTransactional(sqlSession, SqlSessionTemplate.this.sqlSessionFactory)) { // force commit even on non-dirty sessions because some databases require // a commit/rollback before calling close() sqlSession.commit(true); } return result; } catch (Throwable t) { Throwable unwrapped = unwrapThrowable(t); if (SqlSessionTemplate.this.exceptionTranslator != null && unwrapped instanceof PersistenceException) { // release the connection to avoid a deadlock if the translator is no loaded. See issue #22 closeSqlSession(sqlSession, SqlSessionTemplate.this.sqlSessionFactory); sqlSession = null; Throwable translated = SqlSessionTemplate.this.exceptionTranslator.translateExceptionIfPossible((PersistenceException) unwrapped); if (translated != null) { unwrapped = translated; } } throw unwrapped; } finally { if (sqlSession != null) { closeSqlSession(sqlSession, SqlSessionTemplate.this.sqlSessionFactory); } } } } SqlSessionUtils.getSqlSession /** * Gets an SqlSession from Spring Transaction Manager or creates a new one if needed. * Tries to get a SqlSession out of current transaction. If there is not any, it creates a new one. * Then, it synchronizes the SqlSession with the transaction if Spring TX is active and * SpringManagedTransactionFactory is configured as a transaction manager. * * @param sessionFactory a MyBatis {@code SqlSessionFactory} to create new sessions * @param executorType The executor type of the SqlSession to create * @param exceptionTranslator Optional. Translates SqlSession.commit() exceptions to Spring exceptions. * @return an SqlSession managed by Spring Transaction Manager * @throws TransientDataAccessResourceException if a transaction is active and the * {@code SqlSessionFactory} is not using a {@code SpringManagedTransactionFactory} * @see SpringManagedTransactionFactory */ public static SqlSession getSqlSession(SqlSessionFactory sessionFactory, ExecutorType executorType, PersistenceExceptionTranslator exceptionTranslator) { notNull(sessionFactory, NO_SQL_SESSION_FACTORY_SPECIFIED); notNull(executorType, NO_EXECUTOR_TYPE_SPECIFIED); SqlSessionHolder holder = (SqlSessionHolder) TransactionSynchronizationManager.getResource(sessionFactory); SqlSession session = sessionHolder(executorType, holder); if (session != null) { return session; } LOGGER.debug(() -> \"Creating a new SqlSession\"); session = sessionFactory.openSession(executorType); registerSessionHolder(sessionFactory, executorType, exceptionTranslator, session); return session; } /** * Register session holder if synchronization is active (i.e. a Spring TX is active). * * Note: The DataSource used by the Environment should be synchronized with the * transaction either through DataSourceTxMgr or another tx synchronization. * Further assume that if an exception is thrown, whatever started the transaction will * handle closing / rolling back the Connection associated with the SqlSession. * * @param sessionFactory sqlSessionFactory used for registration. * @param executorType executorType used for registration. * @param exceptionTranslator persistenceExceptionTranslator used for registration. * @param session sqlSession used for registration. */ private static void registerSessionHolder(SqlSessionFactory sessionFactory, ExecutorType executorType, PersistenceExceptionTranslator exceptionTranslator, SqlSession session) { SqlSessionHolder holder; if (TransactionSynchronizationManager.isSynchronizationActive()) { Environment environment = sessionFactory.getConfiguration().getEnvironment(); if (environment.getTransactionFactory() instanceof SpringManagedTransactionFactory) { LOGGER.debug(() -> \"Registering transaction synchronization for SqlSession [\" + session + \"]\"); holder = new SqlSessionHolder(session, executorType, exceptionTranslator); TransactionSynchronizationManager.bindResource(sessionFactory, holder); TransactionSynchronizationManager.registerSynchronization(new SqlSessionSynchronization(holder, sessionFactory)); holder.setSynchronizedWithTransaction(true); holder.requested(); } else { if (TransactionSynchronizationManager.getResource(environment.getDataSource()) == null) { LOGGER.debug(() -> \"SqlSession [\" + session + \"] was not registered for synchronization because DataSource is not transactional\"); } else { throw new TransientDataAccessResourceException( \"SqlSessionFactory must be using a SpringManagedTransactionFactory in order to use Spring transaction synchronization\"); } } } else { LOGGER.debug(() -> \"SqlSession [\" + session + \"] was not registered for synchronization because synchronization is not active\"); } } private static SqlSession sessionHolder(ExecutorType executorType, SqlSessionHolder holder) { SqlSession session = null; if (holder != null && holder.isSynchronizedWithTransaction()) { if (holder.getExecutorType() != executorType) { throw new TransientDataAccessResourceException(\"Cannot change the ExecutorType when there is an existing transaction\"); } holder.requested(); LOGGER.debug(() -> \"Fetched SqlSession [\" + holder.getSqlSession() + \"] from current transaction\"); session = holder.getSqlSession(); } return session; } SqlSessionFactoryBean public class SqlSessionFactoryBean implements FactoryBean, InitializingBean, ApplicationListener { @Override public void afterPropertiesSet() throws Exception { notNull(dataSource, \"Property 'dataSource' is required\"); notNull(sqlSessionFactoryBuilder, \"Property 'sqlSessionFactoryBuilder' is required\"); state((configuration == null && configLocation == null) || !(configuration != null && configLocation != null), \"Property 'configuration' and 'configLocation' can not specified with together\"); this.sqlSessionFactory = buildSqlSessionFactory(); } protected SqlSessionFactory buildSqlSessionFactory() throws Exception { final Configuration targetConfiguration; XMLConfigBuilder xmlConfigBuilder = null; if (this.configuration != null) { targetConfiguration = this.configuration; if (targetConfiguration.getVariables() == null) { targetConfiguration.setVariables(this.configurationProperties); } else if (this.configurationProperties != null) { targetConfiguration.getVariables().putAll(this.configurationProperties); } } else if (this.configLocation != null) { xmlConfigBuilder = new XMLConfigBuilder(this.configLocation.getInputStream(), null, this.configurationProperties); targetConfiguration = xmlConfigBuilder.getConfiguration(); } else { LOGGER.debug(() -> \"Property 'configuration' or 'configLocation' not specified, using default MyBatis Configuration\"); targetConfiguration = new Configuration(); Optional.ofNullable(this.configurationProperties).ifPresent(targetConfiguration::setVariables); } Optional.ofNullable(this.objectFactory).ifPresent(targetConfiguration::setObjectFactory); Optional.ofNullable(this.objectWrapperFactory).ifPresent(targetConfiguration::setObjectWrapperFactory); Optional.ofNullable(this.vfs).ifPresent(targetConfiguration::setVfsImpl); if (hasLength(this.typeAliasesPackage)) { scanClasses(this.typeAliasesPackage, this.typeAliasesSuperType) .forEach(targetConfiguration.getTypeAliasRegistry()::registerAlias); } if (!isEmpty(this.typeAliases)) { Stream.of(this.typeAliases).forEach(typeAlias -> { targetConfiguration.getTypeAliasRegistry().registerAlias(typeAlias); LOGGER.debug(() -> \"Registered type alias: '\" + typeAlias + \"'\"); }); } if (!isEmpty(this.plugins)) { Stream.of(this.plugins).forEach(plugin -> { targetConfiguration.addInterceptor(plugin); LOGGER.debug(() -> \"Registered plugin: '\" + plugin + \"'\"); }); } if (hasLength(this.typeHandlersPackage)) { scanClasses(this.typeHandlersPackage, TypeHandler.class).stream() .filter(clazz -> !clazz.isInterface()) .filter(clazz -> !Modifier.isAbstract(clazz.getModifiers())) .filter(clazz -> ClassUtils.getConstructorIfAvailable(clazz) != null) .forEach(targetConfiguration.getTypeHandlerRegistry()::register); } if (!isEmpty(this.typeHandlers)) { Stream.of(this.typeHandlers).forEach(typeHandler -> { targetConfiguration.getTypeHandlerRegistry().register(typeHandler); LOGGER.debug(() -> \"Registered type handler: '\" + typeHandler + \"'\"); }); } if (this.databaseIdProvider != null) {//fix #64 set databaseId before parse mapper xmls try { targetConfiguration.setDatabaseId(this.databaseIdProvider.getDatabaseId(this.dataSource)); } catch (SQLException e) { throw new NestedIOException(\"Failed getting a databaseId\", e); } } Optional.ofNullable(this.cache).ifPresent(targetConfiguration::addCache); if (xmlConfigBuilder != null) { try { xmlConfigBuilder.parse(); LOGGER.debug(() -> \"Parsed configuration file: '\" + this.configLocation + \"'\"); } catch (Exception ex) { throw new NestedIOException(\"Failed to parse config resource: \" + this.configLocation, ex); } finally { ErrorContext.instance().reset(); } } targetConfiguration.setEnvironment(new Environment(this.environment, this.transactionFactory == null ? new SpringManagedTransactionFactory() : this.transactionFactory, this.dataSource)); if (this.mapperLocations != null) { if (this.mapperLocations.length == 0) { LOGGER.warn(() -> \"Property 'mapperLocations' was specified but matching resources are not found.\"); } else { for (Resource mapperLocation : this.mapperLocations) { if (mapperLocation == null) { continue; } try { XMLMapperBuilder xmlMapperBuilder = new XMLMapperBuilder(mapperLocation.getInputStream(), targetConfiguration, mapperLocation.toString(), targetConfiguration.getSqlFragments()); xmlMapperBuilder.parse(); } catch (Exception e) { throw new NestedIOException(\"Failed to parse mapping resource: '\" + mapperLocation + \"'\", e); } finally { ErrorContext.instance().reset(); } LOGGER.debug(() -> \"Parsed mapper file: '\" + mapperLocation + \"'\"); } } } else { LOGGER.debug(() -> \"Property 'mapperLocations' was not specified.\"); } return this.sqlSessionFactoryBuilder.build(targetConfiguration); } /** * {@inheritDoc} */ @Override public SqlSessionFactory getObject() throws Exception { if (this.sqlSessionFactory == null) { afterPropertiesSet(); } return this.sqlSessionFactory; } "},"mybatis/mybatis源码学习总结1.html":{"url":"mybatis/mybatis源码学习总结1.html","title":"mybatis源码学习总结1","keywords":"","body":"1、如何进行参数绑定和转换的 2、如何进行statement绑定的 MapperMethod MapperProxy SqlSessionTemplate MapperBuilderAssistant.java方法addMappedStatement 数据源配置MybatisSqlSessionFactoryBean是入口，其中buildSqlSessionFactory方法里有new XMLMapperBuilder,这个里面是解析*Mapper.xml文件的，包括namespace\\parameterMap\\resultMap\\sql\\select|insert|update|delete\\cache标签， @Bean public SqlSessionFactory sqlSessionFactory() throws Exception { MybatisSqlSessionFactoryBean factoryBean = new MybatisSqlSessionFactoryBean(); //DataSourceConfig.java里配置必须配置dataSource数据源对应的事务，否则事务失效 factoryBean.setDataSource(datasource()); org.springframework.core.io.Resource[] mapperLocations = new PathMatchingResourcePatternResolver() .getResources(\"classpath*:**/*Mapper.xml\"); factoryBean.setMapperLocations(mapperLocations); factoryBean.setConfiguration(mybatisPlusProperties.getConfiguration()); return factoryBean.getObject(); } private void configurationElement(XNode context) { try { String namespace = context.getStringAttribute(\"namespace\"); if (namespace == null || namespace.equals(\"\")) { throw new BuilderException(\"Mapper's namespace cannot be empty\"); } builderAssistant.setCurrentNamespace(namespace); cacheRefElement(context.evalNode(\"cache-ref\")); cacheElement(context.evalNode(\"cache\")); parameterMapElement(context.evalNodes(\"/mapper/parameterMap\")); resultMapElements(context.evalNodes(\"/mapper/resultMap\")); sqlElement(context.evalNodes(\"/mapper/sql\")); buildStatementFromContext(context.evalNodes(\"select|insert|update|delete\")); } catch (Exception e) { throw new BuilderException(\"Error parsing Mapper XML. Cause: \" + e, e); } } 一直跟踪buildStatementFromContext下去，先到XMLStatementBuilder，接着会找到MapperBuilderAssistant.java方法addMappedStatement ，此方法里有configuration.addMappedStatement(statement); 而获取MappedStatement则是在MapperMethod中的SqlCommand构造方法获取的，SqlCommand中的name就是mapper的namespace+\".\"+方法名 3、如何读取配置文件 MybatisSqlSessionFactoryBean 4、mybtis-plus如何生成mapper具体方法的 MapperBuilderAssistant.java方法addMappedStatement 5、mybatis缓存实现原理 6、mybatis如何扩展类型处理器 Configguration.java里有个typeHandlerRegistry变量 MybatisSqlSessionFactoryBean.java 也有个typeHandlersPackage 还有个方法setTypeHandlers通过设置这个来注册 7、mybatis整体架构设计 session executor -statement transaction binding -MapperMethod -MapperProxy -MapperProxyFacotry -MapperRegistry type mapping builder -XMLMapperBuilder -XMLConfigBuilder -MapperBuilderAssistant -MapperAnnotationBuilder cache 8、mybatis用了什么设计模式 1、工厂模式 SqlsessionFactoryTransactionFactory 2、代理模式MapperProxy https://blog.csdn.net/xiaokang123456kao/article/details/76228684 3、建造者模式SqlsessionFactoryBuilder 此类在MybatisSqlSessionFactoryBean中实例化，其中build方法用了构造SqlSessionFactory，默认返回DefaultSqlSessionFactory http://www.runoob.com/design-pattern/builder-pattern.html 9、mybatis常见配置 mybatis.mapperBeanPackage=com.tobe.**.dao mybatis-plus.mapper-locations=classpath:/mapper/*Mapper.xml mybatis-plus.typeAliasesPackage=com.tobe.**.pojo mybatis-plus.global-config.id-type=2 mybatis-plus.global-config.field-strategy=2 mybatis-plus.global-config.db-column-underline=true mybatis-plus.global-config.refresh-mapper=true mybatis-plus.configuration.map-underscore-to-camel-case=true mybatis-plus.configuration.cache-enabled=false 10、mybatis动态代理原理 https://blog.csdn.net/limengliang4007/article/details/80993345 11、mybatis如何扫描mapper包 @MapperScan(basePackages = {\"com.tobe.spbusiness.catchorder.mapper.**\"}, sqlSessionFactoryRef = \"sqlSessionFactory\") MapperScan注解类 mybatis-spring包中MapperScannerRegistrar类 12、mybatis如何管理事务的，如何实现动态数据源的 MybatisSqlSessionFactoryBean中新建SpringManagedTransactionFactory事务工厂，赋值给transactionFactory，而SpringManagedTransactionFactory是mybatis-spring包中的， SpringManagedTransactionFactory创建SpringManagedTransaction， SpringManagedTransaction中有openConnection()方法，该方法中调用了spring的DataSourceUtils的获取连接方法，该静态方法里 this.connection = DataSourceUtils.getConnection(this.dataSource); 13、mybatis如何实例化mapper接口，添加mapper的,绑定mapper 1、添加mapper Configuration类中addMappers方法有调用mapperRegistry.addMappers(packageName); XMLConfigBuilder类中parseConfiguration方法用来解析mybatis配置文件的，该方法调用mapperElement方法来调用Configuration类中addMappers方法 而XMLConfigBuilder由SqlSessionFactoryBuilder的build方法实例化，但是除了下面这个build方法，其他build方法好像都没用到， public SqlSessionFactory build(Configuration config) { return new DefaultSqlSessionFactory(config); } 所以如果不是采用xml配置文件的话，XMLConfigBuilder暂时是没用到解析的，但是Configuration类中addMappers方法又是在哪里调用到呢？看后面的 看下面的配置，很多配置比如datasource和Mapper.xml文件和Properties已经通过代码设置到Configuration对象里了的 @Bean public SqlSessionFactory sqlSessionFactory() throws Exception { MybatisSqlSessionFactoryBean factoryBean = new MybatisSqlSessionFactoryBean(); //DataSourceConfig.java里配置必须配置dataSource数据源对应的事务，否则事务失效 factoryBean.setDataSource(datasource()); org.springframework.core.io.Resource[] mapperLocations = new PathMatchingResourcePatternResolver() .getResources(\"classpath*:**/*Mapper.xml\"); factoryBean.setMapperLocations(mapperLocations); factoryBean.setConfiguration(mybatisPlusProperties.getConfiguration()); return factoryBean.getObject(); } 跟踪调式 org.springframework.core.io.Resource[] mapperLocations = new PathMatchingResourcePatternResolver() .getResources(\"classpath*:**/*Mapper.xml\"); 该方法获取到的值如下 file [D:\\idea-workspace2\\ThirdPlatform-Parent\\catch\\target\\classes\\mapper\\CatchOrderCallLogMapper.xml] file [D:\\idea-workspace2\\ThirdPlatform-Parent\\catch\\target\\classes\\mapper\\CatchOrderDetailMapper.xml] file [D:\\idea-workspace2\\ThirdPlatform-Parent\\catch\\target\\classes\\mapper\\CatchOrderDownloadConfigMapper.xml] file [D:\\idea-workspace2\\ThirdPlatform-Parent\\catch\\target\\classes\\mapper\\CatchOrderMapper.xml] file [D:\\idea-workspace2\\ThirdPlatform-Parent\\catch\\target\\classes\\mapper\\ShopMapper.xml] 然后再通过factoryBean.getObject()方法里面的方法，迭代去解析每个mapper文件，具体则涉及到XMLMapperBuilder， XMLMapperBuilder会被MybatisSqlSessionFactoryBean类的buildSqlSessionFactory方法中实例化来，这个是解析具体业务的*mapper.xml文件的 调试跟踪XMLMapperBuilder类的bindMapperForNamespace方法，里面调用了configuration.addMapper(boundType);这个来添加mapper和绑定mapper 2、实例化mapper 具体实例化mapper是MapperRegistry类中的getMapper时才实例化的 "},"mybatis/mybatis源码学习总结2.html":{"url":"mybatis/mybatis源码学习总结2.html","title":"mybatis源码学习总结2","keywords":"","body":"1、mapper如何被代理的，如何被调用、mapper接口方法是怎样被调用到的 调用baseMapper就转到mapperProxy了，这过程是怎么样的， mapperProxy是在mapperRegistry中被添加和调用的，在getMapper时才被实例化， https://www.jianshu.com/p/c5ecd4de6fd1 2、sqlsessionTemplate（实现了sqlSession）如何传入到mapperProxy的 DataSourceConfig定义了SqlSessionTemplate @Bean public SqlSessionTemplate sqlSessionTemplate() throws Exception { SqlSessionTemplate template = new SqlSessionTemplate(sqlSessionFactory()); return template; } https://www.jianshu.com/p/c5ecd4de6fd1 3、sqlsessionInterceptor如何被调用的 是由SqlSessionTemplate中直接用调用newProxyInstance代理类对象来创建的 public SqlSessionTemplate(SqlSessionFactory sqlSessionFactory, ExecutorType executorType, PersistenceExceptionTranslator exceptionTranslator) { notNull(sqlSessionFactory, \"Property 'sqlSessionFactory' is required\"); notNull(executorType, \"Property 'executorType' is required\"); this.sqlSessionFactory = sqlSessionFactory; this.executorType = executorType; this.exceptionTranslator = exceptionTranslator; this.sqlSessionProxy = (SqlSession) newProxyInstance( SqlSessionFactory.class.getClassLoader(), new Class[] { SqlSession.class }, new SqlSessionInterceptor()); } 4、Transaction如何注入到SimpleExecutor的 DefaultSqlSessionFactory里有如下方法： 1、从配置configuration获取environment 2、从environment获取事务工厂transactionFactory 3、利用事务工厂transaction和从环境environment中获取的数据源databasesource创建事务 4、利用创建的事务作为参数，调用配置configuration创建执行器executor（注意是这里将事务注入到执行器executor的） 5、利用执行器和配置创建DefaultSqlSession private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) { Transaction tx = null; try { final Environment environment = configuration.getEnvironment(); final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit); } catch (Exception e) { closeTransaction(tx); // may have fetched a connection so lets call close() throw ExceptionFactory.wrapException(\"Error opening session. Cause: \" + e, e); } finally { ErrorContext.instance().reset(); } } 5、SpringManageTransaction如何实例化的 MybatisSqlSessionFactoryBean 中直接new 一个SpringManagedTransactionFactory if (this.transactionFactory == null) { this.transactionFactory = new SpringManagedTransactionFactory(); } 6、三处addMapper的地方 第一处 第二处 SqlSessionFactoryBean xmlConfigBuilder = new XMLConfigBuilder(this.configLocation.getInputStream(), null, this.configurationProperties); xmlConfigBuilder.parse(); xmlConfigBuilder public Configuration parse() { if (parsed) { throw new BuilderException(\"Each XMLConfigBuilder can only be used once.\"); } parsed = true; parseConfiguration(parser.evalNode(\"/configuration\")); return configuration; } private void parseConfiguration(XNode root) { try { //issue #117 read properties first propertiesElement(root.evalNode(\"properties\")); Properties settings = settingsAsProperties(root.evalNode(\"settings\")); loadCustomVfs(settings); loadCustomLogImpl(settings); typeAliasesElement(root.evalNode(\"typeAliases\")); pluginElement(root.evalNode(\"plugins\")); objectFactoryElement(root.evalNode(\"objectFactory\")); objectWrapperFactoryElement(root.evalNode(\"objectWrapperFactory\")); reflectorFactoryElement(root.evalNode(\"reflectorFactory\")); settingsElement(settings); // read it after objectFactory and objectWrapperFactory issue #631 environmentsElement(root.evalNode(\"environments\")); databaseIdProviderElement(root.evalNode(\"databaseIdProvider\")); typeHandlerElement(root.evalNode(\"typeHandlers\")); mapperElement(root.evalNode(\"mappers\")); } catch (Exception e) { throw new BuilderException(\"Error parsing SQL Mapper Configuration. Cause: \" + e, e); } } private void mapperElement(XNode parent) throws Exception { if (parent != null) { for (XNode child : parent.getChildren()) { if (\"package\".equals(child.getName())) { String mapperPackage = child.getStringAttribute(\"name\"); configuration.addMappers(mapperPackage); } else { String resource = child.getStringAttribute(\"resource\"); String url = child.getStringAttribute(\"url\"); String mapperClass = child.getStringAttribute(\"class\"); if (resource != null && url == null && mapperClass == null) { ErrorContext.instance().resource(resource); InputStream inputStream = Resources.getResourceAsStream(resource); XMLMapperBuilder mapperParser = new XMLMapperBuilder(inputStream, configuration, resource, configuration.getSqlFragments()); mapperParser.parse(); } else if (resource == null && url != null && mapperClass == null) { ErrorContext.instance().resource(url); InputStream inputStream = Resources.getUrlAsStream(url); XMLMapperBuilder mapperParser = new XMLMapperBuilder(inputStream, configuration, url, configuration.getSqlFragments()); mapperParser.parse(); } else if (resource == null && url == null && mapperClass != null) { Class mapperInterface = Resources.classForName(mapperClass); configuration.addMapper(mapperInterface); } else { throw new BuilderException(\"A mapper element may only specify a url, resource or class, but not more than one.\"); } } } } } 第三处 SqlSessionFactoryBean XMLMapperBuilder xmlMapperBuilder = new XMLMapperBuilder(mapperLocation.getInputStream(), targetConfiguration, mapperLocation.toString(), targetConfiguration.getSqlFragments()); xmlMapperBuilder.parse(); xmlMapperBuilder public void parse() { if (!configuration.isResourceLoaded(resource)) { configurationElement(parser.evalNode(\"/mapper\")); configuration.addLoadedResource(resource); bindMapperForNamespace(); } parsePendingResultMaps(); parsePendingCacheRefs(); parsePendingStatements(); } private void bindMapperForNamespace() { String namespace = builderAssistant.getCurrentNamespace(); if (namespace != null) { Class boundType = null; try { boundType = Resources.classForName(namespace); } catch (ClassNotFoundException e) { //ignore, bound type is not required } if (boundType != null) { if (!configuration.hasMapper(boundType)) { // Spring may not know the real resource name so we set a flag // to prevent loading again this resource from the mapper interface // look at MapperAnnotationBuilder#loadXmlResource configuration.addLoadedResource(\"namespace:\" + namespace); configuration.addMapper(boundType); } } } } ``` 7、mybatis如何控制事务提交的 "},"mybatis/mybatis用法.html":{"url":"mybatis/mybatis用法.html","title":"mybatis用法","keywords":"","body":"http://www.mybatis.org/mybatis-3/zh/dynamic-sql.html "},"mybatis/常见mybatis用法.html":{"url":"mybatis/常见mybatis用法.html","title":"常见mybatis用法","keywords":"","body":"模糊查询 and lower(platform) like CONCAT('%',#{platform},'%') in集合查询 0\"> and product_manager in #{item} 模糊查询和集合查询例子： xx.xml SELECT * FROM sp_common.infringe_brand_examine ibe is_deleted = FALSE and lower(platform) like CONCAT('%',#{platform},'%') and lower(shop) like CONCAT('%',#{shop},'%') and product_code=#{productCode} 0\"> and product_manager in #{item} and lower(brand_name) like CONCAT('%',#{brandName},'%') 0\"> and status in #{item} limit #{pageSize} OFFSET '${offSetParam}' mapper.java List selectExaminesByPage(InfringeBrandExaminePageQueryVo infringeBrandExaminePageQueryVo);//, "},"mybatis-spring/Mybatis-Spring源码分析.html":{"url":"mybatis-spring/Mybatis-Spring源码分析.html","title":"Mybatis-Spring源码分析","keywords":"","body":" 1、MapperScan扫描过程 org.springframework.boot.SpringApplication#run(java.lang.String...){ ConfigurableApplicationContext context = null; context = createApplicationContext(); //主要经调式这里创建的context对象实际上是 AnnotationConfigServletWebServerApplicationContext refreshContext(context); } org.springframework.boot.SpringApplication#refresh{ ((AbstractApplicationContext) applicationContext).refresh(); } org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext#refresh{ super.refresh(); } org.springframework.context.support.AbstractApplicationContext#refresh{ invokeBeanFactoryPostProcessors(beanFactory); } org.springframework.context.support.AbstractApplicationContext#invokeBeanFactoryPostProcessors{ PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, getBeanFactoryPostProcessors()); } org.springframework.context.support.PostProcessorRegistrationDelegate#invokeBeanFactoryPostProcessors(org.springframework.beans.factory.config.ConfigurableListableBeanFactory, java.util.List){ invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); } 具体实现： { List currentRegistryProcessors = new ArrayList<>(); // First, invoke the BeanDefinitionRegistryPostProcessors that implement PriorityOrdered. // 根据BeanDefinitionRegistryPostProcessor类型从beanFactory取出相关处理器的名称 String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) { if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) { currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); } } //这里取出的currentRegistryProcessors里面只有ConfigurationClassPostProcessor这个处理器 invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); } org.springframework.context.annotation.ConfigurationClassPostProcessor#postProcessBeanDefinitionRegistry{ processConfigBeanDefinitions(registry); } private ConfigurationClassBeanDefinitionReader reader; org.springframework.context.annotation.ConfigurationClassPostProcessor#processConfigBeanDefinitions{ this.reader.loadBeanDefinitions(configClasses); } org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader#loadBeanDefinitions{ loadBeanDefinitionsForConfigurationClass(configClass, trackedConditionEvaluator); } org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader#loadBeanDefinitionsForConfigurationClass{ loadBeanDefinitionsFromRegistrars(configClass.getImportBeanDefinitionRegistrars()); } org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader#loadBeanDefinitionsFromRegistrars{ registrars.forEach((registrar, metadata) -> registrar.registerBeanDefinitions(metadata, this.registry)); } org.mybatis.spring.annotation.MapperScannerRegistrar#registerBeanDefinitions(org.springframework.core.type.AnnotationMetadata, org.springframework.beans.factory.support.BeanDefinitionRegistry){ AnnotationAttributes mapperScanAttrs = AnnotationAttributes .fromMap(importingClassMetadata.getAnnotationAttributes(MapperScan.class.getName())); if (mapperScanAttrs != null) { registerBeanDefinitions(mapperScanAttrs, registry); } } org.mybatis.spring.annotation.MapperScannerRegistrar#registerBeanDefinitions(org.springframework.core.annotation.AnnotationAttributes, org.springframework.beans.factory.support.BeanDefinitionRegistry){ } MapperScannerRegistrar#registerBeanDefinitions分析： org.mybatis.spring.annotation.MapperScannerRegistrar#registerBeanDefinitions(org.springframework.core.type.AnnotationMetadata, org.springframework.beans.factory.support.BeanDefinitionRegistry){ //从MapperScan接口注解里获取所有注解属性值 AnnotationAttributes mapperScanAttrs = AnnotationAttributes .fromMap(importingClassMetadata.getAnnotationAttributes(MapperScan.class.getName())); if (mapperScanAttrs != null) { registerBeanDefinitions(mapperScanAttrs, registry); } } org.mybatis.spring.annotation.MapperScannerRegistrar#registerBeanDefinitions(org.springframework.core.annotation.AnnotationAttributes, org.springframework.beans.factory.support.BeanDefinitionRegistry){ } ConfigurationClassBeanDefinitionReader#loadBeanDefinitions分析： org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader#loadBeanDefinitions{ TrackedConditionEvaluator trackedConditionEvaluator = new TrackedConditionEvaluator(); for (ConfigurationClass configClass : configurationModel) { loadBeanDefinitionsForConfigurationClass(configClass, trackedConditionEvaluator); } } mybatisplus的自动扫描如下： com.baomidou.mybatisplus.autoconfigure.MybatisPlusAutoConfiguration.AutoConfiguredMapperScannerRegistrar#registerBeanDefinitions{ } ConfigurableEnvironment 配置类解析分析： org.springframework.context.annotation.ConfigurationClassParser#doProcessConfigurationClass{ if (configClass.getMetadata().isAnnotated(Component.class.getName())) { // Recursively process any member (nested) classes first processMemberClasses(configClass, sourceClass); } // Process any @PropertySource annotations for (AnnotationAttributes propertySource : AnnotationConfigUtils.attributesForRepeatable( sourceClass.getMetadata(), PropertySources.class, org.springframework.context.annotation.PropertySource.class)) { if (this.environment instanceof ConfigurableEnvironment) { processPropertySource(propertySource); } else { logger.info(\"Ignoring @PropertySource annotation on [\" + sourceClass.getMetadata().getClassName() + \"]. Reason: Environment must implement ConfigurableEnvironment\"); } } // Process any @ComponentScan annotations Set componentScans = AnnotationConfigUtils.attributesForRepeatable( sourceClass.getMetadata(), ComponentScans.class, ComponentScan.class); if (!componentScans.isEmpty() && !this.conditionEvaluator.shouldSkip(sourceClass.getMetadata(), ConfigurationPhase.REGISTER_BEAN)) { for (AnnotationAttributes componentScan : componentScans) { // The config class is annotated with @ComponentScan -> perform the scan immediately Set scannedBeanDefinitions = this.componentScanParser.parse(componentScan, sourceClass.getMetadata().getClassName()); // Check the set of scanned definitions for any further config classes and parse recursively if needed for (BeanDefinitionHolder holder : scannedBeanDefinitions) { BeanDefinition bdCand = holder.getBeanDefinition().getOriginatingBeanDefinition(); if (bdCand == null) { bdCand = holder.getBeanDefinition(); } if (ConfigurationClassUtils.checkConfigurationClassCandidate(bdCand, this.metadataReaderFactory)) { parse(bdCand.getBeanClassName(), holder.getBeanName()); } } } } // Process any @Import annotations processImports(configClass, sourceClass, getImports(sourceClass), true); // Process any @ImportResource annotations AnnotationAttributes importResource = AnnotationConfigUtils.attributesFor(sourceClass.getMetadata(), ImportResource.class); if (importResource != null) { String[] resources = importResource.getStringArray(\"locations\"); Class readerClass = importResource.getClass(\"reader\"); for (String resource : resources) { String resolvedResource = this.environment.resolveRequiredPlaceholders(resource); configClass.addImportedResource(resolvedResource, readerClass); } } // Process individual @Bean methods Set beanMethods = retrieveBeanMethodMetadata(sourceClass); for (MethodMetadata methodMetadata : beanMethods) { configClass.addBeanMethod(new BeanMethod(methodMetadata, configClass)); } // Process default methods on interfaces processInterfaces(configClass, sourceClass); // Process superclass, if any if (sourceClass.getMetadata().hasSuperClass()) { String superclass = sourceClass.getMetadata().getSuperClassName(); if (superclass != null && !superclass.startsWith(\"java\") && !this.knownSuperclasses.containsKey(superclass)) { this.knownSuperclasses.put(superclass, configClass); // Superclass found, return its annotation metadata and recurse return sourceClass.getSuperClass(); } } // No superclass -> processing is complete return null; } 2、 3、 "},"redis/redis相关.html":{"url":"redis/redis相关.html","title":"redis相关","keywords":"","body":" 你了解Redis么，你知道Redis里面的字典是如何扩容的么？ 2、 3、 "},"redis/缓存.html":{"url":"redis/缓存.html","title":"缓存","keywords":"","body":" 缓存雪崩 1 缓存雪崩：由于缓存层承载着大量请求，有效的保护了存储层，但如果存储层由于某些原因不能提供服务，存储层调用暴增，造成存储层宕机。 处理： 保证缓存层服务高可用性。 对缓存系统做实时监控，报警等。 依赖隔离组件为后端限流并降级。 做好持久化，以便数据的快速恢复。 2 大量的 key 设置了相同的过期时间，导致在缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。 解决方案 可以给缓存设置过期时间时加上一个随机值时间，使得每个 key 的过期时间分布开来，不会集中在同一时刻失效。 缓存击穿 对于一些设置了过期时间的key，如果这些key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：缓存被“击穿”的问题，这个和缓存雪崩的区别在于这里针对某一 key 缓存，前者则是很多key。 缓存在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端 DB 压垮。 解决方案 在缓存失效的时候（判断拿出来的值为空），不是立即去 load db ，而是先使用缓存工具的某些带成功操作返回值的操作（比如Redis的 SETNX）去 set 一个 mutex key ，当操作返回成功时，再进行 load db 的操作并回设缓存；否则，就重试整个 get 缓存的方法。 缓存穿透 1 缓存穿透：缓存层不命中，存储层不命中。 处理方式1:缓存空对象，不过此时会占用更多内存空间，所以根据大家业务特性去设置超时时间来控制内存占用的问题。 处理方式2:布隆过滤器。 2 访问一个不存在的key，缓存不起作用，请求会穿透到 DB，流量大时 DB 会挂掉。 解决方案 采用布隆过滤器，使用一个足够大的bitmap，用于存储可能访问的 key，不存在的key直接被过滤； 访问key未在DB查询到值，也将空值写进缓存，但可以设置较短过期时间。 什么是布隆过滤器？ 是1970年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都比一般的算法要好的多，缺点是有一定的误识别率和删除困难。 Tips:当判断一定存在时，可能会误判，当判断不存在时，就一定不存在。 "},"spring/aop/aop动态代理学习.html":{"url":"spring/aop/aop动态代理学习.html","title":"aop动态代理学习","keywords":"","body":" 概念 连接点（Joinpoint）: 程序执行过程中明确的点，如方法的调用或特定的异常被抛出。 切入点（Pointcut）: 指定一个通知将被引发的一系列连接点的集合。AOP框架必须允许开发者指定切入点：例如，使用正则表达式。 Spring定义了Pointcut接口，用来组合MethodMatcher和ClassFilter，可以通过名字很清楚的理解， MethodMatcher是用来检查目标类的方法是否可以被应用此通知，而ClassFilter是用来检查Pointcut是否应该应用到目标类上 通知（Advice）: 在特定的连接点，AOP框架执行的动作。各种类型的通知包括“around”、“before”和“throws”通知。通知类型将在下面讨论。许多AOP框架包括Spring都是以拦截器做通知模型，维护一个“围绕”连接点的拦截器链。Spring中定义了四个advice: BeforeAdvice, AfterAdvice, ThrowAdvice和DynamicIntroductionAdvice Spring支持五种类型的通知： Before(前) org.apringframework.aop.MethodBeforeAdvice After-returning(返回后) org.springframework.aop.AfterReturningAdvice After-throwing(抛出后) org.springframework.aop.ThrowsAdvice Arround(周围) org.aopaliance.intercept.MethodInterceptor Introduction(引入) org.springframework.aop.IntroductionInterceptor 切面(Aspect) 通知、连接点、切入点共同组成了切面：时间、地点和要发生的“故事”.一个关注点的模块化，这个关注点实现可能另外横切多个对象。事务管理是J2EE应用中一个很好的横切关注点例子。方面用Spring的 Advisor或拦截器实现。 实现aop的四种方式 配置可以通过xml文件来进行，大概有四种方式： 配置ProxyFactoryBean，显式地设置advisors, advice, target等 ``` @Autowired AfterLogAdvice afterLogAdvice; @Autowired MethodBeforeAdvice methodBeforeAdvice; @Test public void test() { HelloService helloService = new HelloServiceImpl(); ProxyFactoryBean proxyFactoryBean=new ProxyFactoryBean(); proxyFactoryBean.setInterfaces(helloService.getClass().getInterfaces());//可以添加多个接口 proxyFactoryBean.setTarget(helloService);//目标对象 //引入通知 //参考：https://www.cnblogs.com/tjc1996/p/5720447.html proxyFactoryBean.addAdvice(afterLogAdvice); proxyFactoryBean.addAdvice(methodBeforeAdvice); //这样设置是不行的,必须用上面的addAdvice方法 //proxyFactoryBean.setInterceptorNames(\"AfterLogAdvice\",\"BeforeLogAdvice\");这样设置是不行的 HelloService helloServiceProxy = (HelloService) proxyFactoryBean.getAopProxyFactory().createAopProxy(proxyFactoryBean).getProxy(); helloServiceProxy.sayHello(\"小明\"); //直接getBean获取的是ProxyFactoryBean.getObject()返回的对象 //https://blog.csdn.net/c5113620/article/details/83578114 HelloService helloServiceProxy2 = (HelloService) proxyFactoryBean.getObject(); helloServiceProxy2.sayHello(\"小明\"); log.info(\"-------\"); } 2. 配置AutoProxyCreator，这种方式下，还是如以前一样使用定义的bean，但是从容器中获得的其实已经是代理对象 public class AspectJAwareAdvisorAutoProxyCreator implements BeanPostProcessor, BeanFactoryAware { private AbstractBeanFactory beanFactory; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws Exception { return bean; } @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws Exception { if (bean instanceof AspectJExpressionPointcutAdvisor) { return bean; } if (bean instanceof MethodInterceptor) { return bean; } List advisors = beanFactory .getBeansForType(AspectJExpressionPointcutAdvisor.class); for (AspectJExpressionPointcutAdvisor advisor : advisors) { if (advisor.getPointcut().getClassFilter().matches(bean.getClass())) { AdvisedSupport advisedSupport = new AdvisedSupport(); advisedSupport.setMethodInterceptor((MethodInterceptor) advisor.getAdvice()); advisedSupport.setMethodMatcher(advisor.getPointcut().getMethodMatcher()); TargetSource targetSource = new TargetSource(bean, bean.getClass().getInterfaces()); advisedSupport.setTargetSource(targetSource); return new JdkDynamicAopProxy(advisedSupport).getProxy(); } } return bean; } 3. 通过来配置 4. 通过来配置，使用AspectJ的注解来标识通知及切入点 @@Order(2) @Aspect @Component public class ControllerLogAspect } @Pointcut(\"execution(* com.tobe.spbusiness.*.controller..*.*(..))\") public void recordParamLog() { } @Around(\"recordParamLog()\") public Object around(ProceedingJoinPoint joinPoint) throws Throwable { Object controller = joinPoint.getTarget(); MethodSignature signature = (MethodSignature) joinPoint.getSignature(); Method method = signature.getMethod(); Object res = joinPoint.proceed(); joinPoint.getArgs() } #### 学习： 1. https://www.jianshu.com/p/aaeb2355ec5c 2. https://www.cnblogs.com/tjc1996/p/5720447.html 3. https://blog.csdn.net/c5113620/article/details/83578114 https://blog.csdn.net/moreevan/article/details/11977115/ https://blog.csdn.net/zhangliangzi/article/details/52334964 输出生成的代理类，参考：https://www.cnblogs.com/jhxxb/p/10557738.html ### ``` "},"spring/aop/aop学习.html":{"url":"spring/aop/aop学习.html","title":"mybatis学习","keywords":"","body":" spring @EnableAspectJAutoProxy背后的那些事(spring AOP源码赏析) "},"spring/aop/JdkDanymicAopProxy源码.html":{"url":"spring/aop/JdkDanymicAopProxy源码.html","title":"JdkDanymicAopProxy源码","keywords":"","body":" /* * Copyright 2002-2019 the original author or authors. * * Licensed under the Apache License, Version 2.0 (the \"License\"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * https://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ package org.springframework.aop.framework; import java.io.Serializable; import java.lang.reflect.InvocationHandler; import java.lang.reflect.Method; import java.lang.reflect.Proxy; import java.util.List; import org.aopalliance.intercept.MethodInvocation; import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; import org.springframework.aop.AopInvocationException; import org.springframework.aop.RawTargetAccess; import org.springframework.aop.TargetSource; import org.springframework.aop.support.AopUtils; import org.springframework.core.DecoratingProxy; import org.springframework.lang.Nullable; import org.springframework.util.Assert; import org.springframework.util.ClassUtils; /** * JDK-based {@link AopProxy} implementation for the Spring AOP framework, * based on JDK {@link java.lang.reflect.Proxy dynamic proxies}. * * Creates a dynamic proxy, implementing the interfaces exposed by * the AopProxy. Dynamic proxies cannot be used to proxy methods * defined in classes, rather than interfaces. * * Objects of this type should be obtained through proxy factories, * configured by an {@link AdvisedSupport} class. This class is internal * to Spring's AOP framework and need not be used directly by client code. * * Proxies created using this class will be thread-safe if the * underlying (target) class is thread-safe. * * Proxies are serializable so long as all Advisors (including Advices * and Pointcuts) and the TargetSource are serializable. * * @author Rod Johnson * @author Juergen Hoeller * @author Rob Harrop * @author Dave Syer * @see java.lang.reflect.Proxy * @see AdvisedSupport * @see ProxyFactory */ final class JdkDynamicAopProxy implements AopProxy, InvocationHandler, Serializable { /** use serialVersionUID from Spring 1.2 for interoperability. */ private static final long serialVersionUID = 5531744639992436476L; /* * NOTE: We could avoid the code duplication between this class and the CGLIB * proxies by refactoring \"invoke\" into a template method. However, this approach * adds at least 10% performance overhead versus a copy-paste solution, so we sacrifice * elegance for performance. (We have a good test suite to ensure that the different * proxies behave the same :-) * This way, we can also more easily take advantage of minor optimizations in each class. */ /** We use a static Log to avoid serialization issues. */ private static final Log logger = LogFactory.getLog(JdkDynamicAopProxy.class); /** Config used to configure this proxy. */ private final AdvisedSupport advised; /** * Is the {@link #equals} method defined on the proxied interfaces? */ private boolean equalsDefined; /** * Is the {@link #hashCode} method defined on the proxied interfaces? */ private boolean hashCodeDefined; /** * Construct a new JdkDynamicAopProxy for the given AOP configuration. * @param config the AOP configuration as AdvisedSupport object * @throws AopConfigException if the config is invalid. We try to throw an informative * exception in this case, rather than let a mysterious failure happen later. */ public JdkDynamicAopProxy(AdvisedSupport config) throws AopConfigException { Assert.notNull(config, \"AdvisedSupport must not be null\"); if (config.getAdvisors().length == 0 && config.getTargetSource() == AdvisedSupport.EMPTY_TARGET_SOURCE) { throw new AopConfigException(\"No advisors and no TargetSource specified\"); } this.advised = config; } @Override public Object getProxy() { return getProxy(ClassUtils.getDefaultClassLoader()); } @Override public Object getProxy(@Nullable ClassLoader classLoader) { if (logger.isTraceEnabled()) { logger.trace(\"Creating JDK dynamic proxy: \" + this.advised.getTargetSource()); } Class[] proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised, true); findDefinedEqualsAndHashCodeMethods(proxiedInterfaces); return Proxy.newProxyInstance(classLoader, proxiedInterfaces, this); } /** * Finds any {@link #equals} or {@link #hashCode} method that may be defined * on the supplied set of interfaces. * @param proxiedInterfaces the interfaces to introspect */ private void findDefinedEqualsAndHashCodeMethods(Class[] proxiedInterfaces) { for (Class proxiedInterface : proxiedInterfaces) { Method[] methods = proxiedInterface.getDeclaredMethods(); for (Method method : methods) { if (AopUtils.isEqualsMethod(method)) { this.equalsDefined = true; } if (AopUtils.isHashCodeMethod(method)) { this.hashCodeDefined = true; } if (this.equalsDefined && this.hashCodeDefined) { return; } } } } /** * Implementation of {@code InvocationHandler.invoke}. * Callers will see exactly the exception thrown by the target, * unless a hook method throws an exception. */ @Override @Nullable public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { Object oldProxy = null; boolean setProxyContext = false; TargetSource targetSource = this.advised.targetSource; Object target = null; try { if (!this.equalsDefined && AopUtils.isEqualsMethod(method)) { // The target does not implement the equals(Object) method itself. return equals(args[0]); } else if (!this.hashCodeDefined && AopUtils.isHashCodeMethod(method)) { // The target does not implement the hashCode() method itself. return hashCode(); } else if (method.getDeclaringClass() == DecoratingProxy.class) { // There is only getDecoratedClass() declared -> dispatch to proxy config. return AopProxyUtils.ultimateTargetClass(this.advised); } else if (!this.advised.opaque && method.getDeclaringClass().isInterface() && method.getDeclaringClass().isAssignableFrom(Advised.class)) { // Service invocations on ProxyConfig with the proxy config... return AopUtils.invokeJoinpointUsingReflection(this.advised, method, args); } Object retVal; if (this.advised.exposeProxy) { // Make invocation available if necessary. oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; } // Get as late as possible to minimize the time we \"own\" the target, // in case it comes from a pool. target = targetSource.getTarget(); Class targetClass = (target != null ? target.getClass() : null); // Get the interception chain for this method. List chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); // Check whether we have any advice. If we don't, we can fallback on direct // reflective invocation of the target, and avoid creating a MethodInvocation. if (chain.isEmpty()) { // We can skip creating a MethodInvocation: just invoke the target directly // Note that the final invoker must be an InvokerInterceptor so we know it does // nothing but a reflective operation on the target, and no hot swapping or fancy proxying. Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); retVal = AopUtils.invokeJoinpointUsingReflection(target, method, argsToUse); } else { // We need to create a method invocation... MethodInvocation invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); // Proceed to the joinpoint through the interceptor chain. retVal = invocation.proceed(); } // Massage return value if necessary. Class returnType = method.getReturnType(); if (retVal != null && retVal == target && returnType != Object.class && returnType.isInstance(proxy) && !RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) { // Special case: it returned \"this\" and the return type of the method // is type-compatible. Note that we can't help if the target sets // a reference to itself in another returned object. retVal = proxy; } else if (retVal == null && returnType != Void.TYPE && returnType.isPrimitive()) { throw new AopInvocationException( \"Null return value from advice does not match primitive return type for: \" + method); } return retVal; } finally { if (target != null && !targetSource.isStatic()) { // Must have come from TargetSource. targetSource.releaseTarget(target); } if (setProxyContext) { // Restore old proxy. AopContext.setCurrentProxy(oldProxy); } } } /** * Equality means interfaces, advisors and TargetSource are equal. * The compared object may be a JdkDynamicAopProxy instance itself * or a dynamic proxy wrapping a JdkDynamicAopProxy instance. */ @Override public boolean equals(@Nullable Object other) { if (other == this) { return true; } if (other == null) { return false; } JdkDynamicAopProxy otherProxy; if (other instanceof JdkDynamicAopProxy) { otherProxy = (JdkDynamicAopProxy) other; } else if (Proxy.isProxyClass(other.getClass())) { InvocationHandler ih = Proxy.getInvocationHandler(other); if (!(ih instanceof JdkDynamicAopProxy)) { return false; } otherProxy = (JdkDynamicAopProxy) ih; } else { // Not a valid comparison... return false; } // If we get here, otherProxy is the other AopProxy. return AopProxyUtils.equalsInProxy(this.advised, otherProxy.advised); } /** * Proxy uses the hash code of the TargetSource. */ @Override public int hashCode() { return JdkDynamicAopProxy.class.hashCode() * 13 + this.advised.getTargetSource().hashCode(); } } "},"spring/ioc/bean实例化过程.html":{"url":"spring/ioc/bean实例化过程.html","title":"bean实例化过程","keywords":"","body":" 1、 org.springframework.beans.factory.support.AbstractBeanFactory#doGetBean{ } 2、 public abstract class AbstractBeanFactory extends FactoryBeanRegistrySupport implements ConfigurableBeanFactory public abstract class FactoryBeanRegistrySupport extends DefaultSingletonBeanRegistry 3、 org.springframework.beans.factory.support.AbstractBeanDefinition{ /** * Constant for the default scope name: {@code \"\"}, equivalent to singleton * status unless overridden from a parent bean definition (if applicable). */ public static final String SCOPE_DEFAULT = \"\"; /** * Constant that indicates no external autowiring at all. * @see #setAutowireMode */ public static final int AUTOWIRE_NO = AutowireCapableBeanFactory.AUTOWIRE_NO; /** * Constant that indicates autowiring bean properties by name. * @see #setAutowireMode */ public static final int AUTOWIRE_BY_NAME = AutowireCapableBeanFactory.AUTOWIRE_BY_NAME; /** * Constant that indicates autowiring bean properties by type. * @see #setAutowireMode */ public static final int AUTOWIRE_BY_TYPE = AutowireCapableBeanFactory.AUTOWIRE_BY_TYPE; /** * Constant that indicates autowiring a constructor. * @see #setAutowireMode */ public static final int AUTOWIRE_CONSTRUCTOR = AutowireCapableBeanFactory.AUTOWIRE_CONSTRUCTOR; /** * Constant that indicates determining an appropriate autowire strategy * through introspection of the bean class. * @see #setAutowireMode * @deprecated as of Spring 3.0: If you are using mixed autowiring strategies, * use annotation-based autowiring for clearer demarcation of autowiring needs. */ @Deprecated public static final int AUTOWIRE_AUTODETECT = AutowireCapableBeanFactory.AUTOWIRE_AUTODETECT; } org.springframework.beans.factory.config.AutowireCapableBeanFactory{ /** * Constant that indicates no externally defined autowiring. Note that * BeanFactoryAware etc and annotation-driven injection will still be applied. * @see #createBean * @see #autowire * @see #autowireBeanProperties */ int AUTOWIRE_NO = 0; /** * Constant that indicates autowiring bean properties by name * (applying to all bean property setters). * @see #createBean * @see #autowire * @see #autowireBeanProperties */ int AUTOWIRE_BY_NAME = 1; /** * Constant that indicates autowiring bean properties by type * (applying to all bean property setters). * @see #createBean * @see #autowire * @see #autowireBeanProperties */ int AUTOWIRE_BY_TYPE = 2; /** * Constant that indicates autowiring the greediest constructor that * can be satisfied (involves resolving the appropriate constructor). * @see #createBean * @see #autowire */ int AUTOWIRE_CONSTRUCTOR = 3; /** * Constant that indicates determining an appropriate autowire strategy * through introspection of the bean class. * @see #createBean * @see #autowire * @deprecated as of Spring 3.0: If you are using mixed autowiring strategies, * prefer annotation-based autowiring for clearer demarcation of autowiring needs. */ @Deprecated int AUTOWIRE_AUTODETECT = 4; /** * Suffix for the \"original instance\" convention when initializing an existing * bean instance: to be appended to the fully-qualified bean class name, * e.g. \"com.mypackage.MyClass.ORIGINAL\", in order to enforce the given instance * to be returned, i.e. no proxies etc. * @since 5.1 * @see #initializeBean(Object, String) * @see #applyBeanPostProcessorsBeforeInitialization(Object, String) * @see #applyBeanPostProcessorsAfterInitialization(Object, String) */ String ORIGINAL_INSTANCE_SUFFIX = \".ORIGINAL\"; } "},"spring/ioc/spring-ioc原理.html":{"url":"spring/ioc/spring-ioc原理.html","title":"spring-ioc原理","keywords":"","body":" bean的生命周期 bean的作用域 初始化InitializingBean/@PostConstruct "},"spring/事务/spring事务管理.html":{"url":"spring/事务/spring事务管理.html","title":"spring事务管理","keywords":"","body":" "},"springboot/springboot配置/springboot配置文件加载分析.html":{"url":"springboot/springboot配置/springboot配置文件加载分析.html","title":"springboot配置文件加载分析","keywords":"","body":" 1、 2、 3、 ConfigFileApplicationListener https://my.oschina.net/u/1178126/blog/1822846 "},"springboot/springboot配置/修改springboot默认加载application-properties文件.html":{"url":"springboot/springboot配置/修改springboot默认加载application-properties文件.html","title":"修改springboot默认加载application.properties文件","keywords":"","body":" 1、重写默认加载文件 public class BasicServerApplication { public static void main(String[] args) throws IOException { // SpringApplication.run(PcAdminApplication.class, args); Properties properties = new Properties(); String defaultfile = \"application-main.properties\"; InputStream inputStream = BasicServerApplication.class.getClassLoader().getResourceAsStream(defaultfile); properties.load(inputStream); SpringApplication app = new SpringApplication(BasicServerApplication.class); app.setDefaultProperties(properties); app.setBannerMode(Banner.Mode.OFF); app.run(args); } 2、EnvironmentPostProcessor实现自定义加载配置文件 package com.basic.config.environment; import lombok.extern.slf4j.Slf4j; import org.springframework.boot.SpringApplication; import org.springframework.boot.env.EnvironmentPostProcessor; import org.springframework.boot.env.PropertySourceLoader; import org.springframework.core.env.ConfigurableEnvironment; import org.springframework.core.env.PropertySource; import org.springframework.core.io.Resource; import org.springframework.core.io.support.PathMatchingResourcePatternResolver; import org.springframework.core.io.support.ResourcePatternResolver; import org.springframework.core.io.support.SpringFactoriesLoader; import org.springframework.util.ResourceUtils; import java.util.List; /** * https://www.jianshu.com/p/7ab1a62b04ed * https://docs.spring.io/spring-boot/docs/2.0.4.RELEASE/reference/htmlsingle/#howto-customize-the-environment-or-application-context * */ @Slf4j public class BasicEnvironmentPostProcessor implements EnvironmentPostProcessor { private final ResourcePatternResolver resourcePatternResolver = new PathMatchingResourcePatternResolver(); private final List propertySourceLoaders; /* public BasicEnvironmentPostProcessor(List propertySourceLoaders) { this.propertySourceLoaders = propertySourceLoaders; }*/ public BasicEnvironmentPostProcessor() { super(); this.propertySourceLoaders = SpringFactoriesLoader.loadFactories(PropertySourceLoader.class, getClass().getClassLoader()); } @Override public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application) { String[] activeProfiles = environment.getActiveProfiles(); for (String activeProfile : activeProfiles) { for (PropertySourceLoader propertySourceLoader : this.propertySourceLoaders) { for (String fileExtension : propertySourceLoader.getFileExtensions()) { String location = ResourceUtils.CLASSPATH_URL_PREFIX + activeProfile + \"/application-*\" + fileExtension; try { Resource[] resources = this.resourcePatternResolver.getResources(location); for (Resource resource : resources) { List> propertySources = propertySourceLoader.load(resource.getFilename(), resource); if (propertySources != null && !propertySources.isEmpty()) { propertySources.stream().forEach(environment.getPropertySources()::addLast); } } } catch (Exception e) { log.error(\"加载配置文件失败：{}\", e.getMessage(), e); } } } } } } 如果既想加载basic-view中的配置文件，又想加载basic-server中的配置文件，则可以在basic-view下的application.yaml 中设置spring.profiles.active=dev,config-dev 其中config-dev是basic-server中的 参考：https://blog.csdn.net/u012988901/article/details/83024406 3、获取应用启动的端口和添加额外的配置文件 ConfigurableApplicationContext appContext = app.run(args); Environment env = appContext.getEnvironment(); appContext.getApplicationName() env.getActiveProfiles() env.getDefaultProfiles() env.getProperty(\"server.port\") InetAddress.getLocalHost().getHostAddress() app.setAdditionalProfiles(SPRING_PROFILE_DEVELOPMENT);// 没有激活的active文件，就增加额外的文件 Environment继承了PropertyResolver，所以可以调用getProperty获取端口等信息 public interface Environment extends PropertyResolver { public class DemoDubboProviderApplication { private static Logger log = LoggerFactory.getLogger(DemoDubboProviderApplication.class); public static final String SPRING_PROFILE_DEVELOPMENT = \"dev\"; @Autowired private Environment env; @Bean public CountDownLatch closeLatch() { return new CountDownLatch(1); } @PostConstruct public void init() { if (env.getActiveProfiles().length == 0) { log.warn(\"---No Spring profile configured, running with default configuration\"); } else { log.info(\"---Running with Spring profile(s) : {}\", Arrays.toString(env.getActiveProfiles())); @SuppressWarnings(\"rawtypes\") Collection activeProfiles = Arrays.asList(env.getActiveProfiles()); if (activeProfiles.contains(\"dev\") && activeProfiles.contains(\"prod\")) { log.error(\"---You have misconfigured your application! \" + \"It should not run with both the 'dev' and 'prod' profiles at the same time.\"); } } } public static void main(String[] args) throws UnknownHostException, InterruptedException { SpringApplication app = new SpringApplication(DemoDubboProviderApplication.class); app.setWebEnvironment(false); SimpleCommandLinePropertySource source = new SimpleCommandLinePropertySource(args); addDefaultProfile(app, source); //ApplicationContext appContext = app.run(args); ConfigurableApplicationContext appContext = app.run(args); Environment env = appContext.getEnvironment(); log.info(\"---ApplicationName: \" + appContext.getApplicationName()); log.info(\"---Active Profiles: \" + Arrays.toString(env.getActiveProfiles())); log.info(\"---Default Profiles: \" + Arrays.toString(env.getDefaultProfiles())); String a=\"Access URLs:\\n----------------------------------------------------------\\n\\t\" + \"Local: \\t\\thttp://127.0.0.1:\"+env.getProperty(\"server.port\")+\"/\"+appContext.getApplicationName()+\"\\n\\t\" + \"External: \\thttp://\"+ InetAddress.getLocalHost().getHostAddress()+\":\"+env.getProperty(\"server.port\")+\"/\"+appContext.getApplicationName() +\"\\n----------------------------------------------------------\"; log.info(a); /* try { System.in.read(); //用于模拟服务一直开着 } catch (Exception e) { e.printStackTrace(); } */ /*synchronized (EleasticSearchApplication.class) { while (true) { try { EleasticSearchApplication.class.wait(); } catch (Throwable e) { } } } */ // make main thread blocking CountDownLatch closeLatch = appContext.getBean(CountDownLatch.class); System.out.println(\"closeLatch=\" + closeLatch); closeLatch.await(); } /** * If no profile has been configured, set by default the \"dev\" profile. * * 问题：若没有指定spring.profiles.active（比如直接在eclipse里右键启动而没有设置参数的话） spring boot * 默认会加载类路径下application.properties或application-default.properties, * 这里设置默认加载classpath下的application-dev.properties文件， * 但是如果这个文件不在classpath下而是在deploy目录下呢(将配置文件统一放到一个目录)，怎么加载 * * * 方法1：右键该文件，在debug as ->debug configurations里面的Arguments->Program * arguments配置--spring.config.location=classpath:/deploy/application-dev. * properties这个也行 * * * 方法2：如果用代码不会，那只能将所有文件放到config目录下，然后在pom.xml里面配置在打包时过滤这几个文件，因为打出来的包不能有这几个文件 * ，这几个文件要单独 放在linux上某个文件夹下，方便修改和安全控制 * * * 方法3：为了兼容eclipse,eclipse不设置spring.config.location,将application-dev. * properties放到classpath下（放到config文件夹下也可以）， 其他prod，test放到deploy下 * */ private static void addDefaultProfile(SpringApplication app, SimpleCommandLinePropertySource source) { if (!source.containsProperty(\"spring.profiles.active\") && !System.getenv().containsKey(\"SPRING_PROFILES_ACTIVE\")) { log.info(\"---Setting more profile \" + SPRING_PROFILE_DEVELOPMENT); app.setAdditionalProfiles(SPRING_PROFILE_DEVELOPMENT);// 没有激活的active文件，就增加额外的文件 } else { log.info(\"---NOT Setting more profile\"); } } } "},"springboot/监听器/springboot监听器.html":{"url":"springboot/监听器/springboot监听器.html","title":"springboot监听器","keywords":"","body":" 1、META-INF/spring.factories文件 有10个监听器，其中ConfigFileApplicationListener是对配置文件进行加载的 SpringFactoriesLoader # Application Listeners org.springframework.context.ApplicationListener=\\ org.springframework.boot.ClearCachesApplicationListener,\\ org.springframework.boot.builder.ParentContextCloserApplicationListener,\\ org.springframework.boot.context.FileEncodingApplicationListener,\\ org.springframework.boot.context.config.AnsiOutputApplicationListener,\\ org.springframework.boot.context.config.ConfigFileApplicationListener,\\ org.springframework.boot.context.config.DelegatingApplicationListener,\\ org.springframework.boot.context.logging.ClasspathLoggingApplicationListener,\\ org.springframework.boot.context.logging.LoggingApplicationListener,\\ org.springframework.boot.liquibase.LiquibaseServiceLocatorApplicationListener 2、 3、 "},"springboot/SpringApplication源码分析.html":{"url":"springboot/SpringApplication源码分析.html","title":"SpringApplication源码分析","keywords":"","body":" 1、 private Collection getSpringFactoriesInstances(Class type, Class[] parameterTypes, Object... args) { ClassLoader classLoader = getClassLoader(); // Use names and ensure unique to protect against duplicates Set names = new LinkedHashSet<>(SpringFactoriesLoader.loadFactoryNames(type, classLoader)); List instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names); AnnotationAwareOrderComparator.sort(instances); return instances; } 调用SpringFactoriesLoader的loadFactoryNames方法从META-INF/spring.factories文件中读取某种type类型的多个实现名称(比如10大监听器的名称)# Application Listeners org.springframework.context.ApplicationListener=\\ org.springframework.boot.ClearCachesApplicationListener,\\ org.springframework.boot.builder.ParentContextCloserApplicationListener,\\ org.springframework.boot.context.FileEncodingApplicationListener,\\ org.springframework.boot.context.config.AnsiOutputApplicationListener,\\ org.springframework.boot.context.config.ConfigFileApplicationListener,\\ org.springframework.boot.context.config.DelegatingApplicationListener,\\ org.springframework.boot.context.logging.ClasspathLoggingApplicationListener,\\ org.springframework.boot.context.logging.LoggingApplicationListener,\\ org.springframework.boot.liquibase.LiquibaseServiceLocatorApplicationListener 然后根据名称调用createSpringFactoriesInstances来实例化bean 主要通过反射来创建ClassLoader clToUse = classLoader; if (clToUse == null) { clToUse = getDefaultClassLoader(); } Class.forName(name, false, clToUse); 2、 3、 "},"zother1-JavaFaceNotes/Dubbo.html":{"url":"zother1-JavaFaceNotes/Dubbo.html","title":"Dubbo","keywords":"","body":"Dubbo 1.什么是Dubbo? Dubbo是基于Java的高性能轻量级的RPC分布式服务框架，现已成为 Apache 基金会孵化项目。 官网：http://dubbo.apache.org/en-us/ 2.为什么要使用Dubbo? 背景: 随着互联网的快速发展，Web应用程序的规模不断扩大，最后我们发现传统的垂直体系结构（整体式）已无法解决。分布式服务体系结构和流计算体系结构势在必行，迫切需要一个治理系统来确保体系结构的有序发展。 开源免费 一些核心业务被提取并作为独立的服务提供服务，逐渐形成一个稳定的服务中心，这样前端应用程序就可以更好地响应变化多端的市场需求 分布式框架能承受更大规模的流量 内部基于netty性能高 3.Dubbo提供了哪3个关键功能？ 基于接口的远程调用 容错和负载均衡 自动服务注册和发现 4.你知道哪些机构在用Dubbo吗？ 5.Dubbo服务的关键节点有哪些? 6.说一下Dubbo服务注册流程? 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 7.能画一下服务注册流程图吗？ 8.Dubbo架构的特点？ 连通性、健壮性、伸缩性、以及向未来架构的升级性。 9.对jdk的最小版本需求？ jdk1.6+ 10.注册中心的选择？ 一般来说选中Zookeeper更稳定更合适。 除了Zookeeper还有Redis注册中心、Multicast注册中心、Simple注册中心。 11.Dubbo的核心配置？用途？ 12.配置优先级规则？ 优先级从高到低： JVM -D参数，当你部署或者启动应用时，它可以轻易地重写配置，比如，改变dubbo协议端口； XML, XML中的当前配置会重写dubbo.properties中的； Properties，默认配置，仅仅作用于以上两者没有配置时。 13.如何用代码方式绕过注册中心点对点直连？ … ReferenceConfig reference = new ReferenceConfig(); // 此实例很重，封装了与注册中心的连接以及与提供者的连接，请自行缓存，否则可能造成内存和连接泄漏 // 如果点对点直连，可以用reference.setUrl()指定目标地址，设置url后将绕过注册中心， // 其中，协议对应provider.setProtocol()的值，端口对应provider.setPort()的值， // 路径对应service.setPath()的值，如果未设置path，缺省path为接口名 reference.setUrl(\"dubbo://10.20.130.230:20880/com.xxx.XxxService\"); … 14.Dubbo配置来源有几种？分别是？ 4种 JVM System Properties，-D参数 Externalized Configuration，外部化配置 ServiceConfig、ReferenceConfig等编程接口采集的配置 本地配置文件dubbo.properties 15.如何禁用某个服务的启动检查？ 16.Dubbo 负载均衡策略？默认是？ 随机负载平衡(默认) RoundRobin负载平衡 最小活动负载平衡 一致的哈希负载平衡 17.上线兼容老版本？ 多版本号(version) 18.开发测试环境，想绕过注册中心如何配置？ xml -D java -Dcom.alibaba.xxx.XxxService=dubbo://localhost:20890 .properties java -Ddubbo.resolve.file=xxx.properties com.alibaba.xxx.XxxService=dubbo://localhost:20890 19.集群容错几种方法？ 20.Dubbo有几种配置方式？ Spring Java API 21.Dubbo有哪些协议？推荐？ dubbo://(推荐) rmi:// hessian:// http:// webservice:// thrift:// memcached:// redis:// rest:// 22.Dubbo使用什么通信框架? dubbo使用netty。 23.dubbo协议默认端口号？http协议默认端口？hessian?rmi? dubbo:20880 http:80 hessian:80 rmi:80 24.Dubbo默认序列化框架?其他的你还知道？ dubbo协议缺省为hessian2 rmi协议缺省为java http协议缺省为json 25.一个服务有多重实现时，如何处理？ 可以用group分组，服务提供方和消费放都指定同一个group。 26.Dubbo服务调用默认是阻塞的？还有其他的？ 默认是同步等待结果阻塞的，同时也支持异步调用。 Dubbo 是基于 NIO 的非阻塞实现并行调用，客户端不需要启动多线程即可完成并行调用多个远程服务，相对多线程开销较小，异步调用会返回一个 Future 对象。 27.Dubbo服务追踪解决方案? Zipkin Pinpoint SkyWalking 28.Dubbo不维护了吗？Dubbo和Dubbox有什么区别？ 现在进入了Apache,由apache维护。 Dubbox是当当的扩展项目。 29.Dubbox有什么新功能？ 支持REST风格远程调用（HTTP + JSON/XML) 支持基于Kryo和FST的Java高效序列化实现 支持基于嵌入式Tomcat的HTTP remoting体系 升级Spring 升级ZooKeeper客户端 30.io线程池大小默认？ cpu个数 + 1 31.dubbo://协议适合什么样的服务调用？ 采用单一长链接和NIO异步通讯，适用于小数量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。 不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。 32.自动剔除服务什么原理？ zookeeper临时节点，会话保持原理。 33.从 2.0.5 版本开始，dubbo支持通过x命令来进行服务治理? telnet 34.如何用命令查看服务列表？ telnet localhost 20880 进入命令行。然后执行 ls相关命令: ls: 显示服务列表 ls -l: 显示服务详细信息列表 ls XxxService: 显示服务的方法列表 ls -l XxxService: 显示服务的方法详细信息列表 35.Dubbo框架设计是怎样的？ 各层说明: config 配置层：对外配置接口，以 ServiceConfig, ReferenceConfig 为中心，可以直接初始化配置类，也可以通过 spring 解析配置生成配置类 proxy 服务代理层：服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton, 以 ServiceProxy 为中心，扩展接口为 ProxyFactory registry 注册中心层：封装服务地址的注册与发现，以服务 URL 为中心，扩展接口为 RegistryFactory, Registry, RegistryService cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心，扩展接口为 Cluster, Directory, Router, LoadBalance monitor 监控层：RPC 调用次数和调用时间监控，以 Statistics 为中心，扩展接口为 MonitorFactory, Monitor, MonitorService protocol 远程调用层：封装 RPC 调用，以 Invocation, Result 为中心，扩展接口为 Protocol, Invoker, Exporter exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心，扩展接口为 Exchanger, ExchangeChannel, ExchangeClient, ExchangeServer transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为 Channel, Transporter, Client, Server, Codec serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput, ThreadPool 36.你读过Dubbo的源码吗？ 这个问题其实面试中如果问dubbo的话，基本就会带这个问题。有时间的话，大家可以下载源码，读一读，如果大家有兴趣的话，我会出后续文章。 参考：http://dubbo.apache.org/en-us/ "},"zother1-JavaFaceNotes/IO&NIO.html":{"url":"zother1-JavaFaceNotes/IO&NIO.html","title":"IO NIO","keywords":"","body":"IO&NIO 1.什么是IO流？ 它是一种数据的流从源头流到目的地。比如文件拷贝，输入流和输出流都包括了。输入流从文件中读取数据存储到进程(process)中，输出流从进程中读取数据然后写入到目标文件。 2.java中有几种类型的流？ 按照单位大小：字符流、字节流。按照流的方向：输出流、输入流。 3.字节流和字符流哪个好？怎么选择？ 缓大多数情况下使用字节流会更好，因为字节流是字符流的包装，而大多数时候 IO 操作都是直接操作磁盘文件，所以这些流在传输时都是以字节的方式进行的（图片等都是按字节存储的） 如果对于操作需要通过 IO 在内存中频繁处理字符串的情况使用字符流会好些，因为字符流具备缓冲区，提高了性能 4.读取数据量大的文件时，速度会很慢，如何选择流？ 字节流时，选择BufferedInputStream和BufferedOutputStream。 字符流时，选择BufferedReader 和 BufferedWriter 5. IO模型有几种？ 阻塞IO、非阻塞IO、多路复用IO、信号驱动IO以及异步IO。 6.阻塞IO（blocking IO） 应用程序调用一个IO函数，导致应用程序阻塞，如果数据已经准备好，从内核拷贝到用户空间，否则一直等待下去。一个典型的读操作流程大致如下图，当用户进程调用recvfrom这个系统调用时，kernel就开始了IO的第一个阶段：准备数据，就是数据被拷贝到内核缓冲区中的一个过程（很多网络IO数据不会那么快到达，如没收一个完整的UDP包），等数据到操作系统内核缓冲区了，就到了第二阶段：将数据从内核缓冲区拷贝到用户内存，然后kernel返回结果，用户进程才会解除block状态，重新运行起来。blocking IO的特点就是在IO执行的两个阶段用户进程都会block住； 7.非阻塞I/O（nonblocking IO） 非阻塞I/O模型，我们把一个套接口设置为非阻塞就是告诉内核，当所请求的I/O操作无法完成时，不要将进程睡眠，而是返回一个错误。这样我们的I/O操作函数将不断的测试数据是否已经准备好，如果没有准备好，继续测试，直到数据准备好为止。在这个不断测试的过程中，会大量的占用CPU的时间。 ​ 当用户进程发出read操作时，如果kernel中数据还没准备好，那么并不会block用户进程，而是立即返回error，用户进程判断结果是error，就知道数据还没准备好，用户可以再次发read，直到kernel中数据准备好，并且用户再一次发read操作，产生system call，那么kernel 马上将数据拷贝到用户内存，然后返回；所以nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有。 　　阻塞IO一个线程只能处理一个IO流事件，要想同时处理多个IO流事件要么多线程要么多进程，这样做效率显然不会高，而非阻塞IO可以一个线程处理多个流事件，只要不停地询所有流事件即可，当然这个方式也不好，当大多数流没数据时，也是会大量浪费CPU资源；为了避免CPU空转，引进代理(select和poll，两种方式相差不大)，代理可以观察多个流I/O事件，空闲时会把当前线程阻塞掉，当有一个或多个I/O事件时，就从阻塞态醒过来，把所有IO流都轮询一遍，于是没有IO事件我们的程序就阻塞在select方法处，即便这样依然存在问题，我们从select出只是知道有IO事件发生，却不知道是哪几个流，还是只能轮询所有流，epoll这样的代理就可以把哪个流发生怎样的IO事件通知我们；　 8.I/O多路复用模型(IO multiplexing) I/O多路复用就在于单个进程可以同时处理多个网络连接IO,基本原理就是select，poll，epoll这些个函数会不断轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程，这三个functon会阻塞进程，但和IO阻塞不同，这些函数可以同时阻塞多个IO操作，而且可以同时对多个读操作，写操作IO进行检验，直到有数据到达，才真正调用IO操作函数，调用过程如下图；所以IO多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符(套接字描述符)其中任意一个进入就绪状态，select函数就可以返回。 　　IO多路复用的优势在于并发数比较高的IO操作情况，可以同时处理多个连接，和bloking IO一样socket是被阻塞的，只不过在多路复用中socket是被select阻塞，而在阻塞IO中是被socket IO给阻塞。 9.信号驱动I/O模型 可以用信号，让内核在描述符就绪时发送SIGIO信号通知我们，通过sigaction系统调用安装一个信号处理函数。该系统调用将立即返回，我们的进程继续工作，也就是说它没有被阻塞。当数据报准备好读取时，内核就为该进程产生一个SIGIO信号。我们随后既可以在信号处理函数中调用recvfrom读取数据报，并通知主循环数据已经准备好待处理。特点：等待数据报到达期间进程不被阻塞。主循环可以继续执行，只要等待来自信号处理函数的通知：既可以是数据已准备好被处理，也可以是数据报已准备好被读取 10.异步 I/O(asynchronous IO) 异步IO告知内核启动某个操作，并让内核在整个操作(包括将内核数据复制到我们自己的缓冲区)完成后通知我们，调用aioread（Posix异步I/O函数以aio或lio_开头）函数，给内核传递描述字、缓冲区指针、缓冲区大小（与read相同的3个参数）、文件偏移以及通知的方式，然后系统立即返回。我们的进程不阻塞于等待I/0操作的完成。当内核将数据拷贝到缓冲区后，再通知应用程序。 用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了 11.NIO与IO的区别？  NIO即New IO，这个库是在JDK1.4中才引入的。NIO和IO有相同的作用和目的，但实现方式不同，NIO主要用到的是块，所以NIO的效率要比IO高很多。在Java API中提供了两套NIO，一套是针对标准输入输出NIO，另一套就是网络编程NIO。 12.NIO和IO适用场景  NIO是为弥补传统IO的不足而诞生的，但是尺有所短寸有所长，NIO也有缺点，因为NIO是面向缓冲区的操作，每一次的数据处理都是对缓冲区进行的，那么就会有一个问题，在数据处理之前必须要判断缓冲区的数据是否完整或者已经读取完毕，如果没有，假设数据只读取了一部分，那么对不完整的数据处理没有任何意义。所以每次数据处理之前都要检测缓冲区数据。   那么NIO和IO各适用的场景是什么呢？   如果需要管理同时打开的成千上万个连接，这些连接每次只是发送少量的数据，例如聊天服务器，这时候用NIO处理数据可能是个很好的选择。   而如果只有少量的连接，而这些连接每次要发送大量的数据，这时候传统的IO更合适。使用哪种处理数据，需要在数据的响应等待时间和检查缓冲区数据的时间上作比较来权衡选择。 13.NIO核心组件 channel、buffer、selector 14.什么是channel 一个Channel（通道）代表和某一实体的连接，这个实体可以是文件、网络套接字等。也就是说，通道是Java NIO提供的一座桥梁，用于我们的程序和操作系统底层I/O服务进行交互。 通道是一种很基本很抽象的描述，和不同的I/O服务交互，执行不同的I/O操作，实现不一样，因此具体的有FileChannel、SocketChannel等。 通道使用起来跟Stream比较像，可以读取数据到Buffer中，也可以把Buffer中的数据写入通道。 当然，也有区别，主要体现在如下两点： 一个通道，既可以读又可以写，而一个Stream是单向的（所以分 InputStream 和 OutputStream） 通道有非阻塞I/O模式 15.Java NIO中最常用的通道实现? FileChannel：读写文件 DatagramChannel: UDP协议网络通信 SocketChannel：TCP协议网络通信 ServerSocketChannel：监听TCP连接 16.Buffer是什么? NIO中所使用的缓冲区不是一个简单的byte数组，而是封装过的Buffer类，通过它提供的API，我们可以灵活的操纵数据。 与Java基本类型相对应，NIO提供了多种 Buffer 类型，如ByteBuffer、CharBuffer、IntBuffer等，区别就是读写缓冲区时的单位长度不一样（以对应类型的变量为单位进行读写）。 17.核心Buffer实现有哪些？ 核心的buffer实现有这些：ByteBuffer、CharBuffer、DoubleBuffer、FloatBuffer、IntBuffer、LongBuffer、ShortBuffer，涵盖了所有的基本数据类型（4类8种，除了Boolean）。也有其他的buffer如MappedByteBuffer。 18.buffer读写数据基本操作 1）、将数据写入buffer 2）、调用buffer.flip() 3）、将数据从buffer中读取出来 4）、调用buffer.clear()或者buffer.compact() 在写buffer的时候，buffer会跟踪写入了多少数据，需要读buffer的时候，需要调用flip()来将buffer从写模式切换成读模式，读模式中只能读取写入的数据，而非整个buffer。   当数据都读完了，你需要清空buffer以供下次使用，可以有2种方法来操作：调用clear() 或者 调用compact()。   区别：clear方法清空整个buffer，compact方法只清除你已经读取的数据，未读取的数据会被移到buffer的开头，此时写入数据会从当前数据的末尾开始。 // 创建一个容量为48的ByteBuffer ByteBuffer buf = ByteBuffer.allocate(48); // 从channel中读（取数据然后写）入buffer int bytesRead = inChannel.read(buf); // 下面是读取buffer while (bytesRead != -1) { buf.flip(); // 转换buffer为读模式 System.out.print((char) buf.get()); // 一次读取一个byte buf.clear(); //清空buffer准备下一次写入 } 19.Selector是什么? Selector（选择器）是一个特殊的组件，用于采集各个通道的状态（或者说事件）。我们先将通道注册到选择器，并设置好关心的事件，然后就可以通过调用select()方法，静静地等待事件发生。 20.通道可以监听那几个事件？ 通道有如下4个事件可供我们监听： Accept：有可以接受的连接 Connect：连接成功 Read：有数据可读 Write：可以写入数据了 21.为什么要用Selector？ 如果用阻塞I/O，需要多线程（浪费内存），如果用非阻塞I/O，需要不断重试（耗费CPU）。Selector的出现解决了这尴尬的问题，非阻塞模式下，通过Selector，我们的线程只为已就绪的通道工作，不用盲目的重试了。比如，当所有通道都没有数据到达时，也就没有Read事件发生，我们的线程会在select()方法处被挂起，从而让出了CPU资源。 22.Selector处理多Channel图文说明 要使用一个Selector，你要先注册这个Selector的Channels。然后你调用Selector的select()方法。这个方法会阻塞，直到它注册的Channels当中有一个准备好了的事件发生了。当select()方法返回的时候，线程可以处理这些事件，如新的连接的到来，数据收到了等。 参考: https://www.cnblogs.com/sharing-java/p/10791802.html https://blog.csdn.net/zengxiantao1994/article/details/88094910 https://www.cnblogs.com/xueSpring/p/9513266.html "},"zother1-JavaFaceNotes/Java基础.html":{"url":"zother1-JavaFaceNotes/Java基础.html","title":"Java基础","keywords":"","body":"Java基础题 1.Java语言的三大特性 封装： 首先，属性可用来描述同一类事物的特征，方法可描述一类事物可做的操作。封装就是把属于同一类事物的共性（包括属性与方法）归到一个类中，以方便使用。 1)概念：封装也称为信息隐藏，是指利用抽象数据类型将数据和基于数据的操作封装在一起，使其构成一个不可分割的独立实体，数据被保护在抽象数据类型的内部，尽可能地隐藏内部的细节，只保留一些对外接口使之与外部发生联系。系统的其他部分只有通过包裹在数据外面的被授权的操作来与这个抽象数据类型交流与交互。也就是说，用户无需知道对象内部方法的实现细节，但可以根据对象提供的外部接口(对象名和参数)访问该对象。 2)好处：(1)实现了专业的分工。将能实现某一特定功能的代码封装成一个独立的实体后，各程序员可以在需要的时候调用，从而实现了专业的分工。(2)隐藏信息，实现细节。通过控制访问权限可以将可以将不想让客户端程序员看到的信息隐藏起来，如某客户的银行的密码需要保密，只能对该客户开发权限。 继承： 就是个性对共性的属性与方法的接受，并加入个性特有的属性与方法 1.概念：一个类继承另一个类，则称继承的类为子类，被继承的类为父类。 2.目的：实现代码的复用。 3.理解：子类与父类的关系并不是日常生活中的父子关系，子类与父类而是一种特殊化与一般化的关系，是is-a的关系，子类是父类更加详细的分类。如class dog extends animal,就可以理解为dog is a animal.注意设计继承的时候，若要让某个类能继承，父类需适当开放访问权限，遵循里氏代换原则，即向修改关闭对扩展开放，也就是开-闭原则。 4.结果：继承后子类自动拥有了父类的属性和方法，但特别注意的是，父类的私有属性和构造方法并不能被继承。 另外子类可以写自己特有的属性和方法，目的是实现功能的扩展，子类也可以复写父类的方法即方法的重写。 多态： 多态的概念发展出来，是以封装和继承为基础的。 多态就是在抽象的层面上实施一个统一的行为，到个体（具体）的层面上时，这个统一的行为会因为个体（具体）的形态特征而实施自己的特征行为。（针对一个抽象的事，对于内部个体又能找到其自身的行为去执行。） 1.概念：相同的事物，调用其相同的方法，参数也相同时，但表现的行为却不同。 2.理解：子类以父类的身份出现，但做事情时还是以自己的方法实现。子类以父类的身份出现需要向上转型(upcast)，其中向上转型是由JVM自动实现的，是安全的，但向下转型(downcast)是不安全的，需要强制转换。子类以父类的身份出现时自己特有的属性和方法将不能使用。 2.Java语言主要特性 Java语言是易学的。Java语言的语法与C语言和C++语言很接近，使得大多数程序员很容易学习和使用Java。 Java语言是强制面向对象的。Java语言提供类、接口和继承等原语，为了简单起见，只支持类之间的单继承，但支持接口之间的多继承，并支持类与接口之间的实现机制（关键字为implements）。 Java语言是分布式的。Java语言支持Internet应用的开发，在基本的Java应用编程接口中有一个网络应用编程接口（java net），它提供了用于网络应用编程的类库，包括URL、URLConnection、Socket、ServerSocket等。Java的RMI（远程方法激活）机制也是开发分布式应用的重要手段。 Java语言是健壮的。Java的强类型机制、异常处理、垃圾的自动收集等是Java程序健壮性的重要保证。对指针的丢弃是Java的明智选择。 Java语言是安全的。Java通常被用在网络环境中，为此，Java提供了一个安全机制以防恶意代码的攻击。如：安全防范机制（类ClassLoader），如分配不同的名字空间以防替代本地的同名类、字节代码检查。 Java语言是体系结构中立的。Java程序（后缀为java的文件）在Java平台上被编译为体系结构中立的字节码格式（后缀为class的文件），然后可以在实现这个Java平台的任何系统中运行。 Java语言是解释型的。如前所述，Java程序在Java平台上被编译为字节码格式，然后可以在实现这个Java平台的任何系统的解释器中运行。（一次编译，到处运行） Java是性能略高的。与那些解释型的高级脚本语言相比，Java的性能还是较优的。 Java语言是原生支持多线程的。在Java语言中，线程是一种特殊的对象，它必须由Thread类或其子（孙）类来创建。 3. JDK 和 JRE 有什么区别 JDK：Java Development Kit 的简称，Java 开发工具包，提供了 Java 的开发环境和运行环境。 JRE：Java Runtime Environment 的简称，Java 运行环境，为 Java 的运行提供了所需环境。 具体来说 JDK 其实包含了 JRE，同时还包含了编译 Java 源码的编译器 Javac，还包含了很多 Java 程序调试和分析的工具。简单来说：如果你需要运行 Java 程序，只需安装 JRE 就可以了，如果你需要编写 Java 程序，需要安装 JDK。 4.Java基本数据类型及其封装类 Tips:boolean类型占了单独使用是4个字节，在数组中又是1个字节 基本类型所占的存储空间是不变的。这种不变性也是Java具有可移植性的原因之一。 基本类型放在栈中，直接存储值。 所有数值类型都有正负号，没有无符号的数值类型。 为什么需要封装类？ 因为泛型类包括预定义的集合，使用的参数都是对象类型，无法直接使用基本数据类型，所以Java又提供了这些基本类型的封装类。 基本类型和对应的封装类由于本质的不同。具有一些区别： 1.基本类型只能按值传递，而封装类按引用传递。 2.基本类型会在栈中创建，而对于对象类型，对象在堆中创建，对象的引用在栈中创建，基本类型由于在栈中，效率会比较高，但是可能存在内存泄漏的问题。 5.如果main方法被声明为private会怎样？ 能正常编译，但运行的时候会提示”main方法不是public的”。在idea中如果不用public修饰，则会自动去掉可运行的按钮。 6.说明一下public static void main(String args[])这段声明里每个关键字的作用 public: main方法是Java程序运行时调用的第一个方法，因此它必须对Java环境可见。所以可见性设置为pulic. static: Java平台调用这个方法时不会创建这个类的一个实例，因此这个方法必须声明为static。 void: main方法没有返回值。 String是命令行传进参数的类型，args是指命令行传进的字符串数组。 7.==与equals的区别 ==比较两个对象在内存里是不是同一个对象，就是说在内存里的存储位置一致。两个String对象存储的值是一样的，但有可能在内存里存储在不同的地方 . ==比较的是引用而equals方法比较的是内容。public boolean equals(Object obj) 这个方法是由Object对象提供的，可以由子类进行重写。默认的实现只有当对象和自身进行比较时才会返回true,这个时候和==是等价的。String, BitSet, Date, 和File都对equals方法进行了重写，对两个String对象 而言，值相等意味着它们包含同样的字符序列。对于基本类型的包装类来说，值相等意味着对应的基本类型的值一样。 public class EqualsTest { public static void main(String[] args) { String s1 = “abc”; String s2 = s1; String s5 = “abc”; String s3 = new String(”abc”); String s4 = new String(”abc”); System.out.println(”== comparison : ” + (s1 == s5)); System.out.println(”== comparison : ” + (s1 == s2)); System.out.println(”Using equals method : ” + s1.equals(s2)); System.out.println(”== comparison : ” + s3 == s4); System.out.println(”Using equals method : ” + s3.equals(s4)); } } 结果： == comparison : true == comparison : true Using equals method : true false Using equals method :true 8.Object有哪些公用方法 Object是所有类的父类，任何类都默认继承Object clone 保护方法，实现对象的浅复制，只有实现了Cloneable接口才可以调用该方法，否则抛出CloneNotSupportedException异常。 equals 在Object中与==是一样的，子类一般需要重写该方法。 hashCode 该方法用于哈希查找，重写了equals方法一般都要重写hashCode方法。这个方法在一些具有哈希功能的Collection中用到。 getClass final方法，获得运行时类型 wait 使当前线程等待该对象的锁，当前线程必须是该对象的拥有者，也就是具有该对象的锁。 wait() 方法一直等待，直到获得锁或者被中断。 wait(long timeout) 设定一个超时间隔，如果在规定时间内没有获得锁就返回。 调用该方法后当前线程进入睡眠状态，直到以下事件发生 1、其他线程调用了该对象的notify方法。 2、其他线程调用了该对象的notifyAll方法。 3、其他线程调用了interrupt中断该线程。 4、时间间隔到了。 5、此时该线程就可以被调度了，如果是被中断的话就抛出一个InterruptedException异常。 notify 唤醒在该对象上等待的某个线程。 notifyAll 唤醒在该对象上等待的所有线程。 toString 转换成字符串，一般子类都有重写，否则打印句柄。 9.为什么Java里没有全局变量? 全局变量是全局可见的，Java不支持全局可见的变量，因为：全局变量破坏了引用透明性原则。全局变量导致了命名空间的冲突。 10.while循环和do循环有什么不同？ while结构在循环的开始判断下一个迭代是否应该继续。do/while结构在循环的结尾来判断是否将继续下一轮迭代。do结构至少会执行一次循环体。 11.char型变量中能不能存储一个中文汉字?为什么？ 可以。Java默认Unicode编码。Unicode码占16位。 char两个字节刚好16位。 12.public，private，protected的区别，继承方法与访问权限 Tips:不写默认default 13.float f=3.4;是否正确？ 不正确。3.4是双精度数，将双精度型（double）赋值给浮点型（float）属于下转型（down-casting，也称为窄化）会造成精度损失，因此需要强制类型转换float f =(float)3.4; 或者写成float f =3.4F。 14.short s1 = 1; s1 = s1 + 1;有错吗？short s1 = 1; s1 += 1;有错吗？ 对于short s1 = 1; s1 = s1 + 1；由于1是int类型，因此s1+1运算结果也是int 型，需要强制转换类型才能赋值给short型。而short s1 = 1; s1 += 1；+=操作符会进行隐式自动类型转换，是 Java 语言规定的运算符；Java编译器会对它进行特殊处理，因此可以正确编译。因为s1+= 1;相当于s1 = (short)(s1 + 1)。 15.&和&&的区别？ &：(1)按位与；(2)逻辑与。 按位与: 0 & 1 = 0 ; 0 & 0 = 0; 1 & 1 = 1 逻辑与: a == b & b ==c （即使a==b已经是 false了，程序还会继续判断b是否等于c) 2.&&: 短路与 ​ a== b && b== c （当a==b 为false则不会继续判断b是否等与c) 比如判断某对象中的属性是否等于某值，则必须用&&,否则会出现空指针问题。 16.IntegerCache public class IntegerTest { public static void main(String[] args) { Integer a = 100, b = 100 ,c = 129,d = 129; System.out.println(a==b); System.out.println(c==d); } } 结果: true false 小朋友，你是否有很多问号? 来解释一下: /** * Cache to support the object identity semantics of autoboxing for values between * -128 and 127 (inclusive) as required by JLS. * * The cache is initialized on first usage. The size of the cache * may be controlled by the {@code -XX:AutoBoxCacheMax=} option. * During VM initialization, java.lang.Integer.IntegerCache.high property * may be set and saved in the private system properties in the * sun.misc.VM class. */ private static class IntegerCache { static final int low = -128; static final int high; static final Integer cache[]; static { // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(\"java.lang.Integer.IntegerCache.high\"); if (integerCacheHighPropValue != null) { try { int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); } catch( NumberFormatException nfe) { // If the property cannot be parsed into an int, ignore it. } } high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k = 127; } private IntegerCache() {} } public static Integer valueOf(int i) { assert IntegerCache.high >= 127; if (i >= IntegerCache.low && i 通过源码，我们可以看出, -128~127之间做了缓存。考虑到高频数值的复用场景，这样做还是很合理的，合理优化。最大边界可以通过-XX:AutoBoxCacheMax进行配置。 17.Locale类是什么？ Locale类用来根据语言环境来动态调整程序的输出。 18.Java中final、finally、finalize的区别与用法 final final是一个修饰符也是一个关键字。 被final修饰的类无法被继承 对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改； 如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。但是它指向的对象的内容是可变的。 被final修饰的方法将无法被重写，但允许重载 注意：类的private方法会隐式地被指定为final方法。 finally finally是一个关键字。 finally在异常处理时提供finally块来执行任何清除操作。不管有没有异常被抛出或者捕获，finally块都会执行，通常用于释放资源。 finally块正常情况下一定会被执行。但是有至少两个极端情况： 如果对应的try块没有执行，则这个try块的finally块并不会被执行 如果在try块中jvm关机，例如system.exit(n)，则finally块也不会执行（都拔电源了，怎么执行） finally块中如果有return语句，则会覆盖try或者catch中的return语句，导致二者无法return，所以强烈建议finally块中不要存在return关键字 finalize finalize()是Object类的protected方法，子类可以覆盖该方法以实现资源清理工作。 GC在回收对象之前都会调用该方法 finalize()方法是存在很多问题的： java语言规范并不保证finalize方法会被及时地执行，更根本不会保证它们一定会被执行 finalize()方法可能带来性能问题，因为JVM通常在单独的低优先级线程中完成finalize的执行 finalize()方法中，可将待回收对象赋值给GC Roots可达的对象引用，从而达到对象再生的目的 finalize方法最多由GC执行一次（但可以手动调用对象的finalize方法） 19.hashCode()和equals()的区别 下边从两个角度介绍了他们的区别：一个是性能，一个是可靠性。他们之间的主要区别也基本体现在这里。 1.equals()既然已经能实现对比的功能了，为什么还要hashCode()呢？ 因为重写的equals（）里一般比较的比较全面比较复杂，这样效率就比较低，而利用hashCode()进行对比，则只要生成一个hash值进行比较就可以了，效率很高。 2.hashCode()既然效率这么高为什么还要equals()呢？ 因为hashCode()并不是完全可靠，有时候不同的对象他们生成的hashcode也会一样（生成hash值得公式可能存在的问题），所以hashCode()只能说是大部分时候可靠，并不是绝对可靠，所以我们可以得出（PS：以下两条结论是重点，很多人面试的时候都说不出来）： equals()相等的两个对象他们的hashCode()肯定相等，也就是用equals()对比是绝对可靠的。 hashCode()相等的两个对象他们的equals()不一定相等，也就是hashCode()不是绝对可靠的。 扩展 1.阿里巴巴开发规范明确规定： 只要重写 equals，就必须重写 hashCode； 因为 Set 存储的是不重复的对象，依据 hashCode 和 equals 进行判断，所以 Set 存储的对象必须重写这两个方法； 如果自定义对象做为 Map 的键，那么必须重写 hashCode 和 equals； String 重写了 hashCode 和 equals 方法，所以我们可以非常愉快地使用 String 对象作为 key 来使用； 2、什么时候需要重写？ 一般的地方不需要重载hashCode，只有当类需要放在HashTable、HashMap、HashSet等等hash结构的集合时才会重载hashCode。 3、那么为什么要重载hashCode呢？ 如果你重写了equals，比如说是基于对象的内容实现的，而保留hashCode的实现不变，那么很可能某两个对象明明是“相等”，而hashCode却不一样。 这样，当你用其中的一个作为键保存到hashMap、hasoTable或hashSet中，再以“相等的”找另一个作为键值去查找他们的时候，则根本找不到。 4、为什么equals()相等，hashCode就一定要相等，而hashCode相等，却不要求equals相等? 因为是按照hashCode来访问小内存块，所以hashCode必须相等。 HashMap获取一个对象是比较key的hashCode相等和equals为true。 之所以hashCode相等，却可以equal不等，就比如ObjectA和ObjectB他们都有属性name，那么hashCode都以name计算，所以hashCode一样，但是两个对象属于不同类型，所以equals为false。 5、为什么需要hashCode? 通过hashCode可以很快的查到小内存块。 通过hashCode比较比equals方法快，当get时先比较hashCode，如果hashCode不同，直接返回false。 20.深拷贝和浅拷贝的区别是什么? 浅拷贝 （1）、定义 被复制对象的所有变量都含有与原来的对象相同的值，而所有的对其他对象的引用仍然指向原来的对象。即对象的浅拷贝会对“主”对象进行拷贝，但不会复制主对象里面的对象。”里面的对象“会在原来的对象和它的副本之间共享。 简而言之，浅拷贝仅仅复制所考虑的对象，而不复制它所引用的对象 （2）、浅拷贝实例 package com.test; public class ShallowCopy { public static void main(String[] args) throws CloneNotSupportedException { Teacher teacher = new Teacher(); teacher.setName(\"riemann\"); teacher.setAge(27); Student2 student1 = new Student2(); student1.setName(\"edgar\"); student1.setAge(18); student1.setTeacher(teacher); Student2 student2 = (Student2) student1.clone(); System.out.println(\"拷贝后\"); System.out.println(student2.getName()); System.out.println(student2.getAge()); System.out.println(student2.getTeacher().getName()); System.out.println(student2.getTeacher().getAge()); System.out.println(\"修改老师的信息后——————\"); // 修改老师的信息 teacher.setName(\"Games\"); System.out.println(student1.getTeacher().getName()); System.out.println(student2.getTeacher().getName()); } } class Teacher implements Cloneable { private String name; private int age; public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } } class Student2 implements Cloneable { private String name; private int age; private Teacher teacher; public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } public Teacher getTeacher() { return teacher; } public void setTeacher(Teacher teacher) { this.teacher = teacher; } public Object clone() throws CloneNotSupportedException { Object object = super.clone(); return object; } } 输出结果: 拷贝后 edgar 18 riemann 27 修改老师的信息后—————— Games Games 结果分析： 两个引用student1和student2指向不同的两个对象，但是两个引用student1和student2中的两个teacher引用指向的是同一个对象，所以说明是浅拷贝。 深拷贝 （1）、定义 深拷贝是一个整个独立的对象拷贝，深拷贝会拷贝所有的属性,并拷贝属性指向的动态分配的内存。当对象和它所引用的对象一起拷贝时即发生深拷贝。深拷贝相比于浅拷贝速度较慢并且花销较大。 简而言之，深拷贝把要复制的对象所引用的对象都复制了一遍。 （2）、深拷贝实例 package com.test; public class DeepCopy { public static void main(String[] args) throws CloneNotSupportedException { Teacher2 teacher = new Teacher2(); teacher.setName(\"riemann\"); teacher.setAge(27); Student3 student1 = new Student3(); student1.setName(\"edgar\"); student1.setAge(18); student1.setTeacher(teacher); Student3 student2 = (Student3) student1.clone(); System.out.println(\"拷贝后\"); System.out.println(student2.getName()); System.out.println(student2.getAge()); System.out.println(student2.getTeacher().getName()); System.out.println(student2.getTeacher().getAge()); System.out.println(\"修改老师的信息后——————\"); // 修改老师的信息 teacher.setName(\"Games\"); System.out.println(student1.getTeacher().getName()); System.out.println(student2.getTeacher().getName()); } } class Teacher2 implements Cloneable { private String name; private int age; public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } public Object clone() throws CloneNotSupportedException { return super.clone(); } } class Student3 implements Cloneable { private String name; private int age; private Teacher2 teacher; public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } public Teacher2 getTeacher() { return teacher; } public void setTeacher(Teacher2 teacher) { this.teacher = teacher; } public Object clone() throws CloneNotSupportedException { // 浅复制时： // Object object = super.clone(); // return object; // 改为深复制： Student3 student = (Student3) super.clone(); // 本来是浅复制，现在将Teacher对象复制一份并重新set进来 student.setTeacher((Teacher2) student.getTeacher().clone()); return student; } } 输出结果： 拷贝后 edgar 18 riemann 27 修改老师的信息后—————— Games riemann 结果分析： 两个引用student1和student2指向不同的两个对象，两个引用student1和student2中的两个teacher引用指向的是两个对象，但对teacher对象的修改只能影响student1对象,所以说是深拷贝。 21.Java 中操作字符串都有哪些类？它们之间有什么区别？ String、StringBuffer、StringBuilder。 String 和 StringBuffer、StringBuilder 的区别在于 String 声明的是不可变的对象，每次操作都会生成新的 String 对象，然后将指针指向新的 String 对象，而 StringBuffer、StringBuilder 可以在原有对象的基础上进行操作，所以在经常改变字符串内容的情况下最好不要使用 String。 StringBuffer 和 StringBuilder 最大的区别在于，StringBuffer 是线程安全的，而 StringBuilder 是非线程安全的，但 StringBuilder 的性能却高于 StringBuffer，所以在单线程环境下推荐使用 StringBuilder，多线程环境下推荐使用 StringBuffer。 22.String str=\"a\"与 String str=new String(\"a\")一样吗？ 不一样，因为内存的分配方式不一样。 String str=\"a\"; -> 常量池 String str=new String(\"a\") -> 堆内存 23.抽象类能使用 final 修饰吗？ 不能。定义抽象类就是让其他类继承的，而 final修饰的类不能被继承。 24.static关键字5连问 （1）抽象的（abstract）方法是否可同时是静态的（static）？ ​ 抽象方法将来是要被重写的，而静态方法是不能重写的，所以这个是错误的。 (2)是否可以从一个静态（static）方法内部发出对非静态方法的调用？ ​ 不可以，静态方法只能访问静态成员，非静态方法的调用要先创建对象。 (3) static 可否用来修饰局部变量？ ​ static 不允许用来修饰局部变量 （4）内部类与静态内部类的区别？ 静态内部类相对与外部类是独立存在的，在静态内部类中无法直接访问外部类中变量、方法。如果 要访问的话，必须要new一个外部类的对象，使用new出来的对象来访问。但是可以直接访问静态的变量、调用静态的方法； 普通内部类作为外部类一个成员而存在，在普通内部类中可以直接访问外部类属性，调用外部类的方法。 如果外部类要访问内部类的属性或者调用内部类的方法，必须要创建一个内部类的对象，使用该对象访问属性或者调用方法。 如果其他的类要访问普通内部类的属性或者调用普通内部类的方法，必须要在外部类中创建一个普通内部类的对象作为一个属性，外同类可以通过该属性调用普通内部类的方法或者访问普通内部类的属性 如果其他的类要访问静态内部类的属性或者调用静态内部类的方法，直接创建一个静态内部类对象即可。 （5）Java中是否可以覆盖(override) 一个private或者是static的方法？ Java中static方法不能被覆盖，因为方法覆盖是基于运行时动态绑定的，而static方法是编译时静态绑定的。static方法跟类的任何实例都不相关，所以概念上不适用。 25. 重载（Overload）和重写（Override）的区别。重载的方法能否根据返回类型进行区分？ 方法的重载和重写都是实现多态的方式，区别在于前者实现的是编译时的多态性，而后者实现的是运行时的多态性。重载发生在一个类中，同名的方法如果有不同的参数列表（参数类型不同、参数个数不同或者二者都不同）则视为重载；重写发生在子类与父类之间，重写要求子类被重写方法与父类被重写方法有相同的参数列表，有兼容的返回类型，比父类被重写方法更好访问，不能比父类被重写方法声明更多的异常（里氏代换原则）。重载对返回类型没有特殊的要求，不能根据返回类型进行区分。 26.Java的四种引用 1、强引用 最普遍的一种引用方式，如String s = \"abc\"，变量s就是字符串“abc”的强引用，只要强引用存在，则垃圾回收器就不会回收这个对象。 2、软引用（SoftReference） 用于描述还有用但非必须的对象，如果内存足够，不回收，如果内存不足，则回收。一般用于实现内存敏感的高速缓存，软引用可以和引用队列ReferenceQueue联合使用，如果软引用的对象被垃圾回收，JVM就会把这个软引用加入到与之关联的引用队列中。 3、弱引用（WeakReference） 弱引用和软引用大致相同，弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。 4、虚引用（PhantomReference） 就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。 虚引用主要用来跟踪对象被垃圾回收器回收的活动。 虚引用与软引用和弱引用的一个区别在于： 虚引用必须和引用队列 （ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。 27.Java 中Comparator 与Comparable 有什么不同？ Comparable 接口用于定义对象的自然顺序，是排序接口，而 comparator 通常用于定义用户定制的顺序，是比较接口。我们如果需要控制某个类的次序，而该类本身不支持排序(即没有实现Comparable接口)，那么我们就可以建立一个“该类的比较器”来进行排序。Comparable 总是只有一个，但是可以有多个 comparator 来定义对象的顺序。 28. Java 序列化,反序列化? Java 序列化就是指将对象转换为字节序列的过程，反序列化是指将字节序列转换成目标对象的过程。 29.什么情况需要Java序列化? 当 Java 对象需要在网络上传输 或者 持久化存储到文件中时。 30.序列化的实现? 让类实现Serializable接口,标注该类对象是可被序列。 31.如果某些数据不想序列化，如何处理? 在字段面前加 transient 关键字，例如: transient private String phone;//不参与序列化 32.Java泛型和类型擦除？ 泛型即参数化类型，在创建集合时，指定集合元素的类型，此集合只能传入该类型的参数。类型擦除：java编译器生成的字节码不包含泛型信息，所以在编译时擦除：1.泛型用最顶级父类替换；2.移除。 "},"zother1-JavaFaceNotes/java集合.html":{"url":"zother1-JavaFaceNotes/java集合.html","title":"java集合","keywords":"","body":"java集合 1.Java集合框架的基础接口有哪些？ Collection为集合层级的根接口。一个集合代表一组对象，这些对象即为它的元素。Java平台不提供这个接口任何直接的实现。 Set是一个不能包含重复元素的集合。这个接口对数学集合抽象进行建模，被用来代表集合，就如一副牌。 List是一个有序集合，可以包含重复元素。你可以通过它的索引来访问任何元素。List更像长度动态变换的数组。 Map是一个将key映射到value的对象.一个Map不能包含重复的key：每个key最多只能映射一个value。 一些其它的接口有Queue、Dequeue、SortedSet、SortedMap和ListIterator。 2.Collection 和 Collections 有什么区别？ Collection 是一个集合接口，它提供了对集合对象进行基本操作的通用接口方法，所有集合都是它的子类，比如 List、Set 等。 Collections 是一个包装类，包含了很多静态方法，不能被实例化，就像一个工具类，比如提供的排序方法： Collections. sort(list)。 3.List、Set、Map是否继承自Collection接口？ List、Set 是，Map 不是。Map是键值对映射容器，与List和Set有明显的区别，而Set存储的零散的元素且不允许有重复元素（数学中的集合也是如此），List是线性结构的容器，适用于按数值索引访问元素的情形。 4.Collections.sort排序内部原理 在Java 6中Arrays.sort()和Collections.sort()使用的是MergeSort，而在Java 7中，内部实现换成了TimSort，其对对象间比较的实现要求更加严格。 5.List、Set、Map 之间的区别是什么？ List、Set、Map 的区别主要体现在两个方面：元素是否有序、是否允许元素重复。 三者之间的区别，如下表： 6.HashMap 和 Hashtable 有什么区别？ （1）HashMap允许key和value为null，而HashTable不允许。 （2）HashTable是同步的，而HashMap不是。所以HashMap适合单线程环境，HashTable适合多线程环境。 （3）在Java1.4中引入了LinkedHashMap，HashMap的一个子类，假如你想要遍历顺序，你很容易从HashMap转向LinkedHashMap，但是HashTable不是这样的，它的顺序是不可预知的。 （4）HashMap提供对key的Set进行遍历，因此它是fail-fast的，但HashTable提供对key的Enumeration进行遍历，它不支持fail-fast。 （5）HashTable被认为是个遗留的类，如果你寻求在迭代的时候修改Map，你应该使用CocurrentHashMap。 7.如何决定使用 HashMap 还是 TreeMap？ 对于在 Map 中插入、删除、定位一个元素这类操作，HashMap 是最好的选择，因为相对而言 HashMap 的插入会更快，但如果你要对一个 key 集合进行有序的遍历，那 TreeMap 是更好的选择。 8.说一下 HashMap 的实现原理？ HashMap 基于 Hash 算法实现的，我们通过 put(key,value)存储，get(key)来获取。当传入 key 时，HashMap 会根据 key. hashCode() 计算出 hash 值，根据 hash 值将 value 保存在 bucket 里。当计算出的 hash 值相同时，我们称之为 hash 冲突，HashMap 的做法是用链表和红黑树存储相同 hash 值的 value。当 hash 冲突的个数比较少时，使用链表否则使用红黑树。 9.说一下 HashSet 的实现原理？ HashSet 是基于 HashMap 实现的，HashSet 底层使用 HashMap 来保存所有元素，因此 HashSet 的实现比较简单，相关 HashSet 的操作，基本上都是直接调用底层 HashMap 的相关方法来完成，HashSet 不允许重复的值。 10.ArrayList 和 LinkedList 的区别是什么？ 数据结构实现：ArrayList 是动态数组的数据结构实现，而 LinkedList 是双向链表的数据结构实现。 随机访问效率：ArrayList 比 LinkedList 在随机访问的时候效率要高，因为 LinkedList 是线性的数据存储方式，所以需要移动指针从前往后依次查找。 增加和删除效率：在非首尾的增加和删除操作，LinkedList 要比 ArrayList 效率要高，因为 ArrayList 增删操作要影响数组内的其他数据的下标。 综合来说，在需要频繁读取集合中的元素时，更推荐使用 ArrayList，而在插入和删除操作较多时，更推荐使用 LinkedList。 11.为何Map接口不继承Collection接口？ 尽管Map接口和它的实现也是集合框架的一部分，但Map不是集合，集合也不是Map。因此，Map继承Collection毫无意义，反之亦然。 如果Map继承Collection接口，那么元素去哪儿？Map包含key-value对，它提供抽取key或value列表集合的方法，但是它不适合“一组对象”规范。 12.ArrayList和Vector有何异同点？ ArrayList和Vector在很多时候都很类似。 （1）两者都是基于索引的，内部由一个数组支持。 （2）两者维护插入的顺序，我们可以根据插入顺序来获取元素。 （3）ArrayList和Vector的迭代器实现都是fail-fast的。 （4）ArrayList和Vector两者允许null值，也可以使用索引值对元素进行随机访问。 以下是ArrayList和Vector的不同点。 （1）Vector是同步的，而ArrayList不是。然而，如果你寻求在迭代的时候对列表进行改变，你应该使用CopyOnWriteArrayList。 （2）ArrayList比Vector快，它因为有同步，不会过载。 （3）ArrayList更加通用，因为我们可以使用Collections工具类轻易地获取同步列表和只读列表。 13.Array 和 ArrayList 有何区别？ Array 可以存储基本数据类型和对象，ArrayList 只能存储对象。 Array 是指定固定大小的，而 ArrayList 大小是自动扩展的。 Array 内置方法没有 ArrayList 多，比如 addAll、removeAll、iteration 等方法只有 ArrayList 有。 14.在 Queue 中 poll()和 remove()有什么区别？ 相同点：都是返回第一个元素，并在队列中删除返回的对象。 不同点：如果没有元素 poll()会返回 null，而 remove()会直接抛出 NoSuchElementException 异常。 代码示例： Queue queue = new LinkedList(); queue. offer(\"string\"); // add System. out. println(queue. poll()); System. out. println(queue. remove()); System. out. println(queue. size()); 15.LinkedHashMap有什么特点? LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 16.HashMap的底层实现原理？（高频问题） 从结构实现来讲，HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的，如下如所示。 这里需要讲明白两个问题：数据底层具体存储的是什么？这样的存储方式有什么优点呢？ (1) 从源码可知，HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组。我们来看Node[JDK1.8]是何物。 static class Node implements Map.Entry { final int hash; //用来定位数组索引位置 final K key; V value; Node next; //链表的下一个node Node(int hash, K key, V value, Node next) { … } public final K getKey(){ … } public final V getValue() { … } public final String toString() { … } public final int hashCode() { … } public final V setValue(V newValue) { … } public final boolean equals(Object o) { … } } Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。 (2) HashMap就是使用哈希表来存储的。哈希表为解决冲突，可以采用开放地址法和链地址法等来解决问题，Java中HashMap采用了链地址法。链地址法，简单来说，就是数组加链表的结合。在每个数组元素上都一个链表结构，当数据被Hash后，得到数组下标，把数据放在对应下标元素的链表上。例如程序执行下面代码： map.put(\"美团\",\"小美\"); 系统将调用\"美团\"这个key的hashCode()方法得到其hashCode 值（该方法适用于每个Java对象），然后再通过Hash算法的后两步运算（高位运算和取模运算，下文有介绍）来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 如果哈希桶数组很大，即使较差的Hash算法也会比较分散，如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小，并在此基础上设计好的hash算法减少Hash碰撞。那么通过什么方式来控制map使得Hash碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？答案就是好的Hash算法和扩容机制。 在理解Hash和扩容流程之前，我们得先了解下HashMap的几个字段。从HashMap的默认构造函数源码可知，构造函数就是对下面几个字段进行初始化，源码如下： int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子 int modCount; int size; 首先，Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 在HashMap中，哈希桶数组table的长度length大小必须为2的n次方(一定是合数)，这是一种非常规的设计，常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数，具体证明可以参考http://blog.csdn.net/liuqiyao_01/article/details/14475159，Hashtable初始化桶大小为11，就是桶大小设计为素数的应用（Hashtable扩容后不能保证还是素数）。HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。 这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。 功能实现-方法 HashMap的内部功能实现很多，本文主要从根据key获取哈希桶数组索引位置、put方法的详细执行、扩容过程三个具有代表性的点深入展开讲解。 1.确定哈希桶数组索引位置 不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap定位数组索引位置，直接决定了hash方法的离散性能。先看看源码的实现(方法一+方法二): 方法一： static final int hash(Object key) { //jdk1.8 & jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h >>> 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); } 方法二： static int indexFor(int h, int length) { //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的 return h & (length-1); //第三步 取模运算 } 这里的Hash算法本质上就是三步：取key的hashCode值、高位运算、取模运算。 对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，在HashMap中是这样做的：调用方法二来计算该对象应该保存在table数组的哪个索引处。 这个方法非常巧妙，它通过h & (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h& (length-1)运算等价于对length取模，也就是h%length，但是&比%具有更高的效率。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h >>> 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。 下面举例说明下，n为table的长度。 2.HashMap的put方法 HashMap的put方法执行过程可以通过下图来理解，自己有兴趣可以去对比源码更清楚地研究学习。 ①.判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容； ②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③； ③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals； ④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤； ⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可； ⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。 JDK1.8HashMap的put方法源码如下: public V put(K key, V value) { 2 // 对key的hashCode()做hash 3 return putVal(hash(key), key, value, false, true); 4 } 5 6 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, 7 boolean evict) { 8 Node[] tab; Node p; int n, i; 9 // 步骤①：tab为空则创建 10 if ((tab = table) == null || (n = tab.length) == 0) 11 n = (tab = resize()).length; 12 // 步骤②：计算index，并对null做处理 13 if ((p = tab[i = (n - 1) & hash]) == null) 14 tab[i] = newNode(hash, key, value, null); 15 else { 16 Node e; K k; 17 // 步骤③：节点key存在，直接覆盖value 18 if (p.hash == hash && 19 ((k = p.key) == key || (key != null && key.equals(k)))) 20 e = p; 21 // 步骤④：判断该链为红黑树 22 else if (p instanceof TreeNode) 23 e = ((TreeNode)p).putTreeVal(this, tab, hash, key, value); 24 // 步骤⑤：该链为链表 25 else { 26 for (int binCount = 0; ; ++binCount) { 27 if ((e = p.next) == null) { 28 p.next = newNode(hash, key,value,null); //链表长度大于8转换为红黑树进行处理 29 if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st 30 treeifyBin(tab, hash); 31 break; 32 } // key已经存在直接覆盖value 33 if (e.hash == hash && 34 ((k = e.key) == key || (key != null && key.equals(k)))) break; 36 p = e; 37 } 38 } 39 40 if (e != null) { // existing mapping for key 41 V oldValue = e.value; 42 if (!onlyIfAbsent || oldValue == null) 43 e.value = value; 44 afterNodeAccess(e); 45 return oldValue; 46 } 47 } 48 ++modCount; 49 // 步骤⑥：超过最大容量 就扩容 50 if (++size > threshold) 51 resize(); 52 afterNodeInsertion(evict); 53 return null; 54 } 3.扩容机制 扩容(resize)就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。 我们分析下resize的源码，鉴于JDK1.8融入了红黑树，较复杂，为了便于理解我们仍然使用JDK1.7的代码，好理解一些，本质上区别不大，具体区别后文再说。 1 void resize(int newCapacity) { //传入新的容量 2 Entry[] oldTable = table; //引用扩容前的Entry数组 3 int oldCapacity = oldTable.length; 4 if (oldCapacity == MAXIMUM_CAPACITY) { //扩容前的数组大小如果已经达到最大(2^30)了 5 threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 6 return; 7 } 8 9 Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组 10 transfer(newTable); //！！将数据转移到新的Entry数组里 11 table = newTable; //HashMap的table属性引用新的Entry数组 12 threshold = (int)(newCapacity * loadFactor);//修改阈值 13 } 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有Entry数组的元素拷贝到新的Entry数组里。 1 void transfer(Entry[] newTable) { 2 Entry[] src = table; //src引用了旧的Entry数组 3 int newCapacity = newTable.length; 4 for (int j = 0; j e = src[j]; //取得旧Entry数组的每个元素 6 if (e != null) { 7 src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象） 8 do { 9 Entry next = e.next; 10 int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置 11 e.next = newTable[i]; //标记[1] 12 newTable[i] = e; //将元素放在数组上 13 e = next; //访问下一个Entry链上的元素 14 } while (e != null); 15 } 16 } 17 } newTable[i]的引用赋给了e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到Entry链的尾部(如果发生了hash冲突的话），这一点和Jdk1.8有区别，下文详解。在旧数组中同一条Entry链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。 下面举个例子说明下扩容过程。假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。其中的哈希桶数组table的size=2， 所以key = 3、7、5，put顺序依次为 5、7、3。在mod 2以后都冲突在table[1]这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小size 大于 table的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize成4，然后所有的Node重新rehash的过程。 下面我们讲解下JDK1.8做了哪些优化。经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。看下图可以明白这句话的意思，n为table的长度，图（a）表示扩容前的key1和key2两种key确定索引位置的示例，图（b）表示扩容后key1和key2两种key确定索引位置的示例，其中hash1是key1对应的哈希与高位运算结果。 元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”，可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK1.8新增的优化点。有一点注意区别，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8不会倒置。有兴趣的同学可以研究下JDK1.8的resize源码，写的很赞，如下: 1 final Node[] resize() { 2 Node[] oldTab = table; 3 int oldCap = (oldTab == null) ? 0 : oldTab.length; 4 int oldThr = threshold; 5 int newCap, newThr = 0; 6 if (oldCap > 0) { 7 // 超过最大值就不再扩充了，就只好随你碰撞去吧 8 if (oldCap >= MAXIMUM_CAPACITY) { 9 threshold = Integer.MAX_VALUE; 10 return oldTab; 11 } 12 // 没超过最大值，就扩充为原来的2倍 13 else if ((newCap = oldCap = DEFAULT_INITIAL_CAPACITY) 15 newThr = oldThr 0) // initial capacity was placed in threshold 18 newCap = oldThr; 19 else { // zero initial threshold signifies using defaults 20 newCap = DEFAULT_INITIAL_CAPACITY; 21 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); 22 } 23 // 计算新的resize上限 24 if (newThr == 0) { 25 26 float ft = (float)newCap * loadFactor; 27 newThr = (newCap [] newTab = (Node[])new Node[newCap]; 33 table = newTab; 34 if (oldTab != null) { 35 // 把每个bucket都移动到新的buckets中 36 for (int j = 0; j e; 38 if ((e = oldTab[j]) != null) { 39 oldTab[j] = null; 40 if (e.next == null) 41 newTab[e.hash & (newCap - 1)] = e; 42 else if (e instanceof TreeNode) 43 ((TreeNode)e).split(this, newTab, j, oldCap); 44 else { // 链表优化重hash的代码块 45 Node loHead = null, loTail = null; 46 Node hiHead = null, hiTail = null; 47 Node next; 48 do { 49 next = e.next; 50 // 原索引 51 if ((e.hash & oldCap) == 0) { 52 if (loTail == null) 53 loHead = e; 54 else 55 loTail.next = e; 56 loTail = e; 57 } 58 // 原索引+oldCap 59 else { 60 if (hiTail == null) 61 hiHead = e; 62 else 63 hiTail.next = e; 64 hiTail = e; 65 } 66 } while ((e = next) != null); 67 // 原索引放到bucket里 68 if (loTail != null) { 69 loTail.next = null; 70 newTab[j] = loHead; 71 } 72 // 原索引+oldCap放到bucket里 73 if (hiTail != null) { 74 hiTail.next = null; 75 newTab[j + oldCap] = hiHead; 76 } 77 } 78 } 79 } 80 } 81 return newTab; 82 } 17.HashMap并发安全的问题 并发的多线程使用场景中使用HashMap可能造成死循环。代码例子如下(便于理解，仍然使用JDK1.7的环境)： public class HashMapInfiniteLoop { private static HashMap map = new HashMap(2，0.75f); public static void main(String[] args) { map.put(5， \"C\"); new Thread(\"Thread1\") { public void run() { map.put(7, \"B\"); System.out.println(map); }; }.start(); new Thread(\"Thread2\") { public void run() { map.put(3, \"A); System.out.println(map); }; }.start(); } } 其中，map初始化为一个长度为2的数组，loadFactor=0.75，threshold=2*0.75=1，也就是说当put第二个key的时候，map就需要进行resize。 通过设置断点让线程1和线程2同时debug到transfer方法(3.3小节代码块)的首行。注意此时两个线程已经成功添加数据。放开thread1的断点至transfer方法的“Entry next = e.next;” 这一行；然后放开线程2的的断点，让线程2进行resize。结果如下图。 注意，Thread1的 e 指向了key(3)，而next指向了key(7)，其在线程二rehash后，指向了线程二重组后的链表。 线程一被调度回来执行，先是执行 newTalbe[i] = e， 然后是e = next，导致了e指向了key(7)，而下一次循环的next = e.next导致了next指向了key(3)。 e.next = newTable[i] 导致 key(3).next 指向了 key(7)。注意：此时的key(7).next 已经指向了key(3)， 环形链表就这样出现了。 于是，当我们用线程一调用map.get(11)时，悲剧就出现了——Infinite Loop。 18.JDK1.8与JDK1.7的性能对比 HashMap中，如果key经过hash算法得出的数组索引位置全部不相同，即Hash算法非常好，那样的话，getKey方法的时间复杂度就是O(1)，如果Hash算法技术的结果碰撞非常多，假如Hash算极其差，所有的Hash算法结果得出的索引位置一样，那样所有的键值对都集中到一个桶中，或者在一个链表中，或者在一个红黑树中，时间复杂度分别为O(n)和O(lgn)。 鉴于JDK1.8做了多方面的优化，总体性能优于JDK1.7，下面我们从两个方面用例子证明这一点。 Hash较均匀的情况 为了便于测试，我们先写一个类Key，如下： class Key implements Comparable { private final int value; Key(int value) { this.value = value; } @Override public int compareTo(Key o) { return Integer.compare(this.value, o.value); } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Key key = (Key) o; return value == key.value; } @Override public int hashCode() { return value; } } 这个类复写了equals方法，并且提供了相当好的hashCode函数，任何一个值的hashCode都不会相同，因为直接使用value当做hashcode。为了避免频繁的GC，我将不变的Key实例缓存了起来，而不是一遍一遍的创建它们。代码如下： public class Keys { public static final int MAX_KEY = 10_000_000; private static final Key[] KEYS_CACHE = new Key[MAX_KEY]; static { for (int i = 0; i 现在开始我们的试验，测试需要做的仅仅是，创建不同size的HashMap（1、10、100、……10000000），屏蔽了扩容的情况，代码如下： static void test(int mapSize) { HashMap map = new HashMap(mapSize); for (int i = 0; i 在测试中会查找不同的值，然后度量花费的时间，为了计算getKey的平均时间，我们遍历所有的get方法，计算总的时间，除以key的数量，计算一个平均值，主要用来比较，绝对值可能会受很多环境因素的影响。结果如下： 通过观测测试结果可知，JDK1.8的性能要高于JDK1.7 15%以上，在某些size的区域上，甚至高于100%。由于Hash算法较均匀，JDK1.8引入的红黑树效果不明显，下面我们看看Hash不均匀的的情况。 Hash极不均匀的情况 假设我们又一个非常差的Key，它们所有的实例都返回相同的hashCode值。这是使用HashMap最坏的情况。代码修改如下： class Key implements Comparable { //… @Override public int hashCode() { return 1; } } 仍然执行main方法，得出的结果如下表所示： 从表中结果中可知，随着size的变大，JDK1.7的花费时间是增长的趋势，而JDK1.8是明显的降低趋势，并且呈现对数增长稳定。当一个链表太长的时候，HashMap会动态的将它替换成一个红黑树，这话的话会将时间复杂度从O(n)降为O(logn)。hash算法均匀和不均匀所花费的时间明显也不相同，这两种情况的相对比较，可以说明一个好的hash算法的重要性。 测试环境：处理器为2.2 GHz Intel Core i7，内存为16 GB 1600 MHz DDR3，SSD硬盘，使用默认的JVM参数，运行在64位的OS X 10.10.1上。 18.HashMap操作注意事项以及优化? (1) 扩容是一个特别耗性能的操作，所以当程序员在使用HashMap的时候，估算map的大小，初始化的时候给一个大致的数值，避免map进行频繁的扩容。 (2) 负载因子是可以修改的，也可以大于1，但是建议不要轻易修改，除非情况非常特殊。 (3) HashMap是线程不安全的，不要在并发的环境中同时操作HashMap，建议使用ConcurrentHashMap。 (4) JDK1.8引入红黑树大程度优化了HashMap的性能。 (5) 还没升级JDK1.8的，现在开始升级吧。HashMap的性能提升仅仅是JDK1.8的冰山一角。 参考： https://zhuanlan.zhihu.com/p/21673805 https://www.jianshu.com/p/939b8a672070 https://www.jianshu.com/p/c45b6d782e91 https://blog.csdn.net/Mrs_chens/article/details/92761868 https://blog.csdn.net/riemann_/article/details/87217229 https://blog.csdn.net/qq_34626097/article/details/83053004 https://blog.csdn.net/u010887744/article/details/50575735 "},"zother1-JavaFaceNotes/JVM.html":{"url":"zother1-JavaFaceNotes/JVM.html","title":"JVM","keywords":"","body":"JVM 1.JDK、JRE、JVM关系？ Jdk (Java Development Kit) : java语言的软件开发包。包括Java运行时环境Jre。 Jre （Java Runtime Environment) :Java运行时环境，包括Jvm。 Jvm (Java Virtual Machine) : 一种用于计算机设备的规范。 Java语言在不同平台上运行时不需要重新编译。Java语言使用Java虚拟机屏蔽了与具体平台相关的信息，使得Java语言编译程序只需生成在Java虚拟机上运行的目标代码（字节码），就可以在多种平台上不加修改地运行。 Jdk包括Jre，Jre包括jvm。 2.启动程序如何查看加载了哪些类，以及加载顺序？ java -XX:+TraceClassLoading 具体类 Java -verbose 具体类 3. class字节码文件10个主要组成部分? MagicNumber Version Constant_pool Access_flag This_class Super_class Interfaces Fields Methods Attributes 4.画一下jvm内存结构图？ 5.程序计数器 属于线程私有内存。占用一块非常小的空间，它的作用可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的指令的字节码，分支、循环、跳转、异常处理、线程恢复等基础功能都依赖这个计数器来完成。 6.Java虚拟机栈 属于线程私有内存。它的生命周期与线程相同，虚拟机栈描述的是Java方法执行内存模型；每个方法被执行的时候都会同时创建一个栈桢用于存储局部变量表、操作栈、动态链接、方法出口信息等。每一个方法被调用直至执行完成的过程，就对应着一个栈帧再虚拟机中从入栈到出栈的过程。 7.本地方法栈 本地方法栈与虚拟机栈所发挥的作用是非常相似的，只不过虚拟机栈对虚拟机执行Java方法服务，而本地栈是为虚拟机使用到Native方法服务。 8.Java堆 是Java虚拟机所管理的内存中最大的一块。Java堆事被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。 Tips：但随着JIT编译器的发展与逃逸分析技术的逐渐成熟，栈上分配、标亮替换优化技术将会导师一些微妙的变化发生，所有的对象都分配在堆上就不那么绝对了。 9.方法区 是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 10.运行时常量池？ 是方法区的一部分，Class文件中除了有类的版本、字段、方法、接口等描述信息，还有一项是常量池（Constant PoolTable）用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后存放道方法区的运行时常量池中。 11.什么时候抛出StackOverflowError? 如果线程请求的栈深度大于虚拟机所允许的深度，则抛出StackOverflowError。 12.Java7和Java8在内存模型上有什么区别？ Java8取消了永久代，用元空间（Metaspace）代替了，元空间是存在本地内存（Native memory)中。 13.程序员最关注的两个内存区域？ 堆(Heap)和栈（Stack)，一般大家会把Java内存分为堆内存和栈内存，这是一种比较粗糙的划分方式，但实际上Java内存区域是很复杂的。 14.直接内存是什么？ 直接内存并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域，但这部分内存也频繁被实用，也有OutOfMemoryError异常的出现的可能。 Tips:JDK1.4中加入了NIO（new input/output)类，引入了一种基于通道(Channe)与缓冲区（Buffer)的I/O方式，也可以使用Native函数库直接分配堆外内存,然后通过一个存储在Java堆里面的DirectByteBuffer的对象作为这块内存的引用进行操作。 15.除了哪个区域外，虚拟机内存其他运行时区域都会发生OutOfMemoryError？ 程序计数器。 16.什么情况下会出现堆内存溢出？ 堆内存存储对象实例。我们只要不断地创建对象。并保证gc roots到对象之间有可达路径来避免垃圾回收机制清除这些对象。就会在对象数量到达最大。堆容量限制后，产生内存溢出异常。 17.如何实现一个堆内存溢出？ public class Cat { public static void main(String[] args) { List list = new ArrayList(); while (true) { list.add(new Cat()); } } } 18.空间什么情况下会抛出OutOfMemoryError？ 如果虚拟机在扩展栈时无法申请到足够的内存空间，则抛出OutOfMemoryError。 19.如何实现StrackOverflowError? public static void main(String[] args) { eat(); } public static void eat () { eat(); } 20.如何设置直接内存容量? 通过 -XX:MaxDirectMemorySize指定，如果不指定，则默认与Java堆的最大值一样。 21.Java堆内存组成？ 堆大小 = 新生代 + 老年代。如果是Java8则没有Permanent Generation。 其中新生代(Young) 被分为 Eden和S0（from)和S1(to)。 22.Edem : from : to默认比例是? Edem : from : to = 8 : 1 : 1 此比例可以通过 –XX:SurvivorRatio 来设定 23.垃圾标记阶段？ 在GC执行垃圾回收之前，为了区分对象存活与否，当对象被标记为死亡时，GC才回执行垃圾回收，这个过程就是垃圾标记阶段。 24.引用计数法? 比如对象a,只要任何一个对象引用了a,则a的引用计数器就加1，当引用失效时，引用计数器就减1，当计数器为0时，就可以对其回收。 但是无法解决循环引用的问题。 25.根搜索算法？ 跟搜索算法是以跟为起始点，按照从上到下的方式搜索被根对象集合所连接的目标对象是否可达(使用根搜索算法后，内存中 的存活对象都会被根对象集合直接或间接连接着)，如果目标对象不可达，就意味着该对象己经死亡，便可以在 instanceOopDesc的 Mark World 中将其标记为垃圾对象。 在根搜索算法中， 只有能够被根对象集合直接或者间接连接的对象才是存活对象。 26.JVM中三种常见的垃圾收集算法？ 标记-清除算法（Mark_Sweep) 复制算法(Copying) 标记-压缩算法（Mark-Compact) 27.标记-清除算法？ 首先标记出所有需要回收的对象，在标记完成后统一回收掉所有的被标记对象。 缺点： 标记和清除的效率都不高。 空间问题，清除后产生大量不连续的内存随便。如果有大对象会出现空间不够的现象从而不得不提前触发另一次垃圾收集动作。 28.复制算法？ 他将可用内存按容量划分为大小相等的两块，每次只使用其中的一块，当这一块内存用完了，就将还存活的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 优点: 解决了内存碎片问题。 缺点： 将原来的内存缩小为原来的一半，存活对象越多效率越低。 29.标记-整理算法？ 先标记出要被回收的对象，然后让所有存活的对象都向一端移动，然后直接清理掉边界以外的内存。解决了复制算法和标记清理算法的问题。 30.分代收集算法？ 当前商业虚拟机的垃圾收集都采用“分代手机算法”，其实就根据对象存活周期的不同将内存划分为几块，一般是新老年代。根据各个年代的特点采用最适当的收集算法。 31.垃圾收集器？ 如果说垃圾收集算法是方法论，那么垃圾收集器就是具体实现。连线代表可以搭配使用。 32.Stop The World? 进行垃圾收集时，必须暂停其他所有工作线程，Sun将这种事情叫做\"Stop The World\"。 33.Serial收集器? 单线程收集器，单线程的含义在于它会 stop the world。垃圾回收时需要stop the world ,直到它收集结束。所以这种收集器体验比较差。 34.PartNew收集器? Serial收集器的多线程版本，除了使用采用并行收回的方式回收内存外，其他行为几乎和Serial没区别。 可以通过选项“-XX:+UseParNewGC”手动指定使用 ParNew收集器执行内存回收任务。 36.Parallel Scavenge? 是一个新生代收集器，也是复制算法的收集器，同时也是多线程并行收集器，与PartNew 不同是，它重点关注的是程序达到一个可控制的吞吐量(Thoughput，CPU 用于运行用户代码 的时间/CPU 总消耗时间，即吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间))， 高吞吐量可以最高效率地利用 CPU 时间，尽快地完成程序的运算任务，主要适用于在后台运算而不需要太多交互的任务。 他可以通过2个参数精确的控制吞吐量,更高效的利用cpu。 分别是: -XX:MaxCcPauseMillis 和 -XX:GCTimeRatio 37.Parallel Old收集器? Parallel Scavenge收集器的老年代版本，使用多线程和标记-整理算法。JDK 1.6中才开始提供。 38.CMS 收集器? Concurrent Mar Sweep 收集器是一种以获取最短回收停顿时间为目标的收集器。重视服务的响应速度，希望系统停顿时间最短。采用标记-清除的算法来进行垃圾回收。 39.CMS垃圾回收的步骤? 初始标记 (stop the world) 并发标记 重新标记 (stop the world) 并发清除 初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快。 并发标记就是进行Gc Roots Tracing的过程。 重新标记则是为了修正并发标记期间，因用户程序继续运行而导致的标记产生变动的那一部分对象的标记记录，这个阶段停顿时间一般比初始标记时间长，但是远比并发标记时间短。 整个过程中并发标记时间最长，但此时可以和用户线程一起工作。 41.CMS收集器优点？缺点？ 优点： 并发收集、低停顿 缺点： 对cpu资源非常敏感。 无法处理浮动垃圾。 内存碎片问题。 42.G1收集器? Garbage First 收集器是当前收集器技术发展的最前沿成果。jdk 1.6_update14中提供了 g1收集器。 G1收集器是基于标记-整理算法的收集器，它避免了内存碎片的问题。 可以非常精确控制停顿时间，既能让使用者明确指定一个长度为 M毫秒的时间片段内，消耗在垃圾收集上的时间不多超过N毫秒，这几乎已经是实时java(rtsj)的垃圾收集器特征了。 42. G1收集器是如何改进收集方式的？ 极力避免全区域垃圾收集，之前的收集器进行收集的范围都是整个新生代或者老年代。而g1将整个Java堆（包括新生代、老年代）划分为多个大小固定的独立区域，并且跟踪这些区域垃圾堆积程度，维护一个优先级李彪，每次根据允许的收集时间，优先回收垃圾最多的区域。从而获得更高的效率。 43.虚拟机进程状况工具？ jps (Jvm process status tool ),他的功能与ps类似。 可以列出正在运行的虚拟机进程，并显示执行主类(Main Class,main()函数所在的类）的名称，以及浙西进程的本地虚拟机的唯一ID。 语法 ： jps [options] [hostid] -q 主输出lvmid,省略主类的名称 -m 输出虚拟机进程启动时传递给主类main()函数的参数 -l 输出主类全名，如果进程执行是Jar包，输出Jar路径 -v 输出虚拟机进程启动时JVM参数 44.虚拟机统计信息工具？ jstat(JVM Statistics Montoring Tool)是用于监视虚拟机各种运行状态信息命令行工机具。他可以显示本地或远程虚拟机进程中的类装载、内存、垃圾收集、jit编译等运行数据。 jstat [option vmid [interval[s|ms] [count]] ] interval 查询间隔 count 查询次数 如果不用这两个参数，就默认查询一次。 option代表用户希望查询的虚拟机信息，主要分3类： 类装载 垃圾收集 运行期编译状况 45.jstat 工具主要选项？ -class 监视类装载、卸载数量、总空间及类装载锁消耗的时间 -gc 监视Java堆状况，包括Eden区，2个survivor区、老年代 -gccapacity 监视内容与-gc基本相同，但输出主要关注Java堆各个区域使用的最大和最小空间 -gcutil 监视内容与-gc基本相同，主要关注已经使用空间站空间百分比 -gccause 与-gcutil 功能一样，但是会额外输出导致上一次GC产生的原因 -gcnew 监视新生代的GC的状况 -gcnewcapacity 监视内容与 -gcnew基本相同，输出主要关注使用到的最大最小空间 -gcold 监视老年代的GC情况 -gcoldcapacity 监控内容与 -gcold基本相同，主要关注使用到的最大最小空间 -compiler 输出jit 编译器编译过的方法、耗时等信息 45.配置信息工具? jinfo(Configuration Info for Java)的作用是实时地查看和调整虚拟机的各项参数。 使用jps 命令的 -v 参数可以查看虚拟机启动时显示指定的参数列表。 jinfo 语法: jinfo [option] pid 46.内存映像工具？ jmap(Memory Map for Java) 命令用于生成堆转储快照（一般称为heapdump或dump文件）。 语法 ：jmap [option] vmid 它还可以查询finalize执行队列，Java堆和永久代的详细信息，如果空间使用率、当前用的是哪种收集器等。 -dump 生成Java堆转储快照，其中live自参数说明是否只dump出存活对象 -finalizerinfo 显示在F -Queue 中等待Finalizer线程执行finalize方法的对象。只在Linux/Solaris平台下有效 -heap 显示Java堆详细信息，如使用哪种回收器、参数配置、分代状况。 -histo 显示堆中对象统计信息、包括类、实例数量和合计容量。 -F 当虚拟机进程对-dump选项没有响应时，可使用这个选项强制生成dump快照。 47.虚拟机堆转存储快照分析工具？ jhat ( JVM Heap Analysis Tool) 用来分析jmap生成的堆转储快照。 48.堆栈跟踪工具？ jstack(Stack Trace for Java) 命令用于生成虚拟机当前时刻的线程快照（一般称为thread dump 或javacore文件）。线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因。 jstack [option] vmid -F 当正常输出的请求不被响应时，强制输出线程堆栈 -l 除堆栈外，显示关于锁的附加信息 -m 如果调用本地方法的花，可以显示C/C++ 的堆栈 49.除了命令行，还有什么可视化工具？ JConsole 和 VisualVM，这两个工具是JDK的正式成员。 50.类加载过程? 加载-》验证-》准备-》解析-》初始化-》使用-》卸载 参考： 《深入理解JVM & G1 GC》 《深入理解Java虚拟机：JVM高级特性与最佳实践》 《实战Java虚拟机：JVM故障诊断与性能优化》 "},"zother1-JavaFaceNotes/Kafka.html":{"url":"zother1-JavaFaceNotes/Kafka.html","title":"Kafka","keywords":"","body":"Kafka 1.什么是kafka? Apache Kafka是由Apache开发的一种发布订阅消息系统。 2.kafka的3个关键功能？ 发布和订阅记录流，类似于消息队列或企业消息传递系统。 以容错的持久方式存储记录流。 处理记录流。 3.kafka通常用于两大类应用？ 建立实时流数据管道，以可靠地在系统或应用程序之间获取数据 构建实时流应用程序，以转换或响应数据流 4.kafka特性? 消息持久化 高吞吐量 扩展性 多客户端支持 Kafka Streams 安全机制 数据备份 轻量级 消息压缩 5.kafka的5个核心Api? Producer API Consumer API Streams API Connector API Admin API 6.什么是Broker（代理）? Kafka集群中，一个kafka实例被称为一个代理(Broker)节点。 7.什么是Producer（生产者）? 消息的生产者被称为Producer。 Producer将消息发送到集群指定的主题中存储，同时也自定义算法决定将消息记录发送到哪个分区? 8.什么是Consumer（消费者）? 消息的消费者，从kafka集群中指定的主题读取消息。 9.什么是Topic（主题）? 主题，kafka通过不同的主题却分不同的业务类型的消息记录。 10.什么是Partition（分区）? 每一个Topic可以有一个或者多个分区(Partition)。 11.分区和代理节点的关系？ 一个分区只对应一个Broker,一个Broker可以管理多个分区。 12.什么是副本(Replication)? 每个主题在创建时会要求制定它的副本数（默认1）。 13.什么是记录(Record)? 实际写入到kafka集群并且可以被消费者读取的数据。 每条记录包含一个键、值和时间戳。 14.kafka适合哪些场景？ 日志收集、消息系统、活动追踪、运营指标、流式处理、时间源等。 15.kafka磁盘选用上？ SSD的性能比普通的磁盘好，这个大家都知道，实际中我们用普通磁盘即可。它使用的方式多是顺序读写操作，一定程度上规避了机械磁盘最大的劣势，即随机读写操作慢，因此SSD的没有太大优势。 16.使用RAID的优势? 提供冗余的磁盘存储空间 提供负载均衡 17.磁盘容量规划需要考虑到几个因素？ 新增消息数 消息留存时间 平均消息大小 备份数 是否启用压缩 18.Broker使用单个？多个文件目录路径参数？ log.dirs 多个 log.dir 单个 19.一般来说选择哪个参数配置路径？好处？ log.dirs 好处: 提升读写性能，多块物理磁盘同时读写高吞吐。 故障转移。一块磁盘挂了转移到另一个上。 20.自动创建主题的相关参数是? auto.create.topics.enable 21.解决kafka消息丢失问题？ 不要使用 producer.send(msg)，而要使用 producer.send(msg, callback)。 设置 acks = all。 设置 retries 为一个较大的值。 设置 unclean.leader.election.enable = false。 设置 replication.factor >= 3。 设置 min.insync.replicas > 1。 确保 replication.factor > min.insync.replicas。 确保消息消费完成再提交。 22.如何自定分区策略？ 显式地配置生产者端的参数partitioner.class 参数为你实现类的 全限定类名，一般来说实现partition方法即可。 23.kafka压缩消息可能发生的地方？ Producer 、Broker。 24.kafka消息重复问题？ 做好幂等。 数据库方面可以（唯一键和主键）避免重复。 在业务上做控制。 25.你知道的kafka监控工具？ JMXTool 工具 Kafka Manager Burrow JMXTrans + InfluxDB + Grafana Confluent Control Center 参考: 《Kafka并不难学》 《kafka入门与实践》 极客时间：Kafka核心技术与实战 http://kafka.apache.org/ "},"zother1-JavaFaceNotes/Linux.html":{"url":"zother1-JavaFaceNotes/Linux.html","title":"Linux","keywords":"","body":"Linux 1.什么是Linux？ 是一套免费使用和自由传播的类UNIX操作系统，其内核由林纳斯·本纳第克特·托瓦兹于1991年第一次释出，它主要受到Minix和Unix思想的启发，是一个基于POSIX和Unix的多用户、多任务、支持多线程和多CPU的操作系统。它能运行主要的Unix工具软件、应用程序和网络协议。它支持32位和64位硬件。 2.Linux内核主要负责哪些功能 系统内存管理 软件程序管理 硬件设备管理 文件系统管理 3.交互方式 控制台终端、图形化终端 4.启动shell GNU bash shell能提供对linux 系统的交互式访问。作为普通程序运行，通常在用户登陆终端时启动。登录时系统启动的shell依赖与用户账户的配置。 5.bash手册 大多数linux发行版自带以查找shell命令及其他GNU工具信息的在线手册。man命令用来访问linux系统上的手册页面。当用man命令查看手册，使用分页的程序来现实的。 6.登陆后你在的位置？ 一般登陆后，你的位置位于自己的主目录中。 7.绝对文件路径?相对文件路径？快捷方式？ 绝对文件路径：描述了在虚拟目录结构中该目录的确切位置，以虚拟目录跟目录开始，相当于目录全名。 以正斜线(/)开始，比如 /usr/local。 相对文件路径：允许用户执行一个基于当前位置的目标文件路径。 比如:当前在/usr/local ➜ local ls Caskroom Frameworks bin go lib sbin var Cellar Homebrew etc include opt share ➜ local cd go 快捷方式(在相对路径中使用): 单点符(.) : 表示当前目录; 双点符(..) : 表示当前目录的父目录。 8.迷路，我的当前位置在哪？ pwd 显示当前目录 [root@iz2ze76ybn73dvwmdij06zz local]# pwd /usr/local 9.如何切换目录？ 语法: cd destination destination : 相对文件路径或绝对文件路径 可以跳到存在的任意目录。 10.如何查看目录中的文件？区分哪些是文件哪些是目录?递归查? ls 命令会用最基本的形式显示当前目录下的文件和目录： ➜ local ls Caskroom Frameworks bin go lib sbin var Cellar Homebrew etc include opt share 可以看出默认是按照字母序展示的 一般来说，ls命令回显示不同的颜色区分不同的文件类型,如果没有安装颜色插件可以用ls -F来区分哪些是目录（目录带/)，哪些是文件（文件不带/) ls -R 递归展示出目录下以及子目录的文件，目录越多输出越多 11.创建文件？创建目录？批量创建? 创建文件:touch 文件名 批量创建文件: touch 文件名 文件名 … ➜ test touch a ➜ test ls a ➜ test touch b c ➜ test ls a b c 创建目录：mkdir 目录名 批量创建目录: mkdir 目录名 目录名 … ➜ test mkdir aa ➜ test mkdir bb cc ➜ test ls a aa b bb c cc ➜ test ls -F a aa/ b bb/ c cc/ 12.删除文件?强制删除？递归删除? 语法: rm destination -i 询问是否删除,-r 递归删除，-f 强制删除。 rm不能删除有文件的目录,需要递归删除。 ➜ xktest rm jdk rm: jdk: is a directory ➜ xktest rm -r jdk ➜ xktest ls rm -i 询问删除,建议大家平时删除多用 -i，确定一下再删除。 ➜ xktest touch tomcat ➜ xktest rm -i tomcat remove tomcat? n rm -rf 会直接删除，没有警告信息，使用必须谨慎**。 13.制表符自动补全？ 有的时候文件的名字很长，很容易拼出错即使拼写对了也很浪费时间。 ➜ xktest ls java* javaxiaokaxiu 比如操作javaxiaokaxiu这个文件时，输入到java的时候，然后按制表键(tab)就会补全成javaxiaokaxiu,是不是方便多了。 14.复制文件 语法: cp source target 如果target不存在则直接创建，如果存在，默认不会提醒你是否需要覆盖，需要加-i就会询问你是否覆盖,n否y是。 ➜ xktest cp a c ➜ xktest cp -i a c overwrite c? (y/n [n]) y ➜ xktest ls a c 15.重新命名文件？移动文件？ 语法 ： mv soucre target 重命名: ➜ xktest ls ➜ xktest touch java ➜ xktest ls java ➜ xktest mv java java1.8 ➜ xktest ls java1.8 移动文件: 新建jdk目录把java1.8文件移动到jdk目录下。 ➜ xktest ls java1.8 ➜ xktest mkdir jdk ➜ xktest mv java1.8 jdk ➜ xktest ls -R jdk ./jdk: java1.8 16.什么是链接文件？ 如过需要在系统上维护同一文件的两份或者多份副本，除了保存多分单独的物理文件副本之外。还可以采用保存一份物理文件副本和多个虚拟副本的方法，这种虚拟的副本就叫做链接。 17.查看文件类型?字符编码？ 语法: file destination ➜ apache file tomcat tomcat: ASCII text 可以看出，file命令可以显示文件的类型text以及字符编码ASCII 18.查看整个文件？按照有文本显示行号？无文本显示行号？ 语法 : cat destination -n 显示行号，-b 有文本的显示行号。 （默认是不显示行号的) ➜ apache cat -n tomcat 1 text 2 text 3 4 start 5 stop 6 restart 7 end ➜ apache cat -b tomcat 1 text 2 text 3 start 4 stop 5 restart 6 end 19.查看部分文件 语法 : tail destination 默认情况会展示文件的末尾10行。 -n 行数，显示最后n行。 ➜ apache tail -n 2 tomcat restart end 语法: head destination 默认情况会展示文件的开头10行。 -n 行数，显示开头n行。 ➜ apache head -n 2 tomcat text text 20.数据排序?对数字进行排序？对月份排序？ 默认情况下，文件的数据展示是按照原顺序展示的。sort命令可以对文本文件中的数据进行排序。sort默认会把数据当成字符处理。 语法: sort destination sort -n 所以排序数字时需要用-n，它的含义是说当前排序是的数字。 sort -M 比如月份Jan、Feb、Mar，如果希望它按照月份排序，加入-M就会按照月份的大小来排序。 21.查找匹配数据？反向搜？ 语法: grep [options] pattern [file] 该命令会查找匹配执行模式的字符串的行，并输出。 ➜ apache grep start tomcat start restart -v 反向搜 ➜ apache grep -v start tomcat text text stop end -n 显示行号 -c 显示匹配的行数 22.压缩工具有哪些? 23.如何压缩文件？如何解压文件? 比如以.gz的格式举例。 压缩语法: gzip destination ➜ apache gzip tomcat ➜ apache ls tomcat.gz 解压语法: gunzip destination ➜ apache gunzip tomcat.gz ➜ apache ls tomcat 24.Linux广泛使用的归档数据方法? 虽然zip命令能压缩和解压单个文件，但是更多的时候广泛使用tar命令来做归档。 语法: tar function [options] obj1 obj2 ➜ apache tar -cvf service.tar service1 service2 // 创建规定文件service.tar a service1 a service2 ➜ apache tar -tf service.tar //查看文件中的目录内容 service1 service2 ➜ apache tar zxvf service.tar //解压 x service1 x service2 25.如何查看命令历史记录? history 命令可以展示你用的命令的历史记录。 4463 touch service1 service2 4464 ls 4465 tar -cvf service.tar service1 service2 4466 tar -tf service.tar 4467 tar zxvf service 4468 tar zxvf service.t 4469 tar zxvf service.tar 4470 ls 4471 tar -zxvf service.tar 4472 ls 26.查看已有别名?建立属于自己的别名? alias -p 查看当前可用别名 [root@iz2ze76ybn73dvwmdij06zz ~]# alias -p alias cp='cp -i' alias egrep='egrep —color=auto' alias fgrep='fgrep —color=auto' alias grep='grep —color=auto' alias l.='ls -d .* —color=auto' alias ll='ls -l —color=auto' alias li = 'ls -li' 创建别名 27.什么是环境变量？ bash shell用一个叫作环境变量(environment variable)的特性来存储有关shell会话和工作环境的信息。这项特性允许你在内存中存储数据，以便程序或shell中运行的脚本能够轻松访问到它们。这也是存储持久数据的一种简便方法。 在bash shell中，环境变量分为两类: 全局变量：对于 shell会话和所有生成的子shell都是可见的。 局部变量： 只对创建他们的shell可见。 28.储存用户的文件是?包括哪些信息？ /etc/passwd存储来一些用户有关的信息。 [root@iz2ze76ybn73dvwmdij06zz ~]# cat /etc/passwd root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin 文件信息包括如下内容。 登录用户名 用户密码 用户账户的UID(数字形式) 用户账户的组ID(GID)(数字形式) 用户账户的文本描述(称为备注字段) 用户HOME目录的位置 用户的默认shell 29.账户默认信息？添加账户？删除用户？ [root@iz2ze76ybn73dvwmdij06zz ~]# useradd -D//查看系统默认创建用户信息 GROUP=100 HOME=/home INACTIVE=-1 EXPIRE= SHELL=/bin/bash SKEL=/etc/skel CREATE_MAIL_SPOOL=yes [root@iz2ze76ybn73dvwmdij06zz ~]# useradd xiaoka//添加用户 [root@iz2ze76ybn73dvwmdij06zz /]# userdel xiaoka//删除用户 30.查看组信息？如何创建组？删除组？ [root@iz2ze76ybn73dvwmdij06zz ~]# cat /etc/group root:x:0: bin:x:1: daemon:x:2: sys:x:3: adm:x:4: tty:x:5: disk:x:6: [root@iz2ze76ybn73dvwmdij06zz ~]# groupadd java //创建组 [root@iz2ze76ybn73dvwmdij06zz ~]# groupdel java //创建组 31.文件描述符?每个描述符的含义? [root@iz2ze76ybn73dvwmdij06zz xiaoka]# ls -l 总用量 0 -rw-r—r— 1 root root 0 4月 21 13:17 a -rw-r—r— 1 root root 0 4月 21 13:17 b -rw-r—r— 1 root root 0 4月 21 13:17 c -rw-r—r— 1 root root 0 4月 21 13:17 d -rw-r—r— 1 root root 0 4月 21 13:17 e 1、文件类型: -代表文件 d代表目录 l代表链接 c代表字符型设备 b代表块设备 n代表网络设备 2、访问权限符号: r代表对象是可读的 w代表对象是可写的 x代表对象是可执行的 若没有某种权限，在该权限位会出现单破折线。 3、这3组权限分别对应对象的3个安全级别: 对象的属主 对象的属组 系统其他用户 31.修改权限? chmod options mode file 比如给文件附加可以执行权限: [root@xiaoka ~]# chmod +x filename 32.如何执行可以执行文件? [root@xiaoka ~]# sh sleep.sh hello,xiaoka [root@xiaoka ~]# ./sleep.sh hello,xiaoka 33.列出已经安装的包？安装软件？更新软件？卸载? 列出已经安装的包: yum list installed 安装软件: yum install package_name 更新软件: yum update package_name 卸载软件:yum remove package_name //只删除软件包保留数据文件和配置文件 如果不希望保留数据文件和配置文件 可以执行:yum erase package_name 34.源码安装通常的路子? tar -zxvf xx.gz //解包 cd xx ./configure make make install 35.vim编辑器几种操作模式？基本操作? 操作模式: 普通模式 插入模式 基础操作: h:左移一个字符。 j:下移一行(文本中的下一行)。 k:上移一行(文本中的上一行)。 l:右移一个字符。 vim提供了一些能够提高移动速度的命令: PageDown(或Ctrl+F):下翻一屏 PageUp(或Ctrl+B):上翻一屏。 G:移到缓冲区的最后一行。 num G:移动到缓冲区中的第num行。 gg:移到缓冲区的第一行。 退出vim: q:如果未修改缓冲区数据，退出。 q!:取消所有对缓冲区数据的修改并退出。 w filename:将文件保存到另一个文件中。 wq:将缓冲区数据保存到文件中并退出。 36.查看设备还有多少磁盘空间? df 可以查看所有已挂在磁盘的使用情况。 -m 用兆字节，G代替g字节 [root@iz2ze76ybn73dvwmdij06zz ~]# df 文件系统 1K-块 已用 可用 已用% 挂载点 devtmpfs 1931568 0 1931568 0% /dev tmpfs 1940960 0 1940960 0% /dev/shm tmpfs 1940960 720 1940240 1% /run tmpfs 1940960 0 1940960 0% /sys/fs/cgroup /dev/vda1 41152812 9068544 30180560 24% / tmpfs 388192 0 388192 0% /run/user/0 快速判断某个特定目录是否有超大文件？ 默认情况，du会显示当前目录的所有文件、目录、子目录的磁盘使用情况。 [root@iz2ze76ybn73dvwmdij06zz src]# du 4 ./debug 4 ./kernels 12 37.默认进程信息显示? ps它能输出运行在系统上的所有程序的许多信息。 默认情况下ps值显示运行在当前控制台下的当前用户的进程。 [root@iz2ze76ybn73dvwmdij06zz ~]# ps PID TTY TIME CMD 10102 pts/0 00:00:00 bash 10131 pts/0 00:00:00 ps 38.实时监测进程 与ps相比，top可以实时监控进程信息。 平均负载有3个值:最近1分钟的、最近5分钟的和最近15分钟的平均负载。值越大说明系统 的负载越高。由于进程短期的突发性活动，出现最近1分钟的高负载值也很常见，但如果近15分 钟内的平均负载都很高，就说明系统可能有问题。 39.如何中断一个进程? 在一个终端中， Ctrl + c 通过这个命令许多（不是全部）命令行程序都可以被中断。 40.如何把一个进程放到后台运行？ [root@iz2ze76ybn73dvwmdij06zz ~]# ./sleep.sh & 此时，进程并不能被Ctrl + c 中断。 41.如何停止一个进程? kill命令被用来给程序发送信号。如果没有指定信号，默认发送TERM(终止）信号。 语法 : kill [-signal] PID … #### 42.验证网络可链接命令是什么？什么原理？ ping。这个 ping 命令发送一个特殊的网络数据包(叫做 IMCP ECHO REQUEST)到一台指定的主机。大多数接收这个包的网络设备将会回复它，来允许网络连接验证。 一旦启动，ping会持续在特定时间（默认1秒）发送数据包。 43.查看某端口是否被占用? netstat -ntulp|grep 8080 [root@iz2ze76ybn73dvwmdij06zz ~]# netstat -ntulp|grep 8080 tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 4517/java 参数说明: -t (tcp) 仅显示tcp相关选项 -u (udp)仅显示udp相关选项 -n 拒绝显示别名，能显示数字的全部转化为数字 -l 仅列出在Listen(监听)的服务状态 -p 显示建立相关链接的程序名 44.如何查找匹配的文件？基于文件属性？ find 程序能基于各种各样的属性，搜索一个给 定目录(以及它的子目录)，来查找文件。 find 命令的最简单使用是，搜索一个或多个目录。 普通查找，按照name查找: [root@iz2ze76ybn73dvwmdij06zz ~]# find -name xiaoka ./xiaoka 文件类型查找: 比如,输出我们的家目录文件数量 [root@iz2ze76ybn73dvwmdij06zz ~]# find ~|wc -l 17130 根据文件类型查: [root@iz2ze76ybn73dvwmdij06zz ~]# find ~ -type d | wc -l 7340 find支持的类型: b 块设备文件、 c 字符设备文件、d 目录、f 普通文件、l 符号链接 45.如何查看当前主机名？如何修改？如何重启后生效？ [root@iz2ze76ybn73dvwmdij06zz ~]# hostname//查看当前主机名 iz2ze76ybn73dvwmdij06zz [root@iz2ze76ybn73dvwmdij06zz ~]# hostname xiaoka//修改当前主机名 [root@iz2ze76ybn73dvwmdij06zz ~]# hostname xiaoka 大家知道一般来讲命令重启就会失效，目前基本上用的centos7的比较多，两种方式可以支持重启生效。 一、命令 [root@iz2ze76ybn73dvwmdij06zz ~]# hostnamectl set-hostname xiaoka [root@iz2ze76ybn73dvwmdij06zz ~]# hostname xiaoka [root@xiaoka ~]# 二、修改配置文件:/etc/hostname [root@xiaoka ~]# vim /etc/hostname 46.如何写一条规则，拒绝某个ip访问本机8080端口? iptables -I INPUT -s ip -p tcp —dport 8080 -j REJECT 47.哪个文件包含了主机名和ip的映射关系? /etc/hosts 48.如何用sed只打印第5行?删除第一行？替换字符串? 只打印第5行: ➜ apache sed -n \"5p\" tomcat stop 删除第一行: [root@xiaoka ~]# cat story Long ago a lion and a bear saw a kid. They sprang upon it at the same time. The lion said to the bear, “I caught this kid first, and so this is mine.” [root@xiaoka ~]# cat story They sprang upon it at the same time. The lion said to the bear, “I caught this kid first, and so this is mine.” 替换字符串: ➜ apache cat story Long ago a lion and a bear saw a kid. They sprang upon it at the same time. The lion said to the bear, “I caught this kid first, and so this is mine.” ➜ apache sed 's#this#that#g' story Long ago a lion and a bear saw a kid. They sprang upon it at the same time. The lion said to the bear, “I caught that kid first, and so that is mine.” 49.打印文件第一行到第三行? ​ 文件tomcat中内容: ➜ apache cat tomcat text21 text22 text23 start stop restart end ➜ apache head -3 tomcat text21 text22 text23 ➜ apache sed -n '1,3p' tomcat text21 text22 text23 ➜ apache awk 'NR>=1&&NR50.如何用awk查看第2行倒数第3个字段? ➜ apache awk 'NR==3{print $(NF-2)}' story this ➜ apache cat story Long ago a lion and a bear saw a kid. They sprang upon it at the same time. The lion said to the bear, “I caught this kid first, and so this is mine.” 参考: 《鸟哥Linux私房菜》 《快乐的命令行》 《Linux命令行与shell脚本编程大全(第3版）》 《Linux从入门到精通》 百度百科 ​ 公众号：《马里奥玩Python》 "},"zother1-JavaFaceNotes/Mybatis.html":{"url":"zother1-JavaFaceNotes/Mybatis.html","title":"Mybatis","keywords":"","body":"Mybatis 1.什么是Mybatis? MyBatis 是一款优秀的支持自定义 SQL 查询、存储过程和高级映射的持久层框架，消除了 几乎所有的 JDBC 代码和参数的手动设置以及结果集的检索 。 MyBatis 可以使用 XML 或注解进 行配置和映射， MyBatis 通过将参数映射到配置的 SQL 形成最终执行的 SQL 语句 ，最后将执行 SQL 的结果映射成 Java对象返回。 2.Hibernate优点？ Hibernate建立在POJO和数据库表模型的直接映射关系上。通过xml或注解即可和数据库表做映射。通过pojo直接可以操作数据库的数据。它提供的是全表的映射模型。 消除代码映射规则，被分离到xml或注解里配置。 无需在管理数据库连接，配置在xml中即可。 一个会话中，不要操作多个对象，只要操作Session对象即可。 关闭资源只需关闭Session即可。 3.Hibernate缺点？ 全表映射带来的不便，比如更新需要发送所有的字段。 无法根据不同的条件组装不同的sql。 对多表关联和复杂的sql查询支持较差，需要自己写sql，返回后，需要自己将数据组成pojo。 不能有效支持存储过程。 虽然有hql,但是性能较差，大型互联网需要优化sql，而hibernate做不到。 4.Mybatis优点？ 小巧，学习成本低，会写sql上手就很快了。 比jdbc，基本上配置好了，大部分的工作量就专注在sql的部分。 方便维护管理，sql不需要在Java代码中找，sql代码可以分离出来，重用。 接近jdbc,灵活，支持动态sql。 支持对象与数据库orm字段关系映射。 5.Mybatis缺点? 由于工作量在sql上，需要 sql的熟练度高。 移植性差。sql语法依赖数据库。不同数据库的切换会因语法差异，会报错。 6.什么时候用Mybatis? 如果你需要一个灵活的、可以动态生成映射关系的框架。目前来说，因为适合，互联网项目用Mybatis的比例还是很高滴。 7.Mybatis的核心组件有哪些？分别是？ SqlSessionFactoryBuilder(构造器) :它会根据配置信息或者代码来生成SqlSessionFactory。 SqlSessionFactory（工厂接口）:依靠工厂来生成SqlSession。 SqlSession（会话):是一个既可以发送 sql去执行返回结果，也可以获取Mapper接口。 SQL Mapper:它是新设计的组件，是由一个Java接口和XML文件（或注解）构成的。需要给出对象的SQl和映射规则。它负责发送SQL去执行，并返回结果。 8.#{}和${}的区别是什么？ ${}是字符串替换，#{}是预编译处理。一般用#{}防止 sql注入问题。 9.Mybatis中9个动态标签是？ if c h o o s e (when 、 oterwise) trim (where、 set) foreach bind 10.xml映射文件中，有哪些标签？ select|insert|updae|delete|resultMap|parameterMap|sql|include|selectKey 加上9个动态标签，其中为sql片段标签，通过标签引入sql片段，为不支持自增的主键生成策略标签。 11.Mybatis支持注解吗？优点？缺点？ 支持。 优点：对于需求简单sql逻辑简单的系统，效率较高。 缺点： 当sql变化需要重新编译代码，sql复杂时，写起来更不方便，不好维护。 12.Mybatis动态sql？ Mybatis 动态 sql 可以让我们在 Xml 映射文件内，以标签的形式编写动态 sql，完成逻辑 判断和动态拼接 sql 的功能 13.Mybatis 是如何进行分页的?分页插件的原理是什么? 1）Mybatis 使用 RowBounds 对象进行分页，也可以直接编写 sql 实现分页，也可以使用 Mybatis 的分页插件。 2）分页插件的原理：实现 Mybatis 提供的接口，实现自定义插件，在插件的拦截方法内拦 截待执行的 sql，然后重写 sql。 举例：select from student，拦截 sql 后重写为：select t. from （select from student）t limit 0，10 14.如何获取自增主键? 注解： @Options(useGeneratedKeys =true, keyProperty =”id”) int insert( ); Xml： sql 15.为什么Mapper接口没有实现类，却能被正常调用? 这是因为MyBatis在Mapper接口上使用了动态代理。 16.用注解好还是xml好? 简单的增删改查可以注解。 复杂的sql还是用xml,官方也比较推荐xml方式。 xml的方式更便于统一维护管理代码。 17.如果不想手动指定别名，如何用驼峰的形式自动映射？ mapUnderscoreToCamelCase=true 18.当实体属性名和表中字段不一致，怎么办？ 一、别名: 比如 order_num select order_num orderNum 二、通过做映射 ​ 三、如果是驼峰注解用17问的方式。 19.嵌套查询用什么标签？ association 标签的嵌套查询常用的属性如下 。 select:另一个映射查询的 id, MyBatis会额外执行这个查询获取嵌套对象的结果。 column:列名(或别名)，将主查询中列的结果作为嵌套查询的 参数，配置 方式如 column={propl=coll , prop2=col2}, propl 和 prop2 将作为嵌套查询的参数。 fetch Type:数据加载方式，可选值为 lazy 和 eager，分别为延迟加载和积极加载， 这个配置会覆盖全局的 lazyLoadingEnabled 配置 。 20.like模糊查询怎么写？ （1）'%${value}%' 不推荐 （2）CONCAT('%',#{value},'%') 推荐 (3) like '%'|| #{value} || '%' （4）使用bind LIKE #{pattern} 21.Mybatis支持枚举吗？ 支持。有转换器EnumTypeHandler和EnumOrdinalTypeHandler 22.SqlSessionFactoryBuilder生命周期? 利用xml或者Java代码获得资源构建SqlSessionFactoryBuilder，它可以创建多个SessionFactory,一旦构建成功SessionFactory,就可以回收它了。 23.一级缓存的结构?如何开启一级缓存？如何不使用一级缓存？ Map 。默认情况下，一级缓存是开启的。标签内加属性flushCache=true。 24.二级缓存如何配置？ 〈 !一其他自己直一 〉 这个参数是二级缓存的全局开关，默认值是 true，初始状态 为启用状态 。 如果把这个参数设置为 false，即使有后面 的 二级缓 存配置，也不会生效 。 25.简述 Mybatis 的插件运行原理，以及如何编写一个插件？ 1）Mybatis 仅可以编写针对 ParameterHandler、ResultSetHandler、StatementHandler、 Executor 这 4 种接口的插件，Mybatis 通过动态代理，为需要拦截的接口生成代理对象以实 现接口方法拦截功能，每当执行这 4 种接口对象的方法时，就会进入拦截方法，具体就是 InvocationHandler 的 invoke()方法，当然，只会拦截那些你指定需要拦截的方法。 2）实现 Mybatis 的 Interceptor 接口并复写 intercept()方法，然后在给插件编写注解，指定 要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。 26.二级缓存的回收策略有哪些？ eviction (收回策略) LRU(最近最少使用的) : 移除最长时间不被使用的对象，这是默认值 。 IFO(先进先出〉 : 按对象进入缓存的顺序来移除它们 。 SOFT(软引用) : 移除基于垃圾回收器状态和软引用规则的对象 。 WEAK (弱引用) : 更积极地移除基于垃圾收集器状态和弱引用规则的对象 。 27.Mybatis的Xml文件中id可以重复吗? 同一namespace下，id不可重复。不同namespace下，可以重复。 28. 和Mybatis搭配java框架中比较好用的缓存框架？有哪些特点? EhCache是一个纯牌的 Java进程内的缓存框架，具有快速、精干等特点。 具体来说， EhCache主要的特性如下 。 快速。 简单。 多种缓存策略 。 缓存数据有内存和磁盘两级，无须担心容量问题 。 缓存数据会在虚拟机重启 的过程中写入磁盘。 可 以通过 RMI、可插入 API 等方式进行分布式缓存。 .具有缓存和缓存管理器的侦 昕接口。 支持多缓存管理器实例 以及一个实例的多个缓存区域。 参考： 《 MyBatis从入门到精通》 《深入浅出 MyBatis技术原理与实战》 《MyBatis技术》 "},"zother1-JavaFaceNotes/MySql.html":{"url":"zother1-JavaFaceNotes/MySql.html","title":"My Sql","keywords":"","body":"Mysql 1.什么是数据库? 数据库是“按照数据结构来组织、存储和管理数据的仓库”。是一个长期存储在计算机内的、有组织的、可共享的、统一管理的大量数据的集合。 2.如何查看某个操作的语法? 比如看建表的语法： mysql> ? create table Name: 'CREATE TABLE' Description: Syntax: CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name (create_definition,…) [table_options] [partition_options] CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name [(create_definition,…)] [table_options] [partition_options] [IGNORE | REPLACE] [AS] query_expression CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name 3.MySql的存储引擎有哪些? MyISAM、 InnoDB、BDB、MEMORY、MERGE、EXAMPLE、NDB Cluster、 ARCHIVE、CSV、BLACKHOLE、FEDERATED。 Tips:InnoDB和BDB提供事务安全表，其他存储引擎都是非事务安全表。 4.常用的2种存储引擎？ 1.Myisam是Mysql的默认存储引擎，当create创建新表时，未指定新表的存储引擎时，默认使用Myisam。 每个MyISAM 在磁盘上存储成三个文件。文件名都和表名相同，扩展名分别是 .frm (存储表定义) 、.MYD (MYData，存储数据)、.MYI (MYIndex，存储索引)。 数据文件和索引文件可以放置在不同的目录，平均分布io，获得更快的速度。 2.InnoDB 存储引擎提供了具有提交、回滚和崩溃恢复能力的事务安全。但是对比 Myisam 的存储引擎，InnoDB 写的处理效率差一些并且会占用更多的磁盘空间以保留数据和索引。 6.可以针对表设置引擎吗？如何设置？ 可以, ENGINE=xxx 设置引擎。 代码示例: create table person( id int primary key auto_increment, username varchar(32) ) ENGINE=InnoDB 6.选择合适的存储引擎？ 选择标准: 根据应用特点选择合适的存储引擎,对于复杂的应用系统可以根据实际情况选择 多种存储引擎进行组合. 下面是常用存储引擎的适用环境: MyISAM: 默认的 MySQL 插件式存储引擎, 它是在 Web、 数据仓储和其他应用环境下最常使用的存储引擎之一。 InnoDB:用于事务处理应用程序，具有众多特性，包括 ACID 事务支持。 Memory: 将 所有数据保存在RAM 中， 在 需要快速查找引用和其他类似数据的环境下，可 提供极快的访问。 Merge:允许 MySQL DBA 或开发人员将一系列等同的 MyISAM 表以逻辑方式组合在一起,并作为 1 个对象引用它们。对于诸如数据仓储等 VLDB 环境十分适合。 7.选择合适的数据类型 前提: 使用适合存储引擎。 选择原则: 根据选定的存储引擎,确定如何选择合适的数据类型下面的选择方法按存储引擎分类 : MyISAM 数据存储引擎和数据列 MyISAM数据表，最好使用固定长度的数据列代替可变长度的数据列。 MEMORY存储引擎和数据列 MEMORY数据表目前都使用固定长度的数据行存储，因此无论使用CHAR或VARCHAR列都没有关系。两者都是作为CHAR类型处理的。 InnoDB 存储引擎和数据列 建议使用 VARCHAR类型 对于InnoDB数据表，内部的行存储格式没有区分固定长度和可变长度列(所有数据行 都使用指向数据列值的头指针) ，因此在本质上，使用固定长度的CHAR列不一定比使 用可变长度VARCHAR列简单。 因而， 主要的性能因素是数据行使用的存储总量。 由于 CHAR 平均占用的空间多于VARCHAR，因此使用VARCHAR来最小化需要处理的数据行的存储总 量和磁盘I/O是比较好的。 8.char & varchar 保存和检索的方式不同。它们的最大长度和是否尾部空格被保留等方面也不同。在存储或检索过程中不进行大小写转换。 9.Mysql字符集 mysql服务器可以支持多种字符集 (可以用show character set命令查看所有mysql支持 的字符集) ，在同一台服务器、同一个数据库、甚至同一个表的不同字段都可以指定使用不 同的字符集。 mysql的字符集包括字符集(CHARACTER)和校对规则(COLLATION)两个概念。 10.如何选择字符集? 建议在能够完全满足应用的前提下,尽量使用小的字符集。因为更小的字符集意味着能够节省空间、 减少网络传输字节数,同时由于存储空间的较小间接的提高了系统的性能。 有很多字符集可以保存汉字，比如 utf8、gb2312、gbk、latin1 等等，但是常用的是 gb2312 和 gbk。因为 gb2312 字库比 gbk 字库小，有些偏僻字(例如:洺)不能保存，因此 在选择字符集的时候一定要权衡这些偏僻字在应用出现的几率以及造成的影响， 不能做出肯 定答复的话最好选用 gbk。 11.什么是索引？ 在关系数据库中，索引是一种单独的、物理的对数据库表中一列或多列的值进行排序的一种存储结构，它是某个表中一列或若干列值的集合和相应的指向表中物理标识这些值的数据页的逻辑指针清单。索引的作用相当于图书的目录，可以根据目录中的页码快速找到所需的内容。 12.索引设计原则？ 搜索的索引列，不 一定是所要选择的列。最适合索引的列是出现在WHERE子句中的列，或连接子句中指定的列，而不是出现在SELECT 关键字后的选择列表中的列。 使用惟一索引。考虑某列中值的分布。 对于惟一值的列，索引的效果最好，而具有多个 重复值的列，其索引效果最差。 使用短索引。如果对串列进行索引，应该指定一个前缀长度,只要有可能就应该这做样。 例如，如果有一个 CHAR(200) 列，如果在前 10 个或 20 个字符内，多数值是惟一的， 那么就不要对整个列进行索引。 利用最左前缀。在创建 一个 n 列的索引时，实际是创建了 MySQL 可利用的 n 个索引。 多列索引可起几个索引的作用，因为可利用索引中最左边的列集来匹配行。 这样的列集 称为最左前缀。 (这与索引一个列的前缀不同，索引一个列的前缀是利用该的n前个字 符作为索引值 ) 不要过度索引。每个额外的索引都要占用额外的磁盘空间，并降低写操作的性能，这一点我们前面已经介绍 过。在修改表的内容时，索引必须进行更新,有时可能需要重构, 因此， 索引越多，所花的时间越长。 如果有一个索引很少利用或从不使用，那么会不必要地减缓表的修改速度。 此外，MySQL 在生成一个执行计划时，要考虑各个索引，这也要费时间。 创建多余的索引给查询优化带来了更多的工作。索引太多，也可能会使 MySQL选择不到所要使用的 最好索引。 只保持所需的索引有利于查询优化。 如果想给已索引的表增加索引， 应 该考虑所要增加的索引是否是现有多列索引的最左索引。 考虑在列上进行的比较类型。 索引可用于“ =”、“ > ”和 BETWEEN 运算。在模式具有一个直接量前缀时，索引也用于 LIKE 运算。如果只将某个列用于其他类型的运算时(如 STRCMP( )) ，对其进行索引没有价值。 13.MySql有哪些索引? 数据结构角度 BTREE HASH FULLTEXT R-Tree 物理存储角度 1、聚集索引（clustered index） 2、非聚集索引（non-clustered index） 从逻辑角度 普通索引：仅加速查询 唯一索引：加速查询 + 列值唯一（可以有null） 主键索引：加速查询 + 列值唯一（不可以有null）+ 表中只有一个 组合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并 全文索引：对文本的内容进行分词，进行搜索 14.Hash索引和B+树索引的底层实现原理: hash索引底层就是hash表,进行查找时,调用一次hash函数就可以获取到相应的键值,之后进行回表查询获得实际数据.B+树底层实现是多路平衡查找树.对于每一次的查询都是从根节点出发,查找到叶子节点方可以获得所查键值,然后根据查询判断是否需要回表查询数据. 那么可以看出他们有以下的不同: hash索引进行等值查询更快(一般情况下),但是却无法进行范围查询. 因为在hash索引中经过hash函数建立索引之后,索引的顺序与原顺序无法保持一致,不能支持范围查询.而B+树的的所有节点皆遵循(左节点小于父节点,右节点大于父节点,多叉树也类似),天然支持范围. hash索引不支持使用索引进行排序,原理同上. hash索引不支持模糊查询以及多列索引的最左前缀匹配.原理也是因为hash函数的不可预测.AAAA和AAAAB的索引没有相关性. hash索引任何时候都避免不了回表查询数据,而B+树在符合某些条件(聚簇索引,覆盖索引等)的时候可以只通过索引完成查询. hash索引虽然在等值查询上较快,但是不稳定.性能不可预测,当某个键值存在大量重复的时候,发生hash碰撞,此时效率可能极差.而B+树的查询效率比较稳定,对于所有的查询都是从根节点到叶子节点,且树的高度较低. 因此,在大多数情况下,直接选择B+树索引可以获得稳定且较好的查询速度.而不需要使用hash索引. 15. 非聚簇索引一定会回表查询吗? 不一定,这涉及到查询语句所要求的字段是否全部命中了索引,如果全部命中了索引,那么就不必再进行回表查询. 举个简单的例子,假设我们在员工表的年龄上建立了索引,那么当进行select age from employee where age 的查询时,在索引的叶子节点上,已经包含了age信息,不会再次进行回表查询. 16.如何查询最后一行记录? select * from table_name order by id desc limit 1; 17.MySQL自增id不连续问题? 唯一键冲突 事务回滚 批量申请自增id的策略 18.sql注入问题？ 原因:用户传入的参数中注入符合sql的语法，从而破坏原有sql结构语意，达到攻击效果。 19.什么是3NF（范式）? 1NF 指的是数据库表中的任何属性都具有原子性的，不可再分解 2NF 是对记录的惟一性约束，要求记录有惟一标识，即实体的惟一性 3NF是对字段冗余性的约束，即任何字段不能由其他字段派生出来，它要求字段没有冗余 20. NULL和空串判断? NULL值是没有值,，它不是空串。如果指定''(两个单引号，其间没有字符)，这在NOT NULL列中是允许的。空串是一个有效的值，它不是无值。 判断NULL需要用 IS NULL 或者 IS NOT NULL。 21.什么是事务? 可以用来维护数据库的完整性，它保证成批的MySQL操作要么完全执行，要么完全不执行。 22.事务4个特性？ 事务是必须满足4个条件（ACID）： 原子性 Atomicity：一个事务中的所有操作，要么全部完成，要么全部不完成，最小的执行单位。 一致性 Consistency：事务执行前后，都处于一致性状态。 隔离性 Isolation：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。 持久性 Durability：事务执行完成后，对数据的修改就是永久的，即便系统故障也不会丢失。 23.事务隔离级别分别是？ READ_UNCOMMITTED 这是事务最低的隔离级别，它充许另外一个事务可以看到这个事务未提交的数据。解决第一类丢失更新的问题，但是会出现脏读、不可重复读、第二类丢失更新的问题，幻读 。 READ_COMMITTED 保证一个事务修改的数据提交后才能被另外一个事务读取，即另外一个事务不能读取该事务未提交的数据。解决第一类丢失更新和脏读的问题，但会出现不可重复读、第二类丢失更新的问题，幻读问题 REPEATABLE_READ 保证一个事务相同条件下前后两次获取的数据是一致的 （注意是 一个事务，可以理解为事务间的数据互不影响）解决第一类丢失更新，脏读、不可重复读、第二类丢失更新的问题，但会出幻读。 SERIALIZABLE 事务串行执行，解决了脏读、不可重复读、幻读。但效率很差，所以实际中一般不用。 24.InnoDB默认事务隔离级别?如何查看当前隔离级别 可重复读（REPEATABLE-READ） 查看： mysql> select @@global.tx_isolation; +———————————+ | @@global.tx_isolation | +———————————+ | REPEATABLE-READ | +———————————+ 1 row in set, 1 warning (0.01 sec) 25.什么是锁？ 数据库的锁是为了支持对共享资源进行并发访问，提供数据的完整性和一致性，这样才能保证在高并发的情况下，访问数据库的时候，数据不会出现问题。 26.死锁？ 是指两个或两个以上进程执行过程中，因竞争共享资源造成的相互等待现象。 27.如何处理死锁？ 设置超时时间。超时后自动释放。 发起死锁检测，主动回滚其中一条事务，让其他事务继续执行。 28.如何创建用户？授权？ 创建用户: CREATE USER 'username'@'host' IDENTIFIED BY 'password'; 授权： GRANT privileges ON databasename.tablename TO 'username'@'host'; username：用户名 host：可以登陆的主机地址。本地用户用localhost表示，任意远程主机用通配符%。 password：登陆密码，密码可以为空表示不需要密码登陆服务器 databasename: 数据库名称。 tablename:表名称，*代表所有表。 29.如何查看表结构？ ​ desc table_name; mysql> desc zipkin_spans; +———————+———————+———+——+————+———+ | Field | Type | Null | Key | Default | Extra | +———————+———————+———+——+————+———+ | trace_id_high | bigint(20) | NO | PRI | 0 | | | trace_id | bigint(20) | NO | PRI | NULL | | | id | bigint(20) | NO | PRI | NULL | | | name | varchar(255) | NO | MUL | NULL | | | parent_id | bigint(20) | YES | | NULL | | | debug | bit(1) | YES | | NULL | | | start_ts | bigint(20) | YES | MUL | NULL | | | duration | bigint(20) | YES | | NULL | | +———————+———————+———+——+————+———+ 8 rows in set (0.01 sec) 30.Mysql删除表的几种方式？区别？ 1.delete : 仅删除表数据，支持条件过滤，支持回滚。记录日志。因此比较慢。 delete from table_name; 2.truncate: 仅删除所有数据，不支持条件过滤，不支持回滚。不记录日志，效率高于delete。 truncate table table_name; 3.drop:删除表数据同时删除表结构。将表所占的空间都释放掉。删除效率最高。 drop table table_name; 31.like走索引吗? Xxx% 走索引， %xxx不走索引。 32.什么是回表？ 在普通索引查到主键索引后，再去主键索引定位记录。等于说非主键索引需要多走一个索引树。 33.如何避免回表？ 索引覆盖被查询的字段。 34.索引覆盖是什么？ 如果一个索引包含(或覆盖)所有需要查询的字段的值，称为‘覆盖索引’。 35.视图的优缺点？ 优点 简单化，数据所见即所得 安全性，用户只能查询或修改他们所能见到得到的数据 逻辑独立性，可以屏蔽真实表结构变化带来的影响 缺点 性能相对较差，简单的查询也会变得稍显复杂 修改不方便，特变是复杂的聚合视图基本无法修改 36.主键和唯一索引区别？ 本质区别，主键是一种约束，唯一索引是一种索引。 主键不能有空值（非空+唯一），唯一索引可以为空。 主键可以是其他表的外键，唯一索引不可以。 一个表只能有一个主键，唯一索引 可以多个。 都可以建立联合主键或联合唯一索引。 主键-》聚簇索引，唯一索引->非聚簇索引。 37.如何随机获取一条记录? SELECT * FROM table_name ORDER BY rand() LIMIT 1; 38.Mysql中的数值类型？ 39.查看当前表有哪些索引？ show index from table_name; 40.索引不生效的情况？ 使用不等于查询 NULL值 列参与了数学运算或者函数 在字符串like时左边是通配符.比如 %xxx 当mysql分析全表扫描比使用索引快的时候不使用索引. 当使用联合索引,前面一个条件为范围查询,后面的即使符合最左前缀原则,也无法使用索引. 41.MVVC？ MVCC 全称是多版本并发控制系统，InnoDB 的 MVCC 是通过在每行记录后面保存两个隐藏的列来实现，这两个列一个保存了行的创建时间，一个保存行的过期时间（删除时间）。当然存储的并不是真实的时间而是系统版本号（system version number）。每开始一个新的事务，系统版本号都会自动新增，事务开始时刻的系统版本号会作为事务的版本号，用来查询到每行记录的版本号进行比较。 42.sql语句的执行流程？ 客户端连接数据库，验证身份。 获取当前用户权限。 当你查询时，会先去缓存看看，如果有返回。 如果没有，分析器对sql做词法分析。 优化器对sql进行“它认为比较好的优化”。 执行器负责具体执行sql语句。 最后把数据返回给客户端。 43.如何获取select 语句执行计划? explain sql; 44.explain列有哪些？含义？ 一、 id SQL查询中的序列号。 id列数字越大越先执行，如果说数字一样大，那么就从上往下依次执行。 二、select_type 三、table 显示这一行的数据是关于哪张表的。不一定是实际存在的表名。 可以为如下的值： : 引用id为M和N UNION后的结果。 : 引用id为N的结果派生出的表。派生表可以是一个结果集，例如派生自FROM中子查询的结果。 : 引用id为N的子查询结果物化得到的表。即生成一个临时表保存子查询的结果。 四、type 这是最重要的字段之一，显示查询使用了何种类型。从最好到最差的连接类型依次为： system，const，eq_ref，ref，fulltext，ref_or_null，index_merge，unique_subquery，index_subquery，range，index，ALL 1、system 表中只有一行数据或者是空表，这是const类型的一个特例。且只能用于myisam和memory表。如果是Innodb引擎表，type列在这个情况通常都是all或者index 2、const 最多只有一行记录匹配。当联合主键或唯一索引的所有字段跟常量值比较时，join类型为const。其他数据库也叫做唯一索引扫描 3、eq_ref 多表join时，对于来自前面表的每一行，在当前表中只能找到一行。这可能是除了system和const之外最好的类型。当主键或唯一非NULL索引的所有字段都被用作join联接时会使用此类型。 eq_ref可用于使用'='操作符作比较的索引列。比较的值可以是常量，也可以是使用在此表之前读取的表的列的表达式。 相对于下面的ref区别就是它使用的唯一索引，即主键或唯一索引，而ref使用的是非唯一索引或者普通索引。 eq_ref只能找到一行，而ref能找到多行。 4、ref 对于来自前面表的每一行，在此表的索引中可以匹配到多行。若联接只用到索引的最左前缀或索引不是主键或唯一索引时，使用ref类型（也就是说，此联接能够匹配多行记录）。 ref可用于使用'='或''操作符作比较的索引列。 5、 fulltext 使用全文索引的时候是这个类型。要注意，全文索引的优先级很高，若全文索引和普通索引同时存在时，mysql不管代价，优先选择使用全文索引 6、ref_or_null 跟ref类型类似，只是增加了null值的比较。实际用的不多。 7、index_merge 表示查询使用了两个以上的索引，最后取交集或者并集，常见and ，or的条件使用了不同的索引，官方排序这个在ref_or_null之后，但是实际上由于要读取多个索引，性能可能大部分时间都不如range 8、unique_subquery 用于where中的in形式子查询，子查询返回不重复值唯一值，可以完全替换子查询，效率更高。 该类型替换了下面形式的IN子查询的ref： value IN (SELECT primary_key FROM single_table WHERE some_expr) 9、index_subquery 该联接类型类似于unique_subquery。适用于非唯一索引，可以返回重复值。 10、range 索引范围查询，常见于使用 =, <>, >, >=, , BETWEEN, IN()或者like等运算符的查询中。 11、index 索引全表扫描，把索引从头到尾扫一遍。这里包含两种情况： 一种是查询使用了覆盖索引，那么它只需要扫描索引就可以获得数据，这个效率要比全表扫描要快，因为索引通常比数据表小，而且还能避免二次查询。在extra中显示Using index，反之，如果在索引上进行全表扫描，没有Using index的提示。 12、all 全表扫描，性能最差。 五、possible_keys 查询可能使用到的索引都会在这里列出来。 六、Key key列显示MySQL实际使用的键（索引) 要想强制MySQL使用或忽视possible_keys列中的索引，可以使用FORCE INDEX、USE INDEX或者IGNORE INDEX。 select_type为index_merge时，这里可能出现两个以上的索引，其他的select_type这里只会出现一个。 七、key_len 表示索引中使用的字节数。 key_len只计算where条件用到的索引长度，而排序和分组就算用到了索引，也不会计算到key_len中。 不损失精确性的情况下，长度越短越好 。 八、ref 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值。 九、rows rows 也是一个重要的字段。 这是mysql估算的需要扫描的行数（不是精确值）。 十、Extra 该列包含MySQL解决查询的详细信息,有以下几种情况： Using where:列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表的全部的请求列都是同一个索引的部分的时候，表示mysql服务器将在存储引擎检索行后再进行过滤。 Using temporary：表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询。 Using filesort：MySQL中无法利用索引完成的排序操作称为“文件排序”。 Using join buffer：改值强调了在获取连接条件时没有使用索引，并且需要连接缓冲区来存储中间结果。如果出现了这个值，那应该注意，根据查询的具体情况可能需要添加索引来改进能。 Impossible where：这个值强调了where语句会导致没有符合条件的行。 Select tables optimized away：这个值意味着仅通过使用索引，优化器可能仅从聚合函数结果中返回一行。 链接：https://www.jianshu.com/p/8fab76bbf448 45.MySql最多创建多少列索引？ 16 46.为什么最好建立一个主键? 主键是数据库确保数据行在整张表唯一性的保障,即使业务上本张表没有主键,也建议添加一个自增长的ID列作为主键.设定了主键之后,在后续的删改查的时候可能更加快速以及确保操作数据范围安全. 47.字段为什么要求建议为not null? MySQL官网这样介绍: NULL columns require additional space in the rowto record whether their values are NULL. For MyISAM tables, each NULL columntakes one bit extra, rounded up to the nearest byte. null值会占用更多的字节,且会在程序中造成很多与预期不符的情况. 48.varchar(10)和int(10)代表什么含义 varchar的10代表了申请的空间长度,也是可以存储的数据的最大长度,而int的10只是代表了展示的长度,不足10位以0填充.也就是说,int(1)和int(10)所能存储的数字大小以及占用的空间都是相同的,只是在展示时按照长度展示。 49.视图是什么？对比普通表优势? 视图(View)是一种虚拟存在的表，对于使用视图的用户来说基本上是透明的。视图并 不在数据库中实际存在，行和列数据来自定义视图的查询中使用的表，并且是在使用视图时 动态生成的。 视图相对于普通的表的优势主要包括以下几项。 简单:使用视图的用户完全不需要关心后面对应的表的结构、关联条件和筛选条件， 对用户来说已经是过滤好的复合条件的结果集。 安全:使用视图的用户只能访问他们被允许查询的结果集，对表的权限管理并不能 限制到某个行某个列，但是通过视图就可以简单的实现。 数据独立:一旦视图的结构确定了，可以屏蔽表结构变化对用户的影响，源表增加 列对视图没有影响;源表修改列名，则可以通过修改视图来解决，不会造成对访问 者的影响。 50.count(*)在不同引擎的实现方式? MyISAM :把一个表的总行数存在了磁盘上，执行 count(*) 的时候会直接返回这个数，效率很高。 InnoDB : 比较麻烦，它执行 count(*) 的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。 参考: 《深入浅出MySQL》 《高性能MySql》 《MySQL技术内幕（第5版)》 《MySQL必知必会》 极客时间：MySQL实战45讲 百度百科 "},"zother1-JavaFaceNotes/Nginx.html":{"url":"zother1-JavaFaceNotes/Nginx.html","title":"Nginx","keywords":"","body":"Nginx 1.什么是nginx? Nginx是一个高性能的HTTP和反向代理服务器。同时也是一个 IMAP/POP3/SMTP 代理服务器。 官方网站:http://nginx.org。 2.nginx主要特征？ 处理静态文件，索引文件以及自动索引;打开文件描述符缓冲. 无缓存的反向代理加速，简单的负载均衡和容错. FastCGI，简单的负载均衡和容错.模块化的结构。包括 gzipping, byte ranges, chunked responses,以及 SSI-filter 等filter。如果由 FastCGI 或其它代理服务器处理单页中存在的多个 SSI，则这项处理可以并行 运行，而不需要相互等待。 支持 SSL 和 TLSSNI. Nginx 它支持内核 Poll 模型，能经受高负载的考验,有报告表明能支持高达 50,000 个并发连接数。 Nginx 具有很高的稳定性。 例如当前 apache 一旦上到 200 个以上进程，web 响应速度就明显非常缓慢了。而 Nginx 采取了分阶段资源分配技术，使得它的 CPU 与内存占用率非常低。nginx 官方表示保持 10,000 个没有活动的连接，它只占 2.5M 内存，所以类似 DOS 这样的攻击对 nginx 来说基本上是毫无用处的。 Nginx 支持热部署。它的启动特别容易, 并且几乎可以做到 7*24 不间断运行，即使运 行数个月也不需要重新启动。对软件版本进行进行热升级。 Nginx 采用 master-slave 模型,能够充分利用 SMP 的优势，且能够减少工作进程在磁 盘 I/O 的阻塞延迟。当采用 select()/poll()调用时，还可以限制每个进程的连接数。 Nginx 代码质量非常高，代码很规范，手法成熟， 模块扩展也很容易。特别值得一提的是强大的 Upstream 与 Filter 链。 Nginx 采用了一些 os 提供的最新特性如对 sendfile (Linux2.2+)，accept-filter (FreeBSD4.1+)，TCP_DEFER_ACCEPT (Linux 2.4+)的支持，从而大大提高了性能。 免费开源，可以做高并发负载均衡。 3.nginx 常用命令? 启动 nginx 。 停止 nginx -s stop 或 nginx -s quit 。 重载配置 ./sbin/nginx -s reload(平滑重启) 或 service nginx reload 。 重载指定配置文件 .nginx -c /usr/local/nginx/conf/nginx.conf 。 查看 nginx 版本 nginx -v 。 检查配置文件是否正确 nginx -t 。 显示帮助信息 nginx -h 。 4.工作模式及连接数上限? events { use epoll; #epoll 是多路复用 IO(I/O Multiplexing)中的一种方 式,但是仅用于 linux2.6 以上内核,可以大大提高 nginx 的性能 worker_connections 1024;#单个后台 worker process 进程的最大并发链接数 # multi_accept on; } 5.nginx负载均衡几种算法？ 5种。 1.轮询模式（默认） 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 2.权重模式 指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况 3.IP_hash模式 （IP散列） 每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 4.url_hash模式 5.fair模式 按后端服务器的响应时间来分配请求，响应时间短的优先分配。 6.nginx有几种进程模型? 分为master-worker模式和单进程模式。在master-worker模式下，有一个master进程和至少一个的worker进程，单进程模式顾名思义只有一个进程。 7.如何定义错误提示页面？ # 定义错误提示页面 error_page 500 502 503 504 /50x.html; location = /50x.html { root /root; } 8.如何精准匹配路径? location =开头表示精准匹配 location = /get { #规则 A } 9.路径匹配优先级？ 多个 location 配置的情况下匹配顺序为 首先匹配 =，其次匹配^~, 其次是按文件中顺序的正则匹配，最后是交给 / 通用匹配。当 有匹配成功时候，停止匹配，按当前匹配规则处理请求。 10.如何把请求转发给后端应用服务器？ location = / { proxy_pass http://tomcat:8080/index } 11.如何根据文件类型设置过期时间？ location ~* \\.(js|css|jpg|jpeg|gif|png|swf)$ { if (-f $request_filename) { expires 1h; break; } } 12.禁止访问某个目录？ location ^~/path/ { deny all; } 13.nginx负载均衡实现过程？ 首先在 http 模块中配置使用 upstream 模块定义后台的 webserver 的池子，名为 proxy-web，在池子中我们可以添加多台后台 webserver，其中状态 检查、调度算法都是在池子中配置;然后在 serverr 模块中定义虚拟主机，但是这个虚拟主 机不指定自己的 web 目录站点，它将使用 location 匹配 url 然后转发到上面定义好的 web 池子中，最后根据调度策略再转发到后台 web server 上 。 14.负载均衡配置？ Upstream proxy_nginx { server 192.168.0.254 weight=1max_fails=2 fail_timeout=10s ; server 192.168.0.253 weight=2 max_fails=2fail_timeout=10s; server192.168.0.252 backup; server192.168.0.251 down; } server{ listen 80; server_name xiaoka.com; location / { proxy_pass http:// proxy_nginx; proxy_set_header Host proxy_set_header X-Real-IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } 15.设置超时时间？ http { ………. keepalive_timeout 60; ###设置客户端连接保持会话的超时时间，超过这个时间，服务器会关闭该连接。 tcp_nodelay on; \\####打开 tcp_nodelay，在包含了 keepalive 参数才有效 client_header_timeout 15; ####设置客户端请求头读取超时时间，如果超过这个时间，客户端还没有发送任何数据， Nginx 将返回“Request time out(408)”错误 client_body_timeout 15; \\####设置客户端请求主体读取超时时间，如果超过这个时间，客户端还没有发送任何数据， Nginx 将返回“Request time out(408)”错误 send_timeout 15; ####指定响应客户端的超时时间。这个超过仅限于两个连接活动之间的时间，如果超过这 个时间，客户端没有任何活动，Nginx 将会关闭连接。 …… } 16.开启压缩功能好处？坏处？ 好处：压缩是可以节省带宽，提高传输效率 坏处：但是由于是在服务器上进行压缩，会消耗服务器起源 参考: 《Nginx从入门到精通》 《Nginx高性能Web服务器详解》 《深入理解nginx》 "},"zother1-JavaFaceNotes/Redis.html":{"url":"zother1-JavaFaceNotes/Redis.html","title":"Redis","keywords":"","body":"Redis 1.Redis是什么? Redis是一个开放源代码（BSD许可）的内存中数据结构存储，可用作数据库，缓存和消息代理，是一个基于键值对的NoSQl数据库。 2.Redis特性? 速度快 基于键值对的数据结构服务器 丰富的功能、丰富的数据结构 简单稳定 客户端语言多 持久化 主从复制 高可以 & 分布式 3.Redis合适的应用场景？ 缓存 排行榜 计数器 分布式会话 分布式锁 社交网络 最新列表 消息系统 4.除了Redis你还知道哪些NoSQL数据库？ MongoDB、MemcacheDB、Cassandra、CouchDB、Hypertable、Leveldb。 5.Redis和Memcache区别？ 支持的存储类型不同，memcached只支持简单的k/v结构。redis支持更多类型的存储结构类型(详见问题6)。 memcached数据不可恢复，redis则可以把数据持久化到磁盘上。 新版本的redis直接自己构建了VM 机制 ，一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。 redis当物理内存用完时，可以将很久没用到的value交换到磁盘。 6.Redis的有几种数据类型？ 基础：字符串（String）、哈希（hash)、列表（list)、集合(set)、有序集合(zset)。 还有HyperLogLog、流、地理坐标等。 7.Redis有哪些高级功能？ 消息队列、自动过期删除、事务、数据持久化、分布式锁、附近的人、慢查询分析、Sentinel 和集群等多项功能。 8.安装过Redis吗,简单说下步骤？ 1.下载Redis指定版本源码安装包压缩到当前目录。 解压缩Redis源码安装包。 建立一个redis目录软链接，指向解压包。 进入redis目录 编译 安装 对于使用docker的童靴来说就比较容易了。 docker pull redis 9.redis几个比较主要的可执行文件？分别是？ 10.启动Redis的几种方式？ 1.默认配置 : ./redis-server 2.运行启动: redis-server 加上要修改配置名和值（可以是多对），没有配置的将使用默认配置。 例如: redis-server ———port 7359 3.指定配置文件启动: ./redis-server /opt/redis/redis.conf 11.Redis配置需要自己写？如何配置？ redis目录下有一个redis.conf的模板配置。所以只需要复制模板配置然后修改即可。 一般来说大部分生产环境都会用指定配置文件的方式启动redis。 12.Redis客户端命令执行的方式？ 1.交互方式: redis-cli -h 127.0.0.1 -p 6379 连接到redis后，后面执行的命令就可以通过交互方式实现了。 2.命令行方式： redis-cli -h 127.0.0.1 -p 6379 get value 13.如何停止redis服务？ Kill -9 pid (粗暴，请不要使用,数据不仅不会持久化，还会造成缓存区等资源不能被优雅关闭) 可以用redis 的shutdown 命令，可以选择是否在关闭前持久化数据。 redis-cli shutdown nosave|save 14.如何查看当前键是否存在？ exists key 15.如何删除数据？ del key 16.redis为什么快？单线程？ redis使用了单线程架构和I/O多路复用模型模型。 纯内存访问。 由于是单线程避免了线程上下文切换带来的资源消耗。 17.字符串最大不能超过多少？ 512MB 18.redis默认分多少个数据库？ 16 19.redis持久化的几种方式？ RDB、AOF、混合持久化。 20.RDB持久化? RDB（Redis DataBase)持久化是把当前进程数据生成快照保存到硬盘的过程。 Tips:是以二进制的方式写入磁盘。 21.RDB的持久化是如何触发的？ 手动触发: save: 阻塞当前Redis服务器，直到RDB过程完成为止，如果数据比较大的话，会造成长时间的阻塞， 线上不建议。 bgsave:redis进程执行 fork操作创作子进程，持久化由子进程负责，完成后自动结束，阻塞只发生在 fork阶段，一半时间很短。 自动触发： save xsecends n: 表示在x秒内，至少有n个键发生变化，就会触发RDB持久化。也就是说满足了条件就会触发持久化。 flushall : 主从同步触发 22.RDB的优点？ rdb是一个紧凑的二进制文件，代表Redis在某个时间点上的数据快照。 适合于备份，全量复制的场景，对于灾难恢复非常有用。 Redis加载RDB恢复数据的速度远快于AOF方式。 23.RDB的缺点？ RDB没法做到实时的持久化。中途意外终止，会丢失一段时间内的数据。 RDB需要fork()创建子进程，属于重量级操作，可能导致Redis卡顿若干秒。 24.如何禁用持久化？ 一般来说生成环境不会用到，了解一下也有好处的。 config set save \"\" 25.AOF持久化？ AOF(append only file)为了解决rdb不能实时持久化的问题，aof来搞定。以独立的日志方式记录把每次命令记录到aof文件中。 26.如何查询AOF是否开启? config get appendonly 27.如何开启AOF? 命令行方式： 实时生效，但重启后失效。 config set appendonly 配置文件：需要重启生效，重启后依然生效。 appendonly yes 28.AOF工作流程？ 1.所有写入命令追加到aof_buf缓冲区。 2.AOF缓冲区根据对应的策略向硬盘做同步操作。 3.随着AOF文件越来越大，需要定期对AOF文件进行重写，达到压缩的目的。 4.当redis服务器重启时，可以加载AOF文件进行数据恢复。 29.为什么AOF要先把命令追加到缓存区(aof_buf)中？ Redis使用单线程响应命令，如果每次写入文件命令都直接追加到硬盘，性能就会取决于硬盘的负载。如果使用缓冲区，redis提供多种缓冲区策略，在性能和安全性方面做出平衡。 30.AOF持久化如何触发的？ 自动触发：满足设置的策略和满足重写触发。 策略：(在配置文件中配置) ![image-20200427101221526](https://gitee.com/yizhibuerdai/Imagetools/raw/master/images/image-20200427101221526.png 手动触发：（执行命令） bgrewriteaof 31.AOF优点？ AOF提供了3种保存策略：每秒保存、跟系统策略、每次操作保存。实时性比较高，一般来说会选择每秒保存，因此意外发生时顶多失去一秒的数据。 文件追加写形式，所以文件很少有损坏问题，如最后意外发生少写数据，可通过redis-check-aof工具修复。 AOF由于是文本形式，直接采用协议格式，避免二次处理开销，另外对于修改也比较灵活。 32.AOF缺点？ AOF文件要比RDB文件大。 AOF冷备没RDB迅速。 由于执行频率比较高，所以负载高时，性能没有RDB好。 33.混合持久化？优缺点？ 一般来说我们的线上都会采取混合持久化。redis4.0以后添加了新的混合持久化方式。 优点： 在快速加载的同时，避免了丢失过更多的数据。 缺点： 由于混合了两种格式，所以可读性差。 兼容性，需要4.0以后才支持。 34.Redis的Java客户端官方推荐？实际选择？ 官方推荐的有3种：Jedis、Redisson和lettuce。 一般来说用的比较多的有:Jedis|Redisson。 Jedis：更轻量、简介、不支持读写分离需要我们来实现，文档比较少。API提供了比较全面的Redis命令的支持。 Redisson：基于Netty实现，性能高，支持异步请求。提供了很多分布式相关操作服务。高级功能能比较多，文档也比较丰富，但实用上复杂度也相对高。和Jedis相比，功能较为简单，不支持字符串操作，不支持排序、事务、管道、分区等Redis特性。 35.Redis事务？ 事务提供了一种将多个命令请求打包，一次性、按顺序的执行多个命令的机制。并且在事务执行期间，服务器不会中断事务而改去执行其他客户端命令请求，它会 36.Redis事务开始到结束的几个阶段？ 开启事务 命令入队 执行事务/放弃事务 37.Redis中key的过期操作？ ​ 设置key的生存时间为n秒 expire key nseconds ​ 设置key的生存时间为nmilliseconds pxpire key milliseconds ​ 设置过期时间为timestamp所指定的秒数时间戳 expireat key timespamp 设置过期时间为timestamp毫秒级时间戳 pexpireat key millisecondsTimestamp 38.Redis过期键删除策略? 定时删除：在设置的过期时间同时，创建一个定时器在键的过期时间来临时，立即执行队键的操作删除。 惰性删除：放任过期键不管，但每次从键空间中获取键时，都检查取得的键是否过期，如果过期就删除，如果没有就返回该键。 定期删除：每隔一段时间执行一次删除过期键操作，并通过先吃删除操作执行的时长和频率来减少删除操作对cpu时间的影响。 39.Pipeline是什么？为什么要它？ 命令批处理技术，对命令进行组装，然后一次性执行多个命令。 可以有效的节省RTT(Round Trip Time 往返时间)。 经过测试验证： pipeline执行速度一般比逐条执行快。 客户端和服务的网络延越大，pipeline效果越明显。 40.如何获取当前最大内存？如何动态设置？ 获取最大内存: config get maxmemory 设置最大内存： 命令设置: config set maxmemory 1GB 41.Redis内存溢出控制？ 当Redis所用内存达到maxmemory上限时，会出发相应的溢出策略。 42.Redis内存溢出策略？ 1.noeviction(默认策略):拒绝所有写入操作并返回客户端错误信息（error) OOM command not allowed when used memory,只响应读操作。 volatile-lru:根据LRU算法删除设置了超时属性（expire)的键，直到腾出足够空间为止。如果没有可删除的键对象，回退到noeviction策略。 allkeys-lru:根据LRU算法删除键，不管数据有没有设置超时属性， 直到腾出足够空间为止。 allkeys-random:随机删除所有键，直到腾出足够空间为止。 volatile-random:随机删除过期键，直到腾出足够空间为止。 volatile-tth根据键值对象的ttl属性，删除最近将要过期数据。如果没有，回退到noeviction策略。 43.Redis高可用方案？ Redis Sentinel(哨兵)能自动完成故障发现和转移。 44.Redis集群方案？ Twemproxy、Redis Cluster、Codis。 45.Redis Cluster槽范围？ 0~16383 46.Redis锁实现思路? setnx (set if not exists),如果创建成功则表示获取到锁。 setnx lock true 创建锁 del lock 释放锁 如果中途崩溃，无法释放锁？ 此时需要考虑到超时时间的问题。比如 :expire lock 300 由于命令是非原子的，所以还是会死锁,如何解决？ Redis 支持 set 并设置超时时间的功能。 比如: set lock true ex 30 nx 47.什么是布隆过滤器？ 是1970年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都比一般的算法要好的多，缺点是有一定的误识别率和删除困难。 Tips:当判断一定存在时，可能会误判，当判断不存在时，就一定不存在。 48.什么是缓存穿透？处理问题？ 缓存穿透：缓存层不命中，存储层不命中。 处理方式1:缓存空对象，不过此时会占用更多内存空间，所以根据大家业务特性去设置超时时间来控制内存占用的问题。 处理方式2:布隆过滤器。 49.什么是缓存预热？ 就是系统上线后，提前将相关数据加载到缓存系统，避免用户先查库，然后在缓存。 50.什么是缓存雪崩？处理问题？ 缓存雪崩：由于缓存层承载着大量请求，有效的保护了存储层，但如果存储层由于某些原因不能提供服务，存储层调用暴增，造成存储层宕机。 处理： 保证缓存层服务高可用性。 对缓存系统做实时监控，报警等。 依赖隔离组件为后端限流并降级。 做好持久化，以便数据的快速恢复。 参考： 《Redis深度历险:核心原理和应用实践》 《Redis开发与运维》 《Redis设计与实现》 https://redis.io/ 百度百科 "},"zother1-JavaFaceNotes/Spring.html":{"url":"zother1-JavaFaceNotes/Spring.html","title":"Spring","keywords":"","body":"Spring 1.Spring框架? Spring框架是由于软件开发的复杂性而创建的，Spring使用的是基本的JavaBean来完成以前只可能由EJB完成的事。从简单性、可测性和松耦合性角度而言，绝大部分Java应用都可以用Spring。 2.Spring的整体架构？ 大约分为20个模块。 3.Spring可以做什么？ 4.Spring的优点?缺点？ 优点： Spring属于低侵入设计。 IOC将对象之间的依赖关系交给Spring,降低组件之间的耦合，实现各个层之间的解耦，让我们更专注于业务逻辑。 提供面向切面编程。 对各种主流插件提供很好的集成支持。 对事务支持的很好，只要配置即可，无须手动控制。 缺点： 依赖反射，影响性能。 5.你能说几个Spring5的新特性吗？ spring5整个框架基于java8 支持http/2 Spring Web MVC支持最新API Spring WebFlux 响应式编程 支持Kotlin函数式编程 6.IOC? 负责创建对象、管理对象(通过依赖注入)、整合对象、配置对象以及管理这些对象的生命周期。 7.什么是依赖注入? 依赖注入是Spring实现IoC的一种重要手段，将对象间的依赖关系的控制权从开发人员手里转移到容器。 8.IOC注入哪几种方式？ 1.构造器注入 2.setter注入 3.接口注入（我们几乎不用） 9.IOC优点？缺点？ 优点： 组件之间的解耦，提高程序可维护性、灵活性。 缺点： 创建对象步骤复杂，有一定学习成本。 利用反射创建对象，效率上有损。（对于代码的灵活性和可维护性来看，Spring对于我们的开发带来了很大的便利，这点损耗不算什么哦） 10.bean的生命周期? 1.Spring 对bean进行实例化。 2.Spring将值和bean的引用注入到 bean对应的属性中。 3.如果bean实现了BeanNameAware接口，Spring将bean的ID传递给setBeanName()方法。 4.如果bean实现了BeanFactoryAware接口， Spring将调用setBeanFactory()方法，将 bean所在的应用引用传入进来。 5.如果bean实现了ApplicationContextAware接口，Spring将调用setApplicationContext()方法，将bean所在的应用引用传入进来。 6.如果bean实现了BeanPostProcessor 接口，Spring将调用他们的post-ProcessBeforeInitalization()方法。 7.如果bean实现了InitializingBean接口，Spring将调用他们的after-PropertiesSet()方法，类似地，如果bean使用init-method声明了初始化方法，该方法也会被调用。 8.如果bean实现了BeanPostProcessor接口，Spring将调用它们的post-ProcessAfterInitialization()方法。 9.此时， bean已经准备就绪，可以被应用程序使用了，他们将一直驻留在应用上下文中,直到该应用被销毁。 10.如果bean实现了DisposableBean接口，Spring将调用它的destory()接口方法，同样，如果bean使用destroy-method声明了销毁方法，该方法也会被调用。 11.Spring有几种配置方式？ 基于xml 基于注解 基于Java 12.Spring中的bean有几种scope? singleton: 单例，每一个bean只创建一个对象实例。 prototype，原型，每次对该bean请求调用都会生成各自的实例。 request，请求，针对每次HTTP请求都会生成一个新的bean。表示在一次 HTTP 请求内有效。 session，在一个http session中，一个bean定义对应一个bean实例。 global session:在一个全局http session中，一个bean定义对应一个bean实例。 13.什么是AOP(面向切面编程)？ 在软件业，AOP为Aspect Oriented Programming的缩写，意为：面向切面编程，通过预编译方式和运行期间动态代理实现程序功能的统一维护的一种技术。 14.切面有几种类型的通知？分别是？ 前置通知(Before): 目标方法被调用之前调用通知功能。 后置通知(After): 目标方法完成之后调用通。 返回通知(After-returning): 目标方法成功执行之后调用通知。 异常通知(After-throwing): 目标方法抛出异常后调用通知。 环绕通知(Around): 在被通知的方法调用之前和调用之后执行自定义的行为。 15.什么是连接点 （Join point)? 连接点是在应用执行过程中能够插入切面的一个点。这个点可以是调用方法时、抛出异常时、甚至修改一个字段时。 16.什么是切点（Pointcut)? 切点的定义会匹配通知所要织入的一个或多个连接点。我们通常使用明确的类和方法名称，或是利用正则表达式定义所匹配的类和方法名称来指定这些切点。有些AOP框架允许我们创建动态的切点，可以根据运行时的决策(比如方法的参数值)来决定是否应用通知。 17.什么是切面(Aspect)? 切面是通知和切点的结合。通知和切点共同定义了切面的全部内容。 18.织入(Weaving)? 织入是把切面应用到目标对象并创建新的代理对象的过程。切面在指定的连接点被织入到目标对象中。 19.引入（Introduction）？ 􏶸􏰫􏶹􏵡􏰸􏱲􏰇􏲘􏱴􏰉􏵇􏶺􏶻􏶼􏰽􏱸􏵼􏶽􏶾􏱎􏶸􏰫􏶹􏵡􏰸􏱲􏰇􏲘􏱴􏰉􏵇􏶺􏶻􏶼􏰽􏱸􏵼􏶽􏶾􏱎引入允许我们向现有的类添加新方法或属性。 20.在目标对象的生命周期里有多个点可以进行织入？ 编译期：切面在目标类编译时被织入。AspectJ的织入编译器就是以这种方式织入切面的。 类加载期：切面在目标类加载到JVM时被织入。它可以在目标类被引入应用之前增强该目标类的字节码。AspectJ 5的加载时织入(load-time weaving，LTW)就支持以这种方式织入切面。 运行期：切面在应用运行的某个时刻被织入。一般情况下，在织入切面时，AOP容器会为目标对象动态地创建一个代理对象。Spring AOP就是以这种方式织入切面的。 21.AOP动态代理策略？ 如果目标对象实现了接口，默认采用JDK 动态代理。可以强制转为CgLib实现AOP。 如果没有实现接口，采用CgLib进行动态代理。 22.什么是MVC框架？ MVC全名是Model View Controller，是模型(model)－视图(view)－控制器(controller)的缩写，一种软件设计典范，用一种业务逻辑、数据、界面显示分离的方法组织代码，将业务逻辑聚集到一个部件里面，在改进和个性化定制界面及用户交互的同时，不需要重新编写业务逻辑。 MVC被独特的发展起来用于映射传统的输入、处理和输出功能在一个逻辑的图形化用户界面的结构中。 23.什么是SpringMVC? SpringMVC是Spring框架的一个模块。是一个基于MVC的框架。 24.SpringMVC的核心？ DispatcherServlet 25.SpringMVC的几个组件？ DispatcherServlet : 前端控制器，也叫中央控制器。相关组件都是它来调度。 HandlerMapping : 处理器映射器，根据URL路径映射到不同的Handler。 HandlerAdapter : 处理器适配器，按照HandlerAdapter的规则去执行Handler。 Handler : 处理器,由我们自己根据业务开发。 ViewResolver : 视图解析器，把逻辑视图解析成具体的视图。 View : 一个接口，它的实现支持不同的视图类型（freeMaker，JSP等） 26.SpringMVC工作流程？ 1.用户请求旅程的第一站是DispatcherServlet。 2.收到请求后，DispatcherServlet调用HandlerMapping，获取对应的Handler。 3.如果有拦截器一并返回。 4.拿到Handler后，找到HandlerAdapter,通过它来访问Handler,并执行处理器。 5.执行Handler的逻辑。 6.Handler会返回一个ModelAndView对象给DispatcherServlet。 7.将获得到的ModelAndView对象返回给DispatcherServlet。 8.请求ViewResolver解析视图，根据逻辑视图名解析成真正的View。 9.返回View给DispatcherServlet。 10.DispatcherServlet对View进行渲染视图。 11.DispatcherServlet响应用户。 27.SpringMVC的优点？ 1.具有Spring的特性。 2.可以支持多种视图(jsp,freemaker)等。 3.配置方便。 4.非侵入。 5.分层更清晰，利于团队开发的代码维护，以及可读性好。 Tips:Jsp目前很少有人用了。 28.单例bean是线程安全的吗？ 不是。具体线程问题需要开发人员来处理。 29.Spring从哪两个角度实现自动装配？ 组件扫描(component scanning):Spring会自动发现应用上下文中所创建的bean。 自动装配(autowiring):Spring自动满足bean之间的依赖。 30.自动装配有几种方式？分别是？ no - 默认设置，表示没有自动装配。 byName ： 根据名称装配。 byType ： 根据类型装配。 constructor ： 把与Bean的构造器入参具有相同类型的其他Bean自动装配到Bean构造器的对应入参中。 autodetect ：先尝试constructor装配，失败再尝试byType方式。 default：由上级标签的default-autowire属性确定。 31.说几个声明Bean 的注解？ @Component @Service @Repository @Controller 32.注入Java集合的标签？ 允许有相同的值。 不允许有相同的值。 键和值都只能为String类型。 键和值可以是任意类型。 33.Spring支持的ORM？ Hibernate iBatis JPA (Java Persistence API) TopLink JDO (Java Data Objects) OJB 34.@Repository注解？ Dao 层实现类注解，扫描注册 bean。 35.@Value注解? 讲常量、配置中的变量值、等注入到变量中。 36.@Controller注解? 定义控制器类。 37.声明一个切面注解是哪个？ @Aspect 38.映射web请求的注解是？ @RequestMapping 39.@ResponseBody注解？ 作用是将返回对象通过适当的转换器转成置顶格式，写进response的body区。通常用来返回json、xml等。 40.@ResponseBody + @Controller =? @RestController 41.接收路径参数用哪个注解？ @PathVariable 42.@Cacheable注解？ 用来标记缓存查询。 43.清空缓存是哪个注解？ @CacheEvict 44.@Component注解？ 泛指组件，不好归类时，可以用它。 45.BeanFactory 和 ApplicationContext区别？ 46.@Qualifier注解？ 当创建多个相同类型的 bean 时，并且想要用一个属性只为它们其中的一个进行装配，在这种情况下，你可以使用 @Qualifier 注释和 @Autowired 注释通过指定哪一个真正的 bean 将会被装配来消除混乱。 47.事务的注解是? @Transaction 48.Spring事务实现方式有？ 声明式：声明式事务也有两种实现方式。 xml 配置文件的方式。 注解方式（在类上添加 @Transaction 注解）。 编码式：提供编码的形式管理和维护事务。 49.什么是事务传播？ 事务在嵌套方法调用中如何传递，具体如何传播，取决于事务传播行为。 50.Spring事务传播行为有哪些？ 参考： 《Spring in action 4》 《SPRING技术内幕》 《Spring源码深度解析》 《Spring5企业级开发实战》 https://spring.io 百度百科 "},"zother1-JavaFaceNotes/SpringBoot.html":{"url":"zother1-JavaFaceNotes/SpringBoot.html","title":"Spring Boot","keywords":"","body":"SpringBoot 1.什么是SpringBoot? 通过Spring Boot，可以轻松地创建独立的，基于生产级别的Spring的应用程序，您可以“运行”它们。大多数Spring Boot应用程序需要最少的Spring配置。 2.SpringBoot的特征？ 创建独立的Spring应用程序 直接嵌入Tomcat，Jetty或Undertow（无需部署WAR文件） 提供固化的“starter”依赖项，以简化构建配置 尽可能自动配置Spring和3rd Party库 提供可用于生产的功能，例如指标，运行状况检查和外部化配置 完全没有代码生成，也不需要XML配置 3.如何快速构建一个SpringBoot项目？ 通过Web界面使用。http://start.spring.io 通过Spring Tool Suite使用。 通过IntelliJ IDEA使用。 使用Spring Boot CLI使用。 4.SpringBoot启动类注解?它是由哪些注解组成？ @SpringBootApplication @SpringBootConfiguration:组合了 @Configuration 注解，实现配置文件的功能。 @EnableAutoConfiguration:打开自动配置的功能，也可以关闭某个自动配置的选项。 @SpringBootApplication(exclude = { DataSourceAutoConfiguration.class }) @ComponentScan:Spring组件扫描 5.什么是yaml? YAML（/ˈjæməl/，尾音类似camel骆驼）是一个可读性高，用来表达数据序列化的格式。YAML参考了其他多种语言，包括：C语言、Python、Perl。更具有结构性。 6.SpringBoot支持配置文件的格式? 1.properties java.xiaokaxiu.name = xiaoka 2.yml java: xiaokaxiu: name: xiaoka 7.SpringBoot启动方式？ main方法 命令行 java -jar 的方式 mvn/gradle 8.SpringBoot需要独立的容器运行？ 不需要，内置了 Tomcat/Jetty。 9.SpringBoot配置途径？ 命令行参数 java:comp/env里的JNDI属性 JVM系统属性 操作系统环境变量 随机生成的带random.*前缀的属性(在设置其他属性时，可以引用它们，比如${random. long}) 应用程序以外的application.properties或者appliaction.yml文件 打包在应用程序内的application.properties或者appliaction.yml文件 通过@PropertySource标注的属性源 默认属性 tips:这个列表按照优先级排序，也就是说，任何在高优先级属性源里设置的属性都会覆盖低优先级的相同属性。 10.application.properties和application.yml文件可放位置?优先级? 外置，在相对于应用程序运行目录的/config子目录里。 外置，在应用程序运行的目录里。 内置，在config包内。 内置，在Classpath根目录。 这个列表按照优先级排序,优先级高的会覆盖优先级低的。 当然我们可以自己指定文件的位置来加载配置文件。 java -jar xiaoka.jar ———spring.config.location=/home/application.yml 11.SpringBoot自动配置原理? @EnableAutoConfiguration (开启自动配置) 该注解引入了AutoConfigurationImportSelector，该类中的方法会扫描所有存在META-INF/spring.factories的jar包。 12.SpringBoot热部署方式？ spring-boot-devtools Spring Loaded Jrebel 模版热部署 13.bootstrap.yml 和application.yml? bootstrap.yml 优先于application.yml 14.SpringBoot如何修改端口号? yml中: server : port : 8888 properties: server.port = 8888 命令1: java -jar xiaoka.jar ——— server.port=8888 命令2: java - Dserver.port=8888 -jar xiaoka.jar 15.开启SpringBoot特性的几种方式? 继承spring-boot-starter-parent项目 导入spring-boot-dependencies项目依赖 16.SpringBoot如何兼容Spring项目? 在启动类加: @ImportResource(locations = {\"classpath:spring.xml\"}) 17.SpringBoot配置监控? org.springframework.boot spring-boot-starter-actuator 18.获得Bean装配报告信息访问哪个端点？ /beans 端点 19.关闭应用程序访问哪个端点? /shutdown 该端点默认是关闭的，如果开启，需要如下设置。 endpoints: shutdown: enabled: true 或者properties格式也是可以的。 20.查看发布应用信息访问哪个端点? /info 21.针对请求访问的几个组合注解？ @PatchMapping @PostMapping @GetMapping @PutMapping @DeleteMapping 22.SpringBoot 中的starter？ 可以理解成对依赖的一种合成，starter会把一个或一套功能相关依赖都包含进来，避免了自己去依赖费事，还有各种包的冲突问题。大大的提升了开发效率。 并且相关配置会有一个默认值，如果我们自己去配置，就会覆盖默认值。 23.SpringBoot集成Mybatis? mybatis-spring-boot-starter 24.什么是SpringProfiles? 一般来说我们从开发到生产，经过开发(dev)、测试（test）、上线(prod)。不同的时刻我们会用不同的配置。Spring Profiles 允许用户根据配置文件（dev，test，prod 等）来注册 bean。它们可以让我们自己选择什么时候用什么配置。 25.不同的环境的配置文件? 可以是 application-{profile}.properties/yml ，但默认是启动主配置文件application.properties,一般来说我们的不同环境配置如下。 application.properties：主配置文件 application-dev.properties：开发环境配置文件 application-test.properties：测试环境配置文件 application.prop-properties：生产环境配置文件 26.如何激活某个环境的配置？ 比如我们激活开发环境。 yml： spring: profiles: active: dev properties: spring.profiles.active=dev 命令行: java -jar xiaoka-v1.0.jar ———spring.profiles.active=dev 27.编写测试用例的注解？ @SpringBootTest 28.SpringBoot异常处理相关注解? @ControllerAdvice @ExceptionHandler 29.SpringBoot 1.x 和 2.x区别?······· SpringBoot 2基于Spring5和JDK8，Spring 1x用的是低版本。 配置变更，参数名等。 SpringBoot2相关的插件最低版本很多都比原来高 2.x配置中的中文可以直接读取，不用转码 Actuator的变化 CacheManager 的变化 30.SpringBoot读取配置相关注解有？ @PropertySource @Value @Environment @ConfigurationProperties 参考： 《SpringBoot实战（第4版）》 《Spring Boot编程思想》 《深入浅出Spring Boot 2.x》 https://spring.io/projects/spring-boot 百度百科 "},"zother1-JavaFaceNotes/SpringCloud.html":{"url":"zother1-JavaFaceNotes/SpringCloud.html","title":"Spring Cloud","keywords":"","body":"SpringCloud 1.什么是SpringCloud? Spring Cloud为开发人员提供了工具，以快速构建分布式系统中的一些常见模式（例如，配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁，领导选举，分布式会话，群集状态）。它们可以在任何分布式环境中正常工作，包括开发人员自己的笔记本电脑，裸机数据中心以及Cloud Foundry等托管平台。 2.什么是微服务？ 所谓的微服务是SOA架构下的最终产物，该架构的设计目标是为了肢解业务，使得服务能够独立运行。微服务设计原则： 1、各司其职 。 2、服务高可用和可扩展性。 3.SpringCloud有哪些特征? Spring Cloud专注于为典型的用例和可扩展性机制（包括其他用例）提供良好的开箱即用体验。 分布式/版本化配置 服务注册和发现 路由 服务到服务的调用 负载均衡 断路器 全局锁 领导选举和集群状态 分布式消息传递 4.SpringCloud核心组件? Eureka : 注册中心 Ribbon ：客服端负载均衡 Hystrix : 服务容错处理 Feign: 声明式REST客户端 Zuul : 服务网关 Config : 分布式配置 5.SpringCloud基于什么协议？ HTTP 6.SpringCloud和Dubbo区别? 7.Eureka是什么？ 云端服务发现，一个基于 REST 的服务，用于定位服务，以实现云端中间层服务发现和故障转移。 8.服务治理的基础角色？ 服务注册中心：提供服务注册与发现的能力。 服务提供者：提供服务的应用，会把自己提供的服务注册到注册中心。 服务消费者：服务的消费者，从注册中心获取服务列表。 9.什么是服务续约？ 在注册完服务以后，服务提供者会维护一个心跳来向注册中心证明自己还活着，以此防止被“剔除服务”。 10.什么是服务下线？ 当服务实例进行正常关闭时，会发送一个REST请求（我要下线了）给注册中心，收到请求后，将该服务状态设置下线（DOWN），并把这事件传播出去。 11.什么是失效剔除？ 当服务非正常下线时，可能服务注册中心没有收到下线请求，注册中心会创建一个定时任务（默认60s)将没有在固定时间(默认90s)内续约的服务剔除掉。 12.什么是自我保护机制？ 在运行期间，注册中心会统计心跳失败比例在15分钟之内是否低于85%,如果低于的情况，注册中心会将当前注册实例信息保护起来，不再删除这些实例信息，当网络恢复后，退出自我保护机制。 自我保护机制让服务集群更稳定、健壮。 13.Ribbon是什么? 提供云端负载均衡，有多种负载均衡策略可供选择，可配合服务发现和断路器使用。 14.Ribbon负载均衡的注解是? @LoadBalanced 15.Ribbon负载均衡策略有哪些？ RandomRule : 随机。 RoundRobinRule : 轮询。 RetryRule : 重试。 WeightedResponseTimeRule : 权重。 ClientConfigEnabledRoundRobinRule : 一般不用，通过继承该策略，默认的choose就实现了线性轮询机制。可以基于它来做扩展。 BestAvailableRule : 通过便利负载均衡器中维护的所有服务实例，会过滤到故障的，并选择并发请求最小的一个。 PredicateBasedRule : 先过滤清单，再轮询。 AvailabilityFilteringRule ：继承了父类的先过滤清单，再轮询。调整了算法。 ZoneAvoidanceRule : 该类也是PredicateBasedRule的子类，它可以组合过滤条件。以ZoneAvoidancePredicate为主过滤条件，以AvailabilityPredicate为次过滤条件。 16.什么是服务熔断？ 服务熔断的作用类似于我们家用的保险丝，当某服务出现不可用或响应超时的情况时，为了防止整个系统出现雪崩，暂时停止对该服务的调用。 17.什么是服务降级？ 服务降级是当服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心任务的正常运行。 18.什么是Hystrix? 熔断器，容错管理工具，旨在通过熔断机制控制服务和第三方库的节点,从而对延迟和故障提供更强大的容错能力。 19.断路器Hystrix的有哪些功能？ 通过第三方客户端访问依赖服务出现高延迟或者失败时,为系统提供保护和控制 。 在复杂的分布式系统中防止级联失败(服务雪崩效应) 。 快速失败 (Failfast) 同时能快速恢复。 提供失败回滚 (Fallback) 和优雅的服务降级机制。 提供近实时的监控、 报警和运维控制手段。 20.Hystrix将远程调用封装到？ HystrixCommand 或者 HystrixObservableCommand对象中。 21.启动熔断降级服务的注解？ @EnableHystrix 22.什么是Feign? Feign是一种声明式、模板化的HTTP客户端。 23.Feign优点？ 1.feign采用的是基于接口的注解。 2.feign整合了ribbon，具有负载均衡的能力。 3.整合了Hystrix，具有熔断的能力。 24.什么是Config? 配置管理工具包，让你可以把配置放到远程服务器，集中化管理集群配置，目前支持本地存储、Git以及Subversion。 25.Config组件中的两个角色? Config Server : 配置中心服务端。 Config Client : 配置中心客户端。 26.什么是Zuul? Zuul 是在云平台上提供动态路由,监控,弹性,安全等边缘服务的框架。Zuul 相当于是设备和 Netflix 流应用的 Web 网站后端所有请求的前门。 27.使用Zuul的优点? 方便监控。可以在微服务网管手机监控数据并将其推送到外部系统进行分析。 方便认证。可在网关进行统一认证，然后在讲请求转发到后端服务。 隐藏架构实现细节，提供统一的入口给客户端请求，减少了客户端和每个微服务的交互次数。 可以统一处理切面任务，避免每个微服务自己开发，提升效率。 高可用高伸缩性的服务，避免单点失效。 28.Zuul的核心是？ 过滤器。 29.Zuul有几种过滤器类型？分别是？ 4种。 pre : 可以在请求被路由之前调用。 适用于身份认证的场景，认证通过后再继续执行下面的流程。 route : 在路由请求时被调用。 适用于灰度发布场景，在将要路由的时候可以做一些自定义的逻辑。 post :在 route 和 error 过滤器之后被调用。 这种过滤器将请求路由到达具体的服务之后执行。适用于需要添加响应头，记录响应日志等应用场景。 error : 处理请求时发生错误时被调用。 在执行过程中发送错误时会进入 error 过滤器，可以用来统一记录错误信息。 30.什么是Sleuth? 日志收集工具包，封装了Dapper和log-based追踪以及Zipkin和HTrace操作，为SpringCloud应用实现了一种分布式追踪解决方案。 31.Sleuth帮助我们做了哪些工作？ 可以方便的了解到每个采样的请求耗时，分析出哪些服务调用比较耗时。 对于程序未捕捉的异常，可以在集成Zipkin服务页面上看到。 识别调用比较频繁的服务，从而进行优化。 32.什么是Bus? 事件、消息总线，用于在集群（例如，配置变化事件）中传播状态变化，可与Spring Cloud Config联合实现热部署。 33.eureka比zookeeper的优势在？ A:高可用 C:一致性，P:分区容错性 Zookeeper保证了CP，Eureka保证了AP。 Eureka可以很好的应对因网络故障导致部分节点失去联系的情况，而不会像Zookeeper那样使整个微服务瘫痪。 34.什么是Stream? 数据流操作开发包，封装了与Redis,Rabbit、Kafka等发送接收消息。 35.更多知识？ SpringCloud这个体系东西还是挺大的，小编会不断的丰富内容以及优化。欢迎大家关注我的公众号获得最新动态《Java小咖秀》。 参考： 《Spring Cloud微服务实战》 《Spring Cloud微服务全栈技术与案例解析》 《Spring Cloud微服务架构开发实战》 ​ https://spring.io/projects/spring-cloud ​ https://www.springcloud.cc/ ​ 百度百科 "},"zother1-JavaFaceNotes/多线程.html":{"url":"zother1-JavaFaceNotes/多线程.html","title":"多线程","keywords":"","body":"多线程 1.什么是进程? 进程是系统中正在运行的一个程序，程序一旦运行就是进程。 进程可以看成程序执行的一个实例。进程是系统资源分配的独立实体，每个进程都拥有独立的地址空间。一个进程无法访问另一个进程的变量和数据结构，如果想让一个进程访问另一个进程的资源，需要使用进程间通信，比如管道，文件，套接字等。 2.什么是线程？ 是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。 3.线程的实现方式? 1.继承Thread类 2.实现Runnable接口 3.使用Callable和Future 4.Thread 类中的start() 和 run() 方法有什么区别? 1.start（）方法来启动线程，真正实现了多线程运行。这时无需等待run方法体代码执行完毕，可以直接继续执行下面的代码；通过调用Thread类的start()方法来启动一个线程， 这时此线程是处于就绪状态， 并没有运行。 然后通过此Thread类调用方法run()来完成其运行操作的， 这里方法run()称为线程体，它包含了要执行的这个线程的内容， Run方法运行结束， 此线程终止。然后CPU再调度其它线程。 2.run（）方法当作普通方法的方式调用。程序还是要顺序执行，要等待run方法体执行完毕后，才可继续执行下面的代码； 程序中只有主线程——这一个线程， 其程序执行路径还是只有一条， 这样就没有达到写线程的目的。 5.线程NEW状态 new创建一个Thread对象时，并没处于执行状态，因为没有调用start方法启动改线程，那么此时的状态就是新建状态。 6.线程RUNNABLE状态 线程对象通过start方法进入runnable状态，启动的线程不一定会立即得到执行，线程的运行与否要看cpu的调度，我们把这个中间状态叫可执行状态（RUNNABLE)。 7.线程的RUNNING状态 一旦cpu通过轮询货其他方式从任务可以执行队列中选中了线程，此时它才能真正的执行自己的逻辑代码。 8.线程的BLOCKED状态 线程正在等待获取锁。 进入BLOCKED状态，比如调用了sleep,或者wait方法 进行某个阻塞的io操作，比如因网络数据的读写进入BLOCKED状态 获取某个锁资源，从而加入到该锁的阻塞队列中而进入BLOCKED状态 9.线程的TERMINATED状态 TERMINATED是一个线程的最终状态，在该状态下线程不会再切换到其他任何状态了，代表整个生命周期都结束了。 下面几种情况会进入TERMINATED状态: 线程运行正常结束，结束生命周期 线程运行出错意外结束 JVM Crash 导致所有的线程都结束 10.线程状态转化图 11.i——————与System.out.println()的异常 示例代码: public class XkThread extends Thread { private int i = 5; @Override public void run() { System.out.println(\"i=\" + (i——————) + \" threadName=\" + Thread.currentThread().getName()); } public static void main(String[] args) { XkThread xk = new XkThread(); Thread t1 = new Thread(xk); Thread t2 = new Thread(xk); Thread t3 = new Thread(xk); Thread t4 = new Thread(xk); Thread t5 = new Thread(xk); t1.start(); t2.start(); t3.start(); t4.start(); t5.start(); } } 结果: i=5 threadName=Thread-1 i=2 threadName=Thread-5 i=5 threadName=Thread-2 i=4 threadName=Thread-3 i=3 threadName=Thread-4 虽然println()方法在内部是同步的，但i——————的操作却是在进入println()之前发生的，所以有发生非线程安全的概率。 println()源码: public void println(String x) { synchronized (this) { print(x); newLine(); } } 12.如何知道代码段被哪个线程调用？ System.out.println(Thread.currentThread().getName()); 13.线程活动状态？ public class XKThread extends Thread { @Override public void run() { System.out.println(\"run run run is \" + this.isAlive() ); } public static void main(String[] args) { XKThread xk = new XKThread(); System.out.println(\"begin ——— \" + xk.isAlive()); xk.start(); System.out.println(\"end ————— \" + xk.isAlive()); } } 14.sleep()方法 方法sleep()的作用是在指定的毫秒数内让当前的“正在执行的线程”休眠（暂停执行）。 15.如何优雅的设置睡眠时间? jdk1.5 后，引入了一个枚举TimeUnit,对sleep方法提供了很好的封装。 比如要表达2小时22分55秒899毫秒。 Thread.sleep(8575899L); TimeUnit.HOURS.sleep(3); TimeUnit.MINUTES.sleep(22); TimeUnit.SECONDS.sleep(55); TimeUnit.MILLISECONDS.sleep(899); 可以看到表达的含义更清晰，更优雅。 16.停止线程 run方法执行完成，自然终止。 stop()方法，suspend()以及resume()都是过期作废方法，使用它们结果不可预期。 大多数停止一个线程的操作使用Thread.interrupt()等于说给线程打一个停止的标记, 此方法不回去终止一个正在运行的线程，需要加入一个判断才能可以完成线程的停止。 17.interrupted 和 isInterrupted interrupted : 判断当前线程是否已经中断,会清除状态。 isInterrupted ：判断线程是否已经中断，不会清除状态。 18.yield 放弃当前cpu资源，将它让给其他的任务占用cpu执行时间。但放弃的时间不确定，有可能刚刚放弃，马上又获得cpu时间片。 测试代码:(cpu独占时间片) public class XKThread extends Thread { @Override public void run() { long beginTime = System.currentTimeMillis(); int count = 0; for (int i = 0; i 结果： 用时 = 20 毫秒! 加入yield，再来测试。(cpu让给其他资源导致速度变慢) public class XKThread extends Thread { @Override public void run() { long beginTime = System.currentTimeMillis(); int count = 0; for (int i = 0; i 结果: 用时 = 38424 毫秒! 19.线程的优先级 在操作系统中，线程可以划分优先级，优先级较高的线程得到cpu资源比较多，也就是cpu有限执行优先级较高的线程对象中的任务，但是不能保证一定优先级高，就先执行。 Java的优先级分为1～10个等级，数字越大优先级越高，默认优先级大小为5。超出范围则抛出：java.lang.IllegalArgumentException。 20.优先级继承特性 线程的优先级具有继承性，比如a线程启动b线程，b线程与a优先级是一样的。 21.谁跑的更快？ 设置优先级高低两个线程，累加数字，看谁跑的快，上代码。 public class Run extends Thread{ public static void main(String[] args) { try { ThreadLow low = new ThreadLow(); low.setPriority(2); low.start(); ThreadHigh high = new ThreadHigh(); high.setPriority(8); high.start(); Thread.sleep(2000); low.stop(); high.stop(); System.out.println(\"low = \" + low.getCount()); System.out.println(\"high = \" + high.getCount()); } catch (InterruptedException e) { e.printStackTrace(); } } } class ThreadHigh extends Thread { private int count = 0; public int getCount() { return count; } @Override public void run() { while (true) { count++; } } } class ThreadLow extends Thread { private int count = 0; public int getCount() { return count; } @Override public void run() { while (true) { count++; } } } 结果: low = 1193854568 high = 1204372373 22.线程种类 Java线程有两种，一种是用户线程，一种是守护线程。 23.守护线程的特点 守护线程是一个比较特殊的线程，主要被用做程序中后台调度以及支持性工作。当Java虚拟机中不存在非守护线程时，守护线程才会随着JVM一同结束工作。 24.Java中典型的守护线程 GC（垃圾回收器） 25.如何设置守护线程 Thread.setDaemon(true) PS:Daemon属性需要再启动线程之前设置，不能再启动后设置。 25.Java虚拟机退出时Daemon线程中的finally块一定会执行？ Java虚拟机退出时Daemon线程中的finally块并不一定会执行。 代码示例: public class XKDaemon { public static void main(String[] args) { Thread thread = new Thread(new DaemonRunner(),\"xkDaemonRunner\"); thread.setDaemon(true); thread.start(); } static class DaemonRunner implements Runnable { @Override public void run() { try { SleepUtils.sleep(10); } finally { System.out.println(\"Java小咖秀 daemonThread finally run …\"); } } } } 结果： 没有任何的输出，说明没有执行finally。 26.设置线程上下文类加载器 ​ 获取线程上下文类加载器 public ClassLoader getContextClassLoader() ​ 设置线程类加载器（可以打破Java类加载器的父类委托机制） public void setContextClassLoader(ClassLoader cl) 27.join join是指把指定的线程加入到当前线程，比如join某个线程a,会让当前线程b进入等待,直到a的生命周期结束，此期间b线程是处于blocked状态。 28.什么是synchronized? synchronized关键字可以时间一个简单的策略来防止线程干扰和内存一致性错误，如果一个对象是对多个线程可见的，那么对该对想的所有读写都将通过同步的方式来进行。 29.synchronized包括哪两个jvm重要的指令？ monitor enter 和 monitor exit 30.synchronized关键字用法? 可以用于对代码块或方法的修饰 31.synchronized锁的是什么? 普通同步方法 —————> 锁的是当前实力对象。 静态同步方法—————> 锁的是当前类的Class对象。 同步方法快 —————> 锁的是synchonized括号里配置的对象。 32.Java对象头 synchronized用的锁是存在Java对象头里的。对象如果是数组类型，虚拟机用3个字宽(Word)存储对象头，如果对象是非数组类型，用2字宽存储对象头。 Tips:32位虚拟机中一个字宽等于4字节。 33.Java对象头长度 34.Java对象头的存储结构 32位JVM的Mark Word 默认存储结构 35.Mark Word的状态变化 Mark Word 存储的数据会随着锁标志为的变化而变化。 64位虚拟机下，Mark Word是64bit大小的 36.锁的升降级规则 Java SE 1.6 为了提高锁的性能。引入了“偏向锁”和轻量级锁“。 Java SE 1.6 中锁有4种状态。级别从低到高依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态。 锁只能升级不能降级。 37.偏向锁 大多数情况，锁不仅不存在多线程竞争，而且总由同一线程多次获得。当一个线程访问同步块并获取锁时，会在对象头和栈帧中记录存储锁偏向的线程ID,以后该线程在进入和退出同步块时不需要进行 cas操作来加锁和解锁，只需测试一下对象头 Mark Word里是否存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁，如果失败，则需要测试下Mark Word中偏向锁的标示是否已经设置成1（表示当前时偏向锁),如果没有设置，则使用cas竞争锁，如果设置了，则尝试使用cas将对象头的偏向锁只想当前线程。 38.关闭偏向锁延迟 java6和7中默认启用，但是会在程序启动几秒后才激活，如果需要关闭延迟， -XX:BiasedLockingStartupDelay=0。 39.如何关闭偏向锁 JVM参数关闭偏向锁:-XX:-UseBiasedLocking=false,那么程序默认会进入轻量级锁状态。 Tips:如果你可以确定程序的所有锁通常情况处于竞态，则可以选择关闭。 40.轻量级锁 线程在执行同步块，jvm会现在当前线程的栈帧中创建用于储存锁记录的空间。并将对象头中的Mark Word复制到锁记录中。然后线程尝试使用cas将对象头中的Mark Word替换为之乡锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。 41.轻量锁的解锁 轻量锁解锁时，会使原子操作cas将 displaced Mark Word 替换回对象头，如果成功则表示没有竞争发生，如果失败，表示存在竞争，此时锁就会膨胀为重量级锁。 42.锁的优缺点对比 43.什么是原子操作 不可被中断的一个或一系列操作 44.Java如何实现原子操作 Java中通过锁和循环cas的方式来实现原子操作，JVM的CAS操作利用了处理器提供的CMPXCHG指令来实现的。自旋CAS实现的基本思路就是循环进行CAS操作直到成功为止。 45.CAS实现原子操作的3大问题 ABA问题，循环时间长消耗资源大，只能保证一个共享变量的原子操作 46.什么是ABA问题 问题： 因为cas需要在操作值的时候，检查值有没有变化，如果没有变化则更新，如果一个值原来是A,变成了B,又变成了A,那么使用cas进行检测时会发现发的值没有发生变化，其实是变过的。 解决： 添加版本号，每次更新的时候追加版本号，A-B-A —> 1A-2B-3A。 从jdk1.5开始,Atomic包提供了一个类AtomicStampedReference来解决ABA的问题。 47.CAS循环时间长占用资源大问题 如果jvm能支持处理器提供的pause指令，那么效率会有一定的提升。 一、它可以延迟流水线执行指令(de-pipeline),使cpu不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，有些处理器延迟时间是0。 二、它可以避免在退出循环的时候因内存顺序冲突而引起的cpu流水线被清空，从而提高cpu执行效率。 48.CAS只能保证一个共享变量原子操作 一、对多个共享变量操作时，可以用锁。 二、可以把多个共享变量合并成一个共享变量来操作。比如,x=1,k=a,合并xk=1a，然后用cas操作xk。 Tips:java 1.5开始,jdk提供了AtomicReference类来保证饮用对象之间的原子性，就可以把多个变量放在一个对象来进行cas操作。 49.volatile关键字 volatile 是轻量级的synchronized,它在多处理器开发中保证了共享变量的“可见性“。 Java语言规范第3版对volatile定义如下，Java允许线程访问共享变量，为了保证共享变量能准确和一致的更新，线程应该确保排它锁单独获得这个变量。如果一个字段被声明为volatile,Java线程内存模型所有线程看到这个变量的值是一致的。 50.等待/通知机制 一个线程修改了一个对象的值，而另一个线程感知到了变化，然后进行相应的操作。 51.wait 方法wait()的作用是使当前执行代码的线程进行等待，wait()是Object类通用的方法，该方法用来将当前线程置入“预执行队列”中，并在 wait()所在的代码处停止执行，直到接到通知或中断为止。 在调用wait之前线程需要获得该对象的对象级别的锁。代码体现上，即只能是同步方法或同步代码块内。调用wait()后当前线程释放锁。 52.notify notify()也是Object类的通用方法，也要在同步方法或同步代码块内调用，该方法用来通知哪些可能灯光该对象的对象锁的其他线程，如果有多个线程等待，则随机挑选出其中一个呈wait状态的线程，对其发出 通知 notify，并让它等待获取该对象的对象锁。 53.notify/notifyAll notify等于说将等待队列中的一个线程移动到同步队列中，而notifyAll是将等待队列中的所有线程全部移动到同步队列中。 54.等待/通知经典范式 等待 synchronized(obj) { while(条件不满足) { obj.wait(); } 执行对应逻辑 } 通知 synchronized(obj) { 改变条件 obj.notifyAll(); } 55.ThreadLocal 主要解决每一个线程想绑定自己的值，存放线程的私有数据。 56.ThreadLocal使用 获取当前的线程的值通过get(),设置set(T) 方式来设置值。 public class XKThreadLocal { public static ThreadLocal threadLocal = new ThreadLocal(); public static void main(String[] args) { if (threadLocal.get() == null) { System.out.println(\"未设置过值\"); threadLocal.set(\"Java小咖秀\"); } System.out.println(threadLocal.get()); } } 输出: 未设置过值 Java小咖秀 Tips:默认值为null 57.解决get()返回null问题 通过继承重写initialValue()方法即可。 代码实现： public class ThreadLocalExt extends ThreadLocal{ static ThreadLocalExt threadLocalExt = new ThreadLocalExt(); @Override protected Object initialValue() { return \"Java小咖秀\"; } public static void main(String[] args) { System.out.println(threadLocalExt.get()); } } 输出结果: Java小咖秀 58.Lock接口 锁可以防止多个线程同时共享资源。Java5前程序是靠synchronized实现锁功能。Java5之后，并发包新增Lock接口来实现锁功能。 59.Lock接口提供 synchronized不具备的主要特性 60.重入锁 ReentrantLock 支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。除此之外，该锁的还支持获取锁时的公平和非公平性选择。 61.重进入是什么意思？ 重进入是指任意线程在获取到锁之后能够再次获锁而不被锁阻塞。 该特性主要解决以下两个问题： 一、锁需要去识别获取锁的线程是否为当前占据锁的线程，如果是则再次成功获取。 二、所得最终释放。线程重复n次是获取了锁，随后在第n次释放该锁后，其他线程能够获取到该锁。 62.ReentrantLock默认锁？ 默认非公平锁 代码为证: final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc 63.公平锁和非公平锁的区别 公平性与否针对获取锁来说的，如果一个锁是公平的，那么锁的获取顺序就应该符合请求的绝对时间顺序，也就是FIFO。 64.读写锁 读写锁允许同一时刻多个读线程访问，但是写线程和其他写线程均被阻塞。读写锁维护一个读锁一个写锁，读写分离，并发性得到了提升。 Java中提供读写锁的实现类是ReentrantReadWriteLock。 65.LockSupport工具 定义了一组公共静态方法，提供了最基本的线程阻塞和唤醒功能。 66.Condition接口 提供了类似Object监视器方法，与 Lock配合使用实现等待/通知模式。 67.Condition使用 代码示例: public class XKCondition { Lock lock = new ReentrantLock(); Condition cd = lock.newCondition(); public void await() throws InterruptedException { lock.lock(); try { cd.await();//相当于Object 方法中的wait() } finally { lock.unlock(); } } public void signal() { lock.lock(); try { cd.signal(); //相当于Object 方法中的notify() } finally { lock.unlock(); } } } 68.ArrayBlockingQueue? 一个由数据支持的有界阻塞队列，此队列FIFO原则对元素进行排序。队列头部在队列中存在的时间最长，队列尾部存在时间最短。 69.PriorityBlockingQueue? 一个支持优先级排序的无界阻塞队列，但它不会阻塞数据生产者，而只会在没有可消费的数据时，阻塞数据的消费者。 70.DelayQueue? 是一个支持延时获取元素的使用优先级队列的实现的无界阻塞队列。队列中的元素必须实现Delayed接口和 Comparable接口，在创建元素时可以指定多久才能从队列中获取当前元素。 71.Java并发容器，你知道几个？ ConcurrentHashMap、CopyOnWriteArrayList 、CopyOnWriteArraySet 、ConcurrentLinkedQueue、 ConcurrentLinkedDeque、ConcurrentSkipListMap、ConcurrentSkipListSet、ArrayBlockingQueue、 LinkedBlockingQueue、LinkedBlockingDeque、PriorityBlockingQueue、SynchronousQueue、 LinkedTransferQueue、DelayQueue 72.ConcurrentHashMap 并发安全版HashMap,java7中采用分段锁技术来提高并发效率，默认分16段。Java8放弃了分段锁，采用CAS，同时当哈希冲突时，当链表的长度到8时，会转化成红黑树。（如需了解细节，见jdk中代码） 73.ConcurrentLinkedQueue 基于链接节点的无界线程安全队列，它采用先进先出的规则对节点进行排序，当我们添加一个元素的时候，它会添加到队列的尾部，当我们获取一个元素时，它会返回队列头部的元素。它采用cas算法来实现。（如需了解细节，见jdk中代码） 74.什么是阻塞队列？ 阻塞队列是一个支持两个附加操作的队列，这两个附加操作支持阻塞的插入和移除方法。 1、支持阻塞的插入方法：当队列满时，队列会阻塞插入元素的线程，直到队列不满。 2、支持阻塞的移除方法：当队列空时，获取元素的线程会等待队列变为非空。 75.阻塞队列常用的应用场景？ 常用于生产者和消费者场景，生产者是往队列里添加元素的线程，消费者是从队列里取元素的线程。阻塞队列正好是生产者存放、消费者来获取的容器。 76.Java里的阻塞的队列 ArrayBlockingQueue： 数组结构组成的 |有界阻塞队列 LinkedBlockingQueue： 链表结构组成的|有界阻塞队列 PriorityBlockingQueue: 支持优先级排序|无界阻塞队列 DelayQueue： 优先级队列实现|无界阻塞队列 SynchronousQueue： 不存储元素| 阻塞队列 LinkedTransferQueue： 链表结构组成|无界阻塞队列 LinkedBlockingDeque： 链表结构组成|双向阻塞队列 77.Fork/Join java7提供的一个用于并行执行任务的框架，把一个大任务分割成若干个小任务，最终汇总每个小任务结果的后得到大任务结果的框架。 78.工作窃取算法 是指某个线程从其他队列里窃取任务来执行。当大任务被分割成小任务时，有的线程可能提前完成任务，此时闲着不如去帮其他没完成工作线程。此时可以去其他队列窃取任务，为了减少竞争，通常使用双端队列，被窃取的线程从头部拿，窃取的线程从尾部拿任务执行。 79.工作窃取算法的有缺点 优点：充分利用线程进行并行计算，减少了线程间的竞争。 缺点：有些情况下还是存在竞争，比如双端队列中只有一个任务。这样就消耗了更多资源。 80.Java中原子操作更新基本类型，Atomic包提供了哪几个类? AtomicBoolean:原子更新布尔类型 AtomicInteger:原子更新整形 AtomicLong:原子更新长整形 81.Java中原子操作更新数组，Atomic包提供了哪几个类? AtomicIntegerArray: 原子更新整形数据里的元素 AtomicLongArray: 原子更新长整形数组里的元素 AtomicReferenceArray: 原子更新饮用类型数组里的元素 AtomicIntegerArray: 主要提供原子方式更新数组里的整形 82.Java中原子操作更新引用类型，Atomic包提供了哪几个类? 如果原子需要更新多个变量，就需要用引用类型了。 AtomicReference : 原子更新引用类型 AtomicReferenceFieldUpdater: 原子更新引用类型里的字段。 AtomicMarkableReference: 原子更新带有标记位的引用类型。标记位用boolean类型表示，构造方法时AtomicMarkableReference(V initialRef,boolean initialMark) 83.Java中原子操作更新字段类，Atomic包提供了哪几个类? AtomiceIntegerFieldUpdater: 原子更新整形字段的更新器 AtomiceLongFieldUpdater: 原子更新长整形字段的更新器 AtomiceStampedFieldUpdater: 原子更新带有版本号的引用类型，将整数值 84.JDK并发包中提供了哪几个比较常见的处理并发的工具类？ 提供并发控制手段: CountDownLatch、CyclicBarrier、Semaphore 线程间数据交换: Exchanger 85.CountDownLatch 允许一个或多个线程等待其他线程完成操作。 CountDownLatch的构造函数接受一个int类型的参数作为计数器，你想等待n个点完成，就传入n。 两个重要的方法: countDown() : 调用时，n会减1。 await() : 调用会阻塞当前线程，直到n变成0。 await(long time,TimeUnit unit) : 等待特定时间后，就不会继续阻塞当前线程。 tips:计数器必须大于等于0，当为0时，await就不会阻塞当前线程。 不提供重新初始化或修改内部计数器的值的功能。 86.CyclicBarrier 可循环使用的屏障。 让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续运行。 CyclicBarrier默认构造放时CyclicBarrier(int parities) ,其参数表示屏障拦截的线程数量，每个线程调用await方法告诉CyclicBarrier我已经到达屏障，然后当前线程被阻塞。 87.CountDownLatch与CyclicBarrier区别 CountDownLatch： 计数器：计数器只能使用一次。 等待： 一个线程或多个等待另外n个线程完成之后才能执行。 CyclicBarrier： 计数器：计数器可以重置（通过reset()方法)。 等待： n个线程相互等待，任何一个线程完成之前，所有的线程都必须等待。 88.Semaphore 用来控制同时访问资源的线程数量，通过协调各个线程，来保证合理的公共资源的访问。 应用场景：流量控制，特别是公共资源有限的应用场景，比如数据链接，限流等。 89.Exchanger Exchanger是一个用于线程间协作的工具类，它提供一个同步点，在这个同步点上，两个线程可以交换彼此的数据。比如第一个线程执行exchange()方法，它会一直等待第二个线程也执行exchange，当两个线程都到同步点，就可以交换数据了。 一般来说为了避免一直等待的情况，可以使用exchange(V x,long timeout,TimeUnit unit),设置最大等待时间。 Exchanger可以用于遗传算法。 90.为什么使用线程池 几乎所有需要异步或者并发执行任务的程序都可以使用线程池。合理使用会给我们带来以下好处。 降低系统消耗：重复利用已经创建的线程降低线程创建和销毁造成的资源消耗。 提高响应速度： 当任务到达时，任务不需要等到线程创建就可以立即执行。 提供线程可以管理性： 可以通过设置合理分配、调优、监控。 91.线程池工作流程 1、判断核心线程池里的线程是否都有在执行任务，否->创建一个新工作线程来执行任务。是->走下个流程。 2、判断工作队列是否已满，否->新任务存储在这个工作队列里，是->走下个流程。 3、判断线程池里的线程是否都在工作状态，否->创建一个新的工作线程来执行任务， 是->走下个流程。 4、按照设置的策略来处理无法执行的任务。 92.创建线程池参数有哪些，作用？ public ThreadPoolExecutor( int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 1.corePoolSize:核心线程池大小，当提交一个任务时，线程池会创建一个线程来执行任务，即使其他空闲的核心线程能够执行新任务也会创建，等待需要执行的任务数大于线程核心大小就不会继续创建。 2.maximumPoolSize:线程池最大数，允许创建的最大线程数，如果队列满了，并且已经创建的线程数小于最大线程数，则会创建新的线程执行任务。如果是无界队列，这个参数基本没用。 3.keepAliveTime: 线程保持活动时间，线程池工作线程空闲后，保持存活的时间，所以如果任务很多，并且每个任务执行时间较短，可以调大时间，提高线程利用率。 4.unit: 线程保持活动时间单位，天（DAYS)、小时(HOURS)、分钟(MINUTES、毫秒MILLISECONDS)、微秒(MICROSECONDS)、纳秒(NANOSECONDS) 5.workQueue: 任务队列，保存等待执行的任务的阻塞队列。 一般来说可以选择如下阻塞队列： ArrayBlockingQueue:基于数组的有界阻塞队列。 LinkedBlockingQueue:基于链表的阻塞队列。 SynchronizedQueue:一个不存储元素的阻塞队列。 PriorityBlockingQueue:一个具有优先级的阻塞队列。 6.threadFactory：设置创建线程的工厂，可以通过线程工厂给每个创建出来的线程设置更有意义的名字。 handler: 饱和策略也叫拒绝策略。当队列和线程池都满了，即达到饱和状态。所以需要采取策略来处理新的任务。默认策略是AbortPolicy。 AbortPolicy:直接抛出异常。 CallerRunsPolicy: 调用者所在的线程来运行任务。 DiscardOldestPolicy:丢弃队列里最近的一个任务，并执行当前任务。 DiscardPolicy:不处理，直接丢掉。 当然可以根据自己的应用场景，实现RejectedExecutionHandler接口自定义策略。 93.向线程池提交任务 可以使用execute()和submit() 两种方式提交任务。 execute():无返回值，所以无法判断任务是否被执行成功。 submit():用于提交需要有返回值的任务。线程池返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future的get()来获取返回值，get()方法会阻塞当前线程知道任务完成。get(long timeout,TimeUnit unit)可以设置超市时间。 94.关闭线程池 可以通过shutdown()或shutdownNow()来关闭线程池。它们的原理是遍历线程池中的工作线程，然后逐个调用线程的interrupt来中断线程，所以无法响应终端的任务可以能永远无法停止。 shutdownNow首先将线程池状态设置成STOP,然后尝试停止所有的正在执行或者暂停的线程，并返回等待执行任务的列表。 shutdown只是将线程池的状态设置成shutdown状态，然后中断所有没有正在执行任务的线程。 只要调用两者之一，isShutdown就会返回true,当所有任务都已关闭，isTerminaed就会返回true。 一般来说调用shutdown方法来关闭线程池，如果任务不一定要执行完，可以直接调用shutdownNow方法。 95.线程池如何合理设置 配置线程池可以从以下几个方面考虑。 任务是cpu密集型、IO密集型或者混合型 任务优先级，高中低。 任务时间执行长短。 任务依赖性：是否依赖其他系统资源。 cpu密集型可以配置可能小的线程,比如 n + 1个线程。 io密集型可以配置较多的线程，如 2n个线程。 混合型可以拆成io密集型任务和cpu密集型任务， 如果两个任务执行时间相差大，否->分解后执行吞吐量将高于串行执行吞吐量。 否->没必要分解。 可以通过Runtime.getRuntime().availableProcessors()来获取cpu个数。 建议使用有界队列，增加系统的预警能力和稳定性。 96.Executor 从JDK5开始，把工作单元和执行机制分开。工作单元包括Runnable和Callable,而执行机制由Executor框架提供。 97.Executor框架的主要成员 ThreadPoolExecutor :可以通过工厂类Executors来创建。 可以创建3种类型的ThreadPoolExecutor：SingleThreadExecutor、FixedThreadPool、CachedThreadPool。 ScheduledThreadPoolExecutor ：可以通过工厂类Executors来创建。 可以创建2中类型的ScheduledThreadPoolExecutor：ScheduledThreadPoolExecutor、SingleThreadScheduledExecutor Future接口:Future和实现Future接口的FutureTask类来表示异步计算的结果。 Runnable和Callable:它们的接口实现类都可以被ThreadPoolExecutor或ScheduledThreadPoolExecutor执行。Runnable不能返回结果，Callable可以返回结果。 98.FixedThreadPool 可重用固定线程数的线程池。 查看源码： public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue());} corePoolSize 和maxPoolSize都被设置成我们设置的nThreads。 当线程池中的线程数大于corePoolSize ,keepAliveTime为多余的空闲线程等待新任务的最长时间，超过这个时间后多余的线程将被终止，如果设为0，表示多余的空闲线程会立即终止。 工作流程： 1.当前线程少于corePoolSize,创建新线程执行任务。 2.当前运行线程等于corePoolSize,将任务加入LinkedBlockingQueue。 3.线程执行完1中的任务，会循环反复从LinkedBlockingQueue获取任务来执行。 LinkedBlockingQueue作为线程池工作队列（默认容量Integer.MAX_VALUE)。因此可能会造成如下赢下。 1.当线程数等于corePoolSize时，新任务将在队列中等待，因为线程池中的线程不会超过corePoolSize。 2.maxnumPoolSize等于说是一个无效参数。 3.keepAliveTime等于说也是一个无效参数。 4.运行中的FixedThreadPool(未执行shundown或shundownNow))则不会调用拒绝策略。 5.由于任务可以不停的加到队列，当任务越来越多时很容易造成OOM。 99.SingleThreadExecutor 是使用单个worker线程的Executor。 查看源码： public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue())); } corePoolSize和maxnumPoolSize被设置为1。其他参数和FixedThreadPool相同。 执行流程以及造成的影响同FixedThreadPool. 100.CachedThreadPool 根据需要创建新线程的线程池。 查看源码： public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue()); corePoolSize设置为0，maxmumPoolSize为Integer.MAX_VALUE。keepAliveTime为60秒。 工作流程： 1.首先执行SynchronousQueue.offer (Runnable task)。如果当前maximumPool 中有空闲线程正在执行S ynchronousQueue.poll(keepAliveTIme,TimeUnit.NANOSECONDS)，那么主线程执行offer操作与空闲线程执行的poll操作配对成功，主线程把任务交给空闲线程执行,execute方 法执行完成;否则执行下面的步骤2。 当初始maximumPool为空或者maximumPool中当前没有空闲线程时，将没有线程执行 SynchronousQueue.poll (keepAliveTime，TimeUnit.NANOSECONDS)。这种情况下，步骤 1将失 败。此时CachedThreadPool会创建一个新线程执行任务，execute()方法执行完成。 3.在步骤2中新创建的线程将任务执行完后，会执行SynchronousQueue.poll (keepAliveTime，TimeUnit.NANOSECONDS)。这个poll操作会让空闲线程最多在SynchronousQueue中等待60秒钟。如果60秒钟内主线程提交了一个新任务(主线程执行步骤1)，那么这个空闲线程将执行主线程提交的新任务;否则，这个空闲线程将终止。由于空闲60秒的空闲线程会被终止,因此长时间保持空闲的CachedThreadPool不会使用任何资源。 一般来说它适合处理时间短、大量的任务。 参考： 《Java多线程编程核心技术》 《Java高并发编程详解》 《Java 并发编程的艺术》 "},"zother1-JavaFaceNotes/异常&反射.html":{"url":"zother1-JavaFaceNotes/异常&反射.html","title":"异常&反射","keywords":"","body":"异常&反射 1.error和exception有什么区别？ error表示系统级的错误，是java运行环境内部错误或者硬件问题，不能指望程序来处理这样的问题，除了退出运行外别无选择，它是Java虚拟机抛出的。 exception 表示程序需要捕捉、需要处理的异常，是由与程序设计的不完善而出现的问题，程序必须处理的问题。 2.说出5个常见的RuntimeException？ (1)Java.lang.NullPointerException 空指针异常;出现原因：调用了未经初始化的对象或者是不存在的对象。 (2)Java.lang.NumberFormatException 字符串转换为数字异常;出现原因：字符型数据中包含非数字型字符。 (3)Java.lang.IndexOutOfBoundsException 数组角标越界异常，常见于操作数组对象时发生。 (4)Java.lang.IllegalArgumentException 方法传递参数错误。 (5)Java.lang.ClassCastException 数据类型转换异常。 3.throw和throws的区别? throw： 　　(1)throw 语句用在方法体内，表示抛出异常，由方法体内的语句处理。 　　(2)throw 是具体向外抛出异常的动作，所以它抛出的是一个异常实例，执行 throw 一定是抛出了某种异常。 throws： ​ (1)@throws 语句是用在方法声明后面，表示如果抛出异常，由该方法的调用者来进行异常的处理。 ​ (2)throws 主要是声明这个方法会抛出某种类型的异常，让它的使用者要知道需要捕获的异常的类型。 ​ (3)throws 表示出现异常的一种可能性，并不一定会发生这种异常。 4.Java中异常分类 按照异常处理时机: 编译时异常(受控异常(CheckedException))和运行时异常(非受控异常(UnCheckedException)) 5.如何自定义异常 继承Exception是检查性异常，继承RuntimeException是非检查性异常，一般要复写两个构造方法，用throw抛出新异常 如果同时有很多异常抛出，那可能就是异常链，就是一个异常引发另一个异常，另一个异常引发更多异常，一般我们会找它的原始异常来解决问题，一般会在开头或结尾，异常可通过initCause串起来，可以通过自定义异常 6.Java中异常处理 首先处理异常主要有两种方式:一种try catch，一种是throws。 try catch: try{} 中放入可能发生异常的代码。catch{}中放入对捕获到异常之后的处理。 2.throw throws： throw是语句抛出异常，出现于函数内部，用来抛出一个具体异常实例，throw被执行后面的语句不起作用，直接转入异常处理阶段。 throws是函数方法抛出异常，一般写在方法的头部，抛出异常，给方法的调用者进行解决。 7.什么是Java反射机制? Java的反射（reflection）机制是指在程序的运行状态中，可以构造任意一个类的对象，可以了解任意一个对象所属的类，可以了解任意一个类的成员变量和方法，可以调用任意一个对象的属性和方法。这种动态获取程序信息以及动态调用对象的功能称为Java语言的反射机制。反射被视为动态语言的关键。 8.举例什么地方用到反射机制？ JDBC中，利用反射动态加载了数据库驱动程序。 Web服务器中利用反射调用了Sevlet的服务方法。 Eclispe等开发工具利用反射动态刨析对象的类型与结构，动态提示对象的属性和方法。 很多框架都用到反射机制，注入属性，调用方法，如Spring。 9.java反射机制的作用 在运行时判定任意一个对象所属的类 在运行时构造任意一个类的对象； 在运行时判定任意一个类所具有的成员变量和方法； 在运行时调用任意一个对象的方法； 生成动态代理； 10.Java反射机制类 java.lang.Class; //类 java.lang.reflect.Constructor;//构造方法 java.lang.reflect.Field; //类的成员变量 java.lang.reflect.Method;//类的方法 java.lang.reflect.Modifier;//访问权限 11.反射机制优缺点？ 优点：运行期类型的判断，动态加载类，提高代码灵活度。 缺点：性能瓶颈：反射相当于一系列解释操作，通知 JVM 要做的事情，性能比直接的java代码要慢很多。 12.利用反射创建对象? 1.通过一个全限类名创建一个对象 Class.forName(“全限类名”); 例如：com.mysql.jdbc.Driver Driver类已经被加载到 jvm中，并且完成了类的初始化工作就行了 类名.class; 获取Class clz 对象 对象.getClass(); 2.获取构造器对象，通过构造器new出一个对象 Clazz.getConstructor([String.class]); Con.newInstance([参数]); 3.通过class对象创建一个实例对象（就相当与new类名（）无参构造器) Cls.newInstance(); 参考： https://blog.csdn.net/qq_37875585/article/details/89340495 https://www.cnblogs.com/whoislcj/p/6038511.html "},"zother1-JavaFaceNotes/简历.html":{"url":"zother1-JavaFaceNotes/简历.html","title":"简历","keywords":"","body":"简历 几个制作简历不错的网站： https://mp.weixin.qq.com/s/z0k922U6jwXe5VYlz33gaA 原文链接:https://www.zhihu.com/question/25002833 ThoughtWorks中国回答 大家伙让一让，这个问题让老司机先答！作为一个潜入IT圈五年之久、看过数万份简历的HR，在这个问题上还是有点发言权的。HR在筛选简历时主要从公司需求出发，重点不一，不过还是有很多“通用”的套路，为了在30秒内判断出这份简历是否值得跟进，我认为程序员写简历的正确姿势是这样的： 一、基本格调 即打开简历之后的第一印象。就好比我们看见一个人，会有一个整体的感觉，他是fashion的、小清新的还是老道的？有了第一印象之后再慢慢分解来看。 加分写法： 简洁明了，逻辑结构清晰。 字体，排版，顺畅，清晰整齐就好。 最好是PDF格式，兼容性强且不易乱序。 减分写法： 设计的过于浮夸或者过于简单的。（eg.有的简历五颜六色、非常酷炫，却半天找不到联系方式，抑或是只有个人基本信息和公司名称) 写了十几页，半天打不开的，或者加载了半天，打开还乱码。 二、基本信息（姓名/性别/毕业院校/电话/邮箱/居住地/期望地） 加分写法: 清晰罗列出以上信息，这样HR就不用在接下来的电话沟通或面试中再去追问这些内容，建立我们接下来电话沟通对你的熟悉度。 再额外能加上QQ或者微信就更好了（以防有时候电话打不通哦，时不时会遇到这种情况） 减分写法： 大部分的基本信息没有写 甩给我一个Github链接，极致简洁的几句描述，需要通过你的链接来找你的联系方式。（如果不是博客写的特别好，基本是要放弃你了） 三、工作经历&项目经历 加分写法： 工作经历项目经历可参照万能的STAR法则来写，STAR不清楚的童鞋点这里啦 效力过哪些公司，我们匹配的公司？ BAT？ 知名大型互联网公司？ 做过什么行业领域，和我们目前的行业是否匹配 擅长的技术语言，应用了哪些技术栈，（Java, Scala，Ruby, React, Vue, Microservice…） 经历的项目复杂度，及在项目中承担什么样的角色(人的变化/技术的变化/环境的变化/不同工作经历相同角色的不同点) 时间节点（空档期） 减分写法： 看了半天，不知所云，没有任何亮点，没有让人有去和你聊一聊深扒的信息。 来几个栗子 栗子1错误打开方式： XX（全栈工程师）2013.06 — 至今 参与需求分析及实现方案设计。 设计数据库表结构，实现后台功能及web页面展示。 产品线上部署及运维。 ay 配置管理工程师 2010.03 — 2013.03 负责公司产品性能测试，及线上数据分析 负责公司配置管理，环境维护等工作 点评：看不出来他做的什么事情，没有逻辑性，甚至不知道他做的什么技术语言。 栗子2正确打开方式： 西安XXX公司 Java工程师 — 2016.2月-2017.2月 1、MOGU推荐架构数据与缓存层设计开发 MOGU是一款时尚资讯app,负责推荐页面资讯feed流的展示及用户历史的展示 负责数据层,处理前端逻辑整个开发工作,分布式rpc服务搭建 负责进行压测监测、缓存处理,对接又进行改进优化,主用redis缓存 2、基于JAVA的电商爬虫开发 使用java搭建爬虫server平台,进行配置和开发,进行网页改版监测功能开发 爬取淘宝时尚品牌与其他电商网站商品品牌与详情等 通过频率、ip池、匿名代理等应对一些网站的反爬 3、同图搜索Solr服务开发 基于算法组的同图策略,使用solr做java接又实现rpc服务搭建,进行索引构建和solr实现 北京XXX java大数据工程师— 2013.4月-2015.12月 1、负责实时流消息处理应用系统构建和实现 在调研了kafka的优势和我们的具体需求之后,用kafka作为消费者,保证高吞吐处理消息,并持久化消息的同时供其它服务使用,进行了系统的设计和搭建使用。 本地日志保证消息不丢失,并通过记录游标滑动重复读取数据。 使用storm 负责搭建消息处理架构,并完成基于业务的消息落地,提供后续的数据 统计分析实时和离线任务,诸如pv、uv等数据,为运营做决策 网站用户行为埋点和基于js的日志收集器开发,定义接又和前端部门配合。主用go 2、hadoop集群搭建和数据分析处理 2、基于CDH的集群搭建工作,后期进行维护 编写MapReduce程序,能将复杂工作逻辑化,尽最大能力发挥大数据应用的特点, 对程序高要求,监控自己程序运行情况,使用内存合理,注重增量和全量运算的利弊 3、调度系统设计与实现 基于quartz2搭建调度平台,带徒弟实现相关功能并定期review代码 4、数据库调优 负责主从搭建,并掌握主从搭建的利弊,了解业界mycat原理,有数据库优化经验,能 正确并擅长使用索引,对锁有深刻的认识 5、网站开发 java web网站业务开发,并能很好的使用缓存技术,对重构有实际的经验,并对面向对 象开发有全面的实战经验。了解java数据结构的使用场景,虽然对于大并发没有太大的 发挥余地,但是掌握了数据结构,对于并发和阻塞等有自己的见解。 点评：非常清晰的告诉简历阅读者自己做了什么事情，负责了什么样的事情，用了什么技术栈，且逻辑连贯。 四、工作期望&个人评价 加分写法： 对自己有一个全方位的一个描述总结，让别人更好的解读你。或者在此处，高亮你的优点特长有哪些。 即使不写个人评价，也一定记得写上工作期望。 减分写法： 完全看不出个性特点，写和没写没什么区别。 来几个栗子 栗子1 错误打开方式 为人性格,诚实谦虚，勤奋，能吃苦耐劳，有耐心，有团队意识，能和同学和谐相处，能虚心接受别人的建议的人。 责任心强，善于沟通，具有良好的团队合作精神；专业扎实，具有较强的钻研精神和学习能力；性格比较乐观外向，喜欢打羽毛球。 栗子2正确打开方式 我对自己的定位: 主攻前端,同时在其他方面打打辅助。我不希望过于依赖别人,即使没有后端没有设计没有产品经理,我依然想要把这个产品做到完美。毕竟全栈才能最高效地解决问题。 我对工作的态度: 第一,要高效完成自己的本职工作。第二,要在完成的基础上寻找完美。第三,要在完美的基础上,与其他同事 互相交流学习,互相提升。工作是一种生活方式,不是一份养家糊口的差事。 我怎样克服困难: 不用百度是第一原则,在遇到技术问题时我往往会去Google、Stack over flow上寻找答案。但通常很多问题 并不一定已经被人解决,所以熟练地阅读源码、在手册、规范甚至 REPL的环境自己做实验才是最终解决问题的办法。相信事实的结果,自己动手去做。 怎样保持自己的视野:我一直认为软件开发中视野极其重要,除了在 Twitter 上关注业界大牛,Github Trending 也是每周必刷。 另外 Podcast、Hacker News、Reddit 以及TechRadar 也是重要的一手资料。保持开阔视野才能找到更酷的解决方案。 我的优势: 热爱技术、自学能力强,有良好的自我认知。全面的技能树与开阔的视野,良好的心态、情商与沟通能力。 我的劣势: 非科班出身没有科班同学对算法的熟练掌握,但我决定死磕技术,弥补不足。 栗子3正确打开方式 极客、热爱技术、热爱开源 Ruby on Rails：精通 Agile/Lean：精通 ReactJS：掌握 Docker：掌握 AWS：掌握 五、 是否有博客，个人技术栈点等 看到有这项的HR两眼已经放光了，加分加分项，说明你真正的热爱技术，善于学习总结，乐于分享，且有投入自己的业余时间到软件事业中。 我喜欢的书籍:《重构》《卓有成效的程序员》《代码整洁之道》等 我喜欢的社区: 图灵社区，知乎，博客园，Stack Over flow，Google Developer Group等 我的博客链接、个人作品链接如下: https://github.com/github http://www.oschina.net/ https://www.cnblogs.com/ https://itunes.apple.com/app/battle-of-crab/id1121917063?l=en&amp;mt=8 六、简历内容真实性 老司机提醒你，你简历的任意一个细节将会是后面面试中的呈堂证供。 基本就这些了，希望对大家能有帮助，看起简历来几十秒，码字还是个体力活。 ​ 扫码回复“404”获取404份简历模版 写在最后: 希望大家都能拿到自己心仪的offer。面试笔记会继续更新优化。 "},"zother5-Java-Interview/":{"url":"zother5-Java-Interview/","title":"Zother 5 Java Interview","keywords":"","body":" 本github最初的版本是一份word文档，目前只是把word刚刚搬上来了，但是有些图片、排版还没来得急整理，看起来可能还是有点困难 所以可以先关注一下我的公众号，在我的公众号后台回复 888 获取这个github仓库的PDF版本，左侧有导航栏，方便大家阅读。 github必须md格式才能看得舒服些，花了很多时间找word转md的工具，找了几款不太好用，于是自己手动把word改成md格式，后来发现有些重复性工作可以写个程序处理，就写了个程序，把word中的标题、代码都变成md格式，虽然能处理不少，但是还是需要人工校对，还有图片需要上传，真的超级费事，要搞吐了。。。各位也别抱怨我的github格式不好了，毕竟也还没完全处理完，体谅一下~ "},"zother5-Java-Interview/Java基础学习.html":{"url":"zother5-Java-Interview/Java基础学习.html","title":"Java基础学习","keywords":"","body":"Java Oracle JDK有部分源码是闭源的，如果确实需要可以查看OpenJDK的源码，可以在该网站获取。 http://grepcode.com/snapshot/repository.grepcode.com/java/root/jdk/openjdk/8u40-b25/ http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/73d5bcd0585d/src 上面这个还可以查看native方法。 JDK&JRE&JVM JDK（Java Development Kit）是针对Java开发员的产品，是整个Java的核心，包括了Java运行环境JRE、Java工具（编译、开发工具）和Java核心类库。 Java Runtime Environment（JRE）是运行JAVA程序所必须的环境的集合，包含JVM标准实现及Java核心类库。 JVM是Java Virtual Machine（Java虚拟机）的缩写，是整个java实现跨平台的最核心的部分，能够运行以Java语言写作的软件程序。 JDK包含JRE和Java编译、开发工具； JRE包含JVM和Java核心类库； 运行Java仅需要JRE；而开发Java需要JDK。 跨平台 字节码是在虚拟机上运行的，而不是编译器。换而言之，是因为JVM能跨平台安装，所以相应JAVA字节码便可以跟着在任何平台上运行。只要JVM自身的代码能在相应平台上运行，即JVM可行，则JAVA的程序员就可以不用考虑所写的程序要在哪里运行，反正都是在虚拟机上运行，然后变成相应平台的机器语言，而这个转变并不是程序员应该关心的。 "},"zother5-Java-Interview/Java集合面试题及答案.html":{"url":"zother5-Java-Interview/Java集合面试题及答案.html","title":"Java集合面试题及答案","keywords":"","body":"一. HashMap https://blog.csdn.net/jiary5201314/article/details/51439982 1. hashMap的原理 hashmap 是数组和链表的结合体，数组每个元素存的是链表的头结点 往 hashmap 里面放键值对的时候先得到 key 的 hashcode，然后重新计算 hashcode， （让 1 分布均匀因为如果分布不均匀，低位全是 0，则后来计算数组下标的时候会 冲突），然后与 length-1 按位与，计算数组出数组下标 如果该下标对应的链表为空，则直接把键值对作为链表头结点，如果不为空，则 遍历链表看是否有 key 值相同的，有就把 value 替换， 没有就把该对象最为链表的第 一个节点，原有的节点最为他的后续节点 2. hashcode的计算 https://www.zhihu.com/question/20733617/answer/111577937 https://blog.csdn.net/justloveyou_/article/details/62893086 Key.hashcode是key的自带的hascode函数是一个int值32位 hashmap jdk1.8 中 hashmap 重计算 hashcode 方法改动： 高 16 位异或低 16 位 return (key == null) ? 0 : h = key.hashCode() ^ (h >>> 16); 首先确认：当 length 总是 2 的n 次方时， h & (length - 1) 等价于 hash 对length 取模 ， 但是&比%具有更高的效率； Jdk1.7 之前：h & (length - 1);//第三步，取模运算 3. hashMap参数以及扩容机制 初始容量 16，达到阀值扩容，阀值等于最大容量*负载因子，扩容每次 2 倍，总是 2 的n 次方 扩容机制： 使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有 Entry 数组的元素拷贝到新的 Entry 数组里，Java1.重新计算每个元素在数组中的位置。Java1.8 中不是重新计算，而是用了一种更巧妙的方式。 4. get()方法 整个过程都不需要加锁 5. put()方法 这里HashMap里面用到链式数据结构的一个概念。上面我们提到过Entry类里面有一个next属性，作用是指向下一个Entry。打个比方， 第一个键值对A进来，通过计算其key的hash得到的index=0，记做:Entry[0] = A。一会后又进来一个键值对B，通过计算其index也等于0，现在怎么办？HashMap会这样做:B.next = A,Entry[0] = B,如果又进来C,index也等于0,那么C.next = B,Entry[0] = C；这样我们发现index=0的地方其实存取了A,B,C三个键值对,他们通过next这个属性链接在一起。所以疑问不用担心。也就是说数组中存储的是最后插入的元素。到这里为止，HashMap的大致实现 6. HashMap问题 jdk1.8优化 （1）HashMap如果有很多相同key，后面的链很长的话，你会怎么优化？或者你会用什么数据结构来存储？针对HashMap中某个Entry链太长，查找的时间复杂度可能达到O(n)，怎么优化？ Java8 做的改变： HashMap 是数组+链表+红黑树（JDK1.8 增加了红黑树部分），当链表长度>=8 时转化为红黑树 在 JDK1.8 版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过 8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高 HashMap 的性能，其中会用到红黑树的插入、删除、查找等算法。 java8 中对 hashmap 扩容不是重新计算所有元素在数组的位置，而是我们使用的是 2 次幂的扩展(指长度扩为原来 2 倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动 2 次幂的位置在扩充 HashMap 的时候，不需要像 JDK1.7 的实现那样重新计算 hash， 只需要看看原来的 hash 值新增的那个 bit 是 1 还是 0 就好了，是 0 的话索引没变，是 1 的话索引变成“原索引+oldCap”。 7. 一些面试题 7.1 HashMap 和 TreeMap 的区别 Hashmap 使用的是数组+链表，treemap 是红黑树 7.2 hashmap 为什么可以插入空值？ HashMap 中添加 key==null 的 Entry 时会调用 putForNullKey 方法直接去遍历 table[0]Entry 链表，寻找 e.key==null 的 Entry 或者没有找到遍历结束 如果找到了 e.key==null，就保存 null 值对应的原值 oldValue，然后覆盖原值，并返回oldValue 如果在 table[0]Entry 链表中没有找到就调用addEntry 方法添加一个 key 为 null 的 Entry 7.3 Hashmap 为什么线程不安全：(hash 碰撞和扩容导致) HashMap 底层是一个Entry 数组，当发生 hash 冲突的时候，hashmap 是采用链表的方式来解决的，在对应的数组位置存放链表的头结点。对链表而言，新加入的节点会从头结点加入。假如 A 线程和B 线程同时对同一个数组位置调用 addEntry，两个线程会同时得到现在的头结点，然后 A 写入新的头结点之后，B 也写入新的头结点，那 B 的写入操作就会覆盖A 的写入操作造成A 的写入操作丢失 删除键值对的代码如上：当多个线程同时操作同一个数组位置的时候，也都会先取得现在状态下该位置存储的头结点，然后各自去进行计算操作，之后再把结果写会到该数组位置去， 其实写回的时候可能其他的线程已经就把这个位置给修改过了，就会覆盖其他线程的修改 当多个线程同时检测到总数量超过门限值的时候就会同时调用 resize 操作，各自生成新的数组并 rehash 后赋给该map 底层的数组 table，结果最终只有最后一个线程生成的新数组被赋给 table 变量，其他线程的均会丢失。而且当某些线程已经完成赋值而其他线程刚开始的时候，就会用已经被赋值的 table 作为原始数组，这样也会有问题。7.4 Hashmap 碰撞严重 可以自定义重写 hash——多 hash 函数7.4 HashMap 高并发情况下会出现什么问题？ 扩容问题7.5 HashMap 的存放自定义类时，需要实现自定义类的什么方法？ 答：hashCode 和equals。通过 hash(hashCode)然后模运算（其实是与的位操作）定位在Entry 数组中的下标，然后遍历这之后的链表，通过 equals 比较有没有相同的 key，如果有直接覆盖 value，如果没有就重新创建一个 Entry。 7.6 Hashmap为什么线程不安全 hash碰撞和扩容导致,HashMap扩容的的时候可能会形成环形链表，造成死循环。 7.7 Hashmap中的key可以为任意对象或数据类型吗？ 可以为null但不能是可变对象,如果是可变对象的话,对象中的属性改变,则对象 HashCode也进行相应的改变,导致下次无法查找到己存在Map中的效据 如果可变对象在 HashMap中被用作键,时就要小心在改变对象状态的时候,不要改变它的哈希值了。我们只需要保证成员变量的改变能保证该对象的哈希值不变即可. 二. CurrentHashMap http://www.importnew.com/21781.html https://blog.csdn.net/dingji_ping/article/details/51005799 https://www.cnblogs.com/chengxiao/p/6842045.html http://ifeve.com/hashmap-concurrenthashmap-%E7%9B%B8%E4%BF%A1%E7%9C%8B%E5%AE%8C%E8%BF%99%E7%AF%87%E6%B2%A1%E4%BA%BA%E8%83%BD%E9%9A%BE%E4%BD%8F%E4%BD%A0%EF%BC%81/ 1. 概述 一个ConcurrentHashMap维护一个Segment数组，一个Segment维护一个HashEntry数组。 2. JDK1.7 ConCurrentHashMap原理 其中 Segment 继承于 ReentrantLock ConcurrentHashMap 使用分段锁技术，将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问， 能够实现真正的并发访问。 Segment继承了ReentrantLock，表明每个segment都可以当做一个锁。这样对每个segment中的数据需要同步操作的话都是使用每个segment容器对象自身的锁来实现。只有对全局需要改变时锁定的是所有的segment。 3. JDK1.7 Get CurrentHashMap是否使用了锁？？？ 它也没有使用锁来同步，只是判断获取的entry的value是否为null，为null时才使用加锁的方式再次去获取。 这里可以看出并没有使用锁，但是value的值为null时候才是使用了加锁！！！ Get原理： 第一步，先判断一下 count != 0；count变量表示segment中存在entry的个数。如果为0就不用找了。假设这个时候恰好另一个线程put或者remove了这个segment中的一个entry，会不会导致两个线程看到的count值不一致呢？看一下count变量的定义： transient volatile int count; 它使用了volatile来修改。我们前文说过，Java5之后，JMM实现了对volatile的保证：对volatile域的写入操作happens-before于每一个后续对同一个域的读写操作。所以，每次判断count变量的时候，即使恰好其他线程改变了segment也会体现出来 3.1 在get代码的①和②之间，另一个线程新增了一个entry 如果另一个线程新增的这个entry又恰好是我们要get的，这事儿就比较微妙了。下图大致描述了put 一个新的entry的过程。 因为每个HashEntry中的next也是final的，没法对链表最后一个元素增加一个后续entry所以新增一个entry的实现方式只能通过头结点来插入了。newEntry对象是通过 new HashEntry(K k , V v, HashEntry next) 来创建的。如果另一个线程刚好new 这个对象时，当前线程来get它。因为没有同步，就可能会出现当前线程得到的newEntry对象是一个没有完全构造好的对象引用。 如果在这个new的对象的后面，则完全不影响，如果刚好是这个new的对象，那么当刚好这个对象没有完全构造好，也就是说这个对象的value值为null,就出现了如下所示的代码，需要重新加锁再次读取这个值！ 3.2 在get代码的①和②之间，另一个线程修改了一个entry的value value是用volitale修饰的，可以保证读取时获取到的是修改后的值。 3.3 在get代码的①之后，另一个线程删除了一个entry 假设我们的链表元素是：e1-> e2 -> e3 -> e4 我们要删除 e3这个entry，因为HashEntry中next的不可变，所以我们无法直接把e2的next指向e4，而是将要删除的节点之前的节点复制一份，形成新的链表。它的实现大致如下图所示： 如果我们get的也恰巧是e3，可能我们顺着链表刚找到e1，这时另一个线程就执行了删除e3的操作，而我们线程还会继续沿着旧的链表找到e3返回。这里没有办法实时保证了，也就是说没办法看到最新的。 我们第①处就判断了count变量，它保障了在 ①处能看到其他线程修改后的。①之后到②之间，如果再次发生了其他线程再删除了entry节点，就没法保证看到最新的了，这时候的get的实际上是未更新过的！！！。 不过这也没什么关系，即使我们返回e3的时候，它被其他线程删除了，暴漏出去的e3也不会对我们新的链表造成影响。 4. JDK1.7 PUT 1.将当前 Segment 中的 table 通过 key 的 hashcode 定位到 HashEntry。 2.遍历该 HashEntry，如果不为空则判断传入的 key 和当前遍历的 key 是否相等，相等则覆盖旧的 value。 3.不为空则需要新建一个 HashEntry 并加入到 Segment 中，同时会先判断是否需要扩容。 4.最后会解除在 1 中所获取当前 Segment 的锁。 5.可以说是首先找到segment，确定是哪一个segment,然后在这个segment中遍历查找 key值是要查找的key值得entry,如果找到，那么就修改该key,如果没找到，那么就在头部新加一个entry. 5. JDK1.7 Remove 6. JDK1.7 & JDK1.8 size() public int size() { long n = sumCount(); return ((n (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); } volatile 保证内存可见，最大是65535. 5.JDK 1.8 CurrentHashMap概述 1.其中抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。 2.大于8的时候才去红黑树链表转红黑树的阀值，当table[i]下面的链表长度大于8时就转化为红黑树结构。 6. JDK1.8 put 根据 key 计算出 hashcode 。 判断是否需要进行初始化。 f即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。 如果当前位置的 hashcode == MOVED == -1,则需要进行扩容。 如果都不满足，则利用 synchronized 锁写入数据(分为链表写入和红黑树写入）。 如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。 7. JDK1.8 get方法 根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值。 如果是红黑树那就按照树的方式获取值。 就不满足那就按照链表的方式遍历获取值。8.rehash过程 Redis rehash ：dictRehash每次增量rehash n个元素，由于在自动调整大小时已设置好了ht[1]的大小，因此rehash的主要过程就是遍历ht[0]，取得key，然后将该key按ht[1]的 桶的大小重新rehash，并在rehash完后将ht[0]指向ht[1],然后将ht[0]清空。在这个过程中rehashidx非常重要，它表示上次rehash时在ht[0]的下标位置。 可以看到，redis对dict的rehash是分批进行的，这样不会阻塞请求，设计的比较优雅。 但是在调用dictFind的时候，可能需要对两张dict表做查询。唯一的优化判断是，当key在ht[0]不存在且不在rehashing状态时，可以速度返回空。如果在rehashing状态，当在ht[0]没值的时候，还需要在ht[1]里查找。 dictAdd的时候，如果状态是rehashing，则把值插入到ht[1]，否则ht[0] 三 Hashtable https://blog.csdn.net/ns_code/article/details/36191279 1.参数 -（1）table是一个Entry[]数组类型，而Entry实际上就是一个单向链表。哈希表的\"key-value键值对\"都是存储在Entry数组中的。-（2）count是Hashtable的大小，它是Hashtable保存的键值对的数量。 （3）threshold是Hashtable的阈值，用于判断是否需要调整Hashtable的容量。threshold的值=\"容量*加载因子\"。 （4）loadFactor就是加载因子。 （5）modCount是用来实现fail-fast机制的1.put 从下面的代码中我们可以看出，Hashtable中的key和value是不允许为空的，当我们想要想Hashtable中添加元素的时候，首先计算key的hash值，然 后通过hash值确定在table数组中的索引位置，最后将value值替换或者插入新的元素，如果容器的数量达到阈值，就会进行扩充。 2.get 3.Remove 在下面代码中，如果prev为null了，那么说明第一个元素就是要删除的元素，那么就直接指向第一个元素的下一个即可。 4.扩容 默认初始容量为11 线程安全，但是速度慢，不允许key/value为null 加载因子为0.75：即当 元素个数 超过 容量长度的0.75倍 时，进行扩容 扩容增量：2*原数组长度+1如 HashTable的容量为11，一次扩容后是容量为23 四. 一些面试题 4.1 hashtable和hashmap的区别 4.2 HashMap和ConCurrentHashMap区别 4.3 ConcurrentHashMap和HashTable区别 ConcurrentHashMap仅仅锁定map的某个部分，而Hashtable则会锁定整个map。 hashtable(同一把锁):使用synchronized来保证线程安全，但效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用put添加元素，另一个线程不能使用put添加元素，也不能使用get，竞争会越来越激烈效率越低。 concurrenthashmap(分段锁):(锁分段技术)每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。 concurrenthashmap是由Segment数组结构和HahEntry数组结构组成。Segment是一种可重入锁ReentrantLock，扮演锁的角色。HashEntry用于存储键值对数据。一个concurrenthashmap里包含一个Segment数组。Segment的结构和Hashmap类似，是一种数组和链表结构，一个Segment包含一个HashEntry数组，每个HashEntry是一个链表结构的元素，每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得对应的Segment。4.4 linkedHashMap https://blog.csdn.net/justloveyou_/article/details/71713781 4.5 Linkedhashmap与 hashmap的区别 1.LinkedHashMap是HashMap的子类 -2.LinkedHashMap中的Entry增加了两个指针 before 和 after，它们分别用于维护双向链接列表。 3.在put操作上，虽然LinkedHashMap完全继承了HashMap的put操作，但是在细节上还是做了一定的调整，比如，在LinkedHashMap中向哈希表中插入新Entry的同时，还会通过Entry的addBefore方法将其链入到双向链表中。 4.在扩容操作上，虽然LinkedHashMap完全继承了HashMap的resize操作，但是鉴于性能和LinkedHashMap自身特点的考量，LinkedHashMap对其中的重哈希过程(transfer方法)进行了重写 5.在读取操作上，LinkedHashMap中重写了HashMap中的get方法，通过HashMap中的getEntry方法获取Entry对象。在此基础上，进一步获取指定键对应的值。4.6 HashSet 对于HashSet而言，它是基于HashMap实现的 Hashset源码 http://zhangshixi.iteye.com/blog/673143 Hashset 如何保证集合的没有重复元素？ 可以看出hashset底层是hashmap但是存储的是一个对象，hashset实际将该元素e作为key放入hashmap,当key值(该元素e)相同时，只是进行更新value，并不会新增加，所以set中的元素不会进行改变。 4.7 hashmap与hashset区别 4.8 Collections.sort 内部原理 重写 Collections.sort() import java.util.*; class xd{ int a; int b; xd(int a,int b){ this.a = a; this.b = b; } } public class Main { public static void main(String[] arg) { xd a = new xd(2,3); xd b = new xd(4,1); xd c = new xd(1,2); ArrayList array = new ArrayList<>(); array.add(a); array.add(b); array.add(c); Collections.sort(array, new Comparator() { @Override public int compare(xd o1, xd o2) { if(o1.a > o2.a) return 1; else if(o1.a 4.9 hash算法 五.ArrayList，LinkedList和Vector的区别和实现原理 Vector : https://blog.csdn.net/chenssy/article/details/37520981 1.ArrayList与LinkedList区别 ArrayList和LinkedList都实现了List接口，他们有以下的不同点： ArrayList是基于索引的数据接口，它的底层是数组。它可以以O(1)时间复杂度对元素进行随机访问。与此对应，LinkedList是以元素列表的形式存储它的数据，每一个元素都和它的前一个和后一个元素链接在一起，在这种情况下，查找某个元素的时间复杂度是O(n)。 相对于ArrayList，LinkedList的插入，添加，删除操作速度更快，因为当元素被添加到集合任意位置的时候，不需要像数组那样重新计算大小或者是更新索引。 LinkedList比ArrayList更占内存，因为LinkedList为每一个节点存储了两个引用，一个指向前一个元素，一个指向下一个元素。 2. Vetor arraylist Linkedlist 区别 ArrayList ArrayList## 就是动态数组，是Array的复杂版本，动态的增加和减少元素.当更多的元素加入到ArrayList中时,其大小将会动态地增长。它的元素可以通过get/set方法直接访问，因为ArrayList本质上是一个数组。初始容量为10。 1.插入元素的时候可能扩容，删除元素时不会缩小容量。 2.扩容增长为Arraylist增长原来的0.5倍 而Arraylist 没有设置增长空间的方法。 4.线程不同步Vector Vector 和ArrayList类似, 区别在于Vector是同步类(synchronized).因此,开销就比ArrayList要大。初始容量为10。实现了随机访问接口，可以随机访问。Vector是内部是以动态数组的形式来存储数据的。 1.Vector还可以设置增长的空间大小， 及Vector增长原来的1倍3.vector 线程同步LinkedList LinkedList 是一个双链表,在添加和删除元素时具有比ArrayList更好的性能.但在get与set方面弱于ArrayList.当然,这些对比都是指数据量很大或者操作很频繁的情况下的对比。它还实现了 Queue 接口,该接口比List提供了更多的方法,包括 offer(),peek(),poll()等. ArrayList和LinkedList的使用场景，其中add方法的实现ArrayList,LinkedList的实现以及插入，查找，删除的过程 3.使用ArrayList的迭代器会出现什么问题？ 单线程和多线程环境下； 常用的迭代器设计模式，iterator方法返回一个父类实现的迭代器。 1、迭代器的hasNext方法的作用是判断当前位置是否是数组最后一个位置，相等为false，否则为true。 2、迭代器next方法用于返回当前的元素，并把指针指向下一个元素，值得注意的是，每次使用next方法的时候，都会判断创建迭代器获取的这个容器的计数器modCount是否与此时的不相等，不相等说明集合的大小被修改过，如果是会抛出ConcurrentModificationException异常，如果相等调用get方法返回元素即可。4. 数组(Array)和列表(ArrayList)有什么区别？什么时候应该使用Array而不是ArrayList？ 答：不同点：定义上：Array可以包含基本类型和对象类型，ArrayList只能包含对象类型。容量上：Array大小固定，ArrayList的大小是动态变化的。操作上：ArrayList提供更多的方法和特性，如：addAll()，removeAll()，iterator()等等。使用基本数据类型或者知道数据元素数量的时候可以考虑Array;ArrayList处理固定数量的基本类型数据类型时会自动装箱来减少编码工作量，但是相对较慢。5.ArrayList和Vector有何异同点？ 相同点： （1）两者都是基于索引的，都是基于数组的。 （2）两者都维护插入顺序，我们可以根据插入顺序来获取元素。 （3）ArrayList和Vector的迭代器实现都是fail-fast的。 （4）ArrayList和Vector两者允许null值，也可以使用索引值对元素进行随机访问。不同点： （1）Vector是同步，线程安全，而ArrayList非同步，线程不安全。对于ArrayList，如果迭代时改变列表，应该使用CopyOnWriteArrayList。 （2）但是，ArrayList比Vector要快，它因为有同步，不会过载。 （3）在使用上，ArrayList更加通用，因为Collections工具类容易获取同步列表和只读列表。6. 快速失败(fail-fast)和安全失败(fail-safe) 快速失败（fail—fast） 在用迭代器遍历一个集合对象时，如果遍历过程中对集合对象的内容进行了修改（增加、删除、修改），则会抛出Concurrent Modification Exception。 原理：迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变modCount的值。每当迭代器使用hashNext()/next()遍历下一个元素之前，都会检测modCount变量是否为expectedmodCount值，是的话就返回遍历；否则抛出异常，终止遍历。 注意：这里异常的抛出条件是检测到 modCount！=expectedmodCount 这个条件。如果集合发生变化时修改modCount值刚好又设置为了expectedmodCount值，则异常不会抛出。因此，不能依赖于这个异常是否抛出而进行并发操作的编程，这个异常只建议用于检测并发修改的bug。 场景：java.util包下的集合类都是快速失败的，不能在多线程下发生并发修改（迭代过程中被修改）。安全失败（fail—safe） 采用安全失败机制的集合容器，在遍历时不是直接在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历。 原理：由于迭代时是对原集合的拷贝进行遍历，所以在遍历过程中对原集合所作的修改并不能被迭代器检测到，所以不会触发Concurrent Modification Exception。 缺点：基于拷贝内容的优点是避免了Concurrent Modification Exception，但同样地，迭代器并不能访问到修改后的内容，即：迭代器遍历的是开始遍历那一刻拿到的集合拷贝，在遍历期间原集合发生的修改迭代器是不知道的。 场景：java.util.concurrent包下的容器都是安全失败，可以在多线程下并发使用，并发修改。 快速失败和安全失败是对迭代器而言的。 快速失败：当在迭代一个集合的时候，如果有另外一个线程在修改这个集合，就会抛出ConcurrentModification异常，java.util下都是快速失败。 安全失败：在迭代时候会在集合二层做一个拷贝，所以在修改集合上层元素不会影响下层。在java.util.concurrent下都是安全失败 Iterator和ListIterator的区别是什么？ 答：Iterator可用来遍历Set和List集合，但是ListIterator只能用来遍历List。Iterator对集合只能是前向遍历，ListIterator既可以前向也可以后向。 ListIterator实现了Iterator接口，并包含其他的功能，比如：增加元素，替换元素，获取前一个和后一个元素的索引，等等。 快速失败(fail-fast)和安全失败(fail-safe)的区别是什么？ 答：Iterator的安全失败是基于对底层集合做拷贝，因此，它不受源集合上修改的影响。java.util包下面的所有的集合类都是快速失败的，而java.util.concurrent包下面的所有的类都是安全失败的。快速失败的迭代器会抛出ConcurrentModificationException异常，而安全失败的迭代器永远不会抛出这样的异常。 Enumeration接口和Iterator接口的区别有哪些？ 答：Enumeration速度是Iterator的2倍，同时占用更少的内存。但是，Iterator远远比Enumeration安全，因为其他线程不能够修改正在被iterator遍历的集合里面的对象。同时，Iterator允许调用者删除底层集合里面的元素，这对Enumeration来说是不可能的。 "},"zother5-Java-Interview/一、Java基础.html":{"url":"zother5-Java-Interview/一、Java基础.html","title":"一、Java基础","keywords":"","body":" 本github最初的版本是一份word文档，目前只是把word刚刚搬上来了，但是有些图片、排版还没来得急整理，看起来可能还是有点困难 所以可以先关注一下我的公众号，在我的公众号后台回复 888 获取这个github仓库的PDF版本，左侧有导航栏，方便大家阅读。 Java Oracle JDK有部分源码是闭源的，如果确实需要可以查看OpenJDK的源码，可以在该网站获取。 http://grepcode.com/snapshot/repository.grepcode.com/java/root/jdk/openjdk/8u40-b25/ http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/73d5bcd0585d/src 上面这个还可以查看native方法。1.1 JDK&JRE&JVM JDK（Java Development Kit)是针对Java开发员的产品，是整个Java的核心，包括了Java运行环境JRE、Java工具（编译、开发工具)和Java核心类库。 Java Runtime Environment（JRE)是运行JAVA程序所必须的环境的集合，包含JVM标准实现及Java核心类库。 JVM是Java Virtual Machine（Java虚拟机)的缩写，是整个java实现跨平台的最核心的部分，能够运行以Java语言写作的软件程序。 JDK包含JRE和Java编译、开发工具； JRE包含JVM和Java核心类库； 运行Java仅需要JRE；而开发Java需要JDK。1.2 跨平台 字节码是在虚拟机上运行的，而不是编译器。换而言之，是因为JVM能跨平台安装，所以相应JAVA字节码便可以跟着在任何平台上运行。只要JVM自身的代码能在相应平台上运行，即JVM可行，则JAVA的程序员就可以不用考虑所写的程序要在哪里运行，反正都是在虚拟机上运行，然后变成相应平台的机器语言，而这个转变并不是程序员应该关心的。1.3 基础数据类型 第一类：整型 byte short int long 第二类：浮点型 float double 第三类：逻辑型 boolean(它只有两个值可取true false) 第四类：字符型 char byte(1)的取值范围为-128~127（-2的7次方到2的7次方-1) short(2)的取值范围为-32768~32767（-2的15次方到2的15次方-1) int(4)的取值范围为（-2147483648~2147483647)（-2的31次方到2的31次方-1) long(8)的取值范围为（-9223372036854774808~9223372036854774807)（-2的63次方到2的63次方-1) float(4) double(8) char(2) boolean(1/8) 内码是程序内部使用的字符编码，特别是某种语言实现其char或String类型在内存里用的内部编码；外码是程序与外部交互时外部使用的字符编码。“外部”相对“内部”而言；不是char或String在内存里用的内部编码的地方都可以认为是“外部”。例如，外部可以是序列化之后的char或String，或者外部的文件、命令行参数之类的。 Java语言规范规定，Java的char类型是UTF-16的code unit，也就是一定是16位（2字节)，然后字符串是UTF-16 code unit的序列。 Java规定了字符的内码要用UTF-16编码。或者至少要让用户无法感知到String内部采用了非UTF-16的编码。 String.getBytes()是一个用于将String的内码转换为指定的外码的方法。无参数版使用平台的默认编码作为外码，有参数版使用参数指定的编码作为外码；将String的内容用外码编码好，结果放在一个新byte[]返回。调用了String.getBytes()之后得到的byte[]只能表明该外码的性质，而无法碰触到String内码的任何特质。 Java标准库实现的对char与String的序列化规定使用UTF-8作为外码。Java的Class文件中的字符串常量与符号名字也都规定用UTF-8编码。这大概是当时设计者为了平衡运行时的时间效率（采用定长编码的UTF-16)与外部存储的空间效率（采用变长的UTF-8编码)而做的取舍。 1.4 引用类型 类、接口、数组都是引用类型四种引用 目的：避免对象长期占用内存， 强引用 StringReference GC时不回收 当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。软引用 SoftReference GC时如果JVM内存不足时会回收 软引用可用来实现内存敏感的高速缓存。 软引用可以和一个引用队列（ReferenceQueue)联合使用，如果软引用所引用的对象被垃圾回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中。弱引用 WeakReference GC时立即回收 弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。 弱引用可以和一个引用队列（ReferenceQueue)联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。虚引用 PhantomReference 如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。 虚引用主要用来跟踪对象被垃圾回收的活动。虚引用与软引用和弱引用的一个区别在于：虚引用必须和引用队列（ReferenceQueue)联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 在Java集合中有一种特殊的Map类型：WeakHashMap， 在这种Map中存放了键对象的弱引用，当一个键对象被垃圾回收，那么相应的值对象的引用会从Map中删除。WeakHashMap能够节约存储空间，可用来缓存那些非必须存在的数据。 基础数据类型包装类 为什么需要 由于基本数据类型不是对象，所以java并不是纯面向对象的语言，好处是效率较高（全部包装为对象效率较低)。 Java是一个面向对象的编程语言，基本类型并不具有对象的性质，为了让基本类型也具有对象的特征，就出现了包装类型（如我们在使用集合类型Collection时就一定要使用包装类型而非基本类型)，它相当于将基本类型“包装起来”，使得它具有了对象的性质，并且为其添加了属性和方法，丰富了基本类型的操作。 有哪些 基本类型 包装器类型boolean Boolean char Character int Integer byte Byte short Short long Long float Float double Double Number是所有数字包装类的父类自动装箱、自动拆箱（编译器行为) 自动装箱：可以将基础数据类型包装成对应的包装类 Integer i = 10000; // 编译器会改为new Integer(10000) 自动拆箱：可以将包装类转为对应的基础数据类型 int i = new Integer(1000);//编译器会修改为 int i = new Integer(1000).intValue(); 自动拆箱时如果包装类是null，那么会抛出NPE Integer.valueOf public static Integer valueOf(int i) { if (i >= IntegerCache.low && i 调用Integer.valueOf时-128~127的对象被缓存起来。 所以在此访问内的Integer对象使用==和equals结果是一样的。 如果Integer的值一致，且在此范围内，因为是同一个对象，所以==返回true；但此访问之外的对象==比较的是内存地址，值相同，也是返回false。 1.5 Object == 与 equals的区别 如果两个引用类型变量使用==运算符，那么比较的是地址，它们分别指向的是否是同一地址的对象。结果一定是false，因为两个对象不可能存放在同一地址处。 要求是两个对象都不是能空值，与空值比较返回false。 ==不能实现比较对象的值是否相同。 所有对象都有equals方法，默认是Object类的equals，其结果与==一样。 如果希望比较对象的值相同，必须重写equals方法。hashCode与equals的区别 Object中的equals: public boolean equals(Object obj) { return (this == obj); } equals 方法要求满足： 自反性 a.equals(a) 对称性 x.equals(y) y.equals(x) 一致性 x.equals(y) 多次调用结果一致 对于任意非空引用x，x.equals(null) 应该返回false Object中的hashCode: public native int hashCode(); 它是一个本地方法，它的实现与本地机器有关，这里我们暂且认为他返回的是对象存储的物理位置。 当equals方法被重写时，通常有必要重写hashCode方法，以维护hashCode方法的常规约定：值相同的对象必须有相同的hashCode。 object1.equals(object2)为true，hashCode也相同； hashCode不同时，object1.equals(object2)为false； hashCode相同时，object1.equals(object2)不一定为true； 当我们向一个Hash结构的集合中添加某个元素，集合会首先调用hashCode方法，这样就可以直接定位它所存储的位置，若该处没有其他元素，则直接保存。若该处已经有元素存在，就调用equals方法来匹配这两个元素是否相同，相同则不存，不同则链到后面（如果是链地址法)。 先调用hashCode，唯一则存储，不唯一则再调用equals，结果相同则不再存储，结果不同则散列到其他位置。因为hashCode效率更高（仅为一个int值)，比较起来更快。 HashMap#put源码 hash是key的hash值，当该hash对应的位置已有元素时会执行以下代码（hashCode相同) if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k)))) e = p; 如果equals返回结果相同，则值一定相同，不再存入。如果重写equals不重写hashCode会怎样 两个值不同的对象的hashCode一定不一样，那么执行equals，结果为true，HashSet或HashMap的键会放入值相同的对象。1.6 String&StringBuffer&StringBuilder 都是final类，不允许继承； String长度不可变，StringBuffer、StringBuilder长度可变； String public final class String implements java.io.Serializable, Comparable, CharSequence {} equals&hashCode String重写了Object的hashCode和equals。 public boolean equals(Object anObject) { if (this == anObject) { return true; } if (anObject instanceof String) { String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) { char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- != 0) { if (v1[i] != v2[i]) return false; i++; } return true; } } return false; } 添加功能 String是final类，不可被继承，也不可重写一个java.lang.String（类加载机制)。 一般是使用StringUtils来增强String的功能。 为什么只加载系统通过的java.lang.String类而不加载用户自定义的java.lang.String类呢？ 双亲委派机制 因加载某个类时，优先使用父类加载器加载需要使用的类。如果我们自定义了java.lang.String这个类， 加载该自定义的String类，该自定义String类使用的加载器是AppClassLoader，根据优先使用父类加载器原理， AppClassLoader加载器的父类为ExtClassLoader，所以这时加载String使用的类加载器是ExtClassLoader， 但是类加载器ExtClassLoader在jre/lib/ext目录下没有找到String.class类。然后使用ExtClassLoader父类的加载器BootStrap， 父类加载器BootStrap在JRE/lib目录的rt.jar找到了String.class，将其加载到内存中。这就是类加载器的委托机制。 所以，用户自定义的java.lang.String不被加载，也就是不会被使用。+ substring 会创建一个新的字符串； 编译时会将+转为StringBuilder的append方法。 注意新的字符串是在运行时在堆里创建的。 String str1 = “ABC”;可能创建一个或者不创建对象，如果”ABC”这个字符串在java String池里不存在，会在java String池里创建一个创建一个String对象(“ABC”)，然后str1指向这个内存地址，无论以后用这种方式创建多少个值为”ABC”的字符串对象，始终只有一个内存地址被分配，之后的都是String的拷贝，Java中称为“字符串驻留”，所有的字符串常量都会在编译之后自动地驻留。 注意只有字符串常量是共享的，+和substring等操作的结果不是共享的，substring也会在堆中重新创建字符串。 public String substring(int beginIndex, int endIndex) { if (beginIndex value.length) { throw new StringIndexOutOfBoundsException(endIndex); } int subLen = endIndex - beginIndex; if (subLen public String(char value[], int offset, int count) { if (offset >>1. if (offset > value.length - count) { throw new StringIndexOutOfBoundsException(offset + count); } this.value = Arrays.copyOfRange(value, offset, offset+count); } 常量池 String str = new String(“ABC”); 至少创建一个对象，也可能两个。因为用到new关键字，肯定会在heap中创建一个str2的String对象，它的value是“ABC”。同时如果这个字符串在字符串常量池里不存在，会在池里创建这个String对象“ABC”。 String s1= “a”; String s2 = “a”; 此时s1 == s2 返回true String s1= new String(“a”); String s2 = new String(“a”); 此时s1 == s2 返回false \"\"创建的字符串在字符串池中。 如果引号中字符串存在在常量池中，则仅在堆中拷贝一份(new String); 如果不在，那么会先在常量池中创建一份(\"abc\")，然后在堆中创建一份(new String)，共创建两个对象。 编译优化 字面量，final 都会在编译期被优化，并且会被直接运算好。 1)注意c和d中，final变量b已经被替换为其字符串常量了。 2)注意f、g中，b被替换为其字符串常量，并且在编译时字符串常量的+运算会被执行，返回拼接后的字符串常量 3)注意j，a1作为final变量，在编译时被替换为其字符串常量 解释 c == h / d == h/ e== h为false：c是运行时使用+拼接，创建了一个新的堆中的字符串ab，与ab字符串常量不是同一个对象； 解释f == h/ g == h为true：f编译时进行优化，其值即为字符串常量ab，h也是，指向字符串常量池中的同一个对象； String#intern（JDK1.7之后) JDK1.7之后JVM里字符串常量池放入了堆中，之前是放在方法区。 intern()方法设计的初衷，就是重用String对象，以节省内存消耗。 一定是new得到的字符串才会调用intern，字符串常量没有必要去intern。 当调用 intern 方法时，如果池已经包含一个等于此 String 对象的字符串（该对象由 equals(Object) 方法确定)，则返回池中的字符串。否则，常量池中直接存储堆中该字符串的引用（1.7之前是常量池中再保存一份该字符串)。 源码 public native String intern(); 实例一： String s = new String(\"1\"); s.intern(); String s2 = \"1\"; System.out.println(s == s2);// false String s3 = new String(\"1\") + new String(\"1\"); s3.intern(); String s4 = \"11\"; System.out.println(s3 == s4);// true String s = newString(\"1\")，生成了常量池中的“1” 和堆空间中的字符串对象。 s.intern()，这一行的作用是s对象去常量池中寻找后发现\"1\"已经存在于常量池中了。 String s2 = \"1\"，这行代码是生成一个s2的引用指向常量池中的“1”对象。 结果就是 s 和 s2 的引用地址明显不同。因此返回了false。 String s3 = new String(\"1\") + newString(\"1\")，这行代码在字符串常量池中生成“1” ，并在堆空间中生成s3引用指向的对象（内容为\"11\")。注意此时常量池中是没有 “11”对象的。 s3.intern()，这一行代码，是将 s3中的“11”字符串放入 String 常量池中，此时常量池中不存在“11”字符串，JDK1.6的做法是直接在常量池中生成一个 \"11\" 的对象。 但是在JDK1.7中，常量池中不需要再存储一份对象了，可以直接存储堆中的引用。这份引用直接指向 s3 引用的对象，也就是说s3.intern() ==s3会返回true。 String s4 = \"11\"， 这一行代码会直接去常量池中创建，但是发现已经有这个对象了，此时也就是指向 s3 引用对象的一个引用。因此s3 == s4返回了true。 实例二： String s3 = new String(\"1\") + new String(\"1\"); String s4 = \"11\"; s3.intern(); System.out.println(s3 == s4);// false String s3 = new String(\"1\") + newString(\"1\")，这行代码在字符串常量池中生成“1” ，并在堆空间中生成s3引用指向的对象（内容为\"11\")。注意此时常量池中是没有 “11”对象的。 String s4 = \"11\"， 这一行代码会直接去生成常量池中的\"11\"。 s3.intern()，这一行在这里就没什么实际作用了。因为\"11\"已经存在了。 结果就是 s3 和 s4 的引用地址明显不同。因此返回了false。 实例三： String str1 = new String(\"SEU\") + new String(\"Calvin\"); System.out.println(str1.intern() == str1);// true System.out.println(str1 == \"SEUCalvin\");// true str1.intern() == str1就是上面例子中的情况，str1.intern()发现常量池中不存在“SEUCalvin”，因此指向了str1。 \"SEUCalvin\"在常量池中创建时，也就直接指向了str1了。两个都返回true就理所当然啦。 实例四： String str2 = \"SEUCalvin\";//新加的一行代码，其余不变String str1 = new String(\"SEU\") + new String(\"Calvin\"); System.out.println(str1.intern() == str1);// false System.out.println(str1 == \"SEUCalvin\");// false 在实例三的基础上加了第一行 str2先在常量池中创建了“SEUCalvin”，那么str1.intern()当然就直接指向了str2，你可以去验证它们两个是返回的true。后面的\"SEUCalvin\"也一样指向str2。所以谁都不搭理在堆空间中的str1了，所以都返回了false。StringBuffer&StringBuilder StringBuffer是线程安全的，StringBuilder不是线程安全的，但它们两个中的所有方法都是相同的。StringBuffer在StringBuilder的方法之上添加了synchronized，保证线程安全。 StringBuilder比StringBuffer性能更好。 1.7 面向对象 抽象类与接口 区别： 1)抽象类中方法可以不是抽象的；接口中的方法必须是抽象方法； 2)抽象类中可以有普通的成员变量；接口中的变量必须是 static final 类型的，必须被初始化 , 接口中只有常量，没有变量。 3)抽象类只能单继承，接口可以继承多个父接口； 4)Java8 中接口中会有 default 方法，即方法可以被实现。 使用场景： 如果要创建不带任何方法定义和成员变量的基类，那么就应该选择接口而不是抽象类。 如果知道某个类应该是基类，那么第一个选择的应该是让它成为一个接口，只有在必须要有方法定义和成员变量的时候，才应该选择抽象类。因为抽象类中允许存在一个或多个被具体实现的方法，只要方法没有被全部实现该类就仍是抽象类。三大特性 面向对象的三个特性：封装；继承；多态 封装：将数据与操作数据的方法绑定起来，隐藏实现细节，对外提供接口。 继承：代码重用；可扩展性 多态：允许不同子类对象对同一消息做出不同响应 多态的三个必要条件：继承、方法的重写、父类引用指向子类对象 重写和重载 根据对象对方法进行选择，称为分派 编译期的静态多分派：overloading重载 根据调用引用类型和方法参数决定调用哪个方法（编译器) 运行期的动态单分派：overriding 重写 根据指向对象的类型决定调用哪个方法（JVM) 1.8 关键类 ThreadLocal（线程局部变量) 在线程之间共享变量是存在风险的，有时可能要避免共享变量，使用ThreadLocal辅助类为各个线程提供各自的实例。 例如有一个静态变量 public static final SimpleDateFormat sdf = new SimpleDateFormat(“yyyy-MM-dd”); 如果两个线程同时调用sdf.format(…) 那么可能会很混乱，因为sdf使用的内部数据结构可能会被并发的访问所破坏。当然可以使用线程同步，但是开销很大；或者也可以在需要时构造一个局部SImpleDateFormat对象。但这很浪费。 希望为每一个线程构造一个对象，即使该线程调用多次方法，也只需要构造一次，不必在局部每次都构造。 public static final ThreadLocal sdf = new ThreadLocal() { @Override protected SimpleDateFormat initialValue() { return new SimpleDateFormat(\"yyyy-MM-dd\"); } }; 实现原理：1)每个线程的变量副本是存储在哪里的 ThreadLocal的get方法就是从当前线程的ThreadLocalMap中取出当前线程对应的变量的副本。该Map的key是ThreadLocal对象，value是当前线程对应的变量。 public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(\"unchecked\") T result = (T)e.value; return result; } } return setInitialValue(); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } 【注意，变量是保存在线程中的，而不是保存在ThreadLocal变量中】。当前线程中，有一个变量引用名字是threadLocals，这个引用是在ThreadLocal类中createmap函数内初始化的。 void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } 每个线程都有一个这样的名为threadLocals 的ThreadLocalMap，以ThreadLocal和ThreadLocal对象声明的变量类型作为key和value。 Thread ThreadLocal.ThreadLocalMap threadLocals = null; 这样，我们所使用的ThreadLocal变量的实际数据，通过get方法取值的时候，就是通过取出Thread中threadLocals引用的map，然后从这个map中根据当前threadLocal作为参数，取出数据。现在，变量的副本从哪里取出来的（本文章提出的第一个问题)已经确认解决了。 每个线程内部都会维护一个类似 HashMap 的对象，称为 ThreadLocalMap，里边会包含若干了 Entry（K-V 键值对)，相应的线程被称为这些 Entry 的属主线程； Entry 的 Key 是一个 ThreadLocal 实例，Value 是一个线程特有对象。Entry 的作用即是：为其属主线程建立起一个 ThreadLocal 实例与一个线程特有对象之间的对应关系； Entry 对 Key 的引用是弱引用；Entry 对 Value 的引用是强引用。2)为什么ThreadLocalMap的Key是弱引用 如果是强引用，ThreadLocal将无法被释放内存。 因为如果这里使用普通的key-value形式来定义存储结构，实质上就会造成节点的生命周期与线程强绑定，只要线程没有销毁，那么节点在GC分析中一直处于可达状态，没办法被回收，而程序本身也无法判断是否可以清理节点。弱引用是Java中四档引用的第三档，比软引用更加弱一些，如果一个对象没有强引用链可达，那么一般活不过下一次GC。当某个ThreadLocal已经没有强引用可达，则随着它被垃圾回收，在ThreadLocalMap里对应的Entry的键值会失效，这为ThreadLocalMap本身的垃圾清理提供了便利。3)ThreadLocalMap是何时初始化的（setInitialValue) 在get时最后一行调用了setInitialValue，它又调用了我们自己重写的initialValue方法获得要线程局部变量对象。ThreadLocalMap没有被初始化的话，便初始化，并设置firstKey和firstValue；如果已经被初始化，那么将key和value放入map。 private T setInitialValue() { T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value; } 4)ThreadLocalMap 原理 static class Entry extends WeakReference> { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal k, Object v) { super(k); value = v; } } 它也是一个类似HashMap的数据结构，但是并没实现Map接口。 也是初始化一个大小16的Entry数组，Entry对象用来保存每一个key-value键值对，只不过这里的key永远都是ThreadLocal对象，通过ThreadLocal对象的set方法，结果把ThreadLocal对象自己当做key，放进了ThreadLoalMap中。 ThreadLoalMap的Entry是继承WeakReference，和HashMap很大的区别是，Entry中没有next字段，所以就不存在链表的情况了。构造方法 ThreadLocalMap(ThreadLocal firstKey, Object firstValue) { // 表的大小始终为2的幂次 table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode & (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; // 设定扩容阈值 setThreshold(INITIAL_CAPACITY); } 在ThreadLocalMap中，形如key.threadLocalHashCode & (table.length - 1)（其中key为一个ThreadLocal实例)这样的代码片段实质上就是在求一个ThreadLocal实例的哈希值，只是在源码实现中没有将其抽为一个公用函数。 对于& (INITIAL_CAPACITY - 1)，相对于2的幂作为模数取模，可以用&(2^n-1)来替代%2^n，位运算比取模效率高很多。至于为什么，因为对2^n取模，只要不是低n位对结果的贡献显然都是0，会影响结果的只能是低n位。 private void setThreshold(int len) { threshold = len * 2 / 3; } getEntry（由ThreadLocal#get调用) private Entry getEntry(ThreadLocal key) { int i = key.threadLocalHashCode & (table.length - 1); Entry e = table[i]; if (e != null && e.get() == key) return e; else // 因为用的是线性探测，所以往后找还是有可能能够找到目标Entry的。 return getEntryAfterMiss(key, i, e); } private Entry getEntryAfterMiss(ThreadLocal key, int i, Entry e) { Entry[] tab = table; int len = tab.length; while (e != null) { ThreadLocal k = e.get(); if (k == key) return e; if (k == null) // 该entry对应的ThreadLocal已经被回收，调用expungeStaleEntry来清理无效的entry expungeStaleEntry(i); else i = nextIndex(i, len); e = tab[i]; } return null; } i是位置 从staleSlot开始遍历，将无效key（弱引用指向对象被回收)清理，即对应entry中的value置为null，将指向这个entry的table[i]置为null，直到扫到空entry。 另外，在过程中还会对非空的entry作rehash。 可以说这个函数的作用就是从staleSlot开始清理连续段中的slot（断开强引用，rehash slot等) private int expungeStaleEntry(int staleSlot) { Entry[] tab = table; int len = tab.length; // expunge entry at staleSlot tab[staleSlot].value = null; tab[staleSlot] = null; size--; // Rehash until we encounter null Entry e; int i; for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) { ThreadLocal k = e.get(); if (k == null) { e.value = null; tab[i] = null; size--; } else { // 对于还没有被回收的情况，需要做一次rehash。 如果对应的ThreadLocal的ID对len取模出来的索引h不为当前位置i， 则从h向后线性探测到第一个空的slot，把当前的entry给挪过去。 int h = k.threadLocalHashCode & (len - 1); if (h != i) { tab[i] = null; // Unlike Knuth 6.4 Algorithm R, we must scan until // null because multiple entries could have been stale. while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; } } } return i; } set（线性探测法解决hash冲突) private void set(ThreadLocal key, Object value) { // We don't use a fast path as with get() because it is at // least as common to use set() to create new entries as // it is to replace existing ones, in which case, a fast // path would fail more often than not. Entry[] tab = table; int len = tab.length; // 计算key的hash值 - int i = key.threadLocalHashCode & (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { ThreadLocal k = e.get(); if (k == key) { // 同一个ThreadLocal赋了新值，则替换原值为新值 e.value = value; return; } if (k == null) { // 该位置的TheadLocal已经被回收，那么会清理slot并在此位置放入当前key和value（stale：陈旧的) replaceStaleEntry(key, value, i); return; } } // 下一个位置为空，那么就放到该位置上 tab[i] = new Entry(key, value); int sz = ++size; // 启发式地清理一些slot,并判断是否是否需要扩容 if (!cleanSomeSlots(i, sz) && sz >= threshold) rehash(); } 每个ThreadLocal对象都有一个hash值 threadLocalHashCode，每初始化一个ThreadLocal对象，hash值就增加一个固定的大小 0x61c88647。 private final int threadLocalHashCode = nextHashCode(); private static final int HASH_INCREMENT = 0x61c88647; private static int nextHashCode() { return nextHashCode.getAndAdd(HASH_INCREMENT); } 由于ThreadLocalMap使用线性探测法来解决散列冲突，所以实际上Entry[]数组在程序逻辑上是作为一个环形存在的。 private static int nextIndex(int i, int len) { return ((i + 1 在插入过程中，根据ThreadLocal对象的hash值，定位到table中的位置i，过程如下： 1、如果当前位置是空的，那么正好，就初始化一个Entry对象放在位置i上； 2、不巧，位置i已经有Entry对象了，如果这个Entry对象的key正好是即将设置的key，那么重新设置Entry中的value； 3、很不巧，位置i的Entry对象，和即将设置的key没关系，那么只能找下一个空位置； 这样的话，在get的时候，也会根据ThreadLocal对象的hash值，定位到table中的位置，然后判断该位置Entry对象中的key是否和get的key一致，如果不一致，就判断下一个位置 可以发现，set和get如果冲突严重的话，效率很低，因为ThreadLoalMap是Thread的一个属性，所以即使在自己的代码中控制了设置的元素个数，但还是不能控制其它代码的行为。 cleanSomeSlots（启发式地清理slot) i是当前位置，n是元素个数 i对应entry是非无效（指向的ThreadLocal没被回收，或者entry本身为空) n是用于控制控制扫描次数的 正常情况下如果log n次扫描没有发现无效slot，函数就结束了 但是如果发现了无效的slot，将n置为table的长度len，做一次连续段的清理 再从下一个空的slot开始继续扫描 这个函数有两处地方会被调用，一处是插入的时候可能会被调用，另外个是在替换无效slot的时候可能会被调用， 区别是前者传入的n为元素个数，后者为table的容量 private boolean cleanSomeSlots(int i, int n) { boolean removed = false; Entry[] tab = table; int len = tab.length; do { i = nextIndex(i, len); Entry e = tab[i]; if (e != null && e.get() == null) { n = len; removed = true; i = expungeStaleEntry(i); } } while ( (n >>>= 1) != 0); return removed; } rehash 先全量清理，如果清理后现有元素个数超过负载，那么扩容 private void rehash() { - // 进行一次全量清理 expungeStaleEntries(); // Use lower threshold for doubling to avoid hysteresis if (size >= threshold - threshold / 4) resize(); } 全量清理 private void expungeStaleEntries() { Entry[] tab = table; int len = tab.length; for (int j = 0; j 扩容，因为需要保证table的容量len为2的幂，所以扩容即扩大2倍 private void resize() { Entry[] oldTab = table; int oldLen = oldTab.length; int newLen = oldLen * 2; Entry[] newTab = new Entry[newLen]; int count = 0; for (int j = 0; j k = e.get(); if (k == null) { e.value = null; // Help the GC } else { int h = k.threadLocalHashCode & (newLen - 1); while (newTab[h] != null) h = nextIndex(h, newLen); newTab[h] = e; count++; } } } setThreshold(newLen); size = count; table = newTab; } remove private void remove(ThreadLocal key) { Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode & (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { if (e.get() == key) { // 显式断开弱引用 e.clear(); // 进行段清理 expungeStaleEntry(i); return; } } } Reference#clear public void clear() { this.referent = null; } 内存泄露 只有调用TheadLocal的remove或者get、set时才会采取措施去清理被回收的ThreadLocal对应的value（但也未必会清理所有的需要被回收的value)。假如一个局部的ThreadLocal不再需要，如果没有去调用remove方法清除，那么有可能会发生内存泄露。 既然已经发现有内存泄露的隐患，自然有应对的策略，在调用ThreadLocal的get()、set()可能会清除ThreadLocalMap中key为null的Entry对象，这样对应的value就没有GC Roots可达了，下次GC的时候就可以被回收，当然如果调用remove方法，肯定会删除对应的Entry对象。 如果使用ThreadLocal的set方法之后，没有显式的调用remove方法，就有可能发生内存泄露，所以养成良好的编程习惯十分重要，使用完ThreadLocal之后，记得调用remove方法。 JDK建议将ThreadLocal变量定义成private static的，这样的话ThreadLocal的生命周期就更长，由于一直存在ThreadLocal的强引用，所以ThreadLocal也就不会被回收，也就能保证任何时候都能根据ThreadLocal的弱引用访问到Entry的value值，然后remove它，防止内存泄露。 Iterator / ListIterator / Iterable 普通for循环时不能删除元素，否则会抛出异常；Iterator可以 public interface Collection extends Iterable {} Collection接口继承了Iterable，Iterable接口定义了iterator抽象方法和forEach default方法。所以ArrayList、LinkedList都可以使用迭代器和forEach，包括增强for循环（编译时转为迭代器)。 public interface Iterable { Iterator iterator(); default void forEach(Consumer action) { Objects.requireNonNull(action); for (T t : this) { action.accept(t); } } default Spliterator spliterator() { return Spliterators.spliteratorUnknownSize(iterator(), 0); } } 注意这些具体的容器类返回的迭代器对象是各不相同的，主要是因为不同的容器遍历方式不同，但是这些迭代器对象都实现Iterator接口，都可以使用一个Iterator对象来统一指向这些不同的子类对象。 ArrayList#iterator public Iterator iterator() { return new Itr(); } ArrayList#Itr private class Itr implements Iterator { int cursor; // index of next element to return int lastRet = -1; // index of last element returned; -1 if no such int expectedModCount = modCount; public boolean hasNext() { return cursor != size; } @SuppressWarnings(\"unchecked\") public E next() { checkForComodification(); int i = cursor; if (i >= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i >= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i]; } public void remove() { if (lastRet consumer) { Objects.requireNonNull(consumer); final int size = ArrayList.this.size; int i = cursor; if (i >= size) { return; } final Object[] elementData = ArrayList.this.elementData; if (i >= elementData.length) { throw new ConcurrentModificationException(); } while (i != size && modCount == expectedModCount) { consumer.accept((E) elementData[i++]); } // update once at end of iteration to reduce heap write traffic cursor = i; lastRet = i - 1; checkForComodification(); } final void checkForComodification() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); } } ArrayList#listIterator public ListIterator listIterator() { return new ListItr(0); } ArrayList#ListItr private class ListItr extends Itr implements ListIterator { ListItr(int index) { super(); cursor = index; } public boolean hasPrevious() { return cursor != 0; } public int nextIndex() { return cursor; } public int previousIndex() { return cursor - 1; } @SuppressWarnings(\"unchecked\") public E previous() { checkForComodification(); int i = cursor - 1; if (i = elementData.length) throw new ConcurrentModificationException(); cursor = i; return (E) elementData[lastRet = i]; } public void set(E e) { if (lastRet for /增强for/ forEach For-each loop Equivalent for loop for (type var : arr) { body-of-loop } for (int i = 0; i iter = coll.iterator(); iter.hasNext(); ) { type var = iter.next(); body-of-loop } 增强for循环在编译时被修改为for循环：数组会被修改为下标式的循环；集合会被修改为Iterator循环。 增强for循环不适合以下情况：（过滤、转换、平行迭代) 对collection或数组中的元素不能做赋值操作； 只能正向遍历，不能反向遍历； 遍历过程中，collection或数组中同时只有一个元素可见，即只有“当前遍历到的元素”可见，而前一个或后一个元素是不可见的； forEach ArrayList#forEach继承自 Iterable接口的default方法 default void forEach(Consumer action) { Objects.requireNonNull(action); for (T t : this) { action.accept(t); } } Comparable与Comparator 基本数据类型包装类和String类均已实现了Comparable接口。 实现了Comparable接口的类的对象的列表或数组可以通过Collections.sort或Arrays.sort进行自动排序，默认为升序。 可以将 Comparator 传递给 sort 方法（如 Collections.sort 或 Arrays.sort)，从而允许在排序顺序上实现精确控制。还可以使用 Comparator 来控制某些数据结构（如TreeSet,TreeMap)的顺序。 1.9 继承 子类继承父类所有的成员变量（即使是private变量，有所有权，但是没有使用权，不能访问父类的private的成员变量)。 子类中可以直接调用父类非private方法，也可以用super.父类方法的形式调用。 子类构造方法中如果没有显式使用super(父类构造方法参数)去构造父类对象的话（如果有必须是方法的第一行)，编译器会在第一行添加super()。 子类的构造函数可否不使用super(父类构造方法参数)调用超类的构造方法？ 可以不用显式的写出super，但前提是“父类中有多个构造方法，且有一个是显式写出的无参的构造方法”。 1.10 内部类 在另一个类的里面定义的类就是内部类 内部类是编译器现象，与虚拟机无关。 编译器会将内部类编译成用$分割外部类名和内部类名的常规类文件，而虚拟机对此一无所知。 内部类可以是static的，也可用public，default，protected和private修饰。（而外部类即类名和文件名相同的只能使用public和default)。 优点 每个内部类都能独立地继承一个（接口的)实现，所以无论外部类是否已经继承了某个（接口的)实现，对于内部类都没有影响。 接口只是解决了部分问题，而内部类使得多重继承的解决方案变得更加完整。 用内部类还能够为我们带来如下特性： 1、内部类可以有多个实例，每个实例都有自己的状态信息，并且与其他外部对象的信息相互独立。 2、在单个外部类中，可以让多个内部类实现不同的接口，或者继承不同的类。外部类想要多继承的类可以分别由内部类继承，并进行Override或者直接复用。然后外部类通过创建内部类的对象来使用该内部对象的方法和成员，从而达到复用的目的，这样外部内就具有多个父类的所有特征。 3、创建内部类对象的时刻并不依赖于外部类对象的创建。 4、内部类并没有令人迷惑的“is-a”关系，他就是一个独立的实体。 5、内部类提供了更好的封装，除了该外部类，其他类都不能访问 只有静态内部类可以同时拥有静态成员和非静态成员，其他内部类只有拥有非静态成员。 成员内部类：就像外部类的一个成员变量 注意内部类的对象总有一个外部类的引用 当创建内部类对象时，会自动将外部类的this引用传递给当前的内部类的构造方法。 静态内部类：就像外部类的一个静态成员变量 public class OuterClass { private static class StaticInnerClass { int id; static int increment = 1; } } //调用方式： //外部类.内部类 instanceName = new 外部类.内部类(); 局部内部类：定义在一个方法或者一个块作用域里面的类 想创建一个类来辅助我们的解决方案，又不希望这个类是公共可用的，所以就产生了局部内部类，局部内部类和成员内部类一样被编译，只是它的作用域发生了改变，它只能在该方法和属性中被使用，出了该方法和属性就会失效。 JDK1.8之前不能访问非final的局部变量！ 生命周期不一致： 方法在栈中，对象在堆中；方法执行完，对象并没有死亡 如果可以使用方法的局部变量，如果方法执行完毕，就会访问一个不存在的内存区域。 而final是常量，就可以将该常量的值复制一份，即使不存在也不影响。 public Destination destination(String str) { class PDestination implements Destination { private String label; private PDestination(String whereTo) { label = whereTo; } public String readLabel() { return label; } } return new PDestination(str); } 匿名内部类：必须继承一个父类或实现一个接口 匿名内部类和局部内部类在JDK1.8 之前都不能访问一个非final的局部变量，只能访问final的局部变量，原因是生命周期不同，可能栈中的局部变量已经被销毁，而堆中的对象仍存活，此时会访问一个不存在的内存区域。假如是final的变量，那么编译时会将其拷贝一份，延长其生命周期。 拷贝引用，为了避免引用值发生改变，例如被外部类的方法修改等，而导致内部类得到的值不一致，于是用final来让该引用不可改变。 但在JDK1.8之后可以访问一个非final的局部变量了，前提是非final的局部变量没有修改，表现得和final变量一样才可以！ interface AnonymousInner { int add(); } public class AnonymousOuter { public AnonymousInner getAnonymousInner(){ int x = 100; return new AnonymousInner() { int y = 100; @Override public int add() { return x + y; } }; } } 1.11 关键字 final try-finally-return 1、不管有没有出现异常，finally块中代码都会执行； 2、当try和catch中有return时，finally仍然会执行；无论try里执行了return语句、break语句、还是continue语句，finally语句块还会继续执行；如果执行try和catch时JVM退出（比如System.exit(0))，那么finally不会被执行； finally是在return后面的表达式运算后执行的（此时并没有返回运算后的值，而是先把要返回的值保存起来，管finally中的代码怎么样，返回的值都不会改变，仍然是之前保存的值)，所以函数返回值是在finally执行前确定的； 【 如果try语句里有return，那么代码的行为如下： 1.如果有返回值，就把返回值保存到局部变量中 2.执行jsr指令跳到finally语句里执行 3.执行完finally语句后，返回之前保存在局部变量表里的值 】 3、当try和finally里都有return时，会忽略try的return，而使用finally的return。 4、如果try块中抛出异常，执行finally块时又抛出异常，此时原始异常信息会丢失，只抛出在finally代码块中的异常。 实例一： public static int test() { int x = 1; try { x++; return x; // 2 } finally { x++; } } 实例二： private static int test2() { try { System.out.println(\"try...\"); return 80; } finally { System.out.println(\"finally...\"); return 100; // 100 } } static static方法就是没有this的方法。在static方法内部不能调用非静态方法，反过来是可以的。而且可以在没有创建任何对象的前提下，仅仅通过类本身来调用static方法。这实际上正是static方法的主要用途。1)修饰成员方法：静态成员方法 在静态方法中不能访问类的非静态成员变量和非静态成员方法； 在非静态成员方法中是可以访问静态成员方法/变量的； 即使没有显式地声明为static，类的构造器实际上也是静态方法 2)修饰成员变量：静态成员变量 静态变量和非静态变量的区别是：静态变量被所有的对象所共享，在内存中只有一个副本，它当且仅当在类初次加载时会被初始化。而非静态变量是对象所拥有的，在创建对象的时候被初始化，存在多个副本，各个对象拥有的副本互不影响。 静态成员变量并发下不是线程安全的，并且对象是单例的情况下，非静态成员变量也不是线程安全的。 怎么保证变量的线程安全? 只有一个线程写，其他线程都是读的时候，加volatile；线程既读又写，可以考虑Atomic原子类和线程安全的集合类；或者考虑ThreadLocal3)修饰代码块：静态代码块 用来构造静态代码块以优化程序性能。static块可以置于类中的任何地方，类中可以有多个static块。在类初次被加载的时候，会按照static块的顺序来执行每个static块，并且只会执行一次。 4)修饰内部类：静态内部类 成员内部类和静态内部类的区别： 1)前者只能拥有非静态成员；后者既可拥有静态成员，又可拥有非静态成员 2)前者持有外部类的的引用，可以访问外部类的静态成员和非静态成员；后者不持有外部类的引用，只能访问外部类的静态成员 3)前者不能脱离外部类而存在；后者可以5)修饰import：静态导包 switch switch字符串实现原理 对比反编译之后的结果： 编译后switch还是基于整数，该整数来自于String的hashCode。 先比较字符串的hashCode，因为hashCode相同未必值相同，又再次检查了equals是否相同。 字节码实现原理（tableswitch / lookupswitch) 编译器会使用tableswitch和lookupswitch指令来生成switch语句的编译代码。当switch语句中的case分支的条件值比较稀疏时，tableswitch指令的空间使用率偏低。这种情况下将使用lookupswitch指令来替代。lookupswitch指令的索引表由int类型的键（来源于case语句块后面的数值)与对应的目标语句偏移量所构成。当lookupswitch指令执行时，switch语句的条件值将和索引表中的键进行比较，如果某个键和条件值相符，那么将转移到这个键对应的分支偏移量继续执行，如果没有键值符合，执行将在default分支执行。 abstract 只要含有抽象方法，这个类必须添加abstract关键字，定义为抽象类。 只要父类是抽象类,内含抽象方法，那么继承这个类的子类的相对应的方法必须重写。如果不重写，就需要把父类的声明抽象方法再写一遍，留给这个子类的子类去实现。同时将这个子类也定义为抽象类。 注意抽象类中可以有抽象方法，也可以有具体实现方法（当然也可以没有)。 抽象方法须加abstract关键字，而具体方法不可加 只要是抽象类，就不能存在这个类的对象（不可以new一个这个类的对象)。this & super this 自身引用；访问成员变量与方法；调用其他构造方法 通过this调用另一个构造方法，用法是this(参数列表)，这个仅在类的构造方法中可以使用 函数参数或者函数中的局部变量和成员变量同名的情况下，成员变量被屏蔽，此时要访问成员变量则需要用“this.成员变量名”的方式来引用成员变量。 需要引用当前对象时候，直接用this（自身引用) super 父类引用；访问父类成员变量与方法；调用父类构造方法 super可以理解为是指向自己超（父)类对象的一个指针，而这个超类指的是离自己最近的一个父类。 super有三种用法： 1.普通的直接引用 与this类似，super相当于是指向当前对象的父类，这样就可以用super.xxx来引用父类的成员，如果不冲突的话也可以不加super。 2.子类中的成员变量或方法与父类中的成员变量或方法同名时，为了区别，调用父类的成员必须要加super 3.调用父类的构造函数访问权限 1.12 枚举 JDK实现 实例： public enum Labels0 { ENVIRONMENT(\"环保\"), TRAFFIC(\"交通\"), PHONE(\"手机\"); private String name; private Labels0(String name) { this.name = name; } public String getName() { return name; } } 编译后生成的字节码反编译： 可以清晰地看到枚举被编译后其实就是一个类，该类被声明成 final，说明其不能被继承，同时它继承了 Enum 类。枚举里面的元素被声明成 static final ，另外生成一个静态代码块 static{}，最后还会生成 values 和 valueOf 两个方法。下面以最简单的 Labels 为例，一个一个模块来看。 Enum 类 Enum 类是一个抽象类，主要有 name 和 ordinal 两个属性，分别用于表示枚举元素的名称和枚举元素的位置索引，而构造函数传入的两个变量刚好与之对应。 toString 方法直接返回 name。 equals 方法直接用 == 比较两个对象。 hashCode 方法调用的是父类的 hashCode 方法。 枚举不支持 clone、finalize 和 readObject 方法。 compareTo 方法可以看到就是比较 ordinal 的大小。 valueOf 方法，根据传入的字符串 name 来返回对应的枚举元素。 静态代码块的实现 在静态代码块中创建对象，对象是单例的！ 可以看到静态代码块主要完成的工作就是先分别创建 Labels 对象，然后将“ENVIRONMENT”、“TRAFFIC”和“PHONE”字符串作为 name ，按照顺序分别分配位置索引0、1、2作为 ordinal，然后将其值设置给创建的三个 Labels 对象的 name 和 ordinal 属性，此外还会创建一个大小为3的 Labels 数组 ENUM$VALUES，将前面创建出来的 Labels 对象分别赋值给数组。values的实现 可以看到它是一个静态方法，主要是使用了前面静态代码块中的 Labels 数组 ENUM$VALUES，调用 System.arraycopy 对其进行复制，然后返回该数组。所以通过 Labels.values()[2]就能获取到数组中索引为2的元素。valueOf 方法 该方法同样是个静态方法，可以看到该方法的实现是间接调用了父类 Enum 类的 valueOf 方法，根据传入的字符串 name 来返回对应的枚举元素，比如可以通过 Labels.valueOf(\"ENVIRONMENT\")获取 Labels.ENVIRONMENT。 枚举本质其实也是一个类，而且都会继承java.lang.Enum类，同时还会生成一个静态代码块 static{}，并且还会生成 values 和 valueOf 两个方法。而上述的工作都需要由编译器来完成，然后我们就可以像使用我们熟悉的类那样去使用枚举了。 用enum代替int常量 将int枚举常量翻译成可打印的字符串，没有很便利的方法。 要遍历一个枚举组中的所有int 枚举常量，甚至获得int枚举组的大小。 使用枚举类型的values方法可以获得该枚举类型的数组 枚举类型没有可以访问的构造器，是真正的final；是实例受控的，它们是单例的泛型化；本质上是单元素的枚举；提供了编译时的类型安全。 单元素的枚举是实现单例的最佳方法！ 可以在枚举类型中放入这段代码，可以实现String2Enum。 注意Operation是枚举类型名。 用实例域代替序数 这种实现不好，不推荐使用ordinal方法，推荐使用下面这种实现： 用EnumSet代替位域 位域是将几个常量合并到一个集合中，我们推荐用枚举代替常量，用EnumSet代替集合 EnumSet.of(enum1,enum2) -> Set用EnumMap代替序数索引 将一个枚举类型的值与一个元素（或一组)对应起来，推荐使用EnumMap数据结构 如果是两个维度的变化，那么可以使用EnumMap> 1.13 序列化 JDK序列化（Serizalizable) 定义：将实现了Serializable接口（标记型接口)的对象转换成一个字节数组，并可以将该字节数组转为原来的对象。 ObjectOutputStream 是专门用来输出对象的输出流； ObjectOutputStream 将 Java 对象写入 OutputStream。可以使用 ObjectInputStream 读取（重构)对象。 serialVersionUID Java的序列化机制是通过在运行时判断类的serialVersionUID来验证版本一致性的。在进行反序列化时，JVM会把传来的字节流中的serialVersionUID与本地相应实体（类)的serialVersionUID进行比较，如果相同就认为是一致的，可以进行反序列化，否则就会出现序列化版本不一致的异常。(InvalidCastException)。 1)如果没有添加serialVersionUID，进行了序列化，而在反序列化的时候，修改了类的结构（添加或删除成员变量，修改成员变量的命名)，此时会报错。 2)如果添加serialVersionUID，进行了序列化，而在反序列化的时候，修改了类的结构（添加或删除成员变量，修改成员变量的命名)，那么可能会恢复部分数据，或者恢复不了数据。 如果设置了serialVersionUID并且一致，那么可能会反序列化部分数据；如果没有设置，那么只要属性不同，那么无法反序列化。 其他序列化工具 XML/JSON Thrift/Protobuf 对象深拷贝与浅拷贝 当拷贝一个变量时，原始引用和拷贝的引用指向同一个对象，改变一个引用所指向的对象会对另一个引用产生影响。 如果需要创建一个对象的浅拷贝，那么需要调用clone方法。 Object 类本身不实现接口 Cloneable，直接调用clone会抛出异常。 如果要在自己定义类中调用clone方法，必须实现Cloneable接口（标记型接口)，因为Object类中的clone方法为protected，所以需要自己重写clone方法，设置为public。 protected native Object clone() throws CloneNotSupportedException; public class Person implements Cloneable { private int age; private String name; private Company company; @Override public Person clone() throws CloneNotSupportedException { return (Person) super.clone(); } } public class Company implements Cloneable{ private String name; @Override public Company clone() throws CloneNotSupportedException { return (Company) super.clone(); } } 使用super（即Object)的clone方法只能进行浅拷贝。 如果希望实现深拷贝，需要修改实现，比如修改为： @Override public Person clone() throws CloneNotSupportedException { Person person = (Person) super.clone(); person.setCompany(company.clone()); // 一个新的Company return person; } 假如说Company中还有持有其他对象的引用，那么Company中也要像Person这样做。 可以说：想要深拷贝一个子类，那么它的所有父类都必须可以实现深拷贝。 另一种实现对象深拷贝的方式是序列化。 @Override protected Object clone() { try { ByteArrayOutputStream baos = new ByteArrayOutputStream(); ObjectOutputStream os = new ObjectOutputStream(baos); os.writeObject(this); os.close(); ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray()); ObjectInputStream in = new ObjectInputStream(bais); Object ret = in.readObject(); in.close(); return ret; }catch(Exception e) { e.printStackTrace(); } return null; } 1.14 异常 Error、Exception Error是程序无法处理的错误，它是由JVM产生和抛出的，比如OutOfMemoryError、ThreadDeath等。这些异常发生时，Java虚拟机（JVM)一般会选择线程终止。 Exception是程序本身可以处理的异常，这种异常分两大类运行时异常和非运行时异常。程序中应当尽可能去处理这些异常。常见RuntimeException IllegalArgumentException - 方法的参数无效 NullPointerException - 试图访问一空对象的变量、方法或空数组的元素 ArrayIndexOutOfBoundsException - 数组越界访问 ClassCastException - 类型转换异常 NumberFormatException 继承IllegalArgumentException，字符串转换为数字时出现。比如int i= Integer.parseInt(\"ab3\"); RuntimeException与非Runtime Exception RuntimeException是运行时异常，也称为未检查异常； 非RuntimeException 也称为CheckedException 受检异常 前者可以不必进行try-catch，后者必须要进行try-catch或者throw。 异常包装 在catch子句中可以抛出一个异常，这样做的目的是改变异常的类型 try{ … }catch(SQLException e){ throw new ServletException(e.getMessage()); } 这样的话ServletException会取代SQLException。 有一种更好的方法，可以保存原有异常的信息，将原始异常设置为新的异常的原因 try{ … }catch(SQLException e){ Throwable se = new ServletException(e.getMessage()); se.initCause(e); throw se; } 当捕获到异常时，可以使用getCause方法来重新得到原始异常 Throwable e = se.getCause(); 建议使用这种包装技术，可以抛出系统的高级异常（自己new的)，又不会丢失原始异常的细节。 早抛出，晚捕获。 1.15 泛型 泛型，即“参数化类型”。一提到参数，最熟悉的就是定义方法时有形参，然后调用此方法时传递实参。那么参数化类型怎么理解呢？顾名思义，就是将类型由原来的具体的类型参数化，类似于方法中的变量参数，此时类型也定义成参数形式（可以称之为类型形参)，然后在使用/调用时传入具体的类型（类型实参)。 泛型接口/类/方法 泛型继承、实现 父类使用泛型，子类要么去指定具体类型参数，要么继续使用泛型 泛型的约束和局限性 - 1)只能使用包装器类型，不能使用基本数据类型； - 2)运行时类型查询只适用于原始类型，不适用于带类型参数的类型； if(a instanceof Pair) //error 3)不能创建带有类型参数的泛型类的数组 Pair [] pairs = new Pair[10];//error 只能使用反射来创建泛型数组 public static T[] minmax(T… a){ T[] mm = (T[]) Array.newInstance(a.getClass().getComponentType(),个数); …复制 } 通配符 ? 未知类型 只可以用于声明时，声明类型或方法参数，不能用于定义时（指定类型参数时) List unknownList; List unknownNumberList; List unknownBaseLineIntgerList; 对于参数值是未知类型的容器类，只能读取其中元素，不能向其中添加元素， 因为，其类型是未知，所以编译器无法识别添加元素的类型和容器的类型是否兼容，唯一的例外是null。 通配符类型 List 与原始类型 List 和具体类型 List都不相同，List表示这个list内的每个元素的类型都相同，但是这种类型具体是什么我们却不知道。注意，List和List可不相同，由于Object是最高层的超类，List表示元素可以是任何类型的对象，但是List可不是这个意思（未知类型)。 extends 指定类型必为自身或其子类 List 这个引用变量如果作为参数，哪些引用可以传入？ 本身及其子类 以及含有通配符及extends的本身及子类 不可传入只含通配符不含extends+指定类 的引用 或者extends的不是指定类及其子类，而是其父类 // Number \"extends\" Number (in this context) List foo3 = new ArrayList(); // Integer extends Number List foo3 = new ArrayList(); // Double extends Number List foo3 = new ArrayList(); 如果实现了多个接口，可以使用&来将接口隔开 T extends Comparable & Serializable List list = new ArrayList(); list.add(new Integer(1)); //error list.add(new Float(1.2f)); //error super 指定类型必为自身或其父类 不能同时声明泛型通配符申明上界和下界PECS（读extends，写super) producer-extends, consumer-super. produce是指参数是producer，consumer是指参数是consumer。 要往泛型类写数据时，用extends； 要从泛型类读数据时，用super； 既要取又要写，就不用通配符（即extends与super都不用)比如List。 如果参数化类型表示一个T生产者，就是；如果它表示一个T消费者，就使用。 Stack的pushAll的参数产生E实例供Stack使用，因此参数类型为Iterable。 popAll的参数提供Stack消费E实例，因此参数类型为Collection。 public void pushAll(Iterable src) { for (E e : src) push(e); } public void popAll(Collection dst) { while (!isEmpty()) dst.add(pop()); } 在调用pushAll方法时生产了E 实例（produces E instances)，在调用popAll方法时dst消费了E 实例（consumes E instances)。Naftalin与Wadler将PECS称为Get and Put Principle。 Collections#copy public static void copy(List dest, List src) { int srcSize = src.size(); if (srcSize > dest.size()) throw new IndexOutOfBoundsException(\"Source does not fit in dest\"); if (srcSize di=dest.listIterator(); ListIterator si=src.listIterator(); for (int i=0; i 泛型擦除（编译时擦除) 编译器生成的bytecode是不包含泛型信息的，泛型类型信息将在编译处理是被擦除，这个过程即泛型擦除。 擦除类型变量，并替换为限定类型（无限定的变量用Object)。 比如 T extends Comparable 那么下面所有出现T的地方都会被替换为Comparable 如果调用时指定某个类，比如 Pair pair = new Pair<>(); 那么Pair 中所有的T都替换为String 泛型擦除带来的问题： 1)无法使用具有不同类型参数的泛型进行方法重载 public void test(List ls) { System.out.println(\"Sting\"); } public void test(List li) { System.ut.println(\"Integer\"); } // 编译出错 或者 public interface Builder { void add(List keyList); void add(List valueList); } 2)泛型类的静态变量是共享的 另外，因为Java泛型的擦除并不是对所有使用泛型的地方都会擦除的，部分地方会保留泛型信息，在运行时可以获得类型参数。 1.16 IO Unix IO模型 - 异步I/O 是指用户程序发起IO请求后，不等待数据，同时操作系统内核负责I/O操作把数据从内核拷贝到用户程序的缓冲区后通知应用程序。数据拷贝是由操作系统内核完成，用户程序从一开始就没有等待数据，发起请求后不参与任何IO操作，等内核通知完成。 - 同步I/O 就是非异步IO的情况，也就是用户程序要参与把数据拷贝到程序缓冲区（例如java的InputStream读字节流过程)。 - 同步IO里的非阻塞 是指用户程序发起IO操作请求后不等待数据，而是调用会立即返回一个标志信息告知条件不满足，数据未准备好，从而用户请求程序继续执行其它任务。执行完其它任务，用户程序会主动轮询查看IO操作条件是否满足，如果满足，则用户程序亲自参与拷贝数据动作。 Unix IO模型的语境下，同步和异步的区别在于数据拷贝阶段是否需要完全由操作系统处理。阻塞和非阻塞操作是针对发起IO请求操作后是否有立刻返回一个标志信息而不让请求线程等待。 BIO NIO AIO介绍 BIO：同步阻塞，每个客户端的连接会对应服务器的一个线程 NIO：同步非阻塞，多路复用器轮询客户端的请求，每个客户端的IO请求会对应服务器的一个线程 AIO： 异步非阻塞，客户端的IO请求由OS完成后再通知服务器启动线程处理（需要OS支持) 1、进程向操作系统请求数据 2、操作系统把外部数据加载到内核的缓冲区中， 3、操作系统把内核的缓冲区拷贝到进程的缓冲区 4、进程获得数据完成自己的功能 Java NIO属于同步非阻塞IO，即IO多路复用，单个线程可以支持多个IO 即询问时从IO没有完毕时直接阻塞，变成了立即返回一个是否完成IO的信号。 异步IO就是指AIO，AIO需要操作系统支持。 Java BIO 使用 Server public class ChatServer { ServerSocket ss = null; boolean started = false; ArrayList clients = new ArrayList(); public static void main(String[] args) { new ChatServer().start(); } public void start() { try { ss = new ServerSocket(6666); started = true; } catch (BindException e) { System.out.println(\"端口使用中....\"); // 用于处理两次启动Server端 System.out.println(\"请重新运行服务器\"); System.exit(-1); } catch (IOException e) { System.out.println(\"服务器启动失败\"); e.printStackTrace(); } try { while (started) { Socket s = ss.accept(); Client c = new Client(s); clients.add(c); c.transmitToAll(c.name + \"进入了聊天室\"); new Thread(c).start(); } } catch (IOException e) { e.printStackTrace(); } finally { // 主方法结束时应该关闭服务器ServerSocket if (ss != null) try { ss.close(); } catch (IOException e) { e.printStackTrace(); } } } class Client implements Runnable { // 包装给一个单独的客户端的线程类，应该保留自己的连接Socket和流 // 保留连接一般使用构造方法，将连接传入 // 一个客户端就new 一个Client 连接 private Socket s = null; private DataInputStream dis = null; private DataOutputStream dos = null;// 每个客户端的线程都有各自的输入输出流，输入流用于读来自当前客户端的数据，输出流用于保存当前客户端的流。 private boolean Connected = false;// 每个客户端都有一个开始结束的标志 private String name; Client(Socket s) { // new 一个Client对象时，要打开Socket和DataInputStream流 this.s = s; try { dis = new DataInputStream(s.getInputStream()); dos = new DataOutputStream(s.getOutputStream()); Connected = true; this.name = dis.readUTF(); } catch (IOException e) { e.printStackTrace(); } } // 如何实现一个客户端与其他客户端的通信？ // 可以考虑在每连到一个客户端就保存与其的连接Socket，当要发送给其他客户端信息时，遍历一遍所有其他客户端 public void run() { try { while (Connected) { String read = dis.readUTF(); if (read.equals(\"EXIT\")) { Connected = false; transmitToAll(this.name + \"已退出\"); continue; } else if (read.startsWith(\"@\")) { String[] msg = read.substring(1).split(\":\"); transmitToPerson(msg[0], msg[1]); continue; } transmitToAll(this.name+\":\"+read); } } catch (EOFException e1) { System.out.println(\"Client closed\"); } catch (IOException e1) { e1.printStackTrace(); } finally { // 关闭资源应该放在finally中 try { CloseUtil.close(dis, dos); if (s != null) s.close(); } catch (IOException e1) { e1.printStackTrace(); } } } /** 将消息发送给所有人 * @param read */ public void transmitToAll(String read) { for (int i = 0; i Client c = clients.get(i); if (c.Connected == true) c.send(read); // 调用每个客户端线程的send方法，一个对象的输出流与对应的客户端连接 dos --> // Client } } /** 将消息发送给某个人，私聊 * @param read @param clientName */ public void transmitToPerson(String clientName, String read) { boolean isFind = false; for (int i = 0; i Client client = clients.get(i); if (client.name.equals(clientName)) { client.send(this.name+\":\"+read); isFind = true; } } send(this.name+\":\"+read + (isFind ? \"\" : \"\\n抱歉，没有找到此用户\")); } public void send(String str) {// 在哪里出错就在哪里捕获 try { dos.writeUTF(str); } catch (SocketException e) { this.Connected = false; clients.remove(this); } catch (IOException e) { e.printStackTrace(); } } } }   Client（一个线程用于读取，一个线程用于发送) public class ChatClient extends Frame { Socket s = null; //将某个对象使得在一个类的各个方法可用，将该对象设置为整个类的成员变量 DataOutputStream dos = null;//在多个方法中都要使用 DataInputStream dis = null; TextField tfText = new TextField(); // 设置为成员变量方便其他类进行访问 TextArea taContent = new TextArea(); boolean started = false; Thread recv = null; ChatClient(String name, int x, int y, int w, int h) { super(name); this.setBounds(x, y, w, h); this.setLayout(new BorderLayout()); this.addWindowListener(new MonitorWindow()); taContent.setEditable(false); this.add(tfText, BorderLayout.SOUTH); this.add(taContent, BorderLayout.NORTH); tfText.addActionListener(new MonitorText());//对于文本框的监视器必须添加在某个文本框上，只有窗口监视器才能添加到Frame上 this.pack(); this.setVisible(true); // 必须放在最后一行，否则之下的组件无法显示 connect(); ClientNameDialog dialog = new ClientNameDialog(this,\"姓名提示框\",true); } private class ClientNameDialog extends JDialog implements ActionListener{ JLabel jl = null; JTextField jf = null; JButton jb = null; ClientNameDialog(Frame owner,String title,boolean model){ super(owner,title,model); this.setLayout(new BorderLayout()); this.setBounds(300, 300, 200, 150); jl = new JLabel(\"请输入您的姓名或昵称:\"); jf = new JTextField(); jb = new JButton(\"确定\"); jb.addActionListener(this); this.addWindowListener(new WindowAdapter(){ public void windowClosing(WindowEvent arg0) { setVisible(false); System.exit(0); } }); this.add(jl,BorderLayout.NORTH); this.add(jf,BorderLayout.CENTER); this.add(jb, BorderLayout.SOUTH); this.setVisible(true); } public void actionPerformed(ActionEvent e) { String name = \"\"; name = jf.getText(); if((name == null || name.equals(\"\"))){ JOptionPane.showMessageDialog(this, \"姓名不可为空!\"); return; } this.setVisible(false); send(name); JOptionPane.showMessageDialog(this, \"欢迎您,\"+name); launchThread(); } } private class MonitorWindow extends WindowAdapter { public void windowClosing(WindowEvent e) { setVisible(false); disConnect(); System.exit(0); } } private class MonitorText implements ActionListener { String str = null; public void actionPerformed(ActionEvent e) { str = tfText.getText().trim();//注意这是内部类，要找到事件源对象直接引用外部类的TextField即可，不需要getSource(平行类可用) tfText.setText(\"\"); //trim可以去掉开头和结尾的空格 send(str); } } public void send(String str){//为发送数据单独建立一个方法 try{ dos.writeUTF(str); dos.flush(); }catch(IOException e1){ e1.printStackTrace(); } } public void connect(){ //应为连接单独建立一个方法 try{ s = new Socket(\"localhost\",6666); dos = new DataOutputStream(s.getOutputStream());//一连接就打开输出流 dis = new DataInputStream(s.getInputStream()); //一连接就打开输入流 started = true; }catch(IOException e){ e.printStackTrace(); } } public void launchThread(){ recv = new Thread(new Receive()); recv.start(); } public void disConnect() { try{ dos.writeUTF(\"EXIT\"); started = false; //加入到主线程，会等待子线程执行完毕，才会执行下面的语句。这就避免了在读数据的时候将流切断，但是在这里是无效的。但是将线程停止应该先考虑使用join方法 CloseUtil.close(dis,dos); s.close(); } catch (IOException e) { e.printStackTrace(); } } private class Receive implements Runnable { //同样原因 readUTF是阻塞式的，处于死循环中，不能执行其他语句，所以为其单独设置一个线程 public void run(){ String str = null; try{ while(started){ str = dis.readUTF(); //如果在阻塞状态，程序被关闭，那么一定会报错SocketException。关闭了Socket之后还在调用readUTF方法 taContent.setText(taContent.getText()+str+'\\n');//解决方法是在关闭程序的同时停止线程，不再读取 (如果使用JTextArea可以使用append方法) } }catch (SocketException e){ //将SocketException视为退出。但这种想法是不好的，将异常视为程序正常的一部分 System.out.println(\"Client has quitted!\"); }catch (EOFException e){ System.out.println(\"Client has quitted!\"); }catch(IOException e){ e.printStackTrace(); } } } public static void main(String[] args) { new ChatClient(\"Client\", 200, 200, 300, 200); } } Java NIO 使用 传统的IO操作面向数据流，意味着每次从流中读一个或多个字节，直至完成，数据没有被缓存在任何地方。 NIO操作面向缓冲区，数据从Channel读取到Buffer缓冲区，随后在Buffer中处理数据。 BIO中的accept是没有客户端连接时阻塞，NIO的accept是没有客户端连接时立即返回。 NIO的三个重要组件：Buffer、Channel、Selector。 Buffer是用于容纳数据的缓冲区，Channel是与IO设备之间的连接，类似于流。 数据可以从Channel读到Buffer中，也可以从Buffer 写到Channel中。 Selector是Channel的多路复用器。 Buffer（缓冲区) clear 是将position置为0，limit置为capacity； flip是将limit置为position，position置为0；MappedByteBuffer（对应OS中的内存映射文件) ByteBuffer有两种模式:直接/间接。间接模式就是HeapByteBuffer,即操作堆内存 (byte[])。 但是内存毕竟有限,如果我要发送一个1G的文件怎么办?不可能真的去分配1G的内存.这时就必须使用\"直接\"模式,即 MappedByteBuffer。 OS中内存映射文件是将一个文件映射为虚拟内存（文件没有真正加载到内存，只是作为虚存)，不需要使用文件系统调用来读写数据，而是直接读写内存。 Java中是使用MappedByteBuffer来将文件映射为内存的。通常可以映射整个文件，如果文件比较大的话可以分段进行映射，只要指定文件的那个部分就可以。 优点：减少一次数据拷贝 之前是 进程空间内核的IO缓冲区文件 现在是 进程空间文件 MappedByteBuffer可以使用FileChannel.map方法获取。 它有更多的优点： a. 读取快 b. 写入快 c. 随机读写 MappedByteBuffer使用虚拟内存，因此分配(map)的内存大小不受JVM的-Xmx参数限制，但是也是有大小限制的。 那么可用堆外内存到底是多少？，即默认堆外内存有多大： ① 如果我们没有通过-XX:MaxDirectMemorySize来指定最大的堆外内存。则 ② 如果我们没通过-Dsun.nio.MaxDirectMemorySize指定了这个属性，且它不等于-1。则 ③ 那么最大堆外内存的值来自于directMemory = Runtime.getRuntime().maxMemory()，这是一个native方法。 在我们使用CMS GC的情况下的实现如下：其实是新生代的最大值-一个survivor的大小+老生代的最大值，也就是我们设置的-Xmx的值里除去一个survivor的大小就是默认的堆外内存的大小了。 如果当文件过大，内存不足时，可以通过position参数重新map文件后面的内容。 MappedByteBuffer在处理大文件时的确性能很高，但也存在一些问题，如内存占用、文件关闭不确定，被其打开的文件只有在垃圾回收的才会被关闭，而且这个时间点是不确定的。 DirectByteBuffer（堆外内存) DirectByteBuffer继承自MappedByteBuffer，它们都是使用的堆外内存，不受JVM堆大小的限制，只是前者仅仅是分配内存，后者是将文件映射到内存中。 可以通过ByteBuf.allocateDirect方法获取。 堆外内存的特点（大对象；加快内存拷贝；减轻GC压力) - 对于大内存有良好的伸缩性（支持分配大块内存) - 对垃圾回收停顿的改善可以明显感觉到（堆外内存，减少GC对堆内存回收的压力) - 在进程间可以共享，减少虚拟机间的复制，加快复制速度（减少堆内内存拷贝到堆外内存的过程) - 还可以使用 池+堆外内存 的组合方式，来对生命周期较短，但涉及到I/O操作的对象进行堆外内存的再使用。( Netty中就使用了该方式 ) 堆外内存的一些问题 1)堆外内存回收问题（不手工回收会导致内存溢出，手工回收就失去了Java的优势)； 2) 数据结构变得有些别扭。要么就是需要一个简单的数据结构以便于直接映射到堆外内存，要么就使用复杂的数据结构并序列化及反序列化到内存中。很明显使用序列化的话会比较头疼且存在性能瓶颈。使用序列化比使用堆对象的性能还差。 堆外内存的释放 java.nio.DirectByteBuffer对象在创建过程中会先通过Unsafe接口直接通过os::malloc来分配内存，然后将内存的起始地址和大小存到java.nio.DirectByteBuffer对象里，这样就可以直接操作这些内存。这些内存只有在DirectByteBuffer回收掉之后才有机会被回收，因此如果这些对象大部分都移到了old，但是一直没有触发CMS GC或者Full GC，那么悲剧将会发生，因为你的物理内存被他们耗尽了，因此为了避免这种悲剧的发生，通过-XX:MaxDirectMemorySize来指定最大的堆外内存大小，当使用达到了阈值的时候将调用System.gc来做一次full gc，以此来回收掉没有被使用的堆外内存。 GC方式： 存在于堆内的DirectByteBuffer对象很小，只存着基地址和大小等几个属性，和一个Cleaner，但它代表着后面所分配的一大段内存，是所谓的冰山对象。通过前面说的Cleaner，堆内的DirectByteBuffer对象被GC时，它背后的堆外内存也会被回收。 当新生代满了，就会发生minor gc；如果此时对象还没失效，就不会被回收；撑过几次minorgc后，对象被迁移到老生代；当老生代也满了，就会发生full gc。 这里可以看到一种尴尬的情况，因为DirectByteBuffer本身的个头很小，只要熬过了minor gc，即使已经失效了也能在老生代里舒服的呆着，不容易把老生代撑爆触发full gc，如果没有别的大块头进入老生代触发full gc，就一直在那耗着，占着一大片堆外内存不释放。 这时，就只能靠前面提到的申请额度超限时触发的System.gc()来救场了。但这道最后的保险其实也不很好，首先它会中断整个进程，然后它让当前线程睡了整整一百毫秒，而且如果gc没在一百毫秒内完成，它仍然会无情的抛出OOM异常。 那为什么System.gc()会释放DirectByteBuffer呢? 每个DirectByteBuffer关联着其对应的Cleaner，Cleaner是PhantomReference的子类，虚引用主要被用来跟踪对象被垃圾回收的状态，通过查看ReferenceQueue中是否包含对象所对应的虚引用来判断它是否即将被垃圾回收。 当GC时发现DirectByteBuffer除了PhantomReference外已不可达，就会把它放进 Reference类pending list静态变量里。然后另有一条ReferenceHandler线程，名字叫 \"Reference Handler\"的，关注着这个pending list，如果看到有对象类型是Cleaner，就会执行它的clean()，其他类型就放入应用构造Reference时传入的ReferenceQueue中，这样应用的代码可以从Queue里拖出这些理论上已死的对象，做爱做的事情——这是一种比finalizer更轻量更好的机制。 手工方式： 如果想立即释放掉一个MappedByteBuffer/DirectByteBuffer，因为JDK没有提供公开API，只能使用反射的方法去unmap； 或者使用Cleaner的clean方法。 public static void main(String[] args) { try { File f = File.createTempFile(\"Test\", null); f.deleteOnExit(); RandomAccessFile file = new RandomAccessFile(f, \"rw\"); file.setLength(1024); FileChannel channel = file.getChannel(); MappedByteBuffer buffer = channel.map( FileChannel.MapMode.READ_WRITE, 0, 1024); channel.close(); file.close(); // 手动unmap Method m = FileChannelImpl.class.getDeclaredMethod(\"unmap\", MappedByteBuffer.class); m.setAccessible(true); m.invoke(FileChannelImpl.class, buffer); if (f.delete()) System.out.println(\"Temporary file deleted: \" + f); else System.err.println(\"Not yet deleted: \" + f); } catch (Exception ex) { ex.printStackTrace(); } } Channel（通道) Channel与IO设备的连接，与Stream是平级的概念。流与通道的区别 1、流是单向的，通道是双向的，可读可写。 2、流读写是阻塞的，通道可以异步读写。 3、流中的数据可以选择性的先读到缓存中，通道的数据总是要先读到一个缓存中，或从缓存中写入 注意，FileChannel 不能设置为非阻塞模式。 分散与聚集 分散（scatter)从Channel中读取是指在读操作时将读取的数据写入多个buffer中。因此，Channel将从Channel中读取的数据“分散（scatter)”到多个Buffer中。 聚集（gather)写入Channel是指在写操作时将多个buffer的数据写入同一个Channel，因此，Channel 将多个Buffer中的数据“聚集（gather)”后发送到Channel。 Pipe public class PipeTest { public static void main(String[] args) { Pipe pipe = null; ExecutorService exec = Executors.newFixedThreadPool(2); try { pipe = Pipe.open(); final Pipe pipeTemp = pipe; exec.submit(new Callable() { @Override public Object call() throws Exception { Pipe.SinkChannel sinkChannel = pipeTemp.sink();//向通道中写数据 while (true) { TimeUnit.SECONDS.sleep(1); String newData = \"Pipe Test At Time \" + System.currentTimeMillis(); ByteBuffer buf = ByteBuffer.allocate(1024); buf.clear(); buf.put(newData.getBytes()); buf.flip(); while (buf.hasRemaining()) { System.out.println(buf); sinkChannel.write(buf); } } } }); exec.submit(new Callable() { @Override public Object call() throws Exception { Pipe.SourceChannel sourceChannel = pipeTemp.source();//向通道中读数据 while (true) { TimeUnit.SECONDS.sleep(1); ByteBuffer buf = ByteBuffer.allocate(1024); buf.clear(); int bytesRead = sourceChannel.read(buf); System.out.println(\"bytesRead=\" + bytesRead); while (bytesRead > 0) { buf.flip(); byte b[] = new byte[bytesRead]; int i = 0; while (buf.hasRemaining()) { b[i] = buf.get(); System.out.printf(\"%X\", b[i]); i++; } String s = new String(b); System.out.println(\"=================||\" + s); bytesRead = sourceChannel.read(buf); } } } }); } catch (IOException e) { e.printStackTrace(); } finally { exec.shutdown(); } } } FileChannel与文件锁 在通道中我们可以对文件或者部分文件进行上锁。上锁和我们了解的线程锁差不多，都是为了保证数据的一致性。在文件通道FileChannel中可以对文件进行上锁，通过FileLock可以对文件进行锁的释放。 文件加锁是建立在文件通道（FileChannel)之上的，套接字通道（SockeChannel)不考虑文件加锁，因为它是不共享的。它对文件加锁有两种方式： ①lock ②tryLock 两种加锁方式默认都是对整个文件加锁，如果自己配置的话就可以控制加锁的文件范围：position是加锁的开始位置，size是加锁长度，shared是用于控制该锁是共享的还是独占的。 lock是阻塞式的，当有进程对锁进行读取时会等待锁的释放，在此期间它会一直等待；tryLock是非阻塞式的，它尝试获得锁，如果这个锁不能获得，那么它会立即返回。 release可以释放锁。 在一个进程中在锁没有释放之前是无法再次获得锁的 在java的NIO中，通道包下面有一个FileLock类，它主要是对文件锁工具的一个描述。在上一小节中对文件的锁获取其实是FileChannel获取的（lock与trylock是FileChannel的方法)，它们返回一个FileLock对象。这个类的核心方法有如下这些： boolean isShared() :判断锁是否为共享类型 abstract boolean isValid() ：判断锁是否有效 boolean overlaps()：判断此锁定是否与给定的锁定区域重叠 long position()：返回文件内锁定区域中第一个字节的位置。 abstract void release() ：释放锁 long size() ：返回锁定区域的大小，以字节为单位 在文件锁中有3种方式可以释放文件锁：①锁类释放锁，调用FileLock的release方法； ②通道类关闭通道，调用FileChannel的close方法；③jvm虚拟机会在特定情况释放锁。 锁类型（独占式和共享式) 我们先区分一下在文件锁中两种锁的区别：①独占式的锁就想我们上面测试的那样，只要有一个进程获取了独占锁，那么别的进程只能等待。②共享锁在一个进程获取的情况下，别的进程还是可以读取被锁定的文件，但是别的进程不能写只能读。 Selector （Channel的多路复用器) Selector可以用单线程去管理多个Channel（多个连接)。 放在网络编程的环境下：Selector使用单线程，轮询客户端对应的Channel的请求，如果某个Channel需要进行IO，那么分配一个线程去执行IO操作。 Selector可以去监听的请求有以下几类： 1、connect：客户端连接服务端事件，对应值为SelectionKey.OPCONNECT(8) 2、accept：服务端接收客户端连接事件，对应值为SelectionKey.OPACCEPT(16) 3、read：读事件，对应值为SelectionKey.OPREAD(1) 4、write：写事件，对应值为SelectionKey.OPWRITE(4) 每次请求到达服务器，都是从connect开始，connect成功后，服务端开始准备accept，准备就绪，开始读数据，并处理，最后写回数据返回。 SelectionKey是一个复合事件，绑定到某个selector对应的某个channel上，可能是多个事件的复合或单一事件。 Java NIO 实例（文件上传) 服务器主线程先创建Socket，并注册到selector，然后轮询selector。 1)如果有客户端需要进行连接，那么selector返回ACCEPT事件，主线程建立连接（accept)，并将该客户端连接注册到selector，结束，继续轮询selector等待下一个客户端事件； 2)如果有已连接的客户端需要进行读写，那么selector返回READ/WRITE事件，主线程将该请求交给IO线程池中的某个线程执行操作，结束，继续轮询selector等待下一个客户端事件。服务器 public class NIOTCPServer { private ServerSocketChannel serverSocketChannel; private final String FILE_PATH = \"E:/uploads/\"; private AtomicInteger i; private final String RESPONSE_MSG = \"服务器接收数据成功\"; private Selector selector; private ExecutorService acceptPool; private ExecutorService readPool; public NIOTCPServer() { try { serverSocketChannel = ServerSocketChannel.open(); //切换为非阻塞模式 serverSocketChannel.configureBlocking(false); serverSocketChannel.bind(new InetSocketAddress(9000)); //获得选择器 selector = Selector.open(); //将channel注册到selector上 //第二个参数是选择键，用于说明selector监控channel的状态 //可能的取值：SelectionKey.OP_READ OP_WRITE OP_CONNECT OP_ACCEPT //监控的是channel的接收状态 serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); acceptPool = new ThreadPoolExecutor(50, 100, 1000, TimeUnit.MILLISECONDS, new LinkedBlockingDeque<>()); readPool = new ThreadPoolExecutor(50, 100, 1000, TimeUnit.MILLISECONDS, new LinkedBlockingDeque<>()); i = new AtomicInteger(0); System.out.println(\"服务器启动\"); } catch (IOException e) { e.printStackTrace(); } } public void receive() { try { //如果有一个及以上的客户端的数据准备就绪 while (selector.select() > 0) { //获取当前选择器中所有注册的监听事件 for (Iterator it = selector.selectedKeys().iterator(); it.hasNext(); ) { SelectionKey key = it.next(); //如果\"接收\"事件已就绪 if (key.isAcceptable()) { //交由接收事件的处理器处理 acceptPool.submit(new ReceiveEventHander()); } else if (key.isReadable()) { //如果\"读取\"事件已就绪 //交由读取事件的处理器处理 readPool.submit(new ReadEventHandler((SocketChannel) key.channel())); } //处理完毕后，需要取消当前的选择键 it.remove(); } } } catch (IOException e) { e.printStackTrace(); } } class ReceiveEventHander implements Runnable { public ReceiveEventHander() { } @Override public void run() { SocketChannel client = null; try { client = serverSocketChannel.accept(); // 接收的客户端也要切换为非阻塞模式 client.configureBlocking(false); // 监控客户端的读操作是否就绪 client.register(selector, SelectionKey.OP_READ); System.out.println(\"服务器连接客户端:\" + client.toString()); } catch (IOException e) { e.printStackTrace(); } } } class ReadEventHandler implements Runnable { private ByteBuffer buf; private SocketChannel client; public ReadEventHandler(SocketChannel client) { this.client = client; buf = ByteBuffer.allocate(1024); } @Override public void run() { FileChannel fileChannel = null; try { int index = 0; synchronized (client) { while (client.read(buf) != -1) { if (fileChannel == null) { index = i.getAndIncrement(); fileChannel = FileChannel.open(Paths.get(FILE_PATH, index + \".jpeg\"), StandardOpenOption.WRITE, StandardOpenOption.CREATE); } buf.flip(); fileChannel.write(buf); buf.clear(); } } if (fileChannel != null) { fileChannel.close(); System.out.println(\"服务器写来自客户端\" + client + \" 文件\" + index + \" 完毕\"); } } catch (IOException e) { e.printStackTrace(); } } } public static void main(String[] args) { NIOTCPServer server = new NIOTCPServer(); server.receive(); } } 客户端 public class NIOTCPClient { private SocketChannel clientChannel; private ByteBuffer buf; public NIOTCPClient() { try { clientChannel = SocketChannel.open(new InetSocketAddress(\"127.0.0.1\", 9000)); //设置客户端为非阻塞模式 clientChannel.configureBlocking(false); buf = ByteBuffer.allocate(1024); } catch (IOException e) { e.printStackTrace(); } } public void send(String fileName) { try { FileChannel fileChannel = FileChannel.open(Paths.get(fileName), StandardOpenOption.READ); while (fileChannel.read(buf) != -1) { buf.flip(); clientChannel.write(buf); buf.clear(); } System.out.println(\"客户端已发送文件\" + fileName); fileChannel.close(); clientChannel.close(); } catch (IOException e) { e.printStackTrace(); } } public static void main(String[] args) { ExecutorService pool = new ThreadPoolExecutor(50, 100, 1000, TimeUnit.MILLISECONDS, new LinkedBlockingDeque<>()); Instant begin = Instant.now(); for (int i = 0; i { NIOTCPClient client = new NIOTCPClient(); client.send(\"E:/1.jpeg\"); }); } pool.shutdown(); Instant end = Instant.now(); System.out.println(Duration.between(begin,end)); } } Java AIO 使用 对AIO来说，它不是在IO准备好时再通知线程，而是在IO操作已经完成后，再给线程发出通知。因此AIO是不会阻塞的，此时我们的业务逻辑将变成一个回调函数，等待IO操作完成后，由系统自动触发。 AIO的四步： 1、进程向操作系统请求数据 2、操作系统把外部数据加载到内核的缓冲区中， 3、操作系统把内核的缓冲区拷贝到进程的缓冲区 4、进程获得数据完成自己的功能 JDK1.7主要增加了三个新的异步通道： AsynchronousFileChannel: 用于文件异步读写； AsynchronousSocketChannel: 客户端异步socket； AsynchronousServerSocketChannel: 服务器异步socket。 因为AIO的实施需充分调用OS参与，IO需要操作系统支持、并发也同样需要操作系统的 在AIO socket编程中，服务端通道是AsynchronousServerSocketChannel，这个类提供了一个open()静态工厂，一个bind()方法用于绑定服务端IP地址（还有端口号)，另外还提供了accept()用于接收用户连接请求。在客户端使用的通道是AsynchronousSocketChannel,这个通道处理提供open静态工厂方法外，还提供了read和write方法。 在AIO编程中，发出一个事件（accept read write等)之后要指定事件处理类（回调函数)，AIO中的事件处理类是CompletionHandler，这个接口定义了如下两个方法，分别在异步操作成功和失败时被回调。 void completed(V result, A attachment); void failed(Throwable exc, A attachment); public class AIOServer { private static int PORT = 8080; private static int BUFFER_SIZE = 1024; private static String CHARSET = \"utf-8\"; //默认编码 private static CharsetDecoder decoder = Charset.forName(CHARSET).newDecoder(); //解码 private AsynchronousServerSocketChannel serverChannel; public AIOServer() { this.decoder = Charset.forName(CHARSET).newDecoder(); } private void listen() throws Exception { //打开一个服务通道 //绑定服务端口 this.serverChannel = AsynchronousServerSocketChannel.open().bind(new InetSocketAddress(PORT), 100); this.serverChannel.accept(this, new AcceptHandler()); } /** * accept到一个请求时的回调 */ private class AcceptHandler implements CompletionHandler { @Override public void completed(final AsynchronousSocketChannel client, AIOServer server) { try { System.out.println(\"远程地址：\" + client.getRemoteAddress()); //tcp各项参数 client.setOption(StandardSocketOptions.TCP_NODELAY, true); client.setOption(StandardSocketOptions.SO_SNDBUF, 1024); client.setOption(StandardSocketOptions.SO_RCVBUF, 1024); if (client.isOpen()) { System.out.println(\"client.isOpen：\" + client.getRemoteAddress()); final ByteBuffer buffer = ByteBuffer.allocate(BUFFER_SIZE); buffer.clear(); client.read(buffer, client, new ReadHandler(buffer)); } } catch (Exception e) { e.printStackTrace(); } finally { server.serverChannel.accept(server, this);// 监听新的请求，递归调用。 } } @Override public void failed(Throwable e, AIOServer attachment) { try { e.printStackTrace(); } finally { attachment.serverChannel.accept(attachment, this);// 监听新的请求，递归调用。 } } } /** * Read到请求数据的回调 */ private class ReadHandler implements CompletionHandler { private ByteBuffer buffer; public ReadHandler(ByteBuffer buffer) { this.buffer = buffer; } @Override public void completed(Integer result, AsynchronousSocketChannel client) { try { if (result { private ByteBuffer buffer; public WriteHandler(ByteBuffer buffer) { this.buffer = buffer; } @Override public void completed(Integer result, AsynchronousSocketChannel attachment) { buffer.clear(); AIOServer.close(attachment); } @Override public void failed(Throwable exc, AsynchronousSocketChannel attachment) { exc.printStackTrace(); AIOServer.close(attachment); } } private static void close(AsynchronousSocketChannel client) { try { client.close(); } catch (Exception e) { e.printStackTrace(); } } public static void main(String[] args) { try { System.out.println(\"正在启动服务...\"); AIOServer AIOServer = new AIOServer(); AIOServer.listen(); Thread t = new Thread(new Runnable() { @Override public void run() { while (true) { } } }); t.start(); } catch (Exception e) { e.printStackTrace(); } } } Java NIO 源码 关于Selector源码过于难以理解，可以先放过。Buffer public abstract class Buffer { /** * The characteristics of Spliterators that traverse and split elements * maintained in Buffers. */ static final int SPLITERATOR_CHARACTERISTICS = Spliterator.SIZED | Spliterator.SUBSIZED | Spliterator.ORDERED; // Invariants: mark = 0) { if (mark > pos) throw new IllegalArgumentException(\"mark > position: (\" + mark + \" > \" + pos + \")\"); this.mark = mark; } } } ByteBuffer有两种实现：HeapByteBuffer和DirectByteBuffer。 ByteBuffer#allocate public static ByteBuffer allocate(int capacity) { if (capacity ByteBuffer#allocateDirect public static ByteBuffer allocateDirect(int capacity) { return new DirectByteBuffer(capacity); } HeapByteBuffer（间接模式) 底层基于byte数组。初始化 HeapByteBuffer(int cap, int lim) { // package-private super(-1, 0, lim, cap, new byte[cap], 0); } 调用的是ByteBuffer的初始化方法 ByteBuffer(int mark, int pos, int lim, int cap, // package-private byte[] hb, int offset) { super(mark, pos, lim, cap); this.hb = hb; this.offset = offset; } ByteBuffer的独有成员变量： final byte[] hb; // Non-null only for heap buffers final int offset; boolean isReadOnly; // Valid only for heap buffers get public byte get() { return hb[ix(nextGetIndex())]; } final int nextGetIndex() { // package-private if (position >= limit) throw new BufferUnderflowException(); return position++; } protected int ix(int i) { return i + offset; } put public ByteBuffer put(byte x) { hb[ix(nextPutIndex())] = x; return this; } final int nextPutIndex() { // package-private if (position >= limit) throw new BufferOverflowException(); return position++; } DirectByteBuffer（直接模式) 底层基于c++的malloc分配的堆外内存，是使用Unsafe类分配的，底层调用了native方法。 在创建DirectByteBuffer的同时，创建一个与其对应的cleaner，cleaner是一个虚引用。 回收堆外内存的几种情况： 1)程序员手工释放，需要使用sun的非公开API实现。 2)申请新的堆外内存而内存不足时，会进行调用Cleaner（作为一个Reference)的静态方法tryHandlePending(false)，它又会调用cleaner的clean方法释放内存。 3)当DirectByteBuffer失去强引用,只有虚引用时，当等到某一次System.gc（full gc)（比如堆外内存达到XX:MaxDirectMemorySize)时，当DirectByteBuffer对象从pending状态 -> enqueue状态时，会触发Cleaner的clean()，而Cleaner的clean()的方法会实现通过unsafe对堆外内存的释放。初始化 重要成员变量： private final Cleaner cleaner; // Cached unsafe-access object protected static final Unsafe unsafe = Bits.unsafe(); Unsafe中很多都是native方法，底层调用c++代码。 DirectByteBuffer(int cap) { // package-private super(-1, 0, cap, cap); // 内存是否按页分配对齐 boolean pa = VM.isDirectMemoryPageAligned(); // 获取每页内存大小 int ps = Bits.pageSize(); // 分配内存的大小，如果是按页对齐方式，需要再加一页内存的容量 long size = Math.max(1L, (long)cap + (pa ? ps : 0)); // 用Bits类保存总分配内存(按页分配)的大小和实际内存的大小 Bits.reserveMemory(size, cap); long base = 0; try { // 在堆外内存的基地址，指定内存大小 base = unsafe.allocateMemory(size); } catch (OutOfMemoryError x) { Bits.unreserveMemory(size, cap); throw x; } unsafe.setMemory(base, size, (byte) 0); // 计算堆外内存的基地址 if (pa && (base % ps != 0)) { // Round up to page boundary address = base + ps - (base & (ps - 1)); } else { address = base; } cleaner = Cleaner.create(this, new Deallocator(base, size, cap)); att = null; } 第一行super调用的是其父类MappedByteBuffer的构造方法 MappedByteBuffer(int mark, int pos, int lim, int cap) { // package-private super(mark, pos, lim, cap); this.fd = null; } 而它的super又调用了ByteBuffer的构造方法 ByteBuffer(int mark, int pos, int lim, int cap) { // package-private this(mark, pos, lim, cap, null, 0); } Bits#reserveMemory 该方法用于在系统中保存总分配内存(按页分配)的大小和实际内存的大小。 总的来说，Bits.reserveMemory(size, cap)方法在可用堆外内存不足以分配给当前要创建的堆外内存大小时，会实现以下的步骤来尝试完成本次堆外内存的创建： ① 触发一次非堵塞的Reference#tryHandlePending(false)。该方法会将已经被JVM垃圾回收的DirectBuffer对象的堆外内存释放。 ② 如果进行一次堆外内存资源回收后，还不够进行本次堆外内存分配的话，则进行 System.gc()。System.gc()会触发一个full gc，但你需要知道，调用System.gc()并不能够保证full gc马上就能被执行。所以在后面打代码中，会进行最多9次尝试，看是否有足够的可用堆外内存来分配堆外内存。并且每次尝试之前，都对延迟等待时间，已给JVM足够的时间去完成full gc操作。 这里之所以用使用full gc的很重要的一个原因是：System.gc()会对新生代和老生代都会进行内存回收，这样会比较彻底地回收DirectByteBuffer对象以及他们关联的堆外内存. DirectByteBuffer对象本身其实是很小的，但是它后面可能关联了一个非常大的堆外内存，因此我们通常称之为冰山对象. 我们做young gc的时候会将新生代里的不可达的DirectByteBuffer对象及其堆外内存回收了，但是无法对old里的DirectByteBuffer对象及其堆外内存进行回收，这也是我们通常碰到的最大的问题。(并且堆外内存多用于生命期中等或较长的对象) 如果有大量的DirectByteBuffer对象移到了old，但是又一直没有做cms gc或者full gc，而只进行ygc，那么我们的物理内存可能被慢慢耗光，但是我们还不知道发生了什么，因为heap明明剩余的内存还很多(前提是我们禁用了System.gc – JVM参数DisableExplicitGC)。 注意，如果你设置了-XX:+DisableExplicitGC，将会禁用显示GC，这会使System.gc()调用无效。 ③ 如果9次尝试后依旧没有足够的可用堆外内存来分配本次堆外内存，则抛出OutOfMemoryError(\"Direct buffer memory”)异常。 static void reserveMemory(long size, int cap) { if (!memoryLimitSet && VM.isBooted()) { maxMemory = VM.maxDirectMemory(); memoryLimitSet = true; } // optimist! if (tryReserveMemory(size, cap)) { return; } final JavaLangRefAccess jlra = SharedSecrets.getJavaLangRefAccess(); // 如果系统中内存( 即，堆外内存 )不够的话： jlra.tryHandlePendingReference()会触发一次非堵塞的Reference#tryHandlePending(false)。该方法会将已经被JVM垃圾回收的DirectBuffer对象的堆外内存释放。 // retry while helping enqueue pending Reference objects // which includes executing pending Cleaner(s) which includes // Cleaner(s) that free direct buffer memory while (jlra.tryHandlePendingReference()) { if (tryReserveMemory(size, cap)) { return; } } // 如果在进行一次堆外内存资源回收后，还不够进行本次堆外内存分配的话，则 // trigger VM's Reference processing System.gc(); // a retry loop with exponential back-off delays // (this gives VM some time to do it's job) boolean interrupted = false; try { long sleepTime = 1; int sleeps = 0; while (true) { if (tryReserveMemory(size, cap)) { return; } // 9 if (sleeps >= MAX_SLEEPS) { break; } if (!jlra.tryHandlePendingReference()) { try { Thread.sleep(sleepTime); sleepTime } finally { if (interrupted) { // don't swallow interrupts Thread.currentThread().interrupt(); } } } Reference#tryHandlePending static boolean tryHandlePending(boolean waitForNotify) { Reference r; Cleaner c; try { synchronized (lock) { if (pending != null) { r = pending; // 'instanceof' might throw OutOfMemoryError sometimes // so do this before un-linking 'r' from the 'pending' chain... c = r instanceof Cleaner ? (Cleaner) r : null; // unlink 'r' from 'pending' chain pending = r.discovered; r.discovered = null; } else { // The waiting on the lock may cause an OutOfMemoryError // because it may try to allocate exception objects. if (waitForNotify) { lock.wait(); } // retry if waited return waitForNotify; } } } catch (OutOfMemoryError x) { // Give other threads CPU time so they hopefully drop some live references // and GC reclaims some space. // Also prevent CPU intensive spinning in case 'r instanceof Cleaner' above // persistently throws OOME for some time... Thread.yield(); // retry return true; } catch (InterruptedException x) { // retry return true; } // Fast path for cleaners if (c != null) { c.clean(); return true; } ReferenceQueue q = r.queue; if (q != ReferenceQueue.NULL) q.enqueue(r); return true; } Deallocator 后面是调用unsafe的分配堆外内存的方法，然后初始化了该DirectByteBuffer对应的cleaner。 注：在Cleaner 内部中通过一个列表，维护了一个针对每一个 directBuffer 的一个回收堆外内存的 线程对象(Runnable)，回收操作是发生在 Cleaner 的 clean() 方法中。 private static class Deallocator implements Runnable { private static Unsafe unsafe = Unsafe.getUnsafe(); private long address; private long size; private int capacity; private Deallocator(long address, long size, int capacity) { assert (address != 0); this.address = address; this.size = size; this.capacity = capacity; } public void run() { if (address == 0) { // Paranoia return; } unsafe.freeMemory(address); address = 0; Bits.unreserveMemory(size, capacity); } } Cleaner（回收) public class Cleaner extends PhantomReference { private static final ReferenceQueue dummyQueue = new ReferenceQueue(); private static Cleaner first = null; private Cleaner next = null; private Cleaner prev = null; private final Runnable thunk; private static synchronized Cleaner add(Cleaner var0) { if (first != null) { var0.next = first; first.prev = var0; } first = var0; return var0; } private static synchronized boolean remove(Cleaner var0) { if (var0.next == var0) { return false; } else { if (first == var0) { if (var0.next != null) { first = var0.next; } else { first = var0.prev; } } if (var0.next != null) { var0.next.prev = var0.prev; } if (var0.prev != null) { var0.prev.next = var0.next; } var0.next = var0; var0.prev = var0; return true; } } private Cleaner(Object var1, Runnable var2) { super(var1, dummyQueue); this.thunk = var2; } // var0是DirectByteBuffer，var1是Deallocator线程对象 public static Cleaner create(Object var0, Runnable var1) { return var1 == null ? null : add(new Cleaner(var0, var1)); } public void clean() { if (remove(this)) { try { // 回收该DirectByteBuffer对应的堆外内存 this.thunk.run(); } catch (final Throwable var2) { AccessController.doPrivileged(new PrivilegedAction() { public Void run() { if (System.err != null) { (new Error(\"Cleaner terminated abnormally\", var2)).printStackTrace(); } System.exit(1); return null; } }); } } } } Cleaner的构造方法中又调用了父类虚引用的构造方法： public PhantomReference(T referent, ReferenceQueue q) { super(referent, q); } get public byte get() { return ((unsafe.getByte(ix(nextGetIndex())))); } put public ByteBuffer put(byte x) { unsafe.putByte(ix(nextPutIndex()), ((x))); return this; } FileChannel（阻塞式) FileChannel的read、write和map通过其实现类FileChannelImpl实现。 FileChannelImpl的Oracle JDK没有提供源码，只能在OpenJDK中查看。open public static FileChannel open(Path path, OpenOption... options) throws IOException { Set set = new HashSet(options.length); Collections.addAll(set, options); return open(path, set, NO_ATTRIBUTES); } public static FileChannel open(Path path, Set options, FileAttribute... attrs) throws IOException { FileSystemProvider provider = path.getFileSystem().provider(); return provider.newFileChannel(path, options, attrs); } WindowsFileSystemProvider#newFileChannel public FileChannel newFileChannel(Path path, Set options, FileAttribute... attrs) throws IOException { if (path == null) throw new NullPointerException(); if (!(path instanceof WindowsPath)) throw new ProviderMismatchException(); WindowsPath file = (WindowsPath)path; WindowsSecurityDescriptor sd = WindowsSecurityDescriptor.fromAttribute(attrs); try { return WindowsChannelFactory .newFileChannel(file.getPathForWin32Calls(), file.getPathForPermissionCheck(), options, sd.address()); } catch (WindowsException x) { x.rethrowAsIOException(file); return null; } finally { if (sd != null) sd.release(); } } WindowsChannelFactory#newFileChannel static FileChannel newFileChannel(String pathForWindows, String pathToCheck, Set options, long pSecurityDescriptor) throws WindowsException { Flags flags = Flags.toFlags(options); // default is reading; append => writing if (!flags.read && !flags.write) { if (flags.append) { flags.write = true; } else { flags.read = true; } } // validation if (flags.read && flags.append) throw new IllegalArgumentException(\"READ + APPEND not allowed\"); if (flags.append && flags.truncateExisting) throw new IllegalArgumentException(\"APPEND + TRUNCATE_EXISTING not allowed\"); FileDescriptor fdObj = open(pathForWindows, pathToCheck, flags, pSecurityDescriptor); return FileChannelImpl.open(fdObj, pathForWindows, flags.read, flags.write, flags.append, null); } /** * Opens file based on parameters and options, returning a FileDescriptor * encapsulating the handle to the open file. */ private static FileDescriptor open(String pathForWindows, String pathToCheck, Flags flags, long pSecurityDescriptor) throws WindowsException { // set to true if file must be truncated after open boolean truncateAfterOpen = false; // map options int dwDesiredAccess = 0; if (flags.read) dwDesiredAccess |= GENERIC_READ; if (flags.write) dwDesiredAccess |= GENERIC_WRITE; int dwShareMode = 0; if (flags.shareRead) dwShareMode |= FILE_SHARE_READ; if (flags.shareWrite) dwShareMode |= FILE_SHARE_WRITE; if (flags.shareDelete) dwShareMode |= FILE_SHARE_DELETE; int dwFlagsAndAttributes = FILE_ATTRIBUTE_NORMAL; int dwCreationDisposition = OPEN_EXISTING; if (flags.write) { if (flags.createNew) { dwCreationDisposition = CREATE_NEW; // force create to fail if file is orphaned reparse point dwFlagsAndAttributes |= FILE_FLAG_OPEN_REPARSE_POINT; } else { if (flags.create) dwCreationDisposition = OPEN_ALWAYS; if (flags.truncateExisting) { // Windows doesn't have a creation disposition that exactly // corresponds to CREATE + TRUNCATE_EXISTING so we use // the OPEN_ALWAYS mode and then truncate the file. if (dwCreationDisposition == OPEN_ALWAYS) { truncateAfterOpen = true; } else { dwCreationDisposition = TRUNCATE_EXISTING; } } } } if (flags.dsync || flags.sync) dwFlagsAndAttributes |= FILE_FLAG_WRITE_THROUGH; if (flags.overlapped) dwFlagsAndAttributes |= FILE_FLAG_OVERLAPPED; if (flags.deleteOnClose) dwFlagsAndAttributes |= FILE_FLAG_DELETE_ON_CLOSE; // NOFOLLOW_LINKS and NOFOLLOW_REPARSEPOINT mean open reparse point boolean okayToFollowLinks = true; if (dwCreationDisposition != CREATE_NEW && (flags.noFollowLinks || flags.openReparsePoint || flags.deleteOnClose)) { if (flags.noFollowLinks || flags.deleteOnClose) okayToFollowLinks = false; dwFlagsAndAttributes |= FILE_FLAG_OPEN_REPARSE_POINT; } // permission check if (pathToCheck != null) { SecurityManager sm = System.getSecurityManager(); if (sm != null) { if (flags.read) sm.checkRead(pathToCheck); if (flags.write) sm.checkWrite(pathToCheck); if (flags.deleteOnClose) sm.checkDelete(pathToCheck); } } // open file long handle = CreateFile(pathForWindows, dwDesiredAccess, dwShareMode, pSecurityDescriptor, dwCreationDisposition, dwFlagsAndAttributes); // make sure this isn't a symbolic link. if (!okayToFollowLinks) { try { if (WindowsFileAttributes.readAttributes(handle).isSymbolicLink()) throw new WindowsException(\"File is symbolic link\"); } catch (WindowsException x) { CloseHandle(handle); throw x; } } // truncate file (for CREATE + TRUNCATE_EXISTING case) if (truncateAfterOpen) { try { SetEndOfFile(handle); } catch (WindowsException x) { CloseHandle(handle); throw x; } } // make the file sparse if needed if (dwCreationDisposition == CREATE_NEW && flags.sparse) { try { DeviceIoControlSetSparse(handle); } catch (WindowsException x) { // ignore as sparse option is hint } } // create FileDescriptor and return FileDescriptor fdObj = new FileDescriptor(); fdAccess.setHandle(fdObj, handle); return fdObj; } static long CreateFile(String path, int dwDesiredAccess, int dwShareMode, long lpSecurityAttributes, int dwCreationDisposition, int dwFlagsAndAttributes) throws WindowsException { NativeBuffer buffer = asNativeBuffer(path); try { return CreateFile0(buffer.address(), dwDesiredAccess, dwShareMode, lpSecurityAttributes, dwCreationDisposition, dwFlagsAndAttributes); } finally { buffer.release(); } } private static native long CreateFile0(long lpFileName, int dwDesiredAccess, int dwShareMode, long lpSecurityAttributes, int dwCreationDisposition, int dwFlagsAndAttributes) throws WindowsException; read public int read(ByteBuffer dst) throws IOException { ensureOpen(); if (!readable) throw new NonReadableChannelException(); synchronized (positionLock) { int n = 0; int ti = -1; try { begin(); ti = threads.add(); if (!isOpen()) return 0; do { n = IOUtil.read(fd, dst, -1, nd); } while ((n == IOStatus.INTERRUPTED) && isOpen()); return IOStatus.normalize(n); } finally { threads.remove(ti); end(n > 0); assert IOStatus.check(n); } } } IOUtil.read static int read(FileDescriptor fd, ByteBuffer dst, long position, NativeDispatcher nd) IOException { if (dst.isReadOnly()) throw new IllegalArgumentException(\"Read-only buffer\"); if (dst instanceof DirectBuffer) return readIntoNativeBuffer(fd, dst, position, nd); // Substitute a native buffer ByteBuffer bb = Util.getTemporaryDirectBuffer(dst.remaining()); try { int n = readIntoNativeBuffer(fd, bb, position, nd); bb.flip(); if (n > 0) dst.put(bb); return n; } finally { Util.offerFirstTemporaryDirectBuffer(bb); } } 通过上述实现可以看出，基于channel的文件数据读取步骤如下： 1、申请一块和缓存同大小的DirectByteBuffer bb。 2、读取数据到缓存bb，底层由NativeDispatcher的read实现。 3、把bb的数据读取到dst（用户定义的缓存，在jvm中分配内存)。 read方法导致数据复制了两次。 write public int write(ByteBuffer src) throws IOException { ensureOpen(); if (!writable) throw new NonWritableChannelException(); synchronized (positionLock) { int n = 0; int ti = -1; try { begin(); ti = threads.add(); if (!isOpen()) return 0; do { n = IOUtil.write(fd, src, -1, nd); } while ((n == IOStatus.INTERRUPTED) && isOpen()); return IOStatus.normalize(n); } finally { threads.remove(ti); end(n > 0); assert IOStatus.check(n); } } } IOUtil.write static int write(FileDescriptor fd, ByteBuffer src, long position, NativeDispatcher nd) throws IOException { if (src instanceof DirectBuffer) return writeFromNativeBuffer(fd, src, position, nd); // Substitute a native buffer int pos = src.position(); int lim = src.limit(); assert (pos bb.put(src); bb.flip(); // Do not update src until we see how many bytes were written src.position(pos); int n = writeFromNativeBuffer(fd, bb, position, nd); if (n > 0) { // now update src src.position(pos + n); } return n; } finally { Util.offerFirstTemporaryDirectBuffer(bb); } } 基于channel的文件数据写入步骤如下： 1、申请一块DirectByteBuffer，bb大小为byteBuffer中的limit - position。 2、复制byteBuffer中的数据到bb中。 3、把数据从bb中写入到文件，底层由NativeDispatcher的write实现，具体如下： private static int writeFromNativeBuffer(FileDescriptor fd, ByteBuffer bb, long position, NativeDispatcher nd) throws IOException { int pos = bb.position(); int lim = bb.limit(); assert (pos 0) bb.position(pos + written); return written; } write方法也导致了数据复制了两次。 ServerSocketChannel 它的实现类是ServerSocketChannelImpl，同样是闭源的。open public static ServerSocketChannel open() throws IOException { return SelectorProvider.provider().openServerSocketChannel(); } SelectorProvider.provider()方法在windows平台下返回的是SelectorProvider 的实现类 WindowsSelectorProvider类的实例。 WindowsSelectorProvider类的直接父类为SelectorProviderImpl； SelectorProviderImpl 的直接父类是 SelectorProvider。 SelectorProviderImpl# openServerSocketChannel public ServerSocketChannel openServerSocketChannel() throws IOException { return new ServerSocketChannelImpl(this); } - ServerSocketChannelImpl(SelectorProvider var1) throws IOException { super(var1); this.fd = Net.serverSocket(true); this.fdVal = IOUtil.fdVal(this.fd); this.state = 0; } - super(var1)实际上是父类的构造方法 protected AbstractSelectableChannel(SelectorProvider provider) { this.provider = provider; } Net#serverSocket 创建一个FileDescriptor static FileDescriptor serverSocket(boolean stream) { return IOUtil.newFD(socket0(isIPv6Available(), stream, true, fastLoopback)); } bind public ServerSocketChannel bind(SocketAddress local, int backlog) throws IOException { synchronized (lock) { if (!isOpen()) throw new ClosedChannelException(); if (isBound()) throw new AlreadyBoundException(); InetSocketAddress isa = (local == null) ? new InetSocketAddress(0) : Net.checkAddress(local); SecurityManager sm = System.getSecurityManager(); if (sm != null) sm.checkListen(isa.getPort()); NetHooks.beforeTcpBind(fd, isa.getAddress(), isa.getPort()); Net.bind(fd, isa.getAddress(), isa.getPort()); Net.listen(fd, backlog register Selector是通过Selector.open方法获得的。 将这个通道channel注册到指定的selector中，返回一个SelectionKey对象实例。 register这个方法在实现代码上的逻辑有以下四点： 1、首先检查通道channel是否是打开的，如果不是打开的，则抛异常，如果是打开的，则进行 2。 2、检查指定的interest集合是否是有效的。如果没效，则抛异常。否则进行 3。这里要特别强调一下：对于ServerSocketChannel仅仅支持”新的连接”，因此interest集合ops满足ops&~sectionKey.OP_ACCEPT!=0,即对于ServerSocketChannel注册到Selector中时的事件只能包括SelectionKey.OP_ACCEPT。 3、对通道进行了阻塞模式的检查，如果不是阻塞模式，则抛异常，否则进行4. 4、得到当前通道在指定Selector上的SelectionKey，假设结果用k表示。下面对k是否为null有不同的处理。如果k不为null，则说明此通道channel已经在Selector上注册过了，则直接将指定的ops添加进SelectionKey中即可。如果k为null,则说明此通道还没有在Selector上注册，则需要先进行注册，然后为其对应的SelectionKey设置给定值ops。 与Selector一起使用时，Channel必须处于非阻塞模式下。这意味着不能将FileChannel与Selector一起使用，因为FileChannel不能切换到非阻塞模式。而套接字通道都可以。 public final SelectionKey register(Selector sel, int ops, Object att) throws ClosedChannelException { synchronized (regLock) { if (!isOpen()) throw new ClosedChannelException(); if ((ops & ~validOps()) != 0) throw new IllegalArgumentException(); if (blocking) throw new IllegalBlockingModeException(); // 得到当前通道在指定Selector上的SelectionKey（复合事件) SelectionKey k = findKey(sel); 如果k不为null，则说明此通道已经在Selector上注册过了，则直接将指定的ops添加进SelectionKey中即可。 如果k为null,则说明此通道还没有在Selector上注册，则需要先进行注册，然后添加SelectionKey。 if (k != null) { k.interestOps(ops); k.attach(att); } if (k == null) { // New registration synchronized (keyLock) { if (!isOpen()) throw new ClosedChannelException(); k = ((AbstractSelector)sel).register(this, ops, att); addKey(k); } } return k; } private SelectionKey findKey(Selector sel) { synchronized (keyLock) { if (keys == null) return null; for (int i = 0; i Selector（如何实现Channel多路复用) SocketChannel、ServerSocketChannel和Selector的实例初始化都通过SelectorProvider类实现，其中Selector是整个NIO Socket的核心实现。 SelectorProvider在windows和linux下有不同的实现，provider方法会返回对应的实现。成员变量 1.- final class WindowsSelectorImpl extends SelectorImpl2.- {3. private final int INIT_CAP = 8;//选择key集合，key包装集合初始化容量 4. private static final int MAX_SELECTABLE_FDS = 1024;//最大选择key数量 5. private SelectionKeyImpl channelArray[];//选择器关联通道集合 6. private PollArrayWrapper pollWrapper;//存放所有文件描述对象（选择key，唤醒管道的source与sink通道)的集合 7. private int totalChannels;//注册到选择的通道数量 8. private int threadsCount;//选择线程数 9. private final List threads = new ArrayList();//选择操作线程集合 10. private final Pipe wakeupPipe = Pipe.open();//唤醒等待选择操作的管道 11. private final int wakeupSourceFd;//唤醒管道源通道文件描述 12. private final int wakeupSinkFd;//唤醒管道sink通道文件描述 13. private Object closeLock;//选择器关闭同步锁 14. private final FdMap fdMap = new FdMap();//存放选择key文件描述与选择key映射关系的Map 15. private final SubSelector subSelector = new SubSelector();//子选择器 16. private long timeout;//超时时间，具体什么意思，现在还没明白，在后面在看 17. private final Object interruptLock = new Object();//中断同步锁，在唤醒选择操作线程时，用于同步 18. private volatile boolean interruptTriggered;//是否唤醒等待选择操的线程 19. private final StartLock startLock = new StartLock();//选择操作开始锁 20. private final FinishLock finishLock = new FinishLock();//选择操作结束锁 21. private long updateCount;//更新数量，具体什么意思，现在还没明白，在后面在看 22.- static final boolean $assertionsDisabled = !sun/nio/ch/WindowsSelectorImpl.desiredAssertionStatus();23.- static24.- {25.- //加载nio，net资源库26.- Util.load();27.- }28.- } open public static Selector open() throws IOException { return SelectorProvider.provider().openSelector(); } WindowsSelectorProvider#openSelector public AbstractSelector openSelector() throws IOException { return new WindowsSelectorImpl(this); } 初始化一个wakeupPipe WindowsSelectorImpl(SelectorProvider var1) throws IOException { super(var1); this.wakeupSourceFd = ((SelChImpl)this.wakeupPipe.source()).getFDVal(); SinkChannelImpl var2 = (SinkChannelImpl)this.wakeupPipe.sink(); var2.sc.socket().setTcpNoDelay(true); this.wakeupSinkFd = var2.getFDVal(); this.pollWrapper.addWakeupSocket(this.wakeupSourceFd, 0); } SelectorImpl#register 第一个参数是ServerSocketChannel，第二个参数是复合事件，第三个是附件。 1、以当前channel和selector为参数，初始化SelectionKeyImpl 对象selectionKeyImpl ，并添加附件attachment。 2、如果当前channel的数量totalChannels等于SelectionKeyImpl数组大小，对SelectionKeyImpl数组和pollWrapper进行扩容操作。 3、如果totalChannels % MAXSELECTABLEFDS == 0，则多开一个线程处理selector。 4、pollWrapper.addEntry将把selectionKeyImpl中的socket句柄添加到对应的pollfd。 5、k.interestOps(ops)方法最终也会把event添加到对应的pollfd。 所以，不管serverSocketChannel，还是socketChannel，在selector注册的事件，最终都保存在pollArray中。 protected final SelectionKey register(AbstractSelectableChannel ch, int ops, Object attachment) { if (!(ch instanceof SelChImpl)) throw new IllegalSelectorException(); SelectionKeyImpl k = new SelectionKeyImpl((SelChImpl)ch, this); k.attach(attachment); synchronized (publicKeys) { implRegister(k); } k.interestOps(ops); return k; } WindowsSelectorImpl#implRegister protected void implRegister(SelectionKeyImpl ski) { synchronized (closeLock) { if (pollWrapper == null) throw new ClosedSelectorException(); growIfNeeded(); channelArray[totalChannels] = ski; ski.setIndex(totalChannels); fdMap.put(ski); keys.add(ski); pollWrapper.addEntry(totalChannels, ski); totalChannels++; } } select（返回有事件发生的SelectionKey数量) var1是timeout时间，无参数的版本对应的timeout为0. select(long timeout)和select()一样，除了最长会阻塞timeout毫秒(参数)。 这个方法并不能提供精确时间的保证，和当执行wait(long timeout)方法时并不能保证会延时timeout道理一样。 这里的timeout说明如下： 如果 timeout为正，则select(long timeout)在等待有通道被选择时至多会阻塞timeout毫秒 如果timeout为零，则永远阻塞直到有至少一个通道准备就绪。 timeout不能为负数。 public int select(long timeout) throws IOException { if (timeout selectNow（非阻塞版本) public int selectNow() throws IOException { return this.lockAndDoSelect(0L); } private int lockAndDoSelect(long timeout) throws IOException { synchronized (this) { if (!isOpen()) throw new ClosedSelectorException(); synchronized (publicKeys) { synchronized (publicSelectedKeys) { return doSelect(timeout); } } } } WindowsSelectorImpl#doSelect protected int doSelect(long timeout) throws IOException { if (channelArray == null) throw new ClosedSelectorException(); this.timeout = timeout; // set selector timeout processDeregisterQueue(); if (interruptTriggered) { resetWakeupSocket(); return 0; } // Calculate number of helper threads needed for poll. If necessary // threads are created here and start waiting on startLock adjustThreadsCount(); finishLock.reset(); // reset finishLock // Wakeup helper threads, waiting on startLock, so they start polling. // Redundant threads will exit here after wakeup. startLock.startThreads(); // do polling in the main thread. Main thread is responsible for // first MAX_SELECTABLE_FDS entries in pollArray. try { begin(); try { subSelector.poll(); } catch (IOException e) { finishLock.setException(e); // Save this exception } // Main thread is out of poll(). Wakeup others and wait for them if (threads.size() > 0) finishLock.waitForHelperThreads(); } finally { end(); } // Done with poll(). Set wakeupSocket to nonsignaled for the next run. finishLock.checkForException(); processDeregisterQueue(); int updated = updateSelectedKeys(); // Done with poll(). Set wakeupSocket to nonsignaled for the next run. resetWakeupSocket(); return updated; } 其中 subSelector.poll() 是select的核心，由native函数poll0实现，readFds、writeFds 和exceptFds数组用来保存底层select的结果，数组的第一个位置都是存放发生事件的socket的总数，其余位置存放发生事件的socket句柄fd。 private int poll() throws IOException { return this.poll0(WindowsSelectorImpl.this.pollWrapper.pollArrayAddress, Math.min(WindowsSelectorImpl.this.totalChannels, 1024), this.readFds, this.writeFds, this.exceptFds, WindowsSelectorImpl.this.timeout); } private native int poll0(long pollAddress, int numfds, int[] readFds, int[] writeFds, int[] exceptFds, long timeout); 在src/windows/native/sun/nio/ch/WindowsSelectorImpl.c中找到了该方法的实现 define FD_SETSIZE 1024 Java_sun_nio_ch_WindowsSelectorImpl_00024SubSelector_poll0(JNIEnv *env, jobject this, jlong pollAddress, jint numfds, jintArray returnReadFds, jintArray returnWriteFds, jintArray returnExceptFds, jlong timeout) { DWORD result = 0; pollfd *fds = (pollfd *) pollAddress; int i; FD_SET readfds, writefds, exceptfds; struct timeval timevalue, *tv; static struct timeval zerotime = {0, 0}; int read_count = 0, write_count = 0, except_count = 0; #ifdef _WIN64 int resultbuf[FD_SETSIZE + 1]; #endif if (timeout == 0) { tv = &zerotime; } else if (timeout tv_sec = (long)(timeout / 1000); tv->tv_usec = (long)((timeout % 1000) * 1000); } /* Set FD_SET structures required for select */ for (i = 0; i SetIntArrayRegion(env, returnReadFds, 0, readfds.fd_count + 1, resultbuf); resultbuf[0] = writefds.fd_count; for (i = 0; i SetIntArrayRegion(env, returnWriteFds, 0, writefds.fd_count + 1, resultbuf); resultbuf[0] = exceptfds.fd_count; for (i = 0; i SetIntArrayRegion(env, returnExceptFds, 0, exceptfds.fd_count + 1, resultbuf); #else (*env)->SetIntArrayRegion(env, returnReadFds, 0, readfds.fd_count + 1, (jint *)&readfds); (*env)->SetIntArrayRegion(env, returnWriteFds, 0, writefds.fd_count + 1, (jint *)&writefds); (*env)->SetIntArrayRegion(env, returnExceptFds, 0, exceptfds.fd_count + 1, (jint *)&exceptfds); #endif return 0; } 执行 selector.select() ，poll0函数把指向socket句柄和事件的内存地址传给底层函数。 1、如果之前没有发生事件，程序就阻塞在select处，当然不会一直阻塞，因为epoll在timeout时间内如果没有事件，也会返回； 2、一旦有对应的事件发生，poll0方法就会返回； 3、processDeregisterQueue方法会清理那些已经cancelled的SelectionKey； 4、updateSelectedKeys方法统计有事件发生的SelectionKey数量，并把符合条件发生事件的SelectionKey添加到selectedKeys哈希表中，提供给后续使用。 如何判断是否有事件发生?（native) poll0()会监听pollWrapper中的FD有没有数据进出，这会造成IO阻塞，直到有数据读写事件发生。 比如，由于pollWrapper中保存的也有ServerSocketChannel的FD，所以只要ClientSocket发一份数据到ServerSocket,那么poll0()就会返回； 又由于pollWrapper中保存的也有pipe的write端的FD，所以只要pipe的write端向FD发一份数据，也会造成poll0()返回； 如果这两种情况都没有发生，那么poll0()就一直阻塞，也就是selector.select()会一直阻塞；如果有任何一种情况发生，那么selector.select()就会返回。 在早期的JDK1.4和1.5 update10版本之前，Selector基于select/poll模型实现，是基于IO复用技术的非阻塞IO，不是异步IO。在JDK1.5 update10和linux core2.6以上版本，sun优化了Selctor的实现，底层使用epoll替换了select/poll。 epoll是Linux下的一种IO多路复用技术，可以非常高效的处理数以百万计的socket句柄。 在Windows下是IOCP WindowsSelectorImpl.wakeup() public Selector wakeup() { synchronized (interruptLock) { if (!interruptTriggered) { setWakeupSocket(); interruptTriggered = true; } } return this; } private void setWakeupSocket() { setWakeupSocket0(wakeupSinkFd); } private native void setWakeupSocket0(int wakeupSinkFd); Java_sun_nio_ch_WindowsSelectorImpl_setWakeupSocket0(JNIEnv *env, jclass this, jint scoutFd) { /* Write one byte into the pipe */ const char byte = 1; send(scoutFd, &byte, 1, 0); } 这里完成了向最开始建立的pipe的sink端写入了一个字节，source文件描述符就会处于就绪状态，poll方法会返回，从而导致select方法返回。（原来自己建立一个socket链着自己另外一个socket就是为了这个目的) Java AIO 源码 AsynchronousFileChannle（AIO,基于CompletionHandler回调) 在Java 7中，AsynchronousFileChannel被添加到Java NIO。AsynchronousFileChannel使读取数据，并异步地将数据写入文件成为可能。open Path path = Paths.get(\"data/test.xml\"); AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.READ); public static AsynchronousFileChannel open(Path file, Set options, ExecutorService executor, FileAttribute... attrs) throws IOException { FileSystemProvider provider = file.getFileSystem().provider(); return provider.newAsynchronousFileChannel(file, options, executor, attrs); } WindowsChannelFactory#newAsynchronousFileChannel static AsynchronousFileChannel newAsynchronousFileChannel(String pathForWindows, String pathToCheck, Set options, long pSecurityDescriptor, ThreadPool pool) throws IOException { Flags flags = Flags.toFlags(options); // Overlapped I/O required flags.overlapped = true; // default is reading if (!flags.read && !flags.write) { flags.read = true; } // validation if (flags.append) throw new UnsupportedOperationException(\"APPEND not allowed\"); // open file for overlapped I/O FileDescriptor fdObj; try { fdObj = open(pathForWindows, pathToCheck, flags, pSecurityDescriptor); } catch (WindowsException x) { x.rethrowAsIOException(pathForWindows); return null; } // create the AsynchronousFileChannel try { return WindowsAsynchronousFileChannelImpl.open(fdObj, flags.read, flags.write, pool); } catch (IOException x) { // IOException is thrown if the file handle cannot be associated // with the completion port. All we can do is close the file. long handle = fdAccess.getHandle(fdObj); CloseHandle(handle); throw x; } WindowsAsynchronousFileChannelImpl#open public static AsynchronousFileChannel open(FileDescriptor fdo, boolean reading, boolean writing, ThreadPool pool) throws IOException { Iocp iocp; boolean isDefaultIocp; if (pool == null) { iocp = DefaultIocpHolder.defaultIocp; isDefaultIocp = true; } else { iocp = new Iocp(null, pool).start(); isDefaultIocp = false; } try { return new WindowsAsynchronousFileChannelImpl(fdo, reading, writing, iocp, isDefaultIocp); } catch (IOException x) { // error binding to port so need to close it (if created for this channel) if (!isDefaultIocp) iocp.implClose(); throw x; } } read write Netty NIO 基于这个语境，Netty目前的版本是没有把IO操作交过操作系统处理的，所以是属于同步的。如果别人说Netty是异步非阻塞，如果要深究，那真要看看Netty新的版本是否把IO操作交过操作系统处理，或者看看有否使用JDK1.7中的AIO API，否则他们说的异步其实是指客户端程序调用Netty的IO操作API“不停顿等待”。 很多人所讲的异步其实指的是编程模型上的异步（即回调)，而非应用程序的异步。 NIO与Epoll Linux2.6之后支持epoll windows支持select而不支持epoll 不同系统下nio的实现是不一样的，包括Sunos linux 和windows select的复杂度为O(N) select有最大fd限制，默认为1024 修改sys/select.h可以改变select的fd数量限制 epoll的事件模型，无fd数量限制，复杂度O(1),不需要遍历fd 1.17 动态代理 静态代理：代理类是在编译时就实现好的。也就是说 Java 编译完成后代理类是一个实际的 class 文件。 动态代理：代理类是在运行时生成的。也就是说 Java 编译完之后并没有实际的 class 文件，而是在运行时动态生成的类字节码，并加载到JVM中。 JDK动态代理是由Java内部的反射机制+动态生成字节码来实现的，cglib动态代理底层则是借助asm来实现的。总的来说，反射机制在生成类的过程中比较高效，而asm在生成类之后的相关执行过程中比较高效（可以通过将asm生成的类进行缓存，这样解决asm生成类过程低效问题)。还有一点必须注意：JDK动态代理的应用前提，必须是目标类基于统一的接口。如果没有上述前提，JDK动态代理不能应用。由此可以看出，JDK动态代理有一定的局限性，cglib这种第三方类库实现的动态代理应用更加广泛，且在效率上更有优势。 前者必须基于接口，后者不需要接口，是基于继承的，但是不能代理final类和final方法； JDK采用反射机制调用委托类的方法，CGLIB采用类似索引的方式直接调用委托类方法； 前者效率略低于后者效率，CGLIB效率略高（不是一定的)JDK动态代理 使用 Proxy类（代理类)的设计用到代理模式的设计思想，Proxy类对象实现了代理目标的所有接口，并代替目标对象进行实际的操作。代理的目的是在目标对象方法的基础上作增强，这种增强的本质通常就是对目标对象的方法进行拦截。所以，Proxy应该包括一个方法拦截器，来指示当拦截到方法调用时作何种处理。InvocationHandler就是拦截器的接口。 Proxy (代理) 提供用于创建动态代理类和实例的静态方法，它还是由这些方法创建的所有动态代理类的超类。 动态代理类（代理类)是一个实现在创建类时在运行时指定的接口列表的类 ，代理接口是代理类实现的一个接口。代理实例 是代理类的一个实例。 每个代理实例都有一个关联的调用处理程序对象，它可以实现接口 InvocationHandler。（拦截器) 在Java中怎样实现动态代理呢？ 第一步，我们要有一个接口，还要有一个接口的实现类，而这个实现类呢就是我们要代理的对象， 所谓代理呢也就是在调用实现类的方法时，可以在方法执行前后做额外的工作。 第二步，我们要自己写一个在代理类的方法要执行时，能够做额外工作的类（拦截器)，而这个类必须继承InvocationHandler接口， 为什么要继承它呢？因为代理类的实例在调用实现类的方法的时候，不会调用真正的实现类的这个方法， 而是转而调用这个类的invoke方法（继承时必须实现的方法)，在这个方法中你可以调用真正的实现类的这个方法。 JDK动态代理 原理 Proxy#newProxyInstance 会返回一个实现了指定接口的代理对象，对该对象的所有方法调用都会转发给InvocationHandler.invoke()方法。 public static Object newProxyInstance(ClassLoader loader, Class[] interfaces, InvocationHandler h) throws IllegalArgumentException { Objects.requireNonNull(h); final Class[] intfs = interfaces.clone(); final SecurityManager sm = System.getSecurityManager(); if (sm != null) { checkProxyAccess(Reflection.getCallerClass(), loader, intfs); } /* * Look up or generate the designated proxy class. */ // 生成代理类的class Class cl = getProxyClass0(loader, intfs); /* * Invoke its constructor with the designated invocation handler. */ try { if (sm != null) { checkNewProxyPermission(Reflection.getCallerClass(), cl); } // 获取代理对象的构造方法（也就是$Proxy0(InvocationHandler h)) final Constructor cons = cl.getConstructor(constructorParams); final InvocationHandler ih = h; if (!Modifier.isPublic(cl.getModifiers())) { AccessController.doPrivileged(new PrivilegedAction() { public Void run() { cons.setAccessible(true); return null; } }); } // 生成代理类的实例并把InvocationHandlerImpl的实例传给它的构造方法 return cons.newInstance(new Object[]{h}); } catch (IllegalAccessException|InstantiationException e) { throw new InternalError(e.toString(), e); } catch (InvocationTargetException e) { Throwable t = e.getCause(); if (t instanceof RuntimeException) { throw (RuntimeException) t; } else { throw new InternalError(t.toString(), t); } } catch (NoSuchMethodException e) { throw new InternalError(e.toString(), e); } } 1)getProxyClass0（生成代理类的class) 最终生成是通过ProxyGenerator的generateProxyClass方法实现的。 private static Class getProxyClass0(ClassLoader loader, Class... interfaces) { if (interfaces.length > 65535) { throw new IllegalArgumentException(\"interface limit exceeded\"); } // If the proxy class defined by the given loader implementing // the given interfaces exists, this will simply return the cached copy; // otherwise, it will create the proxy class via the ProxyClassFactory return proxyClassCache.get(loader, interfaces); } private static final WeakCache[], Class> proxyClassCache = new WeakCache<>(new KeyFactory(), new ProxyClassFactory()); @param type of keys @param type of parameters @param type of values */ final class WeakCache {} public V get(K key, P parameter) { Objects.requireNonNull(parameter); expungeStaleEntries(); Object cacheKey = CacheKey.valueOf(key, refQueue); // lazily install the 2nd level valuesMap for the particular cacheKey ConcurrentMap> valuesMap = map.get(cacheKey); if (valuesMap == null) { ConcurrentMap> oldValuesMap = map.putIfAbsent(cacheKey, valuesMap = new ConcurrentHashMap<>()); if (oldValuesMap != null) { valuesMap = oldValuesMap; } } // create subKey and retrieve the possible Supplier stored by that // subKey from valuesMap Object subKey = Objects.requireNonNull(subKeyFactory.apply(key, parameter)); Supplier supplier = valuesMap.get(subKey); Factory factory = null; while (true) { if (supplier != null) { // supplier might be a Factory or a CacheValue instance // supplier是Factory,这个类定义在WeakCache的内部。 V value = supplier.get(); if (value != null) { return value; } } // else no supplier in cache // or a supplier that returned null (could be a cleared CacheValue // or a Factory that wasn't successful in installing the CacheValue) // lazily construct a Factory if (factory == null) { factory = new Factory(key, parameter, subKey, valuesMap); } if (supplier == null) { supplier = valuesMap.putIfAbsent(subKey, factory); if (supplier == null) { // successfully installed Factory supplier = factory; } // else retry with winning supplier } else { if (valuesMap.replace(subKey, supplier, factory)) { // successfully replaced // cleared CacheEntry / unsuccessful Factory // with our Factory supplier = factory; } else { // retry with current supplier supplier = valuesMap.get(subKey); } } } } Factory private final class Factory implements Supplier { private final K key; private final P parameter; private final Object subKey; private final ConcurrentMap> valuesMap; Factory(K key, P parameter, Object subKey, ConcurrentMap> valuesMap) { this.key = key; this.parameter = parameter; this.subKey = subKey; this.valuesMap = valuesMap; } @Override public synchronized V get() { // serialize access // re-check Supplier supplier = valuesMap.get(subKey); if (supplier != this) { // something changed while we were waiting: // might be that we were replaced by a CacheValue // or were removed because of failure -> // return null to signal WeakCache.get() to retry // the loop return null; } // else still us (supplier == this) // create new value // 创建新的class V value = null; try { value = Objects.requireNonNull(valueFactory.apply(key, parameter)); } finally { if (value == null) { // remove us on failure valuesMap.remove(subKey, this); } } // the only path to reach here is with non-null value assert value != null; // wrap value with CacheValue (WeakReference) CacheValue cacheValue = new CacheValue<>(value); // try replacing us with CacheValue (this should always succeed) if (valuesMap.replace(subKey, this, cacheValue)) { // put also in reverseMap reverseMap.put(cacheValue, Boolean.TRUE); } else { throw new AssertionError(\"Should not reach here\"); } // successfully replaced us with new CacheValue -> return the value // wrapped by it return value; } } private static final class ProxyClassFactory implements BiFunction[], Class> { // prefix for all proxy class names private static final String proxyClassNamePrefix = \"$Proxy\"; // next number to use for generation of unique proxy class names private static final AtomicLong nextUniqueNumber = new AtomicLong(); @Override public Class apply(ClassLoader loader, Class[] interfaces) { Map, Boolean> interfaceSet = new IdentityHashMap<>(interfaces.length); for (Class intf : interfaces) { /* * Verify that the class loader resolves the name of this * interface to the same Class object. */ Class interfaceClass = null; try { interfaceClass = Class.forName(intf.getName(), false, loader); } catch (ClassNotFoundException e) { } if (interfaceClass != intf) { throw new IllegalArgumentException( intf + \" is not visible from class loader\"); } /* * Verify that the Class object actually represents an * interface. */ if (!interfaceClass.isInterface()) { throw new IllegalArgumentException( interfaceClass.getName() + \" is not an interface\"); } /* * Verify that this interface is not a duplicate. */ if (interfaceSet.put(interfaceClass, Boolean.TRUE) != null) { throw new IllegalArgumentException( \"repeated interface: \" + interfaceClass.getName()); } } String proxyPkg = null; // package to define proxy class in int accessFlags = Modifier.PUBLIC | Modifier.FINAL; /* * Record the package of a non-public proxy interface so that the * proxy class will be defined in the same package. Verify that * all non-public proxy interfaces are in the same package. */ for (Class intf : interfaces) { int flags = intf.getModifiers(); if (!Modifier.isPublic(flags)) { accessFlags = Modifier.FINAL; String name = intf.getName(); int n = name.lastIndexOf('.'); String pkg = ((n == -1) ? \"\" : name.substring(0, n + 1)); if (proxyPkg == null) { proxyPkg = pkg; } else if (!pkg.equals(proxyPkg)) { throw new IllegalArgumentException( \"non-public interfaces from different packages\"); } } } if (proxyPkg == null) { // if no non-public proxy interfaces, use com.sun.proxy package proxyPkg = ReflectUtil.PROXY_PACKAGE + \".\"; } /* * Choose a name for the proxy class to generate. */ long num = nextUniqueNumber.getAndIncrement(); String proxyName = proxyPkg + proxyClassNamePrefix + num; /* * Generate the specified proxy class. */ byte[] proxyClassFile = ProxyGenerator.generateProxyClass( proxyName, interfaces, accessFlags); try { return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length); } catch (ClassFormatError e) { /* * A ClassFormatError here means that (barring bugs in the * proxy class generation code) there was some other * invalid aspect of the arguments supplied to the proxy * class creation (such as virtual machine limitations * exceeded). */ throw new IllegalArgumentException(e.toString()); } } } 重点！ /** * Generate a proxy class given a name and a list of proxy interfaces. * * @param name the class name of the proxy class * @param interfaces proxy interfaces * @param accessFlags access flags of the proxy class */ public static byte[] generateProxyClass(final String name, Class[] interfaces, int accessFlags) { ProxyGenerator gen = new ProxyGenerator(name, interfaces, accessFlags); final byte[] classFile = gen.generateClassFile(); if (saveGeneratedFiles) { java.security.AccessController.doPrivileged( new java.security.PrivilegedAction() { public Void run() { try { int i = name.lastIndexOf('.'); Path path; if (i > 0) { Path dir = Paths.get(name.substring(0, i).replace('.', File.separatorChar)); Files.createDirectories(dir); path = dir.resolve(name.substring(i+1, name.length()) + \".class\"); } else { path = Paths.get(name + \".class\"); } Files.write(path, classFile); return null; } catch (IOException e) { throw new InternalError( \"I/O exception saving generated file: \" + e); } } }); } return classFile; } ProxyGenerator#generateClassFIle /** * Generate a class file for the proxy class. This method drives the * class file generation process. */ private byte[] generateClassFile() { /* ============================================================ * Step 1: Assemble ProxyMethod objects for all methods to * generate proxy dispatching code for. */ /* * Record that proxy methods are needed for the hashCode, equals, * and toString methods of java.lang.Object. This is done before * the methods from the proxy interfaces so that the methods from * java.lang.Object take precedence over duplicate methods in the * proxy interfaces. */ addProxyMethod(hashCodeMethod, Object.class); addProxyMethod(equalsMethod, Object.class); addProxyMethod(toStringMethod, Object.class); /* * Now record all of the methods from the proxy interfaces, giving * earlier interfaces precedence over later ones with duplicate * methods. */ for (Class intf : interfaces) { for (Method m : intf.getMethods()) { addProxyMethod(m, intf); } } /* * For each set of proxy methods with the same signature, * verify that the methods' return types are compatible. */ for (List sigmethods : proxyMethods.values()) { checkReturnTypes(sigmethods); } /* ============================================================ * Step 2: Assemble FieldInfo and MethodInfo structs for all of * fields and methods in the class we are generating. */ try { methods.add(generateConstructor()); for (List sigmethods : proxyMethods.values()) { for (ProxyMethod pm : sigmethods) { // add static field for method's Method object fields.add(new FieldInfo(pm.methodFieldName, \"Ljava/lang/reflect/Method;\", ACC_PRIVATE | ACC_STATIC)); // generate code for proxy method and add it methods.add(pm.generateMethod()); } } methods.add(generateStaticInitializer()); } catch (IOException e) { throw new InternalError(\"unexpected I/O Exception\", e); } if (methods.size() > 65535) { throw new IllegalArgumentException(\"method limit exceeded\"); } if (fields.size() > 65535) { throw new IllegalArgumentException(\"field limit exceeded\"); } /* ============================================================ * Step 3: Write the final class file. */ /* * Make sure that constant pool indexes are reserved for the * following items before starting to write the final class file. */ cp.getClass(dotToSlash(className)); cp.getClass(superclassName); for (Class intf: interfaces) { cp.getClass(dotToSlash(intf.getName())); } /* * Disallow new constant pool additions beyond this point, since * we are about to write the final constant pool table. */ cp.setReadOnly(); ByteArrayOutputStream bout = new ByteArrayOutputStream(); DataOutputStream dout = new DataOutputStream(bout); try { /* * Write all the items of the \"ClassFile\" structure. * See JVMS section 4.1. */ // u4 magic; dout.writeInt(0xCAFEBABE); // u2 minor_version; dout.writeShort(CLASSFILE_MINOR_VERSION); // u2 major_version; dout.writeShort(CLASSFILE_MAJOR_VERSION); cp.write(dout); // (write constant pool) // u2 access_flags; dout.writeShort(accessFlags); // u2 this_class; dout.writeShort(cp.getClass(dotToSlash(className))); // u2 super_class; dout.writeShort(cp.getClass(superclassName)); // u2 interfaces_count; dout.writeShort(interfaces.length); // u2 interfaces[interfaces_count]; for (Class intf : interfaces) { dout.writeShort(cp.getClass( dotToSlash(intf.getName()))); } // u2 fields_count; dout.writeShort(fields.size()); // field_info fields[fields_count]; for (FieldInfo f : fields) { f.write(dout); } // u2 methods_count; dout.writeShort(methods.size()); // method_info methods[methods_count]; for (MethodInfo m : methods) { m.write(dout); } // u2 attributes_count; dout.writeShort(0); // (no ClassFile attributes for proxy classes) } catch (IOException e) { throw new InternalError(\"unexpected I/O Exception\", e); } return bout.toByteArray(); } 2)getConstructor（获取代理类的构造方法) 3)newInstance（初始化代理对象) CGLIB动态代理 使用 CGLIB(Code Generation Library)是一个基于ASM的字节码生成库，它允许我们在运行时对字节码进行修改和动态生成。CGLIB通过继承方式实现代理。 CGLIB的核心类： net.sf.cglib.proxy.Enhancer – 主要的增强类 net.sf.cglib.proxy.MethodInterceptor – 主要的方法拦截类，它是Callback接口的子接口，需要用户实现 net.sf.cglib.proxy.MethodProxy – JDK的java.lang.reflect.Method类的代理类，可以方便的实现对源对象方法的调用,如使用： Object o = methodProxy.invokeSuper(proxy, args);//虽然第一个参数是被代理对象，也不会出现死循环的问题。 net.sf.cglib.proxy.MethodInterceptor接口是最通用的回调（callback)类型，它经常被基于代理的AOP用来实现拦截（intercept)方法的调用。这个接口只定义了一个方法 public Object intercept(Object object, java.lang.reflect.Method method, Object[] args, MethodProxy proxy) throws Throwable; 第一个参数是代理对像，第二和第三个参数分别是拦截的方法和方法的参数。原来的方法可能通过使用java.lang.reflect.Method对象的一般反射调用，或者使用 net.sf.cglib.proxy.MethodProxy对象调用。net.sf.cglib.proxy.MethodProxy通常被首选使用，因为它更快。 public class CglibProxy implements MethodInterceptor { @Override public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { System.out.println(\"++++++before \" + methodProxy.getSuperName() + \"++++++\"); System.out.println(method.getName()); Object o1 = methodProxy.invokeSuper(o, args); System.out.println(\"++++++before \" + methodProxy.getSuperName() + \"++++++\"); return o1; } } public class Main { public static void main(String[] args) { CglibProxy cglibProxy = new CglibProxy(); Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(UserServiceImpl.class); enhancer.setCallback(cglibProxy); UserService o = (UserService)enhancer.create(); o.getName(1); o.getAge(1); } } 我们通过CGLIB的Enhancer来指定要代理的目标对象、实际处理代理逻辑的对象，最终通过调用create()方法得到代理对象，对这个对象所有非final方法的调用都会转发给MethodInterceptor.intercept()方法，在intercept()方法里我们可以加入任何逻辑，比如修改方法参数，加入日志功能、安全检查功能等；通过调用MethodProxy.invokeSuper()方法，我们将调用转发给原始对象，具体到本例，就是HelloConcrete的具体方法。CGLIG中MethodInterceptor的作用跟JDK代理中的InvocationHandler很类似，都是方法调用的中转站。 注意：对于从Object中继承的方法，CGLIB代理也会进行代理，如hashCode()、equals()、toString()等，但是getClass()、wait()等方法不会，因为它是final方法，CGLIB无法代理。 既然是继承就不得不考虑final的问题。我们知道final类型不能有子类，所以CGLIB不能代理final类型。 final方法是不能重载的，所以也不能通过CGLIB代理，遇到这种情况不会抛异常，而是会跳过final方法只代理其他方法。 CGLIB动态代理 原理 1、生成代理类Class的二进制字节码（基于ASM)； 2、通过 Class.forName加载二进制字节码，生成Class对象； 3、通过反射机制获取实例构造，并初始化代理类对象。 调用委托类的方法是使用invokeSuper public Object invokeSuper(Object obj, Object[] args) throws Throwable { try { init(); FastClassInfo fci = fastClassInfo; return fci.f2.invoke(fci.i2, obj, args); } catch (InvocationTargetException e) { throw e.getTargetException(); } } private static class FastClassInfo { FastClass f1; FastClass f2; int i1; int i2; } f1指向委托类对象，f2指向代理类对象 i1是被代理的方法在对象中的索引位置 i2是CGLIB$被代理的方法$0在对象中的索引位置FastClass实现机制 FastClass其实就是对Class对象进行特殊处理，提出下标概念index，通过索引保存方法的引用信息，将原先的反射调用，转化为方法的直接调用，从而体现所谓的fast。 在FastTest中有两个方法， getIndex中对Test类的每个方法根据hash建立索引， invoke根据指定的索引，直接调用目标方法，避免了反射调用。 1.18 反射 Java的动态性体现在：反射机制、动态执行脚本语言、动态操作字节码 反射：在运行时加载、探知、使用编译时未知的类。 Class.forName使用的类加载器是调用者的类加载器 Class 表示Java中的类型（class、interface、enum、annotation、primitive type、void)本身。 一个类被加载之后，JVM会创建一个对应该类的Class对象，类的整个结构信息会放在相应的Class对象中。 这个Class对象就像一个镜子一样，从中可以看到类的所有信息。 反射的核心就是Class 如果多次执行forName等加载类的方法，类只会被加载一次；一个类只会形成一个Class对象，无论执行多少次加载类的方法，获得的Class都是一样的。 用途 性能 反射带来灵活性的同时，也有降低程序执行效率的弊端 setAccessible方法不仅可以标记某些私有的属性方法为可访问的属性方法，并且可以提高程序的执行效率 实际上是启用和禁用访问安全检查的开关。如果做检查就会降低效率；关闭检查就可以提高效率。 反射调用方法比直接调用要慢大约30倍，如果跳过安全检查的话比直接调用要慢大约7倍 开启和不开启安全检查对于反射而言可能会差4倍的执行效率。 为什么慢? 1)验证等防御代码过于繁琐，这一步本来在link阶段，现在却在计算时进行验证 2)产生很多临时对象，造成GC与计算时间消耗 3)由于缺少上下文，丢失了很多运行时的优化，比如JIT(它可以看作JVM的重要评测标准之一) 当然，现代JVM也不是非常慢了，它能够对反射代码进行缓存以及通过方法计数器同样实现JIT优化，所以反射不一定慢。实现 反射在Java中可以直接调用，不过最终调用的仍是native方法，以下为主流反射操作的实现。 Class.forName的实现 Class.forName可以通过包名寻找Class对象，比如Class.forName(\"java.lang.String\")。 在JDK的源码实现中，可以发现最终调用的是native方法forName0()，它在JVM中调用的实际是FindClassFromCaller()，原理与ClassLoader的流程一样。 public static Class forName(String className) throws ClassNotFoundException { Class caller = Reflection.getCallerClass(); return forName0(className, true, ClassLoader.getClassLoader(caller), caller); } private static native Class forName0(String name, boolean initialize, ClassLoader loader, Class caller) throws ClassNotFoundException; Java_java_lang_Class_forName0(JNIEnv *env, jclass this, jstring classname, jboolean initialize, jobject loader, jclass caller) { char *clname; jclass cls = 0; char buf[128]; jsize len; jsize unicode_len; if (classname == NULL) { JNU_ThrowNullPointerException(env, 0); return 0; } len = (*env)->GetStringUTFLength(env, classname); unicode_len = (*env)->GetStringLength(env, classname); if (len >= (jsize)sizeof(buf)) { clname = malloc(len + 1); if (clname == NULL) { JNU_ThrowOutOfMemoryError(env, NULL); return NULL; } } else { clname = buf; } (*env)->GetStringUTFRegion(env, classname, 0, unicode_len, clname); if (VerifyFixClassname(clname) == JNI_TRUE) { /* slashes present in clname, use name b4 translation for exception */ (*env)->GetStringUTFRegion(env, classname, 0, unicode_len, clname); JNU_ThrowClassNotFoundException(env, clname); goto done; } if (!VerifyClassname(clname, JNI_TRUE)) { /* expects slashed name */ JNU_ThrowClassNotFoundException(env, clname); goto done; } cls = JVM_FindClassFromCaller(env, clname, initialize, loader, caller); done: if (clname != buf) { free(clname); } return cls; } JVM_ENTRY(jclass, JVM_FindClassFromClass(JNIEnv env, const char name, jboolean init, jclass from)) JVMWrapper2(\"JVM_FindClassFromClass %s\", name); if (name == NULL || (int)strlen(name) > Symbol::max_length()) { // It's impossible to create this class; the name cannot fit // into the constant pool. THROW_MSG_0(vmSymbols::java_lang_NoClassDefFoundError(), name); } TempNewSymbol h_name = SymbolTable::new_symbol(name, CHECK_NULL); oop from_class_oop = JNIHandles::resolve(from); Klass* from_class = (from_class_oop == NULL) ? (Klass*)NULL : java_lang_Class::as_Klass(from_class_oop); oop class_loader = NULL; oop protection_domain = NULL; if (from_class != NULL) { class_loader = from_class->class_loader(); protection_domain = from_class->protection_domain(); } Handle h_loader(THREAD, class_loader); Handle h_prot (THREAD, protection_domain); jclass result = find_class_from_class_loader(env, h_name, init, h_loader, h_prot, true, thread); if (TraceClassResolution && result != NULL) { // this function is generally only used for class loading during verification. ResourceMark rm; oop from_mirror = JNIHandles::resolve_non_null(from); Klass from_class = java_lang_Class::as_Klass(from_mirror); const char from_name = from_class->external_name(); oop mirror = JNIHandles::resolve_non_null(result); Klass to_class = java_lang_Class::as_Klass(mirror); const char to = to_class->external_name(); tty->print(\"RESOLVE %s %s (verification)\\n\", from_name, to); } return result; JVM_END getDeclaredFields的实现 在JDK源码中，可以知道class.getDeclaredFields()方法实际调用的是native方法getDeclaredFields0()，它在JVM主要实现步骤如下： 1)根据Class结构体信息，获取field_count与fields[]字段，这个字段早已在load过程中被放入了 2)根据field_count的大小分配内存、创建数组 3)将数组进行forEach循环，通过fields[]中的信息依次创建Object对象 4)返回数组指针 主要慢在如下方面： 创建、计算、分配数组对象 对字段进行循环赋值 public Field[] getDeclaredFields() throws SecurityException { checkMemberAccess(Member.DECLARED, Reflection.getCallerClass(), true); return copyFields(privateGetDeclaredFields(false)); } private Field[] privateGetDeclaredFields(boolean publicOnly) { checkInitted(); Field[] res; ReflectionData rd = reflectionData(); if (rd != null) { res = publicOnly ? rd.declaredPublicFields : rd.declaredFields; if (res != null) return res; } // No cached value available; request value from VM res = Reflection.filterFields(this, getDeclaredFields0(publicOnly)); if (rd != null) { if (publicOnly) { rd.declaredPublicFields = res; } else { rd.declaredFields = res; } } return res; } private static Field[] copyFields(Field[] arg) { Field[] out = new Field[arg.length]; ReflectionFactory fact = getReflectionFactory(); for (int i = 0; i Method.invoke的实现 以下为无同步、无异常的情况下调用的步骤 1)创建Frame 2)如果对象flag为native，交给native_handler进行处理 3)在frame中执行java代码 4)弹出Frame 5)返回执行结果的指针 主要慢在如下方面： 需要完全执行ByteCode而缺少JIT等优化 检查参数非常多，这些本来可以在编译器或者加载时完成 public Object invoke(Object obj, Object... args) throws IllegalAccessException, IllegalArgumentException, InvocationTargetException { if (!override) { if (!Reflection.quickCheckMemberAccess(clazz, modifiers)) { Class caller = Reflection.getCallerClass(); checkAccess(caller, clazz, obj, modifiers); } } MethodAccessor ma = methodAccessor; // read volatile if (ma == null) { ma = acquireMethodAccessor(); } return ma.invoke(obj, args); } NativeMethodAccessorImpl#invoke public Object invoke(Object obj, Object[] args) throws IllegalArgumentException, InvocationTargetException { // We can't inflate methods belonging to vm-anonymous classes because // that kind of class can't be referred to by name, hence can't be // found from the generated bytecode. if (++numInvocations > ReflectionFactory.inflationThreshold() && !ReflectUtil.isVMAnonymousClass(method.getDeclaringClass())) { MethodAccessorImpl acc = (MethodAccessorImpl) new MethodAccessorGenerator(). generateMethod(method.getDeclaringClass(), method.getName(), method.getParameterTypes(), method.getReturnType(), method.getExceptionTypes(), method.getModifiers()); parent.setDelegate(acc); } return invoke0(method, obj, args); } private static native Object invoke0(Method m, Object obj, Object[] args); Java_sun_reflect_NativeMethodAccessorImpl_invoke0 (JNIEnv *env, jclass unused, jobject m, jobject obj, jobjectArray args) { return JVM_InvokeMethod(env, m, obj, args); } JVM_ENTRY(jobject, JVM_InvokeMethod(JNIEnv *env, jobject method, jobject obj, jobjectArray args0)) JVMWrapper(\"JVM_InvokeMethod\"); Handle method_handle; if (thread->stack_available((address) &method_handle) >= JVMInvokeMethodSlack) { method_handle = Handle(THREAD, JNIHandles::resolve(method)); Handle receiver(THREAD, JNIHandles::resolve(obj)); objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0))); oop result = Reflection::invoke_method(method_handle(), receiver, args, CHECK_NULL); jobject res = JNIHandles::make_local(env, result); if (JvmtiExport::should_post_vm_object_alloc()) { oop ret_type = java_lang_reflect_Method::return_type(method_handle()); assert(ret_type != NULL, \"sanity check: ret_type oop must not be NULL!\"); if (java_lang_Class::is_primitive(ret_type)) { // Only for primitive type vm allocates memory for java object. // See box() method. JvmtiExport::post_vm_object_alloc(JavaThread::current(), result); } } return res; } else { THROW_0(vmSymbols::java_lang_StackOverflowError()); } JVM_END class.newInstance的实现 - 1)检测权限、预分配空间大小等参数 - 2)创建Object对象，并分配空间 - 3)通过Method.invoke调用构造函数(()) - 4)返回Object指针 主要慢在如下方面： 参数检查不能优化或者遗漏 ()的查表 Method.invoke本身耗时 public T newInstance() throws InstantiationException, IllegalAccessException { if (System.getSecurityManager() != null) { checkMemberAccess(Member.PUBLIC, Reflection.getCallerClass(), false); } // NOTE: the following code may not be strictly correct under // the current Java memory model. // Constructor lookup if (cachedConstructor == null) { if (this == Class.class) { throw new IllegalAccessException( \"Can not call newInstance() on the Class for java.lang.Class\" ); } try { Class[] empty = {}; final Constructor c = getConstructor0(empty, Member.DECLARED); // Disable accessibility checks on the constructor // since we have to do the security check here anyway // (the stack depth is wrong for the Constructor's // security check to work) java.security.AccessController.doPrivileged( new java.security.PrivilegedAction() { public Void run() { c.setAccessible(true); return null; } }); cachedConstructor = c; } catch (NoSuchMethodException e) { throw (InstantiationException) new InstantiationException(getName()).initCause(e); } } Constructor tmpConstructor = cachedConstructor; // Security check (same as in java.lang.reflect.Constructor) int modifiers = tmpConstructor.getModifiers(); if (!Reflection.quickCheckMemberAccess(this, modifiers)) { Class caller = Reflection.getCallerClass(); if (newInstanceCallerCache != caller) { Reflection.ensureMemberAccess(caller, this, null, modifiers); newInstanceCallerCache = caller; } } // Run constructor try { return tmpConstructor.newInstance((Object[])null); } catch (InvocationTargetException e) { Unsafe.getUnsafe().throwException(e.getTargetException()); // Not reached return null; } } 1.19 XML DOM OM是用与平台和语言无关的方式表示XML文档的官方W3C标准。DOM是以层次结构组织的节点或信息片断的集合。这个层次结构允许开发人员在树中寻找特定信息。分析该结构通常需要加载整个文档和构造层次结构，然后才能做任何工作。由于它是基于信息层次的，因而DOM被认为是基于树或基于对象的。 优点 ①允许应用程序对数据和结构做出更改。 ②访问是双向的，可以在任何时候在树中上下导航，获取和操作任意部分的数据。 缺点 ①通常需要加载整个XML文档来构造层次结构，消耗资源大。 SAX SAX处理的优点非常类似于流媒体的优点。分析能够立即开始，而不是等待所有的数据被处理。而且，由于应用程序只是在读取数据时检查数据，因此不需要将数据存储在内存中。这对于大型文档来说是个巨大的优点。事实上，应用程序甚至不必解析整个文档；它可以在某个条件得到满足时停止解析。一般来说，SAX还比它的替代者DOM快许多。 优点 ①不需要等待所有数据都被处理，分析就能立即开始。 ②只在读取数据时检查数据，不需要保存在内存中。 ③可以在某个条件得到满足时停止解析，不必解析整个文档。 ④效率和性能较高，能解析大于系统内存的文档。 缺点 ①需要应用程序自己负责TAG的处理逻辑（例如维护父/子关系等)，文档越复杂程序就越复杂。 ②单向导航，无法定位文档层次，很难同时访问同一文档的不同部分数据，不支持XPath。 JDOM DOM4J 1.20 Java8 Lambda表达式&函数式接口&方法引用&Stream API Java8 stream迭代的优势和区别；lambda表达式？为什么要引入它 1)流（高级Iterator)：对集合对象进行各种非常便利、高效的聚合操作（aggregate operation)，或者大批量数据操作 (bulk data operation)，隐式迭代等，代码简洁 2)方便地实现并行（并行流)，比如实现MapReduce 3)Lamdba：简化匿名内部类的实现，代码更加紧凑 4)方法引用：方法引用是lambda表达式的另一种表达方式 对象::实例方法 类::静态方法 类::实例方法名 Optional Optional仅仅是一个容易：存放T类型的值或者null。它提供了一些有用的接口来避免显式的null检查。CompletableFuture 1)实现异步API（将任务交给另一线程完成，该线程与调用方异步，通过回调函数或阻塞的方式取得任务结果) 2)将批量同步操作转为异步操作（并行流/CompletableFuture) 3)多个异步任务合并时间日期API 新的java.time包包含了所有关于日期、时间、时区、Instant（跟日期类似但是精确到纳秒)、duration（持续时间)和时钟操作的类。新设计的API认真考虑了这些类的不变性（从java.util.Calendar吸取的教训)，如果某个实例需要修改，则返回一个新的对象。 接口中的默认方法与静态方法 默认方法使得开发者可以在不破坏二进制兼容性的前提下，往现存接口中添加新的方法，即不强制那些实现了该接口的类也同时实现这个新加的方法。 默认方法允许在不打破现有继承体系的基础上改进接口。该特性在官方库中的应用是：给java.util.Collection接口添加新方法，如stream()、parallelStream()、forEach()和removeIf()等等。 1.21 Java9 模块化 提供了类似于OSGI框架的功能，模块之间存在相互的依赖关系，可以导出一个公共的API，并且隐藏实现的细节，Java提供该功能的主要的动机在于，减少内存的开销，在JVM启动的时候，至少会有30～60MB的内存加载，主要原因是JVM需要加载rt.jar，不管其中的类是否被classloader加载，第一步整个jar都会被JVM加载到内存当中去，模块化可以根据模块的需要加载程序运行需要的class。 在引入了模块系统之后，JDK 被重新组织成 94 个模块。Java 应用可以通过新增的 jlink 工具，创建出只包含所依赖的 JDK 模块的自定义运行时镜像。这样可以极大的减少 Java 运行时环境的大小。使得JDK可以在更小的设备中使用。采用模块化系统的应用程序只需要这些应用程序所需的那部分JDK模块，而非是整个JDK框架了。HTTP/2 Java 9的版本中引入了一个新的package:java.net.http，里面提供了对Http访问很好的支持，不仅支持Http1.1而且还支持HTTP/2，以及WebSocket，据说性能特别好。JShell java9引入了jshell这个交互性工具，让Java也可以像脚本语言一样来运行，可以从控制台启动 jshell ，在 jshell 中直接输入表达式并查看其执行结果。当需要测试一个方法的运行效果，或是快速的对表达式进行求值时，jshell 都非常实用。 除了表达式之外，还可以创建 Java 类和方法。jshell 也有基本的代码完成功能。不可变集合工厂方法 Java 9增加了List.of()、Set.of()、Map.of()和Map.ofEntries()等工厂方法来创建不可变集合。私有接口方法 Java 8 为我们提供了接口的默认方法和静态方法，接口也可以包含行为，而不仅仅是方法定义。 默认方法和静态方法可以共享接口中的私有方法，因此避免了代码冗余，这也使代码更加清晰。如果私有方法是静态的，那这个方法就属于这个接口的。并且没有静态的私有方法只能被在接口中的实例调用。多版本兼容 JAR 当一个新版本的 Java 出现的时候，你的库用户要花费很长时间才会切换到这个新的版本。这就意味着库要去向后兼容你想要支持的最老的 Java 版本 (许多情况下就是 Java 6 或者 7)。这实际上意味着未来的很长一段时间，你都不能在库中运用 Java 9 所提供的新特性。幸运的是，多版本兼容 JAR 功能能让你创建仅在特定版本的 Java 环境中运行库程序时选择使用的 class 版本。 统一 JVM 日志 Java 9 中 ，JVM 有了统一的日志记录系统，可以使用新的命令行选项-Xlog 来控制 JVM 上 所有组件的日志记录。该日志记录系统可以设置输出的日志消息的标签、级别、修饰符和输出目标等。垃圾收集机制 Java 9 移除了在 Java 8 中 被废弃的垃圾回收器配置组合，同时把G1设为默认的垃圾回收器实现。替代了之前默认使用的Parallel GC，对于这个改变，evens的评论是酱紫的：这项变更是很重要的，因为相对于Parallel来说，G1会在应用线程上做更多的事情，而Parallel几乎没有在应用线程上做任何事情，它基本上完全依赖GC线程完成所有的内存管理。这意味着切换到G1将会为应用线程带来额外的工作，从而直接影响到应用的性能 I/O 流新特性 java.io.InputStream 中增加了新的方法来读取和复制 InputStream 中包含的数据。 - readAllBytes：读取 InputStream 中的所有剩余字节。 - readNBytes： 从 InputStream 中读取指定数量的字节到数组中。 - transferTo：读取 InputStream 中的全部字节并写入到指定的 OutputStream 中 。 "},"zother5-Java-Interview/七、JavaWeb.html":{"url":"zother5-Java-Interview/七、JavaWeb.html","title":"七、JavaWeb","keywords":"","body":"JavaWeb 三层模型 MVC 1、MVC设计模式 MVC设计模式 MVC模式（Model-View-Controller)是软件工程中的一种软件架构模式，把软件系统分为三个基本部分：模型（Model)、视图（View)和控制器（Controller)。 MVC模式最早为Trygve Reenskaug提出，为施乐帕罗奥多研究中心（Xerox PARC)的Smalltalk语言发明的一种软件设计模式。 MVC可对程序的后期维护和扩展提供了方便，并且使程序某些部分的重用提供了方便。而且MVC也使程序简化，更加直观。 - 控制器Controller：对请求进行处理，负责请求转发； - 视图View：界面设计人员进行图形界面设计； - 模型Model：程序编写程序应用的功能（实现算法等等)、数据库管理； 注意，MVC不是Java的东西，几乎现在所有B/S结构的软件都采用了MVC设计模式。但是要注意，MVC在B/S结构软件并没有完全实现，例如在我们今后的B/S软件中并不会有事件驱动！ 2、JavaWeb与MVC 　　JavaWeb的经历了JSP Model1、JSP Model1二代、JSP Model2三个时期。 2.1　JSP Model1第一代 JSP Model1是JavaWeb早期的模型，它适合小型Web项目，开发成本低！Model1第一代时期，服务器端只有JSP页面，所有的操作都在JSP页面中，连访问数据库的API也在JSP页面中完成。也就是说，所有的东西都耦合在一起，对后期的维护和扩展极为不利。 2.2　JSP Model1第二代 　　JSP Model1第二代有所改进，把业务逻辑的内容放到了JavaBean中，而JSP页面负责显示以及请求调度的工作。虽然第二代比第一代好了些，但还让JSP做了过多的工作，JSP中把视图工作和请求调度（控制器)的工作耦合在一起了。 2.3　JSP Model2 JSP Model2模式已经可以清晰的看到MVC完整的结构了。 - JSP：视图层，用来与用户打交道。负责接收用来的数据，以及显示数据给用户； - Servlet：控制层，负责找到合适的模型对象来处理业务逻辑，转发到合适的视图； - JavaBean：模型层，完成具体的业务工作，例如：开启、转账等。 JSP Model2适合多人合作开发大型的Web项目，各司其职，互不干涉，有利于开发中的分工，有利于组件的重用。但是，Web项目的开发难度加大，同时对开发人员的技术要求也提高了。 5.1 JavaWeb经典三层框架 我们常说的三层框架是由JavaWeb提出的，也就是说这是JavaWeb独有的！ 所谓三层是表述层（WEB层)、业务逻辑层（Business Logic)，以及数据访问层（Data Access)。 - WEB层：包含JSP和Servlet等与WEB相关的内容； - 业务层：业务层中不包含JavaWeb API，它只关心业务逻辑； - 数据层：封装了对数据库的访问细节； 　　注意，在业务层中不能出现JavaWeb API，例如request、response等。也就是说，业务层代码是可重用的，甚至可以应用到非Web环境中。业务层的每个方法可以理解成一个万能，例如转账业务方法。业务层依赖数据层，而Web层依赖业务层！ Web服务器 Web服务器的作用是接收客户端的请求，给客户端作出响应。 对于JavaWeb程序而已，还需要有Servlet容器，Servlet容器的基本功能是把动态资源转换成静态资源。 我们需要使用的是Web服务器和Servlet容器，通常这两者会集于一身。下面是对JavaWeb服务器： - Tomcat（Apache)：当前应用最广的JavaWeb服务器； - JBoss（Redhat红帽)：支持JavaEE，应用比较广；EJB容器 - GlassFish（Orcale)：Oracle开发JavaWeb服务器，应用不是很广； - Resin（Caucho)：支持JavaEE，应用越来越广； - Weblogic（Oracle)：收费；支持JavaEE，适合大型项目； - Websphere（IBM)：收费；支持JavaEE，适合大型项目； 支持JavaEE是EJB容器 Servlet 5.2 定义 Servlet是JavaWeb的三大组件之一，它属于动态资源。Servlet的作用是处理请求，服务器会把接收到的请求交给Servlet来处理，在Servlet中通常需要： - 接收请求数据； - 处理请求； - 完成响应。 　　例如客户端发出登录请求，或者输出注册请求，这些请求都应该由Servlet来完成处理！Servlet需要我们自己来编写，每个Servlet必须实现javax.servlet.Servlet接口。5.3 生命周期 生命周期方法： - void init(ServletConfig)：出生之后（1次)； - void service(ServletRequest request, ServletResponse response)：每次处理请求时都会被调用； - void destroy()：临死之前（1次)； 特性： - 单例，一个类只有一个对象；当然可能存在多个Servlet类！ - 多线程的，所以它的效率是高的！（不是线程安全的) Servlet类由我们来写，但对象由服务器来创建，并且由服务器来调用相应的方法。 Servlet的出生 服务器会在Servlet第一次被访问时创建Servlet，或者是在服务器启动时创建Servlet。如果服务器启动时就创建Servlet，那么还需要在web.xml文件中配置。也就是说默认情况下，Servlet是在第一次被访问时由服务器创建的。 而且一个Servlet类型，服务器只创建一个实例对象，例如在我们首次访问http://localhost:8080/helloservlet/helloworld时，服务器通过“/helloworld”找到了绑定的Servlet名称为cn.itcast.servlet.HelloServlet，然后服务器查看这个类型的Servlet是否已经创建过，如果没有创建过，那么服务器才会通过反射来创建HelloServlet的实例。当我们再次访问http://localhost:8080/helloservlet/helloworld时，服务器就不会再次创建HelloServlet实例了，而是直接使用上次创建的实例。 在Servlet被创建后，服务器会马上调用Servlet的void init(ServletConfig)方法。请记住， Servlet出生后马上就会调用init()方法，而且一个Servlet的一生。这个方法只会被调用一次。 我们可以把一些对Servlet的初始化工作放到init方法中！Servlet服务 　　当服务器每次接收到请求时，都会去调用Servlet的service()方法来处理请求。服务器接收到一次请求，就会调用service() 方法一次，所以service()方法是会被调用多次的。正因为如此，所以我们才需要把处理请求的代码写到service()方法中！Servlet的销毁 　　Servlet是不会轻易离去的，通常都是在服务器关闭时Servlet才会离去！在服务器被关闭时，服务器会去销毁Servlet，在销毁Servlet之前服务器会先去调用Servlet的destroy()方法，我们可以把Servlet的临终遗言放到destroy()方法中，例如对某些资源的释放等代码放到destroy()方法中。 Servlet接口相关类型 在Servlet接口中还存在三个我们不熟悉的类型： - ServletRequest：service() 方法的参数，它表示请求对象，它封装了所有与请求相关的数据，它是由服务器创建的； - ServletResponse：service()方法的参数，它表示响应对象，在service()方法中完成对客户端的响应需要使用这个对象； - ServletConfig：init()方法的参数，它表示Servlet配置对象，它对应Servlet的配置 ServletRequest和ServletResponse ServletRequest和ServletResponse是Servlet#service() 方法的两个参数，一个是请求对象，一个是响应对象，可以从ServletRequest对象中获取请求数据，可以使用ServletResponse对象完成响应。 ServletRequest和ServletResponse的实例由服务器创建，然后传递给service()方法。如果在service() 方法中希望使用HTTP相关的功能，那么可以把ServletRequest和ServletResponse强转成HttpServletRequest和HttpServletResponse。这也说明我们经常需要在service()方法中对ServletRequest和ServletResponse进行强转，这是很心烦的事情。不过后面会有一个类来帮我们解决这一问题的。 HttpServletRequest方法： - String getParameter(String paramName)：获取指定请求参数的值； - String getMethod()：获取请求方法，例如GET或POST； - String getHeader(String name)：获取指定请求头的值； - void setCharacterEncoding(String encoding)：设置请求体的编码！ 因为GET请求没有请求体，所以这个方法只对POST请求有效。当调用 request.setCharacterEncoding(“utf-8”)之后，再通过getParameter()方法获取参数值时，那么参数值都已经通过了转码，即转换成了UTF-8编码。所以，这个方法必须在调用getParameter()方法之前调用！ HttpServletResponse方法： - PrintWriter getWriter()：获取字符响应流，使用该流可以向客户端输出响应信息。例如response.getWriter().print(“Hello JavaWeb!”)； - ServletOutputStream getOutputStream()：获取字节响应流，当需要向客户端响应字节数据时，需要使用这个流，例如要向客户端响应图片； - void setCharacterEncoding(String encoding)：用来设置字符响应流的编码，例如在调用setCharacterEncoding(“utf-8”);之后，再response.getWriter()获取字符响应流对象，这时的响应流的编码为utf-8，使用response.getWriter()输出的中文都会转换成utf-8编码后发送给客户端； - void setHeader(String name, String value)：向客户端添加响应头信息， 例如setHeader(“Refresh”, “3;url=http://www.itcast.cn”)，表示3秒后自动刷新到http://www.itcast.cn； - void setContentType(String contentType)：该方法是setHeader(“content-type”, “xxx”)的简便方法，即用来添加名为content-type响应头的方法。content-type响应头用来设置响应数据的MIME类型，例如要向客户端响应jpg的图片，那么可以setContentType(“image/jepg”)，如果响应数据为文本类型，那么还要同时设置编码，例如setContentType(“text/html;chartset=utf-8”)表示响应数据类型为文本类型中的html类型，并且该方法会调用setCharacterEncoding(“utf-8”)方法； - void sendError(int code, String errorMsg)：向客户端发送状态码，以及错误消息。例如给客户端发送404：response(404, “您要查找的资源不存在！”)。 ServletConfig Servlet的配置信息，即web.xml文件中的元素。 一个ServletConfig对象对应着一个servlet元素的配置信息（servlet-name，servlet-class) getServletName获取的是的功能 getServletContext获取的是Servlet上下文对象 ServletConfig对象对应web.xml文件中的元素。例如你想获取当前Servlet在web.xml文件中的配置名，那么可以使用servletConfig.getServletName()方法获取！ ServletConfig对象是由服务器创建的，然后传递给Servlet的init()方法，你可以在init()方法中使用它！ - String getServletName()：获取Servlet在web.xml文件中的配置名称，即指定的名称； - ServletContext getServletContext()：用来获取ServletContext对象； - String getInitParameter(String name)：用来获取在web.xml中配置的初始化参数，通过参数名来获取参数值； - Enumeration getInitParameterNames()：用来获取在web.xml中配置的所有初始化参数名称； 在元素中还可以配置初始化参数： One cn.itcast.servlet.OneServlet paramName1 paramValue1 paramName2 paramValue2 在OneServlet中，可以使用ServletConfig对象的getInitParameter()方法来获取初始化参数，例如： String value1 = servletConfig.getInitParameter(“paramName1”);//获取到paramValue15.4 实现Servlet的方式 实现Servlet有三种方式： - 实现javax.servlet.Servlet接口； - 继承javax.servlet.GenericServlet类； - 继承javax.servlet.http.HttpServlet类； 通常我们会去继承HttpServlet类来完成我们的Servlet GenericServlet GenericServlet概述 GenericServlet是Servlet接口的实现类，我们可以通过继承GenericServlet来编写自己的Servlet。下面是GenericServlet类的源代码： GenericServlet.java public abstract class GenericServlet implements Servlet, ServletConfig, java.io.Serializable { private static final long serialVersionUID = 1L; private transient ServletConfig config; public GenericServlet() {} @Override public void destroy() {} @Override public String getInitParameter(String name) { return getServletConfig().getInitParameter(name); } @Override public Enumeration getInitParameterNames() { return getServletConfig().getInitParameterNames(); } @Override public ServletConfig getServletConfig() { return config; } @Override public ServletContext getServletContext() { return getServletConfig().getServletContext(); } @Override public String getServletInfo() { return \"\"; } @Override public void init(ServletConfig config) throws ServletException { this.config = config; this.init(); } public void init() throws ServletException {} 这个init方法是为了拓展init而设置的，子类可以重写这个无参数的init方法 public void log(String msg) { getServletContext().log(getServletName() + \": \" + msg); } public void log(String message, Throwable t) { getServletContext().log(getServletName() + \": \" + message, t); } @Override public abstract void service(ServletRequest req, ServletResponse res) throws ServletException, IOException; @Override public String getServletName() { return config.getServletName(); } } GenericServlet的init()方法 在GenericServlet中，定义了一个ServletConfig实例变量，并在init(ServletConfig)方法中把参数ServletConfig赋给了实例变量。然后在该类的很多方法中使用了实例变量config。 如果子类覆盖了GenericServlet的init(StringConfig)方法，那么this.config=config这一条语句就会被覆盖了，也就是说GenericServlet的实例变量config的值为null，那么所有依赖config的方法都不能使用了。如果真的希望完成一些初始化操作，那么去覆盖GenericServlet提供的init()方法，它是没有参数的init()方法，它会在init(ServletConfig)方法中被调用。 实现了ServletConfig接口 　　GenericServlet还实现了ServletConfig接口，所以可以直接调用getInitParameter()、getServletContext()等ServletConfig的方法。 HttpServlet HttpServlet概述 HttpServlet类是GenericServlet的子类，它提供了对HTTP请求的特殊支持，所以通常我们都会通过继承HttpServlet来完成自定义的Servlet。 HttpServlet覆盖了service()方法 HttpServlet类中提供了service(HttpServletRequest,HttpServletResponse)方法，这个方法是HttpServlet自己的方法，不是从Servlet继承来的。在HttpServlet的service(ServletRequest,ServletResponse)方法中会把ServletRequest和ServletResponse强转成HttpServletRequest和HttpServletResponse，然后调用 service(HttpServletRequest,HttpServletResponse)方法，这说明子类可以去覆盖service(HttpServletRequest,HttpServletResponse)方法即可，这就不用自己去强转请求和响应对象了。 其实子类也不用去覆盖service(HttpServletRequest,HttpServletResponse)方法，因为HttpServlet还要做另一步简化操作，下面会介绍。 HttpServlet.java public abstract class HttpServlet extends GenericServlet { protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { …… } @Override public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException { HttpServletRequest request; HttpServletResponse response; try { request = (HttpServletRequest) req; response = (HttpServletResponse) res; } catch (ClassCastException e) { throw new ServletException(\"non-HTTP request or response\"); } service(request, response); } …… } doGet()和doPost() 在HttpServlet的service(HttpServletRequest,HttpServletResponse)方法会去判断当前请求是GET还是POST，如果是GET请求，那么会去调用本类的doGet()方法，如果是POST请求会去调用doPost()方法，这说明我们在子类中去覆盖doGet()或doPost()方法即可。 public class AServlet extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { System.out.println(\"hello doGet()...\"); } } public class BServlet extends HttpServlet { public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { System.out.println(\"hello doPost()...\"); } } Servlet细节 - 不要在Servlet中创建成员！创建局部变量即可！ - 可以创建无状态成员！ - 可以创建有状态的成员，但状态必须为只读的！（只有get，没有set) Servlet与线程安全 因为一个类型的Servlet只有一个实例对象，那么就有可能会现时出一个Servlet同时处理多个请求，那么Servlet是否为线程安全的呢？答案是：“不是线程安全的”。这说明Servlet的工作效率很高，但也存在线程安全问题！ 所以我们不应该在Servlet中创建成员变量，因为可能会存在一个线程对这个成员变量进行写操作，另一个线程对这个成员变量进行读操作。 让服务器在启动时就创建Servlet（很少这么做) 默认情况下，服务器会在某个Servlet第一次收到请求时创建它。也可以在web.xml中对Servlet进行配置，使服务器启动时就创建Servlet。 hello1 cn.itcast.servlet.Hello1Servlet 0 hello1 /hello1 hello2 cn.itcast.servlet.Hello2Servlet 1 hello2 /hello2 hello3 cn.itcast.servlet.Hello3Servlet 2 hello3 /hello3 在元素中配置元素可以让服务器在启动时就创建该Servlet，其中元素的值必须是大于等于的整数，它的使用是服务器启动时创建Servlet的顺序。上例中，根据的值可以得知服务器创建Servlet的顺序为Hello1Servlet、Hello2Servlet、Hello3Servlet。 是的子元素，用来指定Servlet的访问路径，即URL。 它必须是以“/”开头！ 1)- 可以在中给出多个，例如： AServlet /AServlet /BServlet 那么这说明一个Servlet绑定了两个URL，无论访问/AServlet还是/BServlet，访问的都是AServlet。 2)- 还可以在中使用通配符，所谓通配符就是星号“*”，星号可以匹配任何URL前缀或后缀，使用通配符可以命名一个Servlet绑定一组URL，例如：  /servlet/*：/servlet/a、/servlet/b，都匹配/servlet/*； - .do：/abc/def/ghi.do、/a.do，都匹配.do；  /*：匹配所有URL； 请注意，通配符要么为前缀，要么为后缀，不能出现在URL中间位置，也不能只有通配符。例如：/*.do就是错误的，因为星号出现在URL的中间位置上了。*.*也是不对的，因为一个URL中最多只能出现一个通配符。 注意，通配符是一种模糊匹配URL的方式，如果存在更具体的，那么访问路径会去匹配具体的。例如： hello1 cn.itcast.servlet.Hello1Servlet hello1 /servlet/hello1 hello2 cn.itcast.servlet.Hello2Servlet hello2 /servlet/* 　　当访问路径为http://localhost:8080/hello/servlet/hello1时，因为访问路径即匹配hello1的，又匹配hello2的，但因为hello1的中没有通配符，所以优先匹配，即设置hello1。 web.xml文件的继承 　　在${CATALINA_HOME}\\conf\\web.xml中的内容，相当于写到了每个项目的web.xml中，它是所有web.xml的父文件。 每个完整的JavaWeb应用中都需要有web.xml，但我们不知道所有的web.xml文件都有一个共同的父文件，它在Tomcat的conf/web.xml路径。 conf/web.xml default org.apache.catalina.servlets.DefaultServlet debug 0 listings false 1 jsp org.apache.jasper.servlet.JspServlet fork false xpoweredBy false 3 default / jsp *.jsp *.jspx 30 bmp image/bmp htm text/html index.html index.htm index.jsp ServletContext（存取数据，获取资源) 一个项目只有一个ServletContext对象！ 我们可以在N多个Servlet中来获取这个唯一的对象，使用它可以给多个Servlet传递数据！ 这个对象在Tomcat启动时就创建，在Tomcat关闭时才会死去！ ServletContext概述 服务器会为每个应用创建一个ServletContext对象： - ServletContext对象的创建是在服务器启动时完成的； - ServletContext对象的销毁是在服务器关闭时完成的。 　　 ServletContext对象的作用是在整个Web应用的动态资源之间共享数据！例如在AServlet中向ServletContext对象中保存一个值，然后在BServlet中就可以获取这个值，这就是共享数据了。 获取ServletContext - ServletConfig#getServletContext()； - GenericServlet#getServletContext(); - HttpSession#getServletContext() - ServletContextEvent#getServletContext() 在Servlet中获取ServletContext对象： - 在void init(ServletConfig config)中：ServletContext context = config.getServletContext();，ServletConfig类的getServletContext()方法可以用来获取ServletContext对象； 在GenericeServlet或HttpServlet中获取ServletContext对象： - GenericServlet类有getServletContext()方法，所以可以直接使用 this.getServletContext()来获取； 　　 public class MyServlet implements Servlet { public void init(ServletConfig config) { ServletContext context = config.getServletContext(); } … } public class MyServlet extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) { ServletContext context = this.getServletContext(); } } 域对象的功能 ServletContext是JavaWeb四大域对象之一： - PageContext； - ServletRequest； - HttpSession； - ServletContext； 所有域对象都有存取数据的功能，因为域对象内部有一个Map，用来存储数据，下面是ServletContext对象用来操作数据的方法： - void setAttribute(String name, Object value)：用来存储一个对象，也可以称之为存储一个域属性，例如：servletContext.setAttribute(“xxx”, “XXX”)，在ServletContext中保存了一个域属性，域属性名称为xxx，域属性的值为XXX。请注意，如果多次调用该方法，并且使用相同的name，那么会覆盖上一次的值，这一特性与Map相同； - Object getAttribute(String name)：用来获取ServletContext中的数据，当前在获取之前需要先去存储才行，例如：String value = (String)servletContext.getAttribute(“xxx”);，获取名为xxx的域属性； - void removeAttribute(String name)：用来移除ServletContext中的域属性，如果参数name指定的域属性不存在，那么本方法什么都不做； - Enumeration getAttributeNames()：获取所有域属性的名称； 获取应用初始化参数 - Servlet也可以获取初始化参数，但它是局部的参数；也就是说，一个Servlet只能获取自己的初始化参数，不能获取别人的，即初始化参数只为一个Servlet准备！ - 可以配置公共的初始化参数，为所有Servlet而用！这需要使用ServletContext才能使用！ 还可以使用ServletContext来获取在web.xml文件中配置的应用初始化参数！注意，应用初始化参数与Servlet初始化参数不同： web.xml ... （为ServletContext设置的公共初始化参数) paramName1 paramValue1 paramName2 paramValue2 ServletContext context = this.getServletContext(); String value1 = context.getInitParameter(\"paramName1\"); String value2 = context.getInitParameter(\"paramName2\"); System.out.println(value1 + \", \" + value2); Enumeration names = context.getInitParameterNames(); while(names.hasMoreElements()) { System.out.println(names.nextElement()); } 5.5 请求&响应 服务器每次收到请求时，都会为这个请求开辟一个新的线程。 Response 1、概述 response是Servlet#service方法的一个参数，类型为 javax.servlet.http.HttpServletResponse。在客户端发出每个请求时，服务器都会创建一个response对象，并传入给Servlet.service()方法。response对象是用来对客户端进行响应的，这说明在service()方法中使用response对象可以完成对客户端的响应工作。 response对象的功能分为以下四种： - 设置响应头信息； - 发送状态码； - 设置响应正文； - 重定向。 2、响应正文 response是响应对象，向客户端输出响应正文（响应体)可以使用response的响应流，repsonse一共提供了两个响应流对象： - PrintWriter out = response.getWriter()：获取字符流； - ServletOutputStream out = response.getOutputStream()：获取字节流； 当然，如果响应正文内容为字符（html)，那么使用response.getWriter()，如果响应内容是字节（图片等)，例如下载时，那么可以使用response.getOutputStream() (ServletOutputStream)。 注意，在一个请求中，不能同时使用这两个流！也就是说，要么你使用repsonse.getWriter()，要么使用response.getOutputStream()，但不能同时使用这两个流。不然会抛出IllegalStateException异常。 字符响应流 - 字符编码 在使用response.getWriter()时需要注意默认字符编码为ISO-8859-1，如果希望设置字符流的字符编码为utf-8，可以使用response.setCharaceterEncoding(“utf-8”)来设置。这样可以保证输出给客户端的字符都是使用UTF-8编码的！ 但客户端浏览器并不知道响应数据是什么编码的！如果希望通知客户端使用UTF-8来解读响应数据，那么还是使用response.setContentType(\"text/html;charset=utf-8\")方法比较好，因为这个方法不只会调用response.setCharaceterEncoding(“utf-8”)，还会设置content-type响应头，客户端浏览器会使用content-type头来解读响应数据。 - 缓冲区 response.getWriter()是PrintWriter类型，所以它有缓冲区，缓冲区的默认大小为8KB。也就是说，在响应数据没有输出8KB之前，数据都是存放在缓冲区中，而不会立刻发送到客户端。当Servlet执行结束后，服务器才会去刷新流，使缓冲区中的数据发送到客户端。 如果希望响应数据马上发送给客户端： - 向流中写入大于8KB的数据； - 调用response.flushBuffer()方法来手动刷新缓冲区； 字节响应流 将一张图片转为字节流写入到response中 读取图片，使用commons-io包的方法。 byte[] image = IOUtils.toByteArray(new FileInputStream(this.getServletContext().getRealPath(\"/images/cat.jpeg\"))); response.getOutputStream().write(image); 3、设置响应头信息 响应头是键值对 　　可以使用response对象的setHeader()方法来设置响应头！使用该方法设置的响应头最终会发送给客户端浏览器！ - response.setHeader(“content-type”, “text/html;charset=utf-8”)：设置content-type响应头，该头的作用是告诉浏览器响应内容为html类型，编码为utf-8。而且同时会设置response的字符流编码为utf-8，即response.setCharaceterEncoding(“utf-8”)； - response.setHeader(\"Refresh\",\"5; URL=http://www.itcast.cn\")：5秒后自动跳转到传智主页。 4、设置状态码及其他方法 - response.setContentType(\"text/html;charset=utf-8\")：等同\\于调用 response.setHeader(“content-type”, “text/html;charset=utf-8”)； - response.setCharacterEncoding(“utf-8”)：设置字符响应流的字符编码为utf-8； - response.setStatus(200)：设置状态码； - response.sendError(404, “您要查找的资源不存在”)：当发送错误状态码时，Tomcat会跳转到固定的错误页面去，但可以显示错误信息。 response.sendError(状态码) 5、重定向 1)什么是重定向 当你访问http://www.sun.com时，你会发现浏览器地址栏中的URL会变成http://www.oracle.com/us/sun/index.htm，这就是重定向了。 重定向是服务器通知浏览器去访问另一个地址，即再发出另一个请求。 2)完成重定向 响应码为200表示响应成功，而响应码为302表示重定向。所以完成重定向的第一步就是设置响应码为302。 因为重定向是通知浏览器发出第二个请求，所以浏览器需要知道第二个请求的URL，所以完成重定向的第二步是设置Location头，指定第二个请求的URL地址。 public class AServlet extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { response.setStatus(302); response.setHeader(\"Location\", \"http://www.itcast.cn\"); } } 　　上面代码的作用是：当访问AServlet后，会通知浏览器重定向到传智主页。客户端浏览器解析到响应码为302后，就知道服务器让它重定向，所以它会马上获取响应头Location，然发出第二个请求。 3)便捷的重定向方式sendRedirect public class AServlet extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { response.sendRedirect(\"http://www.itcast.cn\"); } } response.sendRedirect()方法会设置响应头为302，以设置Location响应头。 如果要重定向的URL是在同一个服务器内，那么可以使用相对路径，例如： public class AServlet extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { response.sendRedirect(\"/hello/BServlet\"); 注意是/项目名/路径（请求URI) } } 重定向的URL地址为：http://localhost:8080/hello/BServlet 4)重定向小结 - 重定向是两次请求； - 重定向的URL可以是其他应用，不局限于当前应用； - 重定向的响应头为302，并且必须要有Location响应头； - 重定向就不要再使用response.getWriter()或response.getOutputStream()输出数据，不然可能会出现异常； Request 1、概述 request是Servlet.service()方法的一个参数，类型为javax.servlet.http.HttpServletRequest。在客户端发出每个请求时，服务器都会创建一个request对象，并把请求数据封装到request中，然后在调用Servlet.service()方法时传递给service()方法，这说明在service()方法中可以通过request对象来获取请求数据。 request的功能可以分为以下几种： - 封装了请求头数据； - 封装了请求正文数据，如果是GET请求，那么就没有正文； - request是一个域对象，可以把它当成Map来添加获取数据； - request提供了请求转发和请求包含功能。 2、域方法 request是域对象！在JavaWeb中一共四个域对象，其中ServletContext就是域对象，它在整个应用中只创建一个ServletContext对象。request其中一个，request可以在一个请求中共享数据。 一个请求会创建一个request对象，如果在一个请求中经历了多个Servlet，那么多个Servlet就可以使用request来共享数据。 下面是request的域方法： - void setAttribute(String name, Object value)：用来存储一个对象，也可以称之为存储一个域属性，例如：servletContext.setAttribute(“xxx”, “XXX”)，在request中保存了一个域属性，域属性名称为xxx，域属性的值为XXX。请注意，如果多次调用该方法，并且使用相同的name，那么会覆盖上一次的值，这一特性与Map相同； - Object getAttribute(String name)：用来获取request中的数据，当前在获取之前需要先去存储才行，例如：String value = (String)request.getAttribute(“xxx”);，获取名为xxx的域属性； - void removeAttribute(String name)：用来移除request中的域属性，如果参数name指定的域属性不存在，那么本方法什么都不做； - Enumeration getAttributeNames()：获取所有域属性的名称； 3、获取请求头数据 request与请求头相关的方法有： - String getHeader(String name)：获取指定名称的请求头； - Enumeration getHeaderNames()：获取多值头； - int getIntHeader(String name)：获取值为int类型的请求头。 - long getDateHeader(String name)：获取值为long类型的请求头 4、获取请求相关的其它方法 request中还提供了与请求相关的其他方法，有些方法是为了我们更加便捷的方法请求头数据而设计，有些是与请求URL相关的方法。 - int getContentLength()：获取请求体的字节数，GET请求没有请求体，没有请求体返回-1； - String getContentType()：获取请求类型，如果请求是GET，那么这个方法返回null；如果是POST请求，那么默认为application/x-www-form-urlencoded，表示请求体内容使用了URL编码； - String getMethod()：返回请求方法，例如：GET/POST - Locale getLocale()：返回当前客户端浏览器的Locale。java.util.Locale表示国家和言语，这个东西在国际化中很有用； - String getCharacterEncoding()：获取请求编码，如果没有setCharacterEncoding()，那么返回null，表示使用ISO-8859-1编码； - void setCharacterEncoding(String code)：设置请求编码，只对请求体有效！注意，对于GET而言，没有请求体！！！所以此方法只能对POST请求中的参数有效！ - String getContextPath()：返回上下文路径（/项目名)，例如：/hello - String getQueryString()：返回请求URL中的参数，例如：name=zhangSan - String getRequestURI()：返回请求URI路径，例如：/hello/oneServlet - StringBuffer getRequestURL()：返回请求URL路径，例如： http://localhost/hello/oneServlet，即返回除了参数以外的路径信息； - String getServletPath()：返回Servlet路径，例如：/oneServlet - String getRemoteAddr()：返回当前客户端的IP地址； - String getRemoteHost()：返回当前客户端的主机名，但这个方法的实现还是获取IP地址； - String getScheme()：返回请求协议，例如：http； - String getServerName()：返回主机名，例如：localhost - int getServerPort()：返回服务器端口号，例如：8080 System.out.println(\"IP:\"+request.getRemoteAddr()); System.out.println(\"请求方式:\"+request.getMethod()); System.out.println(\"User-Agent请求头:\"+request.getHeader(\"User-Agent\")); System.out.println(\"协议名:\"+request.getScheme()); System.out.println(\"主机名:\"+request.getServerName()); System.out.println(\"端口号:\"+request.getServerPort()); System.out.println(\"项目:\"+request.getContextPath()); System.out.println(\"Servlet路径:\"+request.getServletPath()); System.out.println(\"请求参数:\"+request.getQueryString()); System.out.println(\"URI:\"+request.getRequestURI()); System.out.println(\"URL:\"+request.getRequestURL()); ========================================================================= http://localhost:8080/Request&Response/RequestHeaderServlet?username=sxj&password=sxj ========================================================================== IP:127.0.0.1 请求方式:GET User-Agent请求头:Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36 协议名:http 主机名:localhost 端口号:8080 项目:/Request&Response Servlet路径:/RequestHeaderServlet 请求参数: username=sxj&password=sxj URI:/Request&Response/RequestHeaderServlet（RequestURI) URL:http://localhost:8080/Request&Response/RequestHeaderServlet 整个访问的路径就等于URL+QueryString URL=协议名+主机名+端口号+URI 案例：request.getRemoteAddr()：封IP 可以使用request.getRemoteAddr()方法获取客户端的IP地址，然后判断IP是否为禁用IP。 String ip = request.getRemoteAddr(); System.out.println(ip); if(ip.equals(\"127.0.0.1\")) { response.getWriter().print(\"您的IP已被禁止！\"); } else { response.getWriter().print(\"Hello!\"); } 案例：防盗链 可以使用request.getAttribute(“Referer”)如果不是当前页面，那么属于盗链，则跳转到当前页面 如果是从地址栏直接输入URL，那么Referee返回的是null String referer = request.getHeader(\"Referer\"); System.out.println(referer); if(referer == null || !referer.contains(\"localhost\")){ response.sendRedirect(\"http://www.baidu.com\"); }else{ response.getWriter().print(\"hello\");//如果是从指定页面跳转来的 } 5、获取请求参数 最为常见的客户端传递参数方式有两种： - 浏览器地址栏直接输入：一定是GET请求； - 超链接：一定是GET请求； - 表单：可以是GET，也可以是POST，这取决与的method属性值； GET请求和POST请求的区别： - GET请求： - 请求参数会在浏览器的地址栏中显示，所以不安全； - 请求参数长度限制长度在1K之内； - GET请求没有请求体，无法通过request.setCharacterEncoding()来设置参数的编码； - POST请求： - 请求参数不会显示浏览器的地址栏，相对安全； - 请求参数长度没有限制； 无论是GET|POST请求，都可以使用相同的API来获取请求参数。 请求参数有一个key一个value的，也有一个key多个value的。 超链接 参数1： 参数2： 下面是使用request获取请求参数的API： - String getParameter(String name)：通过指定名称获取参数值； public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { String v1 = request.getParameter(\"p1\"); String v2 = request.getParameter(\"p2\"); System.out.println(\"p1=\" + v1); System.out.println(\"p2=\" + v2); } public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { String v1 = request.getParameter(\"p1\"); String v2 = request.getParameter(\"p2\"); System.out.println(\"p1=\" + v1); System.out.println(\"p2=\" + v2); } - String[] getParameterValues(String name)：当多个参数名称相同时，可以使用方法来获取； 超链接 public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { String[] names = request.getParameterValues(\"name\"); System.out.println(Arrays.toString(names)); } - Enumeration getParameterNames()：获取所有参数的名字； 参数1： 参数2： public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { Enumeration names = request.getParameterNames(); while(names.hasMoreElements()) { System.out.println(names.nextElement()); } } - Map getParameterMap()：获取所有参数封装到Map中，其中key为参数名，value为参数值，因为一个参数名称可能有多个值，所以参数值是String[]，而不是String。 超链接 Map paramMap = request.getParameterMap(); for(String name : paramMap.keySet()) { String[] values = paramMap.get(name); System.out.println(name + \": \" + Arrays.toString(values)); } p2: [v2, vv2] p1: [v1, vv1] 示例： 点击这里，测试GET请求 用户名： 密 码： 爱 好：吃饭 睡觉 编程 //遍历Map public class RequestParameterServlet extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { Enumeration names = request.getParameterNames(); while(names.hasMoreElements()){ String name = names.nextElement(); System.out.println(name+\"=\"+request.getParameter(name)); } } public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { Map map = request.getParameterMap(); for(Entry entry:map.entrySet()){ System.out.println(entry.getKey()+\"=\"+Arrays.toString(entry.getValue())); } } } username=sxj password=sxj username=[23] password=[123] hobby=[eat, sleep] String username = request.getParameter(\"username\"); String password = request.getParameter(\"password\"); String [] hobbies = request.getParameterValues(\"hobby\"); System.out.println(\"username:\"+username); System.out.println(\"password:\"+password); System.out.println(\"hobbies:\"+Arrays.toString(hobbies)); 6、请求转发和请求包含 无论是请求转发还是请求包含，都表示由多个Servlet共同来处理一个请求。例如Servlet1来处理请求，然后Servlet1又转发给Servlet2来继续处理这个请求。 1)请求转发（常用) 在AServlet中，把请求转发到BServlet： 参数是Servlet路径（servlet-mapping中的url-pattern) 相当于/项目名 public class AServlet extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { System.out.println(\"AServlet\"); RequestDispatcher rd = request.getRequestDispatcher(\"/BServlet\"); rd.forward(request, response); } } public class BServlet extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { System.out.println(\"BServlet\"); } } Aservlet BServlet 如果转发了，那么在原本请求的Servlet中设置的响应体是失效的。 如果转发前，在原本请求的Servlet中设置的响应体超过24K，那么响应体仍有效（缓冲区溢出)。 2)请求包含（不常用) 在AServlet中，把请求包含到BServlet： public class AServlet extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { System.out.println(\"AServlet\"); RequestDispatcher rd = request.getRequestDispatcher(\"/BServlet\"); rd.include(request, response); } } public class BServlet extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { System.out.println(\"BServlet\"); } } Aservlet BServlet 3)请求转发与请求包含比较 - 如果在AServlet中请求转发到BServlet，那么在AServlet中就不允许再输出响应体，即不能再使用response.getWriter()和response.getOutputStream()向客户端输出，这一工作应该由BServlet来完成；如果是使用请求包含，那么没有这个限制； - 请求转发虽然不能输出响应体，但还是可以设置响应头的，例如：response.setContentType(”text/html;charset=utf-8”); - 请求包含大多是应用在JSP页面中，完成多页面的合并； - 请求转发大多是应用在Servlet中，转发目标大多是JSP页面； 4)请求转发与重定向比较  - 1)请求转发是一个请求，而重定向是两个请求； - 请求转发后浏览器地址栏不会有变化，而重定向会有变化，因为重定向是两个请求；  - 2)请求转发的目标只能是本应用中的资源（给出Servlet路径)，重定向的目标可以是其他应用（请求URI或者其他路径)；  - 3)请求转发对AServlet和BServlet的请求方法是相同的，即要么都是GET，要么都是POST，因为请求转发是一个请求； - 重定向的第二个请求一定是GET；  - 4)请求转发效率更高；当需要地址栏发生变化时，需要使用重定向；需要在下一个Servlet中获取之前Servlet设置的域，需要使用转发 Filter 什么是过滤器 过滤器JavaWeb三大组件之一，它与Servlet很相似！不过过滤器是用来拦截请求的，而不是处理请求的。 当用户请求某个Servlet时，会先执行部署在这个请求上的Filter，如果Filter“放行”，那么会继承执行用户请求的Servlet；如果Filter不“放行”，那么就不会执行用户请求的Servlet。 其实可以这样理解，当用户请求某个Servlet时，Tomcat会去执行注册在这个请求上的Filter，然后是否“放行”由Filter来决定。可以理解为，Filter来决定是否调用Servlet！当执行完成Servlet的代码后，还会执行Filter后面的代码。 过滤器之hello world 　　其实过滤器与Servlet很相似，我们回忆一下如果写的第一个Servlet应用！写一个类，实现Servlet接口！没错，写过滤器就是写一个类，实现Filter接口。 public class HelloFilter implements Filter { public void init(FilterConfig filterConfig) throws ServletException {} public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { System.out.println(\"Hello Filter\"); } public void destroy() {} } 第二步也与Servlet一样，在web.xml文件中部署Filter： helloFilter cn.itcast.filter.HelloFilter helloFilter /*(/*表示拦截所有的访问请求) 当用户访问index.jsp页面时，会执行HelloFilter的doFilter()方法！在我们的示例中，index.jsp页面是不会被执行的，如果想执行index.jsp页面，那么我们需要放行！ public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { System.out.println(\"filter start...\"); chain.doFilter(request, response); System.out.println(\"filter end...\"); } 　　有很多同学总是错误的认为，一个请求在给客户端输出之后就算是结束了，这是不对的！其实很多事情都需要在给客户端响应之后才能完成！ 过滤器的生命周期 - init(FilterConfig)：在服务器启动时会创建Filter实例，并且每个类型的Filter只创建一个实例，从此不再创建！在创建完Filter实例后，会马上调用init()方法完成初始化工作，这个方法只会被执行一次； - doFilter(ServletRequest req,ServletResponse res,FilterChain chain)：这个方法会在用户每次访问“目标资源（pattern>index.jsp)”时执行，如果需要“放行”，那么需要调用FilterChain的doFilter(ServletRequest,ServletResponse)方法，如果不调用FilterChain的doFilter()方法，那么目标资源将无法执行； - destroy()：服务器会在创建Filter对象之后，把Filter放到缓存中一直使用，通常不会销毁它。一般会在服务器关闭时销毁Filter对象，在销毁Filter对象之前，服务器会调用Filter对象的destory()方法。 - 单例，同Servlet一致 FilterConfig Filter接口中的init()方法的参数类型为FilterConfig类型。它的功能与ServletConfig相似，与web.xml文件中的配置信息对应。下面是FilterConfig的功能介绍： - ServletContext getServletContext()：获取ServletContext的方法； - String getFilterName()：获取Filter的配置名称；与元素对应； - String getInitParameter(String name)：获取Filter的初始化配置，与元素对应； - Enumeration getInitParameterNames()：获取所有初始化参数的名称。 FilterChain doFilter()方法的参数中有一个类型为FilterChain的参数，它只有一个方法：doFilter(ServletRequest,ServletResponse)。 前面我们说doFilter()方法的放行，让请求流访问目标资源！但这么说不严谨，其实调用该方法的意思是，“我（当前Filter)”放行了，但不代表其他人（其他过滤器)也放行。 如果当前过滤器是最后一个过滤器，那么调用chain.doFilter()方法表示执行目标资源，而不是最后一个过滤器，那么chain.doFilter()表示执行下一个过滤器的doFilter()方法。 多个过滤器执行顺序 一个目标资源可以指定多个过滤器，过滤器的执行顺序是在web.xml文件中的部署顺序： myFilter1 cn.itcast.filter.MyFilter1 myFilter1 /index.jsp myFilter2 cn.itcast.filter.MyFilter2 myFilter2 /index.jsp public class MyFilter1 extends HttpFilter { public void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws IOException, ServletException { System.out.println(\"filter1 start...\"); chain.doFilter(request, response);//放行，执行MyFilter2的doFilter()方法 System.out.println(\"filter1 end...\"); } } public class MyFilter2 extends HttpFilter { public void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws IOException, ServletException { System.out.println(\"filter2 start...\"); chain.doFilter(request, response);//放行，执行目标资源 System.out.println(\"filter2 end...\"); } } This is my JSP page. index.jsp 当有用户访问index.jsp页面时，输出结果如下： filter1 start... filter2 start... index.jsp filter2 end... filter1 end... AFilter B Filter CFilter 有点像是递归，调用filterchain的doFilter方法会放行给下一个过滤器，这个doFilter前面的代码的执行顺序是A-B-C，而后面的代码的执行顺序是C-B-A 四种拦截方式 我们来做个测试，写一个过滤器，指定过滤的资源为b.jsp，然后我们在浏览器中直接访问b.jsp，你会发现过滤器执行了！ 但是，当我们在a.jsp中request.getRequestDispathcer(“/b.jsp”).forward(request,response)时，就不会再执行过滤器了！也就是说，默认情况下，只能直接访问目标资源才会执行过滤器，而forward执行目标资源，不会执行过滤器！ public class MyFilter extends HttpFilter { public void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws IOException, ServletException { System.out.println(\"myfilter...\"); chain.doFilter(request, response); } } myfilter cn.itcast.filter.MyFilter myfilter /b.jsp b.jsp a.jsp http://localhost:8080/filtertest/b.jsp -->直接访问b.jsp时，会执行过滤器内容； http://localhost:8080/filtertest/a.jsp --> 访问a.jsp，但a.jsp会forward到b.jsp，这时就不会执行过滤器！ 其实过滤器有四种拦截方式！分别是：REQUEST、FORWARD、INCLUDE、ERROR。 - REQUEST：直接访问目标资源时执行过滤器。包括：在地址栏中直接访问、表单提交、超链接、重定向，只要在地址栏中可以看到目标资源的路径，就是REQUEST； - FORWARD：转发访问执行过滤器。包括RequestDispatcher#forward()方法、标签都是转发访问； - INCLUDE：包含访问执行过滤器。包括RequestDispatcher#include()方法、标签都是包含访问； - ERROR：当目标资源在web.xml中配置为中时，并且真的出现了异常，转发到目标资源时，会执行过滤器。 可以在中添加0~n个子元素，来说明当前访问的拦截方式。 如果没有写dispatcher，默认为REQUEST myfilter /b.jsp REQUEST FORWARD myfilter /b.jsp myfilter /b.jsp FORWARD 其实最为常用的就是REQUEST和FORWARD两种拦截方式，而INCLUDE和ERROR都比较少用！下面给出ERROR拦截方式的例子： myfilter /b.jsp ERROR 500 /b.jsp a.jsp 过滤器的应用场景 过滤器的应用场景： - 执行目标资源之前做预处理工作，例如设置编码，这种通常都会放行，只是在目标资源执行之前做一些准备工作； 几乎是所有的Sevlet中都需要写request.setCharacterEndoing() 可以把它放入到一个Filter中 - 通过条件判断是否放行，例如校验当前用户是否已经登录，或者用户IP是否已经被禁用； - 在目标资源执行后，做一些后续的特殊处理工作，例如把目标资源输出的数据进行处理； 设置目标资源 在web.xml文件中部署Filter时，可以通过“*”来执行目标资源： myfilter /* 这一特性与Servlet完全相同！通过这一特性，我们可以在用户访问敏感资源时，执行过滤器，例如：/admin/*，可以把所有管理员才能访问的资源放到/admin路径下，这时可以通过过滤器来校验用户身份。 还可以为指定目标资源为某个Servlet，例如： myservlet cn.itcast.servlet.MyServlet myservlet /abc myfilter cn.itcast.filter.MyFilter myfilter myservlet（可以有多个) 　　当用户访问http://localhost:8080/filtertest/abc时，会执行名字为myservlet的Servlet，这时会执行过滤器。 Filter小结 Filter的三个方法： - void init(FilterConfig)：在Tomcat启动时被调用； - void destroy()：在Tomcat关闭时被调用； - void doFilter(ServletRequest,ServletResponse,FilterChain)：每次有请求时都调用该方法； FilterConfig类：与ServletConfig相似，用来获取Filter的初始化参数 - ServletContext getServletContext()：获取ServletContext的方法； - String getFilterName()：获取Filter的配置名称； - String getInitParameter(String name)：获取Filter的初始化配置，与元素对应； - Enumeration getInitParameterNames()：获取所有初始化参数的名称。 FilterChain类： - void doFilter(ServletRequest,ServletResponse)：放行！表示执行下一个过滤器，或者执行目标资源。可以在调用FilterChain的doFilter()方法的前后添加语句，在FilterChain的doFilter()方法之前的语句会在目标资源执行之前执行，在FilterChain的doFilter()方法之后的语句会在目标资源执行之后执行。 四种拦截方式：REQUEST、FORWARD、INCLUDE、ERROR，默认是REQUEST方式。 - REQUEST：拦截直接请求方式； - FORWARD：拦截请求转发方式； - INCLUDE：拦截请求包含方式； - ERROR：拦截错误转发方式。 Listener 5.6 1、监听器概述 在JavaWeb被监听的事件源为：ServletContext、HttpSession、ServletRequest，即三大域对象。 - 监听域对象“创建”与“销毁”的监听器； - 监听域对象“操作域属性”的监听器； - 监听HttpSession的监听器。 2、创建与销毁监听器 创建与销毁监听器一共有三个： - ServletContextListener：Tomcat启动和关闭时调用下面两个方法 public void contextInitialized(ServletContextEvent evt)：ServletContext对象被创建后调用；  public void contextDestroyed(ServletContextEvent evt)：ServletContext对象被销毁前调用； - HttpSessionListener：开始会话和结束会话时调用下面两个方法  public void sessionCreated(HttpSessionEvent evt)：HttpSession对象被创建后调用；  public void sessionDestroyed(HttpSessionEvent evt)：HttpSession对象被销毁前调用； - ServletRequestListener：开始请求和结束请求时调用下面两个方法  public void requestInitiallized(ServletRequestEvent evt)：ServletRequest对象被创建后调用；  public void requestDestroyed(ServletRequestEvent evt)：ServletRequest对象被销毁前调用。 事件对象 - ServletContextEvent：ServletContext getServletContext()； - HttpSessionEvent：HttpSession getSession()； - ServletRequestEvent： - ServletRequest getServletRequest() - ServletContext getServletContext() 编写测试例子： - 编写MyServletContextListener类，实现ServletContextListener接口； - 在web.xml文件中部署监听器； - 为了看到session销毁的效果，在web.xml文件中设置session失效时间为1分钟； /* ServletContextListener实现类 contextDestroyed() -- 在ServletContext对象被销毁前调用 contextInitialized() -- -- 在ServletContext对象被创建后调用 ServletContextEvent -- 事件类对象 该类有getServletContext()，用来获取ServletContext对象，即获取事件源对象 */ public class MyServletContextListener implements ServletContextListener { public void contextDestroyed(ServletContextEvent evt) { System.out.println(\"销毁ServletContext对象\"); } public void contextInitialized(ServletContextEvent evt) { System.out.println(\"创建ServletContext对象\"); } } /* HttpSessionListener实现类 sessionCreated() -- 在HttpSession对象被创建后被调用 sessionDestroyed() -- -- 在HttpSession对象被销毁前调用 HttpSessionEvent -- 事件类对象 该类有getSession()，用来获取当前HttpSession对象，即获取事件源对象 */ public class MyHttpSessionListener implements HttpSessionListener { public void sessionCreated(HttpSessionEvent evt) { System.out.println(\"创建session对象\"); } public void sessionDestroyed(HttpSessionEvent evt) { System.out.println(\"销毁session对象\"); } } /* ServletRequestListener实现类 requestDestroyed() -- 在ServletRequest对象被销毁前调用 requestInitialized() -- 在ServletRequest对象被创建后调用 ServletRequestEvent -- 事件类对象 该类有getServletContext()，用来获取ServletContext对象 该类有getServletRequest()，用来获取当前ServletRequest对象，即事件源对象 */ public class MyServletRequestListener implements ServletRequestListener { public void requestDestroyed(ServletRequestEvent evt) { System.out.println(\"销毁request对象\"); } public void requestInitialized(ServletRequestEvent evt) { System.out.println(\"创建request对象\"); } } cn.itcast.listener.MyServletContextListener cn.itcast.listener.MyHttpSessionListener cn.itcast.listener.MyServletRequestListener 1 3、操作域属性的监听器 当对域属性进行增、删、改时，执行的监听器一共有三个： - ServletContextAttributeListener：在ServletContext域进行增、删、改属性时调用下面方法。 public void attributeAdded(ServletContextAttributeEvent evt)  public void attributeRemoved(ServletContextAttributeEvent evt)  public void attributeReplaced(ServletContextAttributeEvent evt) - HttpSessionAttributeListener：在HttpSession域进行增、删、改属性时调用下面方法  public void attributeAdded(HttpSessionBindingEvent evt)  public void attributeRemoved (HttpSessionBindingEvent evt)  public void attributeReplaced (HttpSessionBindingEvent evt) - ServletRequestAttributeListener：在ServletRequest域进行增、删、改属性时调用下面方法  public void attributeAdded(ServletRequestAttributeEvent evt)  public void attributeRemoved (ServletRequestAttributeEvent evt)  public void attributeReplaced (ServletRequestAttributeEvent evt) 下面对这三个监听器的事件对象功能进行介绍： - ServletContextAttributeEvent - String getName()：获取当前操作的属性名； - Object getValue()：获取当前操作的属性值； - ServletContext getServletContext()：获取ServletContext对象。 - HttpSessionBindingEvent - String getName()：获取当前操作的属性名； - Object getValue()：获取当前操作的属性值； - HttpSession getSession()：获取当前操作的session对象。 - ServletRequestAttributeEvent - String getName()：获取当前操作的属性名； - Object getValue()：获取当前操作的属性值； - ServletContext getServletContext()：获取ServletContext对象； - ServletRequest getServletRequest()：获取当前操作的ServletRequest对象。 如果是替换，那么getValue返回的是原值；如果想拿到新值，那么去对应的域对象中取。 4、HttpSession的监听器 还有两个与HttpSession相关的特殊的监听器，这两个监听器的特点如下： - 不用在web.xml文件中部署； - 这两个监听器不是给session添加，而是给Bean添加。即让Bean类实现监听器接口，然后再把Bean对象添加到session域中。 下面对这两个监听器介绍一下： - HttpSessionBindingListener：当某个类实现了该接口后，可以感知本类对象添加到session中，以及感知从session中移除。例如让Person类实现HttpSessionBindingListener接口，那么当把Person对象添加到session中，或者把Person对象从session中移除时会调用下面两个方法：  public void valueBound(HttpSessionBindingEvent event)：当把监听器对象添加到session中会调用监听器对象的本方法；  public void valueUnbound(HttpSessionBindingEvent event)：当把监听器对象从session中移除时会调用监听器对象的本方法； 这里要注意，HttpSessionBindingListener监听器的使用与前面介绍的都不相同，当该监听器对象添加到session中，或把该监听器对象从session移除时会调用监听器中的方法。并且无需在web.xml文件中部署这个监听器。 示例步骤： - 编写Person类，让其实现HttpSessionBindingListener监听器接口； - 编写Servlet类，一个方法向session中添加Person对象，另一个从session中移除Person对象； - 在index.jsp中给出两个超链接，分别访问Servlet中的两个方法。 Pseron.java public class Person implements HttpSessionBindingListener { private String name; private int age; private String sex; public Person(String name, int age, String sex) { super(); this.name = name; this.age = age; this.sex = sex; } public Person() { super(); } public String toString() { return \"Person [name=\" + name + \", age=\" + age + \", sex=\" + sex + \"]\"; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } public String getSex() { return sex; } public void setSex(String sex) { this.sex = sex; } public void valueBound(HttpSessionBindingEvent evt) { System.out.println(\"把Person对象存放到session中：\" + evt.getValue()); } public void valueUnbound(HttpSessionBindingEvent evt) { System.out.println(\"从session中移除Pseron对象：\" + evt.getValue()); } } ListenerServlet.java public class ListenerServlet extends BaseServlet { public String addPerson(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { Person p = new Person(\"zhangSan\", 23, \"male\"); request.getSession().setAttribute(\"person\", p); return \"/index.jsp\"; } public String removePerson(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { request.getSession().removeAttribute(\"person\"); return \"/index.jsp\"; } index.jsp \">addPerson \">removePerson Session有一种重生的性质。 在服务器开启时保存了session，然后当服务器被关闭时，会在硬盘上保存一个session属性的序列化文件。 启动服务器时会重新将该序列化文件读入内存，并删除该文件。 在conf/context.xml中进行一些配置可以取消保存session序列化文件的操作。 - HttpSessionActivationListener：Tomcat会在session不被使用时钝化session对象，所谓钝化session，就是把session通过序列化的方式保存到硬盘文件中。当用户再使用session时，Tomcat还会把钝化的对象再活化session，所谓活化就是把硬盘文件中的session在反序列化回内存。当session被Tomcat钝化时，session中存储的对象也被钝化，当session被活化时，也会把session中存储的对象活化。如果某个类实现了HttpSessionActiveationListener接口后，当对象随着session被钝化和活化时，下面两个方法就会被调用： public void sessionWillPassivate(HttpSessionEvent se)：当对象感知被活化时调用本方法；  public void sessionDidActivate(HttpSessionEvent se)：当对象感知被钝化时调用本方法； HttpSessionActivationListener监听器与HttpSessionBindingListener监听器相似，都是感知型的监听器，例如让Person类实现了HttpSessionActivationListener监听器接口，并把Person对象添加到了session中后，当Tomcat钝化session时，同时也会钝化session中的Person对象，这时Person对象就会感知到自己被钝化了，其实就是调用Person对象的sessionWillPassivate()方法。当用户再次使用session时，Tomcat会活化session，这时Person会感知到自己被活化，其实就是调用Person对象的sessionDidActivate()方法。 注意JavaBean同时需要实现Serializable接口。 注意，因为钝化和活化session，其实就是使用序列化和反序列化技术把session从内存保存到硬盘，和把session从硬盘加载到内存。这说明如果Person类没有实现Serializable接口，那么当session钝化时就不会钝化Person，而是把Person从session中移除再钝化！这也说明session活化后，session中就不再有Person对象了。 Cookie(服务器创建，客户端保存) Cookie如果我不想js或http读，应该设置什么属性？里面有个属性httpOnly，这个属性你了解吗？5.7 1、Cookie概述 1.1　什么叫Cookie Cookie翻译成中文是小甜点，小饼干的意思。在HTTP中它表示服务器送给客户端浏览器的小甜点。由HTTP协议制定。其实Cookie就是一个键和一个值构成的，随着服务器端的响应发送给客户端浏览器。然后客户端浏览器会把Cookie保存起来，当下一次再访问服务器时把Cookie再发送给服务器。 　　Cookie是由服务器创建，然后通过响应发送给客户端的一个键值对。客户端会保存Cookie，并会标注出Cookie的来源（哪个服务器的Cookie)。当客户端向服务器发出请求时会把所有这个服务器Cookie包含在请求中发送给服务器，这样服务器就可以识别客户端了！ 1.2　Cookie规范 - Cookie大小上限为4KB； - 一个服务器最多在客户端浏览器上保存20个Cookie； - 一个浏览器最多保存300个Cookie； 上面的数据只是HTTP的Cookie规范，但在浏览器大战的今天，一些浏览器为了打败对手，为了展现自己的能力起见，可能对Cookie规范“扩展”了一些，例如每个Cookie的大小为8KB，最多可保存500个Cookie等！但也不会出现把你硬盘占满的可能！ 注意，不同浏览器之间是不共享Cookie的。也就是说在你使用IE访问服务器时，服务器会把Cookie发给IE，然后由IE保存起来，当你在使用FireFox访问服务器时，不可能把IE保存的Cookie发送给服务器。 1.3　Cookie与HTTP头 Cookie是通过HTTP请求头和响应头在客户端和服务器端传递的： - Cookie：请求头，客户端发送给服务器端； - 格式：Cookie: a=A; b=B; c=C。即多个Cookie用分号分开； 一个Cookie对应多个键值对 - Set-Cookie：响应头，服务器端发送给客户端； - 一个Cookie对象一个Set-Cookie，一个键值对 Set-Cookie: a=A Set-Cookie: b=B Set-Cookie: c=C 比如可以用response.addHeader(“Set-Cookie”,”a=A”);发送一个Cookie 便捷的使用Cookie方式： HttpServletRequest: 没有返回null HttpServletResponse: 1.4　Cookie的覆盖 　　如果服务器端发送重复的Cookie那么会覆盖原有的Cookie，例如客户端的第一个请求服务器端发送的Cookie是：Set-Cookie: a=A；第二请求服务器端发送的是：Set-Cookie: a=AA，那么客户端只留下一个Cookie，即：a=AA。 2、Cookie的生命周期 　　Cookie不只是有name和value，Cookie还是生命。所谓生命就是Cookie在客户端的有效时间，可以通过setMaxAge(int)来设置Cookie的有效时间。  - cookie.setMaxAge(-1)：cookie的maxAge属性的默认值就是-1，表示只在浏览器内存中存活。一旦关闭浏览器窗口，那么cookie就会消失。 - cookie.setMaxAge(60*60)：表示cookie对象可存活1小时。当生命大于0时，浏览器会把Cookie保存到硬盘上，就算关闭浏览器，就算重启客户端电脑，cookie也会存活1小时； - cookie.setMaxAge(0)：cookie生命等于0是一个特殊的值，它表示cookie被作废！也就是说，如果原来浏览器已经保存了这个Cookie，那么可以通过Cookie的setMaxAge(0)来删除这个Cookie。无论是在浏览器内存中，还是在客户端硬盘上都会删除这个Cookie。 3、Cookie的path（服务器的路径) 3.1　什么是Cookie的路径 现在有WEB应用A，向客户端发送了10个Cookie，这就说明客户端无论访问应用A的哪个Servlet都会把这10个Cookie包含在请求中！但是也许只有AServlet需要读取请求中的Cookie，而其他Servlet根本就不会获取请求中的Cookie。这说明客户端浏览器有时发送这些Cookie是多余的！ 可以通过设置Cookie的path来指定浏览器，在访问什么样的路径时，包含什么样的Cookie。 3.2　Cookie路径与请求路径的关系 下面我们来看看Cookie路径的作用： 下面是客户端浏览器保存的3个Cookie的路径： a:　/cookietest； b:　/cookietest/servlet； c:　/cookietest/jsp； 下面是浏览器请求的URL： A:　http://localhost:8080/cookietest/AServlet； B:　http://localhost:8080/cookietest/servlet/BServlet； C:　http://localhost:8080/cookietest/jsp/a.jsp； - 请求A时，会在请求中包含a； - 请求B时，会在请求中包含a、b； - 请求C时，会在请求中包含a、c； 请求路径如果包含了Cookie路径，那么会在请求中包含这个Cookie。 A请求的URL包含了“/cookietest”，所以会在请求中包含路径为“/cookietest”的Cookie； - B请求的URL包含了“/cookietest”，以及“/cookietest/servlet”，所以请求中包含路径为“/cookietest”和“/cookietest/servlet”两个Cookie； - C请求的URL包含了“/cookietest”，以及“/cookietest/jsp”，所以请求中包含路径为“/cookietest”和“/cookietest/jsp”两个Cookie； 3.3　设置Cookie的路径 设置Cookie的路径需要使用setPath()方法，例如： cookie.setPath(“/cookietest/servlet”); 默认路径是访问资源的上一级路径 如果没有设置Cookie的路径，那么Cookie路径的默认值当前访问资源所在路径（上一级)，例如： - 访问http://localhost:8080/cookietest/AServlet时添加的Cookie默认路径为/cookietest； - 访问http://localhost:8080/cookietest/servlet/BServlet时添加的Cookie默认路径为/cookietest/servlet； - 访问http://localhost:8080/cookietest/jsp/BServlet时添加的Cookie默认路径为/cookietest/jsp； 4、Cookie的domain Cookie的domain属性可以让网站中二级域共享Cookie，次要！ 百度你是了解的对吧！ http://www.baidu.com http://zhidao.baidu.com http://news.baidu.com http://tieba.baidu.com 现在我希望在这些主机之间共享Cookie（例如在www.baidu.com中响应的cookie，可以在news.baidu.com请求中包含)。很明显，现在不是路径的问题了，而是主机的问题，即域名的问题。处理这一问题其实很简单，只需要下面两步： - 设置Cookie的path为“/”：c.setPath(“/”)； - 设置Cookie的domain为“.baidu.com”：c.setDomain(“.baidu.com”)。 5、Cookie中保存中文 Cookie的name和value都不能使用中文，如果希望在Cookie中使用中文，那么需要先对中文进行URL编码，然后把编码后的字符串放到Cookie中。 　向客户端响应中添加Cookie String name = URLEncoder.encode(\"姓名\", \"UTF-8\"); String value = URLEncoder.encode(\"张三\", \"UTF-8\"); Cookie c = new Cookie(name, value); c.setMaxAge(3600); response.addCookie(c); 从客户端请求中获取Cookie response.setContentType(\"text/html;charset=utf-8\"); Cookie[] cs = request.getCookies(); if(cs != null) { for(Cookie c : cs) { String name = URLDecoder.decode(c.getName(), \"UTF-8\"); String value = URLDecoder.decode(c.getValue(), \"UTF-8\"); String s = name + \": \" + value + \"\"; response.getWriter().print(s); } } 5.8 6、Cookie中的属性 HttpOnly 如果Cookie中设置了HttpOnly属性，那么通过js脚本将无法读取到cookie信息，这样能有效的防止XSS攻击，窃取cookie内容，这样就增加了cookie的安全性，即便是这样，也不要将重要信息存入cookie。 Servlet3.0支持setHttpOnly(boolean httpOnly)。 非servlet3.0的JAVAEE项目也可以通过设置Header进行设置： response.setHeader(\"Set-Cookie\", \"cookiename=value; Path=/;Domain=domainvalue;Max-Age=seconds;HTTPOnly\"); max-age expires/Max-Age 字段为此cookie超时时间。若设置其值为一个时间，那么当到达此时间后，此cookie失效。不设置的话默认值是Session，意思是cookie会和session一起失效。当浏览器关闭(不是浏览器标签页，而是整个浏览器) 后，此cookie失效。 secure 设置是否只能通过https来传递此条cookie Session(服务器创建，服务器保存) session默认过期时间，过长会怎么样：30min，过长占用内存 如何防止session被别人伪造cookie得到：Cookie的值后面跟上一段防篡改的验证串。防篡改的验证串可以由DES(cookie的内容+盐值)，也可以用 MD5（cookie内容+密钥)，。也可以是SHA1（cookie内容+密钥)，这里的密钥只有站点本身知道，如果这个都泄漏了那就真完蛋了。这个值在服务器接收到Cookie以后，就可以用Cookie的内容+密钥重新计算一次验证串，和提交上来的做比对，如果是一致的，我们就认为cookie没有被篡改，反之，cookie肯定被篡改过，我们就不要相信这一次提交。如果所有的Cookie都经过防篡改验证，那么也就不用担心SessionID被冒名顶替的事情发生了。 如果服务器的负载量很高内存负荷不住要怎么办：尽量规避使用，优先以Cookie（包括加密Cookie)来解决；Session要自己实现分布式储存，比如使用 redis 等做储存。 1、HttpSession概述（属于JavaWeb) 1.1　什么是HttpSesssion javax.servlet.http.HttpSession接口表示一个会话，我们可以把一个会话内需要共享的数据保存到HttSession对象中！ 会话：会话范围是某个用户从首次访问服务器开始，到该用户关闭浏览器结束。 1.2　获取HttpSession对象 - HttpSession request.getSesssion()：如果当前会话已经有了session对象那么直接返回，如果当前会话还不存在会话，那么创建session并返回； - HttpSession request.getSession(boolean)：当参数为true时，与requeset.getSession()相同。如果参数为false，那么如果当前会话中存在session则返回，不存在返回null； - JSP中session是一个内置对象，可以直接使用 1.3　HttpSession是域对象 我们已经学习过HttpServletRequest、ServletContext，它们都是域对象，现在我们又学习了一个HttpSession，它也是域对象。它们三个是Servlet中可以使用的域对象， 而JSP中可以多使用一个域对象PageContext。 HttpServletRequest：一个请求创建一个request对象，所以在同一个请求中可以共享request，例如一个请求从AServlet转发到BServlet，那么AServlet和BServlet可以共享request域中的数据； - ServletContext：一个应用只创建一个ServletContext对象，所以在ServletContext中的数据可以在整个应用中共享，只要不关闭服务器，那么ServletContext中的数据就可以共享； - HttpSession：一个会话创建一个HttpSession对象，同一会话中的多个请求中可以共享session中的数据； 下面是session的域方法： - void setAttribute(String name, Object value)：用来存储一个对象，也可以称之为存储一个域属性，例如：session.setAttribute(“xxx”, “XXX”)，在session中保存了一个域属性，域属性名称为xxx，域属性的值为XXX。请注意，如果多次调用该方法，并且使用相同的name，那么会覆盖上一次的值，这一特性与Map相同； - Object getAttribute(String name)：用来获取session中的数据，当前在获取之前需要先去存储才行，例如：String value = (String) session.getAttribute(“xxx”);，获取名为xxx的域属性； - void removeAttribute(String name)：用来移除HttpSession中的域属性，如果参数name指定的域属性不存在，那么本方法什么都不做； - Enumeration getAttributeNames()：获取所有域属性的名称； 2、登录案例 需要的页面： - login.jsp：登录页面，提供登录表单； - succ1.jsp：主页，显示当前用户名称，如果没有登录，显示您还没登录； - succ2.jsp：同上 Servlet： - LoginServlet：在login.jsp页面提交表单时，请求本Servlet。在本Servlet中获取用户名、密码进行校验，如果用户名、密码错误，显示“用户名或密码错误”，如果正确保存用户名session中，然后重定向到succ1.jsp； 保存用户名到session的目的是保存登录状态 　　当用户没有登录时访问succ1.jsp或succ2.jsp，保存错误信息“您还没有登录”，并转发给login.jsp。如果用户在login.jsp登录成功后到达succ1.jsp页面会显示当前用户名，而且不用再次登录去访问succ2.jsp也会显示用户名。因为多次请求在一个会话范围，succ1.jsp和succ2.jsp都会到session中获取用户名，session对象在一个会话中是相同的，所以都可以获取到用户名！ 用户名的自动填充：如果曾经登录过（session中保存有username)，则将该username保存到Cookie中，发送给客户端。如果再次访问login.jsp，则将username自动填充。 注意要设置Cookie的生命周期 代码： login.jsp String username = \"\"; Cookie[] cookies = request.getCookies(); if (cookies != null) { for (Cookie c : cookies) { if (c.getName().equals(\"username\")) { username = c.getValue(); } } } %> username:\"> password: type=\"submit\" value=\"login\"> String status = (String) request.getAttribute(\"status\"); if (status != null) { out.print(status); } %> LoginServlet public class LoginServlet extends HttpServlet { public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { request.setCharacterEncoding(\"utf-8\"); String username = request.getParameter(\"username\"); String password = request.getParameter(\"password\"); if (username.equals(\"admin\") && password.equals(\"admin\")) { // 使用session保存信息，可以使用重定向，因为处于同一会话 // 避免Servlet的路径出现在地址栏中 //保存一个session的域 request.getSession().setAttribute(\"username\", username); //保存一个cookie Cookie cookie = new Cookie(\"username\", username); cookie.setMaxAge(606024);//一天 response.addCookie(cookie); response.sendRedirect(\"/Session/jsp/succ1.jsp\"); } else { // 使用request保存信息，只能使用转发，因为处于同一请求 request.setAttribute(\"status\", \"用户名或密码错误\"); request.getRequestDispatcher(\"/jsp/login.jsp\").forward(request, response); } } } succ1.jsp String username = (String)session.getAttribute(\"username\"); if(username != null){ out.print(\"欢迎您,\"+username+\"!\"); }else{ request.setAttribute(\"status\", \"您尚未登录\"); request.getRequestDispatcher(\"/jsp/login.jsp\").forward(request, response); } %> 3、session的实现原理 session底层是依赖Cookie的！我们来理解一下session的原理吧！ 当首次使用session时，服务器端要创建session，session是保存在服务器端，而给客户端的session的id（一个cookie中保存了sessionId)。客户端带走的是sessionId，而数据是保存在session中。 当客户端再次访问服务器时，在请求中会带上sessionId，而服务器会通过sessionId找到对应的session，而无需再创建新的session。 如果Servlet中没有创建session，那么响应头中就不会有sessionid的cookie。但是所有的JSP页面中，session可以直接使用，也就是说每个JSP页面都会自动获取session，无论是否使用，访问JSP页面一定会带回来一个sessionid。 4、session与浏览器 session保存在服务器，而sessionId通过Cookie发送给客户端，但这个Cookie的生命不是-1，即只在浏览器内存中存在，也就是说如果用户关闭了浏览器，那么这个Cookie就丢失了。 当用户再次打开浏览器访问服务器时，就不会有sessionId发送给服务器，那么服务器会认为你没有session，所以服务器会创建一个session，并在响应中把sessionId中到Cookie中发送给客户端。　　　　　 你可能会说，那原来的session对象会怎样？当一个session长时间没人使用的话，服务器会把session删除了！这个时长在Tomcat中配置是30分钟，可以在${CATALANA}/conf/web.xml找到这个配置，当然你也可以在自己的web.xml中覆盖这个配置！ web.xml 30 session失效时间也说明一个问题！如果你打开网站的一个页面开始长时间不动，超出了30分钟后，再去点击链接或提交表单时你会发现，你的session已经丢失了！ 5、session其他常用API - String getId()：获取sessionId；32位长，16进制字符串，使用UUID生成随机字符串； - int getMaxInactiveInterval()：获取session可以的最大不活动时间（秒)，默认为30分钟。当session在30分钟内没有使用，那么Tomcat会在session池中移除这个session； - void setMaxInactiveInterval(int interval)：设置session允许的最大不活动时间（秒)，如果设置为1秒，那么只要session在1秒内不被使用，那么session就会被移除； - long getCreationTime()：返回session的创建时间，返回值为当前时间的毫秒值； - long getLastAccessedTime()：返回session的最后活动时间，返回值为当前时间的毫秒值； - void invalidate()：让session失效！调用这个方法会被session失效，当session失效后，客户端再次请求，服务器会给客户端创建一个新的session，并在响应中给客户端新session的sessionId；（比如退出按钮，退出时让session失效) - boolean isNew()：查看session是否为新。当客户端第一次请求时，服务器为客户端创建session，但这时服务器还没有响应客户端，也就是还没有把sessionId响应给客户端时，这时session的状态为新。请求中没有sessionid的Cookie request.getSession().isNew()可以判断这个Session是新的还是旧的。 6、URL重写 我们知道session依赖Cookie，那么session为什么依赖Cookie呢？因为服务器需要在每次请求中获取sessionId，然后找到客户端的session对象。那么如果客户端浏览器关闭了Cookie呢？那么session是不是就会不存在了呢？ 其实还有一种方法让服务器收到的每个请求中都带有sessioinId，那就是URL重写！在每个页面中的每个链接和表单中都添加名为jSessionId的参数，值为当前sessionid。当用户点击链接或提交表单时也服务器可以通过获取jSessionId这个参数来得到客户端的sessionId，找到sessoin对象。 index.jsp URL重写 ' >主页 ' method=\"post\"> - 也可以使用response.encodeURL()对每个请求的URL处理，这个方法会自动追加jsessionid参数，与上面我们手动添加是一样的效果。 ' >主页 ' method=\"post\"> 　　使用response.encodeURL()更加“智能”，它会判断客户端浏览器是否禁用了Cookie，如果禁用了，那么这个方法在URL后面追加jsessionid，否则不会追加。 AsyncServlet&Reactor 5.9 API Servlet异步处理就是让Servlet在处理费时的请求时不要阻塞，而是一部分一部分的显示。 也就是说，在使用Servlet异步处理之后，页面可以一部分一部分的显示数据，而不是一直卡，等到请求响应结束后一起显示。 在使用异步处理之前，一定要在@WebServlet注解中给出asyncSupported=true，不然默认Servlet是不支持异步处理的。如果存在过滤器，也要设置@WebFilter的asyncSupportedt=true。 @WebServlet(urlPatterns = {\"/MyServlet\"}, asyncSupported=true) public class MyServlet extends HttpServlet {…} 　　注意，响应类型必须是text/html，所以：response.setContentType(“text/html;charset=utf-8”); 使用异步处理大致可以分为两步： - Servlet正常响应数据； - Servlet异常响应数据。 在Servlet正常响应数据时，没什么可说的，可通知response.getWriter().print()来向客户端输出，但输出后要使用response.getWriter().flush()刷新，不然数据只是在缓冲区中，不能向客户端发送数据的。 异步响应数据需要使用request.startAsync()方法获取AsyncContext对象。然后调用AsyncContext对象的start()方法启动异步响应，start()方法需要一个Runnable类型的参数。在Runnable的run()方法中给出异步响应的代码。 AsyncContext ac = request.startAsyncContext(request, response); ac.start(new Runnable() {…}); 注意在异步处理线程中使用response做响应后，要使用response.getWriter().flush()来刷新流，不然数据是不能响应到客户端浏览器的。 asyncContext.start(new Runnable() { public void run() { for(char i = 'a'; i Tomcat需要知道异步响应是否结束，如果响应不结束，虽然客户端浏览器会看到响应的数据，但是鼠标上只是有个圈圈的不行的转啊转的，表示还没有结束响应。Tomcat会等待到超时为止，这个超时的时间可以通过AsyncContext类的getTimeout()方法获取，Tomcat默认为20000毫秒。当然也可以通过setTimeOut()方法设置，以毫秒为单位。ac.setTimeout(1000*10)。 如果异步线程已经结束了响应，那么可以在异步线程中调用AsyncContext.complete()方法，这样Tomcat就知道异步线程已经完成了工作了。 @WebServlet(urlPatterns = {\"/AServlet\"}, asyncSupported=true) public class AServlet extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { response.setContentType(\"text/html;charset=utf-8\"); PrintWriter out = response.getWriter(); out.println(\"Servlet begin \"); out.println(\"Servlet begin \"); out.println(\"Servlet begin \"); out.println(\"Servlet begin \"); out.println(\"Servlet begin \"); out.println(\"Servlet begin \"); out.println(\"Servlet begin \"); out.println(\"Servlet begin \"); out.println(\"Servlet begin \"); out.println(\"Servlet begin \"); out.println(\"Servlet begin \"); out.println(\"Servlet begin \"); out.println(\"Servlet begin \"); out.println(\"Servlet begin \"); out.println(\"Servlet begin \"); out.flush(); final AsyncContext asyncContext = request.startAsync(request, response); asyncContext.setTimeout(1000 * 20); asyncContext.start(new Runnable() { public void run() { try { Thread.sleep(1000); asyncContext.getResponse().getWriter().print(\"马上开始\" + \"\"); asyncContext.getResponse().getWriter().flush(); Thread.sleep(2000); } catch (Exception e1) { } for(char i = 'a'; i \"); } } 模型 在引入Servlet3之前我们的线程模型是如下样子的： 整个请求解析、业务处理、生成响应都是由Tomcat线程池进行处理，而且都是在一个线程中处理；不能分离线程处理；比如接收到请求后交给其他线程处理，这样不能灵活定义业务处理模型。 引入Servlet3之后，我们的线程模型可以改造为如下样子： 此处可以看到请求解析使用Tomcat单线程；而解析完成后会扔到业务队列中，由业务线程池进行处理；这种处理方式可以得到如下好处： 1、根据业务重要性对业务进行分级，然后根据分级定义线程池； 2、可以拿到业务线程池，可以进行很多的操作，比如监控、降级等。 分级线程池 好处 更高的并发能力； 请求解析和业务处理线程池分离； 根据业务重要性对业务分级，并分级线程池； 对业务线程池进行监控、运维、降级等处理。 总结 通过异步化我们不会获得更快的响应时间，但是我们获得了整体吞吐量和我们需要的灵活性：请求解析和业务处理线程池分离；根据业务重要性对业务分级，并分级线程池；对业务线程池进行监控、运维、降级等处理。5.10 与SpringWebFlux Servlet3.1 规范其中一个新特性是异步处理支持。 异步处理支持：Servlet 线程不需一直阻塞，即不需要到业务处理完毕再输出响应，然后结束 Servlet线程。异步处理的作用是在接收到请求之后，Servlet 线程可以将耗时的操作委派给另一个线程来完成，在不生成响应的情况下返回至容器。主要应用场景是针对业务处理较耗时的情况，可以减少服务器资源的占用，并且提高并发处理速度。 所以 WebFlux 支持的容器有 Tomcat、Jetty（Non-Blocking IO API) ，也可以像 Netty 和 Undertow 的本身就支持异步容器。在容器中 Spring WebFlux 会将输入流适配成 Mono 或者 Flux 格式进行统一处理。5.11 Reactor Reactive Streams（以下简称为RS)是“一种规范，它为基于非阻塞回压的异步流处理提供了标准”。它是一组包含了TCK工具套件和四个简单接口（Publisher、Subscriber、Subscription和Processor)的规范，这些接口将被集成到Java 9. Reactor不同于其它框架的最关键一点就是RS。Flux和Mono这两者都是RS的Publisher实现，它们都具备了响应式回压的特点。 Reactive Streams的主流实现有RxJava和Reactor，Spring WebFlux默认集成的是Reactor。Flux 和 Mono Flux 和 Mono 是 Reactor 中的两个基本概念。Flux 表示的是包含 0 到 N 个元素的异步序列。在该序列中可以包含三种不同类型的消息通知：正常的包含元素的消息、序列结束的消息和序列出错的消息。当消息通知产生时，订阅者中对应的方法 onNext(), onComplete()和 onError()会被调用。Mono 表示的是包含 0 或者 1 个元素的异步序列。该序列中同样可以包含与 Flux 相同的三种类型的消息通知。Flux 和 Mono 之间可以进行转换。对一个 Flux 序列进行计数操作，得到的结果是一个 Mono对象。把两个 Mono 序列合并在一起，得到的是一个 Flux 对象。 JDBC JDBC的桥接设计模式 jdbc的详细链接过程；底层源码；在Java中调用存储过程的方法 为什么要使用JDBC JDBC可以跨数据库平台 系统可以 小规模时使用MySQL 规模变大时使用Oracle 如果不需要改动API函数，就需要分层。 ODBC统一与数据库的接口，ADO是.NET统一与数据库的接口 JDBC是java与数据库统一的接口 但是注意JDBC还是要求将SQL语句插入到代码中，而SQL语句各数据库有所不同，所以不能完全实现移植。如果要实现完全的移植，需要使用Hibernate技术 Hibernate将不同数据库微小的区别也屏蔽掉了 EJB也实现了屏蔽数据库间微小的区别 JDBC对于java的接口是一致，但对于不同数据库JDBC所用的类库是不同的，对于程序猿而言是透明的。 JDBC编程步骤 先需要找JDBC的类库 java.sql Driver 驱动，用来提供给JDBC连接到数据库 java并不知道使用的是哪种数据库，而java有一个管理数据库的管家DriverManager，它来管理使用哪种数据库的连接。 连接到某个数据库，需要先向DriverManager注册。然后通过DriverManager连接到数据库。 代码示例： import java.sql.*; public class TestJDBC { public static void main(String []args){ Connection conn = null; Statement stmt = null; ResultSet rs = null; try { Class.forName(\"com.mysql.jdbc.Driver\"); conn = DriverManager.getConnection(\"jdbc:mysql://localhost/mydata?user=root&password=130119\"); stmt = conn.createStatement(); rs = stmt.executeQuery(\"select * from emp\"); while(rs.next()){ System.out.println(rs.getString(\"deptno\")); } } catch (SQLException | InstantiationException | IllegalAccessException | ClassNotFoundException e) { e.printStackTrace(); }finally{ try{ if(rs != null){ rs.close(); rs = null; } if(stmt != null ){ stmt.close(); stmt= null; } if(conn != null){ conn.close(); conn = null; } }catch(SQLException e){ e.printStackTrace(); } } } } 总结：JDBC连接流程 1.- 注册：驱动实例化 Class forName() 2.- 连接：获取连接 Connection DriverManager getConnection() 3.- 执行：通过连接执行SQL语句 Statement createStatement() executeQuery() 4.- 接收并输出：循环遍历取出数据 ResultSet next() getString() 5.- 关闭：close 先打开的后关闭 close() 除此之外，还需要抓Exception，并将关闭的代码放到finally里面 关闭前先判断是否为null，如果为null那么关闭时会出错 在close中也会抛Exception，也要写try catch JDBCUtils import java.io.IOException; import java.util.Properties; import java.sql.*; public final class JDBCUtils { private static String driver; private static String url; private static String username; private static String password; private JDBCUtils() { } static { Properties props = new Properties(); try { props.load(JDBCUtils.class.getClassLoader().getResourceAsStream( \"dbinfo.properties\")); driver = props.getProperty(\"driver\"); url = props.getProperty(\"url\"); username = props.getProperty(\"username\"); password = props.getProperty(\"password\"); } catch (IOException e) { throw new ExceptionInInitializerError(e); } try { Class.forName(driver); } catch (ClassNotFoundException e) { throw new ExceptionInInitializerError(e); } } public static Connection getConn() { Connection conn = null; try { conn = DriverManager.getConnection(url, username, password); } catch (SQLException e) { e.printStackTrace(); } return conn; } public static void free(ResultSet rs, Statement stmt, Connection conn) { try { if (rs != null) { rs.close(); rs = null; } } catch (SQLException e) { e.printStackTrace(); } finally { try { if (stmt != null) { stmt.close(); stmt = null; } } catch (SQLException e) { e.printStackTrace(); } finally { try { if (conn != null) { conn.close(); conn = null; } } catch (SQLException e) { e.printStackTrace(); } } } } } 实际调用的代码： import java.sql.*; public class TestTemplate { public static void main(String[] args) { read(); } public static void read(){ Connection c = null; Statement stmt = null; ResultSet rs = null; try{ c = JDBCUtils.getConn(); stmt = c.createStatement(); rs = stmt.executeQuery(\"select * from dept\"); while(rs.next()){ System.out.println(rs.getString(\"deptno\")); } }catch(SQLException e){ e.printStackTrace(); }finally{ JDBCUtils.free(rs, stmt, c); } } } 以下可以忽略 工具类可以采用单例模式 类中有一个静态私有的实例，有一个公开的静态的获取该实例的方法，其他方法是public但不是静态的 当其他的类要调用该工具类的方法时，先获取该类的实例，再调用其方法 还可以采用延迟加载，当第一次使用该类时new出实例；一开始是没有实例的成员变量的 以后使用该类就不会再new第二个实例了。 构造实例的成本很高时多采用该方法 还可以再完善，并发（多线程)控制时将new实例的部分加锁，避免出现两个实例 永远保持单个实例 这是双重检查 DML增删改 注意sql语句不建议用*，并且get获取值时不应该写1,2 ；写为字段名可读性更高 最好将sql语句中列出所需要的字段，全部取出效率较低 这是为了降低维护成本 executeUpdate 返回值是int类型，是执行成功的影响行数 注意关闭执行DML语句的资源也可以使用封装的free方法，因为会判断是否为空，空即不关闭 注意与之前步骤不同，不需要ResultSet这个类，因为不需要接收结果集 示例模板（使用了JDBCUtils工具类)： import java.sql.*; public class TestTemplate { public static void main(String []args){ add(); } private static void add() { Connection c = null; Statement stmt = null; ResultSet rs = null; try{ c = JDBCUtils.getConn(); stmt = c.createStatement(); String sql = \"insert into dept2 values(14,'lala','china')\"; int result = stmt.executeUpdate(sql); System.out.println(result); }catch(SQLException e){ e.printStackTrace(); }finally{ JDBCUtils.free(rs, stmt, c); } } }JDBC进阶 SQL注入攻击指的是通过构建特殊的输入作为参数传入Web应用程序，而这些输入大都是SQL语法里的一些组合，通过执行SQL语句进而执行攻击者所要的操作，其主要原因是程序没有细致地过滤用户输入的数据，致使非法数据侵入系统。 select * from emp where ename = '' or 1 = 1; PreparedStatement类 是Statement接口的子接口 表示预编译的 SQL 语句的对象。 SQL 语句（增改删DML)被预编译并存储在 PreparedStatement 对象中。然后可以使用此对象多次高效地执行该SQL语句。 与Statement不同的是通过preparedStatement方法来获得PreparedStatement对象。 参数： sql - 可能包含一个或多个 '?' IN 参数占位符的 SQL 语句 返回： 包含预编译 SQL 语句的新的默认 PreparedStatement 对象 不同于createStatement不需要参数，这个方法需要一个参数，即预编译的SQL语句，其插入的数据用？占位符表示 PreparedStatement pstmt = conn.prepareStatement(“insert into dept values(?,?,?)”); 之后再使用pstmt来为这三个？赋值 使用的方法有： 这些方法的第一个参数都是表示第几个占位符 1表示第一个？ 2表示第二个？ pstmt.setInt(1,deptno); pstmt.setString(2,dname); pstmt.setString(3,loc); 另一个区别是executeUpdate方法不再需要参数传入。 PreparedStatement类的好处是不必去考虑怎么才能拼凑出格式正确的SQL语句，而是调用方法就可以设置相应的值，十分方便；也易于修改占位符值。尽量使用；可以解决SQL注入问题（过滤掉特殊字符)（最大的优点)；同时效率同Statement相比较高（避免频繁SQL语句，是预编译) 可以执行查询和DML操作。 示例： import java.sql.*; public class TestTemplate { public static void main(String []args){ - if(args.length != 3){ System.out.println(\"Input Error!\"); System.exit(-1); } int deptno =0 ; try{ deptno = Integer.parseInt(args[0]); }catch(NumberFormatException e){ e.printStackTrace(); } String dname = args[1]; String loc = args[2]; Connection c = null; PreparedStatement pstmt = null; ResultSet rs = null; try{ c = JDBCUtils.getConn(); pstmt = c.prepareStatement(\"insert into dept2 values(?,?,?)\"); pstmt.setInt(1, deptno); pstmt.setString(2,dname); pstmt.setString(3,loc); pstmt.executeUpdate(); }catch(SQLException e){ e.printStackTrace(); }finally{ JDBCUtils.free(rs, pstmt, c);//这里仍可以执行，因为PreparedStatement是Statement的子类，而非超类 } } } 在java中SQL语句测定时间的一种方法，在执行之前和执行之后的时间打印出来 JDBC中最耗时间的是建立连接；使用的是Socket连接，三次握手机制 发送用户名密码 比发送执行SQL语句的时间要长得多 注意虽然PreparedStatement是Statement是子类，但是不能调用父类的executeQuery(sql); 会出错 因为该方法会直接将参数原始的sql语句传到数据库，而不管之前的填充sql语句的步骤 示例： import java.sql.*; public class TestTemplate { public static void main(String []args){ read(\"SMITH\"); } private static void read(String name) { Connection c = null; PreparedStatement pstmt = null; ResultSet rs = null; try{ String sql = \"select * from emp where ename = ?\"; c = JDBCUtils.getConn(); pstmt = c.prepareStatement(sql); pstmt.setString(1,name); rs = pstmt.executeQuery();//如果这里加上sql，那么会报错 while(rs.next()){ System.out.println(rs.getString(1)); } }catch(SQLException e){ e.printStackTrace(); }finally{ JDBCUtils.free(rs, pstmt, c); } } } 如何选择？ 一般带有参数的sql都使用PreparedStatement 如果没有条件或条件固定的可以使用Statement。 建议全部使用PreparedStatement CallableStatement(存储过程) 是PreparedStatement的子接口，是Statement的孙子接口。 用于执行 SQL 存储过程的接口。JDBC API 提供了一个存储过程 SQL 转义语法，该语法允许对所有 RDBMS 使用标准方式调用存储过程。此转义语法有一个包含结果参数的形式和一个不包含结果参数的形式。如果使用结果参数，则必须将其注册为 OUT 参数。其他参数可用于输入、输出或同时用于二者。参数是根据编号按顺序引用的，第一个参数的编号是 1。 继承自PreparedStatement Connection 可以创建一个CallableStatement对象 参数是sql语句 sql语句是这样写的 CollableStatement cs = null; cs = conn.prepareCall(“{call pro1(?,?)}”); //参数以？表示，之后赋值 cs.setString(1,”SMITH”); cs.setIntFloat(2,456.7f);//f是表示float类型 第一个参数是1,2,3等 表示第一个、第二个、第三个？ 第二个参数是所要设为的值 设置完后调用execute方法 注意如果在sql 中没有commit提交事务，那么java中无法访问数据库，处于阻塞状态 代码： public static void main(String []args){ Connection c = null; CallableStatement cs = null; try{ c =JDBCUtils.getConn(); cs = c.prepareCall(\"{call pro3(?,?)}\"); cs.setString(1, \"SMITH\"); cs.setFloat(2, 7800f); //这个也可以是setString //注意oracle提供一种自动转换机制，如果该字段是数字类型，那么会自动将字符串转为数字 cs.execute(); 可以在SQL工具类中封装一个可以调用存储过程的方法 工具类中的成员变量均为静态变量，方法都是静态方法 SQLHelper: public static void callProcedure(String sql,String []parameters){ Connection c= null; CallableStatement cs= null; ResultSet rs = null; //所有含有参数的sql语句都使用PreparedStatement，传入时还包括一个字符串数据 然后通过一个循环将参数使用setString将sql语句补充完毕 然后调用 try{ c =JDBCUtils.getConn(); cs = c.prepareCall(sql); if(parameters != null && \"\".equals(parameters)){ for(int i = 0; i cs.setString(i+1,parameters[i]); } } cs.execute(); }catch(SQLException e){ e.printStackTrace(); }finally{ JDBCUtils.free(rs,cs,c); } } 调用此方法的代码： 带返回值参数的存储过程 分页查询 存储过程代码： create or replace procedure paged_query (v_in_table in varchar2,v_in_record_per_page in number, v_in_now_page in number,v_in_col varchar2, v_out_records out number,v_out_pages out number, v_out_result out p1.my_cursor) is v_start number; v_end number; v_sql varchar2(2000); v_sql_get_record varchar2(300); begin v_start := 1+v_in_record_per_page*( v_in_now_page-1) ; v_end := 1+ v_in_record_per_page * v_in_now_page; v_sql_get_record := 'select count(*) from '||v_in_table; execute immediate v_sql_get_record into v_out_records; if mod(v_out_records,v_in_record_per_page) = 0 then v_out_pages := v_out_records / v_in_record_per_page; else v_out_pages := v_out_records / v_in_record_per_page + 1; end if; v_sql := 'select t2. from (select t1. ,rownum rn from (select * from '||v_in_table||' order by '||v_in_col||') t1 where rownum ) t2 where rn >= '||v_start; open v_out_result for v_sql; end; / 使用CallableStatement来接收Connection的prepareCall方法，得到statement 传入的是含有等同于参数个数的sql语句 String sql = “{call pro1(?,?)}”; 然后set方法设置每个？所对应的值；注意数值类型也可以用setString oracle会自动转为相应的数值类型 与不含输出参数的过程的第一个不同是还需要调用这个方法registerOutParameter 注册输出参数的类型 注意不同数据库，参数类型也是不同的 第一个参数是给第n个？赋值；第二个参数是对应的返回值的类型 对oracle而言是oracle.jdbc.OracleTypes.某个类型 最后execute 第二个不同是还需要再取出返回值 get…数据类型(n) 比如getString(第几个？的返回值); 如果返回的是结果集（游标)，那么需要使用结果集来接收游标变量 oracle.jdbc.OracleTypes.CURSOR 取出返回值的集合 方法是getObject 将游标视为一个对象 接收的是ResltSet结果集（需要将Object类型强制转为ResultSet) ResultSet实际上就是游标 之后就可以使用next和get…来获取值了 java调用代码: try { c = JDBCUtils.getConn(); cs = c.prepareCall(\"{call paged_query(?,?,?,?,?,?,?)}\"); int page = 2; cs.setString(1, \"emp\"); cs.setString(2,\"5\"); cs.setString(3,\"\"+page); cs.setString(4, \"sal\"); cs.registerOutParameter(5, oracle.jdbc.OracleTypes.NUMBER); //每个返回的变量都需要注册 cs.registerOutParameter(6, oracle.jdbc.OracleTypes.NUMBER); cs.registerOutParameter(7, oracle.jdbc.OracleTypes.CURSOR); cs.execute(); String recordCount = cs.getString(5); String pageCount = cs.getString(6); System.out.println(\"共有\"+recordCount+\"条记录\"); //执行完后需要逐个取出返回值 System.out.println(\"共有\"+pageCount+\"页\"); System.out.println(\"第\"+page+\"页:\"); rs = (ResultSet)cs.getObject(7); while(rs.next()){ System.out.println(rs.getString(1)+\",\"+rs.getString(2)); }catch ……略 getGeneratedKeys产生主键 用于拿出插入的记录的主键 getGeneratedKeys(获取主键) API介绍： Connection: prepareStatement PreparedStatement prepareStatement(String sql, int autoGeneratedKeys) throws SQLException 创建一个默认 PreparedStatement 对象，该对象能获取自动生成的键。给定常量告知驱动程序是否可以获取自动生成的键。如果 SQL 语句不是一条 INSERT 语句，或者 SQL 语句能够返回自动生成的键（这类语句的列表是特定于供应商的)，则忽略此参数。 该SQL语句是插入语句，可以立刻获得插入记录的主键 第二个参数是Statement中的常量 PreparedStatement: ResultSet getGeneratedKeys() throws SQLException 获取由于执行此 Statement 对象而创建的所有自动生成的键。如果此 Statement 对象没有生成任何键，则返回空的 ResultSet 对象。 注：如果未指定表示自动生成键的列，则 JDBC 驱动程序实现将确定最能表示自动生成键的列。 返回：包含通过执行此 Statement 对象自动生成的键的 ResultSet 对象 执行的是insert语句，返回的是ResultSet ResultSet 可能是多个主键（组合主键)，所以需要ResultSet 代码： public static void add() { Connection c = null; PreparedStatement ps = null; ResultSet rs = null; String sql = \"insert into UserTable(user_name,user_password) values(?,?)\"; try { c = JDBCUtils.getConn(); ps = c.prepareStatement(sql, Statement.RETURN_GENERATED_KEYS); ps.setString(1, \"wulitaotao\"); ps.setString(2, \"666\"); ps.executeUpdate(); rs = ps.getGeneratedKeys(); if(rs.next()){ System.out.println(“刚插入记录的id为”+rs.getInt(1)); } } catch (SQLException e) { e.printStackTrace(); } finally{ JDBCUtils.free(rs,ps,c); } } 实际作用是如果数据库的表是自动主键，那么插入之后是不知道id的。只能从数据库中根据其他的唯一键来找到这条记录再取出主键 而这种方式可以在插入之后立刻得到主键，然后取出赋给对象，ok 优化UserDAOJDBCImpl中的addUser方法 try { c = JDBCUtils.getConn(); String sql = \"insert into UserTable(user_name,user_password,user_birthday) values(?,?,?)\"; pstmt = c.prepareStatement(sql,Statement.RETURN_GENERATED_KEYS); pstmt.setString(1, user.getUsername()); pstmt.setString(2, user.getPassword()); pstmt.setDate(3, new java.sql.Date(user.getBirthday().getTime())); pstmt.executeUpdate(); rs = pstmt.getGeneratedKeys(); if(rs.next()){ user.setId(rs.getInt(1)); } } catch (SQLException e) { throw new DAOException(e.getMessage(),e); } finally { JDBCUtils.free(rs, pstmt, c); } 批处理Batch 可以一次执行多条SQL语句，调用Statement接口的addBatch方法 不必建立多次的连接（建立连接成本很高)，只建立一次连接就可以执行多条语句 将给定的 SQL 命令添加到此 Statement 对象的当前命令列表中。通过调用方法 executeBatch 可以批量执行此列表中的命令。 参数： sql - 通常此参数为 SQL INSERT 或 UPDATE 语句 执行语句时调用 将一批命令提交给数据库来执行 如果全部命令执行成功，则返回更新计数组成的数组。返回数组的 int 元素的排序对应于批中的命令，批中的命令根据被添加到批中的顺序排序。 返回： 包含批中每个命令的一个元素的更新计数所组成的数组。数组的元素根据将命令添加到批中的顺序排序。 批处理适用于Statement，当然适用于其子类PreparedStatement 示例1 Statement： import java.sql.*; public class JDBC { public static void main(String []args){ Connection c = null; Statement stmt = null; ResultSet rs = null; try{ c = JDBCUtils.getConn(); stmt = c.createStatement(); stmt.addBatch(\"insert into dept2 values(88,'aaa','aaa')\"); stmt.addBatch(\"insert into dept2 values(89,'aaa','aaa')\"); stmt.addBatch(\"insert into dept2 values(90,'aaa','aaa')\"); stmt.executeBatch(); }catch(SQLException e){ e.printStackTrace(); }finally{ JDBCUtils.free(rs,stmt,c); } } } 示例2 PreparedStatement： import java.sql.*; public class JDBC { public static void main(String []args){ Connection c = null; PreparedStatement pstmt = null; ResultSet rs = null; try{ c = JDBCUtils.getConn(); pstmt = c.prepareStatement(\"insert into UserTable(user_name,user_password,user_birthday) values(?,?,?)\"); for(int i = 0 ; i pstmt.setString(1, \"user\"+i+1); pstmt.setString(2, \"111\"); pstmt.setDate(3, new Date(System.currentTimeMillis())); pstmt.addBatch(); //设置完一条记录添加一次，然后重新设置下一条记录 字段的值 } int []id = pstmt.executeBatch();//返回值全部是1 for(int i:id){ System.out.println(i); } }catch(SQLException e){ e.printStackTrace(); }finally{ JDBCUtils.free(rs,pstmt,c); } } } addBatch(sql) 也是存在的，可以将sql语句传入 如果记录太多可能会内存溢出 事务处理 在java程序中将多个DML语句视为一个事务，统一提交和回滚 注意java中事务是自动提交的，也就是执行一条DML语句就commit一下 需要我们去设置不自动提交事务，由自己来设定事务 Connection有一个方法setAutoCommit 参数为true or false 当想提交时，执行Connection的commit方法 在执行commit方法之前的DML语句都还未提交，当执行commit方法时将之前的DML语句视为一个事务，整体地执行 当事务执行过程中出现异常，可以在catch到exception后回滚 可以Connection的rollback方法，有重载的方法，可以无参数，对应sql中的rollback； rollback方法可以有参数，是保存点，回滚到这个保存点 public static void testSavepoint() throws Exception{ Connection c = null; Statement stmt = null; ResultSet rs = null; String sql1 = \"update emp2 set sal = sal - 800 where empno = 7369\"; String sql2 = \"update emp2 set sal = sal - 800 where empno = 7369\"; try { c = JDBCUtils.getConn(); c.setAutoCommit(false); stmt = c.createStatement(); stmt.executeUpdate(sql1); int i = 8/0; //故意制造出一些错误 stmt.executeUpdate(sql2); c.commit(); c.setAutoCommit(true);//恢复现场 }catch (Exception e) { //捕捉到任何异常，立刻返回到初始状态 //不能只写SQLException，应该是所有异常，否则ArithmeticException就捕捉不到了，捕捉不到也就无法执行catch块中的代码了 if(c != null) //如果没有建立连接那么没有必要去rollback c.rollback(); c.setAutoCommit(true);//无论如何都应该恢复现场 throw e;//交给上一级去处理 } finally{ JDBCUtils.free(rs,stmt,c); } } } 设置保存点: 假如在执行了第一条sql之后设置保存点，如果在执行第二条sql时出错，回滚到第一条sql之后 Connection: public static void testSavepoint() throws SQLException { Connection c = null; Statement stmt = null; ResultSet rs = null; Savepoint sp = null; String sql1 = \"update emp2 set sal = sal - 800 where empno = 7369\"; String sql2 = \"update emp2 set sal = sal - 800 where empno = 7369\"; String sql3 = \"select sal from emp2 where empno = 7369\"; try { c = JDBCUtils.getConn(); c.setAutoCommit(false); stmt = c.createStatement(); stmt.executeUpdate(sql1); sp = c.setSavepoint(); rs = stmt.executeQuery(sql3); int sal = 0; if(rs.next()) sal = rs.getInt(\"sal\"); if(sal throw new RuntimeException(\"工资过低!\"); //自己new一个异常，以便于与SQLException区分开来 stmt.executeUpdate(sql2); c.commit(); c.setAutoCommit(true);//恢复现场 }catch (RuntimeException e) { if(c != null && sp != null){ c.rollback(sp); //如果工资减去800之后小于1000，那么不再减少，保留第一次的结果 } c.setAutoCommit(true); throw e; } catch (SQLException e) { if(c != null){ c.rollback(); c.setAutoCommit(true); } throw e; } finally{ JDBCUtils.free(rs,stmt,c); } } JTA类似指挥官，第一阶段给所有数据库发送一个准备提交的请求，如果有数据库提出要回滚，那么JTA会通知其他数据库，一起回滚 如果没有数据库没有提出要回滚，那么第二阶段JTA发送提交的命令 JDBC桥接模式源码分析（以MySQL为例) 5.12 Class.forName Class.forName(\"com.mysql.jdbc.Driver\"); Driver#static{} 注册MySQL的Driver public class Driver extends NonRegisteringDriver implements java.sql.Driver { // ~ Static fields/initializers // --------------------------------------------- // // Register ourselves with the DriverManager // static { try { java.sql.DriverManager.registerDriver(new Driver()); } catch (SQLException E) { throw new RuntimeException(\"Can't register driver!\"); } } } DriverManager#registerDriver public static synchronized void registerDriver(java.sql.Driver driver) throws SQLException { registerDriver(driver, null); } public static synchronized void registerDriver(java.sql.Driver driver, DriverAction da) throws SQLException { /* Register the driver if it has not already been added to our list */ if(driver != null) { registeredDrivers.addIfAbsent(new DriverInfo(driver, da)); } else { // This is for compatibility with the original DriverManager throw new NullPointerException(); } println(\"registerDriver: \" + driver); } DriverManager#getConnection public static Connection getConnection(String url) throws SQLException { java.util.Properties info = new java.util.Properties(); return (getConnection(url, info, Reflection.getCallerClass())); } private static Connection getConnection( String url, java.util.Properties info, Class caller) throws SQLException { /* * When callerCl is null, we should check the application's * (which is invoking this class indirectly) * classloader, so that the JDBC driver class outside rt.jar * can be loaded from here. */ ClassLoader callerCL = caller != null ? caller.getClassLoader() : null; synchronized(DriverManager.class) { // synchronize loading of the correct classloader. if (callerCL == null) { callerCL = Thread.currentThread().getContextClassLoader(); } } if(url == null) { throw new SQLException(\"The url cannot be null\", \"08001\"); } println(\"DriverManager.getConnection(\\\"\" + url + \"\\\")\"); // Walk through the loaded registeredDrivers attempting to make a connection. // Remember the first exception that gets raised so we can reraise it. SQLException reason = null; for(DriverInfo aDriver : registeredDrivers) { // If the caller does not have permission to load the driver then // skip it. if(isDriverAllowed(aDriver.driver, callerCL)) { try { println(\" trying \" + aDriver.driver.getClass().getName()); Connection con = aDriver.driver.connect(url, info); if (con != null) { // Success! println(\"getConnection returning \" + aDriver.driver.getClass().getName()); return (con); } } catch (SQLException ex) { if (reason == null) { reason = ex; } } } else { println(\" skipping: \" + aDriver.getClass().getName()); } } // if we got here nobody could connect. if (reason != null) { println(\"getConnection failed: \" + reason); throw reason; } println(\"getConnection: no suitable driver found for \"+ url); throw new SQLException(\"No suitable driver found for \"+ url, \"08001\"); } Driver#connect public java.sql.Connection connect(String url, Properties info) throws SQLException { if (url != null) { if (StringUtils.startsWithIgnoreCase(url, LOADBALANCE_URL_PREFIX)) { return connectLoadBalanced(url, info); } else if (StringUtils.startsWithIgnoreCase(url, REPLICATION_URL_PREFIX)) { return connectReplicationConnection(url, info); } } Properties props = null; if ((props = parseURL(url, info)) == null) { return null; } if (!\"1\".equals(props.getProperty(NUM_HOSTS_PROPERTY_KEY))) { return connectFailover(url, info); } try { Connection newConn = com.mysql.jdbc.ConnectionImpl.getInstance( host(props), port(props), props, database(props), url); return newConn; } catch (SQLException sqlEx) { // Don't wrap SQLExceptions, throw // them un-changed. throw sqlEx; } catch (Exception ex) { SQLException sqlEx = SQLError.createSQLException(Messages .getString(\"NonRegisteringDriver.17\") //$NON-NLS-1$ + ex.toString() + Messages.getString(\"NonRegisteringDriver.18\"), //$NON-NLS-1$ SQLError.SQL_STATE_UNABLE_TO_CONNECT_TO_DATASOURCE, null); sqlEx.initCause(ex); throw sqlEx; } } 5.17 总结 有了抽象部分——JDBC的API，有了具体实现部分——驱动程序，那么它们如何连接起来呢？就是如何桥接呢？ 就是前面提到的DriverManager来把它们桥接起来，从某个侧面来看，DriverManager在这里起到了类似于简单工厂的功能，基于JDBC的应用程序需要使用JDBC的API，如何得到呢？就通过DriverManager来获取相应的对象。 JDBC的这种架构，把抽象和具体分离开来，从而使得抽象和具体部分都可以独立扩展。对于应用程序而言，只要选用不同的驱动，就可以让程序操作不同的数据库，而无需更改应用程序，从而实现在不同的数据库上移植；对于驱动程序而言，为数据库实现不同的驱动程序，并不会影响应用程序。而且，JDBC的这种架构，还合理的划分了应用程序开发人员和驱动程序开发人员的边界。 Class.forName是用MySql还是Oracle，这个Driver一定会实现接口java.sql.Driver，然后通过DriverManager.registerDriver(new Driver());使DriverManager类持有一个Driver，是否可以把DriverManager当成桥，当成桥连接中的抽象类？然后持有一个接口Driver，至于是MySql还是Oracle，不关心，坐等传参。因为DriverManager持有的是一个Driver接口，你传过来什么，我就得到什么的实例化，然后我再通过getConnection用你的实例，去调用你自己的方法connect，去获得Connection的一个实例。 安全 XSS攻击 5.18 原理 跨站脚本攻击（Cross-Site Scripting, XSS)：主要是指在用户浏览器内运行了JavaScript 脚本。比如富文本编辑器，如果不过滤用户输入的数据直接显示用户输入的HTML内容的话，就会有可能运行恶意的 JavaScript 脚本，导致页面结构错乱，Cookies 信息被窃取等问题。 XSS 的原理是恶意攻击者往 Web 页面里插入恶意可执行网页脚本代码，当用户浏览该页之时，嵌入其中 Web 里面的脚本代码会被执行，从而可以达到攻击者盗取用户信息或其他侵犯用户安全隐私的目的。 常见的XSS攻击类型有两种，一种是反射型，一种是持久型。反射型 攻击者诱使用户点击一个嵌入恶意脚本的链接，达到工具的目的。 比如新浪微博中，攻击者发布的微博中含有一个恶意脚本的URL（URL中包含脚本的链接)，用户点击该URL，脚本会自动关注攻击者的新浪微博ID，发布含有恶意脚本URL的微博，攻击就被扩散了。 现实中，攻击者可以采用XSS攻击，偷取用户Cookie、密码等重要数据，进而伪造交易、盗窃用户财产、窃取情报。 持久型 黑客提交含有恶意脚本的请求，保存在被攻击的Web站点的数据库中，用户浏览网页时，恶意脚本被包含在正常页面中，达到攻击的目的。此种攻击经常使用在论坛、博客等Web应用中。 5.19 预防 Web 页面渲染的所有内容或者渲染的数据都必须来自于服务端。 后端在入库前应该选择不相信任何前端数据，将所有的字段统一进行转义处理。 后端在输出给前端数据统一进行转义处理。 前端在渲染页面 DOM 的时候应该选择不相信任何后端数据，任何字段都需要做转义处理。消毒 XSS攻击者一般都是在请求中嵌入恶意脚本达到攻击的目的，这些脚本是一般在用户输入中不常用的，如果进行过滤和消毒处理，即对某些HTML危险字符转义，如”>”转义为”>”，就可以防止大部分的攻击。为了避免对不必要的内容错误转义，如”3HttpOnly 浏览器禁止页面JavaScript访问带有HttpOnly属性的Cookie。SQL注入 5.20 原理 针对 Web 应用使用的数据库，通过运行非法的SQL而产生的攻击。 5.21 预防 所有的查询语句建议使用数据库提供的参数化查询接口，使用SQL预编译和参数绑定。 在应用发布之前建议使用专业的 SQL 注入检测工具进行检测。CSRF攻击 5.22 原理 CSRF（Cross-site request forgery)跨站请求伪造，利用跨站请求，在用户不知情的情况下，以用户的身份伪造请求，其核心是利用了浏览器Cookie或者Session，盗取用户身份。 下面是CSRF的常见特性： 依靠用户标识危害网站 利用网站对用户标识的信任，欺骗用户的浏览器发送HTTP请求给目标站点 攻击者可以盗用你的登陆信息，以你的身份模拟发送各种请求。攻击者只要借助少许的社会工程学的诡计，例如通过 QQ 等聊天软件发送的链接(有些还伪装成短域名，用户无法分辨)，攻击者就能迫使 Web 应用的用户去执行攻击者预设的操作。例如，当用户登录网络银行去查看其存款余额，在他没有退出时，就点击了一个 QQ 好友发来的链接，那么该用户银行帐户中的资金就有可能被转移到攻击者指定的帐户中。 所以遇到 CSRF 攻击时，将对终端用户的数据和操作指令构成严重的威胁。当受攻击的终端用户具有管理员帐户的时候，CSRF 攻击将危及整个 Web 应用程序。 5.23 预防 防御手段主要是识别请求者身份。 1、重要数据交互采用POST进行接收，当然是用POST也不是万能的，伪造一个form表单即可破解 2、使用验证码，只要是涉及到数据交互就先进行验证码验证，这个方法可以完全解决CSRF。但是出于用户体验考虑，网站不能给所有的操作都加上验证码。因此验证码只能作为一种辅助手段，不能作为主要解决方案。 3、验证HTTP Referer字段，该字段记录了此次HTTP请求的来源地址，最常见的应用是图片防盗链。PHP中可以采用APache URL重写规则进行防御，可参考：http://www.cnblogs.com/phpstudy2015-6/p/6715892.html（但是这个也是可以绕过的) 4、为每个表单添加令牌token并验证（推荐) （可以使用cookie或者session进行构造。当然这个token仅仅只是针对CSRF攻击，在这前提需要解决好XSS攻击，否则这里也将会是白忙一场【XSS可以偷取客户端的cookie】) 可以为每一个表单生成一个随机数秘钥，并在服务器端建立一个拦截器来验证这个token，如果请求中没有token或者token内容不正确，则认为可能是CSRF攻击而拒绝该请求。 用户访问某个表单页面。 服务端生成一个Token，放在用户的Session中，或者浏览器的Cookie中。【这里已经不考虑XSS攻击】 在页面表单附带上Token参数。 用户提交请求后， 服务端验证表单中的Token是否与用户Session（或Cookies)中的Token一致，一致为合法请求，不是则非法请求。DDoS 攻击 5.24 原理 DDoS 又叫分布式拒绝服务，全称 Distributed Denial of Service，其原理就是利用大量的请求造成资源过载，导致服务不可用。 TCP的半连接5.25 预防 - 网络架构上做好优化，采用负载均衡分流。 - 确保服务器的系统文件是最新的版本，并及时更新系统补丁。 - 添加抗 DDos 设备，进行流量清洗。 - 限制同时打开的 SYN 半连接数目，缩短 SYN 半连接的 Timeout 时间。 - 限制单 IP 请求频率。 - 防火墙等防护设置禁止 ICMP 包等。 - 严格限制对外开放的服务器的向外访问。 - 运行端口映射程序或端口扫描程序，要认真检查特权端口和非特权端口。 - 关闭不必要的服务。 - 认真检查网络设备和主机/服务器系统的日志。只要日志出现漏洞或是时间变更,那这台机器就可能遭到了攻击。 - 限制在防火墙外与网络文件共享。这样会给黑客截取系统文件的机会，主机的信息暴露给黑客，无疑是给了对方入侵的机会。加密算法 什么是对称加密，什么是非对称加密，知道的加密算法有哪些 BCrypt算法 对称加密在加密和解密的过程中，使用相同的密钥；而非对称加密在加密过程中使用公钥进行加密，使用私钥进行解密。 5.26 对称加密 所谓常规密钥密码体制，即加密密钥与解密密钥是相同的密码体制。这种加密系统又称为对称密钥系统。 对称加密算法有：DES、AES等。 DES 的保密性仅取决于对密钥的保密，而算法是公开的。尽管人们在破译 DES 方面取得了许多进展，但至今仍未能找到比穷举搜索密钥更有效的方法。 DES 是世界上第一个公认的实用密码算法标准，它曾对密码学的发展做出了重大贡献。 目前较为严重的问题是 DES 的密钥的长度。 现在已经设计出来搜索 DES 密钥的专用芯片。 对称加密的加密和解密需要使用相同的密钥，所以需要解决密钥配送问题。 5.27 非对称加密 公钥密码体制使用不同的加密密钥与解密密钥，是一种“由已知加密密钥推导出解密密钥在计算上是不可行的”密码体制。 公钥密码体制的产生主要是因为两个方面的原因，一是由于常规密钥密码体制的密钥分配问题，另一是由于对数字签名的需求。 现有最著名的公钥密码体制是RSA 体制，它基于数论中大数分解问题的体制，由美国三位科学家 Rivest, Shamir 和 Adleman 于 1976 年提出并在 1978 年正式发表。 可以用公钥加密，私钥解密；也可以用私钥加密、公钥解密。 若密钥能够实现安全交换，那么有可能会考虑仅使用公开密钥加密来通信。但是公开密钥加密与共享密钥加密相比，其处理速度要慢。 任何加密方法的安全性取决于密钥的长度，以及攻破密文所需的计算量 在这方面，公钥密码体制并不比传统加密体制更加优越 由于目前公钥加密算法的开销较大，在可见的将来还不会放弃传统的加密方法 公钥需要密钥分配协议，具体的分配过程并不比采用传统加密方法时更简单 消息摘要/单向散列 单向散列函数也称为消息摘要函数（message digest function)，哈希函数，适用于检查消息完整性的加密技术。 单向散列函数有一个输入和一个输出，其中输入称为信息，输出称为散列值。单向散列函数可以根据消息的内容计算出散列值，篡改后的信息的散列值计算结果会不一样，所以散列值可以被用来检查消息的完整性 。 常见消息摘要技术：MD5、SHA-1、SHA-256 CRC、MD5、SHA1都是通过对数据进行计算，来生成一个校验值，该校验值用来校验数据的完整性。 不同点： 算法不同。CRC采用多项式除法，MD5和SHA1使用的是替换、轮转等方法； 校验值的长度不同。CRC校验位的长度跟其多项式有关系，一般为16位或32位；MD5是16个字节（128位)；SHA1是20个字节（160位)； 校验值的称呼不同。CRC一般叫做CRC值；MD5和SHA1一般叫做哈希值（Hash)或散列值； 安全性不同。这里的安全性是指检错的能力，即数据的错误能通过校验位检测出来。CRC的安全性跟多项式有很大关系，相对于MD5和SHA1要弱很多；MD5的安全性很高，不过大概在04年的时候被山东大学的王小云破解了；SHA1的安全性最高（现在SHA-256安全性比较高)。 效率不同，CRC的计算效率很高；MD5和SHA1比较慢。 用途不同。CRC一般用作通信数据的校验；MD5和SHA1用于安全（Security)领域，比如文件校验、数字签名等。 5.28 密码 利用单向散列加密的特性，可以进行密码加密保存，即用户注册时输入的密码不直接保存到数据库，而是对密码进行单向散列加密，将密文存入数据库，用户登录时，进行密码验证，同样计算得到输入密码的密文，并和数据库中的密文比较，如果一致，则密码验证成功。 这样保存在数据库中的是用户输入的密码的密文，而且不可逆地计算得到密码的明文，因此即使数据库被拖库（指网站遭到入侵后，黑客窃取其数据库)，也不会泄露用户的密码信息。 虽然不能通过算法将单向散列密文反算得到明文，但是由于人们设置密码具有一定的模式吗，因此通过彩虹表（建立一个 源数据与加密数据之间对应的hash表。这样在获得加密数据后通过比较，查询或者一定的运算，可以快速定位源数据)等手段可以进行猜测式破解。 为了加强单向散列计算的安全性，还会给散列算法加点盐，salt相当于加密的密钥，增加破解的难度。盐一般都是跟hash一起保存在数据库里，或者作为hash字符串的一部分。salt是由系统随机生成的，并且只有系统知道。这样，即便两个用户使用了同一个密码，由于系统为它们生成的salt值不同，他们的散列值也是不同的。 5.29 算法介绍 sha比md5更安全一些，sha比md5哈希碰撞的概率更小一些。 到目前为止，我们已经了解如何为密码生成安全的 Hash 值以及通过利用 salt 来加强它的安全性。但今天的问题是，硬件的速度已经远远超过任何使用字典或彩虹表进行的暴力攻击，并且任何密码都能被破解，只是使用时间多少的问题。 为了解决这个问题，主要想法是尽可能降低暴力攻击速度来保证最小化的损失。我们下一个算法同样是基于这个概念。目标是使 Hash 函数足够慢以妨碍攻击，并对用户来说仍然非常快且不会感到有明显的延时。 要达到这个目的通常是使用某些 CPU 密集型算法来实现，比如 PBKDF2, Bcrypt 或 Scrypt 。这些算法采用 work factor(也称之为 security factor)或迭代次数作为参数来确定 Hash 函数将变的有多慢，并且随着日后计算能力的提高，可以逐步增大 work factor 来使之与计算能力达到平衡。 bcrypt是单向的，而且经过salt和cost的处理，使其受rainbow攻击破解的概率大大降低，同时破解的难度也提升不少。 因为bcrypt采用了一系列各种不同的Blowfish加密算法，并引入了一个work factor，这个工作因子可以让你决定这个算法的代价有多大。因为这些，这个算法不会因为计算机CPU处理速度变快了，而导致算法的时间会缩短了。因为，你可以增加work factor来把其性能降下来。数字签名 别人不能冒充我的签名（不可伪造)，我也不能否认上面的签名是我的（不可抵赖)。 数字签名又是靠什么保证不可伪造和不可抵赖两个特性呢？ 答案是利用公钥加密系统。 RSA既可以用公钥加密然后私钥解密，也可以用私钥加密然后公钥解密（对称性)。 因为RSA中的每一个公钥都有唯一的私钥与之对应，任一公钥只能解开对应私钥加密的内容。换句话说，其它私钥加密的内容，这个公钥是解不开的。 这样，如果你生成了一对RSA密钥，你把公钥公布出去，并告诉全世界人这个公钥是你的。之后你只要在发送的消息，比如“123456”，后面加上用私钥加密过的密文，其他人拿公钥解密，看解密得到的内容是不是“123456”就可以知道这个“123456”是不是你发的。 其他人因为没有对应的私钥，所以没法生成公钥可以解密的密文，所以是不可伪造的。 又因为公钥对应的私钥只有一个，所以只要能成功解密，那么发消息的一定是你，不会是其他人，所以是不可抵赖的。 由于直接对原消息进行签名有安全性问题，而且原消息往往比较大，直接使用RSA算法进行签名速度会比较慢，所以我们一般对消息计算其摘要（使用SHA-256等安全的摘要算法)，然后对摘要进行签名。只要使用的摘要算法是安全的（MD5、SHA-1已经不安全了)，那么这种方式的数字签名就是安全的。 一个具体的RSA签名过程如下： 小明对外发布公钥，并声明对应的私钥在自己手上 小明对消息M计算摘要，得到摘要D 小明使用私钥对D进行签名，得到签名S 将M和S一起发送出去 验证过程如下： 接收者首先对M使用跟小明一样的摘要算法计算摘要，得到D 使用小明公钥对S进行解签，得到D’ 如果D和D’相同，那么证明M确实是小明发出的，并且没有被篡改过。 报文鉴别——接收者能够核实发送者对报文的签名 报文的完整性——发送者事后不能抵赖对报文的签名 不可否认——接收者不能伪造对报文的签名密钥管理 对称密码的密钥、非对称加密的私钥、salt等都需要保证不被泄露。 改善密钥安全性的方式有两种： 1)把密钥和算法放在一个单独的服务器上，对外提供加密和解密服务，应用系统通过调用这个服务实现数据的加解密。容易成为应用瓶颈，系统性能开销较高。 2)将加解密算法放到应用系统中，密钥放在独立服务器中。实际存储时，密钥被切分成薯片，加密后分别保存在不同存储介质中，兼顾密钥安全性的同时又改善了性能。 HTTP HTTP概述 HTTP（hypertext transport protocol)，即超文本传输协议。这个协议详细规定了浏览器和万维网服务器之间互相通信的规则。 HTTP就是一个通信规则，通信规则规定了客户端发送给服务器的内容格式，也规定了服务器发送给客户端的内容格式。其实我们要学习的就是这个两个格式！客户端发送给服务器的格式叫“请求协议”；服务器发送给客户端的格式叫“响应协议”。 请求协议 请求协议的格式如下： 请求首行； 包括请求类型,要访问的资源以及所使用的HTTP版本. 请求头信息； 空行； \\r\\n DOS/Windows:’\\r\\n’ UNIX/Linux:’\\n’ Mac:’\\r’ 请求体。 　　浏览器发送给服务器的内容就这个格式的，如果不是这个格式服务器将无法解读！在HTTP协议中，请求有很多请求方法，其中最为常用的就是GET和POST。不同的请求方法之间的区别，后面会一点一点的介绍。 5.30 GET请求 　　打开IE，在访问hello项目的index.jsp之间打开HttpWatch，并点击“Record”按钮。然后访问index.jsp页面。查看请求内容如下： GET /hello/index.jsp HTTP/1.1 Host: localhost User-Agent: Mozilla/5.0 (Windows NT 5.1; rv:5.0) Gecko/20100101 Firefox/5.0 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8 Accept-Language: zh-cn,zh;q=0.5 Accept-Encoding: gzip, deflate Accept-Charset: GB2312,utf-8;q=0.7,*;q=0.7 Connection: keep-alive Cookie: JSESSIONID=369766FDF6220F7803433C0B2DE36D98 　　 - GET /hello/index.jsp HTTP/1.1：GET请求，请求服务器路径为/hello/index.jsp，协议为1.1； - Host:localhost：请求的主机名为localhost； - User-Agent: Mozilla/5.0 (Windows NT 5.1; rv:5.0) Gecko/20100101 Firefox/5.0：与浏览器和OS相关的信息。有些网站会显示用户的系统版本和浏览器版本信息，这都是通过获取User-Agent头信息而来的；  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8：告诉服务器，当前客户端可以接收的文档类型，其实这里包含了*/*，就表示什么都可以接收； - Accept-Language: zh-cn,zh;q=0.5：当前客户端支持的语言，可以在浏览器的工具选项中找到语言相关信息； - Accept-Encoding: gzip, deflate：支持的压缩格式。数据在网络上传递时，可能服务器会把数据压缩后再发送； - Accept-Charset: GB2312,utf-8;q=0.7,*;q=0.7：客户端支持的编码； - Connection: keep-alive：客户端支持的链接方式，保持一段时间链接，默认为3000ms；（无状态的) - Cookie: JSESSIONID=369766FDF6220F7803433C0B2DE36D98：因为不是第一次访问这个地址，所以会在请求中把上一次服务器响应中发送过来的Cookie在请求中一并发送去过 5.31 POST请求 为了演示POST请求，我们需要修改index.jsp页面，即添加一个表单： 关键字： 打开HttpWatch，输入hello后点击提交，查看请求内容如下： POST /hello/index.jsp HTTP/1.1 Accept: image/gif, image/jpeg, image/pjpeg, image/pjpeg, application/msword, application/vnd.ms-excel, application/vnd.ms-powerpoint, application/x-ms-application, application/x-ms-xbap, application/vnd.ms-xpsdocument, application/xaml+xml, / Referer: http://localhost:8080/hello/index.jsp Accept-Language: zh-cn,en-US;q=0.5 User-Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; InfoPath.2; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729) Content-Type: application/x-www-form-urlencoded Accept-Encoding: gzip, deflate Host: localhost:8080 Content-Length: 13 Connection: Keep-Alive Cache-Control: no-cache Cookie: JSESSIONID=E365D980343B9307023A1D271CC48E7D keyword=hello POST请求是可以有体的，而GET请求不能有请求体。 - Referer: http://localhost:8080/hello/index.jsp：请求来自哪个页面，例如你在百度上点击链接到了这里，那么Referer:http://www.baidu.com；如果你是在浏览器的地址栏中直接输入的地址，那么就没有Referer这个请求头了； - Content-Type: application/x-www-form-urlencoded：表单的数据类型，说明会使用url格式编码数据；url编码的数据都是以“%”为前缀，后面跟随两位的16进制，例如“传智”这两个字使用UTF-8的url编码用为“%E4%BC%A0%E6%99%BA”； - Content-Length:13：请求体的长度，这里表示13个字节。 - keyword=hello：请求体内容！hello是在表单中输入的数据，keyword是表单字段的名字。 Referer请求头是比较有用的一个请求头，它可以用来做统计工作，也可以用来做防盗链。 统计工作：我公司网站在百度上做了广告，但不知道在百度上做广告对我们网站的访问量是否有影响，那么可以对每个请求中的Referer进行分析，如果Referer为百度的很多，那么说明用户都是通过百度找到我们公司网站的。 防盗链：我公司网站上有一个下载链接，而其他网站盗链了这个地址，例如在我网站上的index.html页面中有一个链接，点击即可下载JDK7.0，但有某个人的微博中盗链了这个资源，它也有一个链接指向我们网站的JDK7.0，也就是说登录它的微博，点击链接就可以从我网站上下载JDK7.0，这导致我们网站的广告没有看，但下载的却是我网站的资源。这时可以使用Referer进行防盗链，在资源被下载之前，我们对Referer进行判断，如果请求来自本网站，那么允许下载，如果非本网站，先跳转到本网站看广告，然后再允许下载。5.32 GET和POST请求的区别 幂等意味着对同一URL的多个请求应该返回同样的结果。 1)前者将请求参数放在URL中，文本格式；后者将请求参数放在请求体中，可以是文本、二进制等格式 2)前者语义上是从服务器获取资源，安全（无副作用)、幂等、可缓存；后者语义上是向服务器提交资源，不安全（有副作用)、不幂等、不可缓存 3)前者的URL是明文传输，会保存在浏览器历史记录中，安全性不足，可能会受到CSRF攻击；后者较为安全（但是如果没有加密的话，都是可以明文获取的)5.33 其他请求方法 GET（SELECT)：从服务器取出资源（一项或多项)。 POST（CREATE)：在服务器新建一个资源。 PUT（UPDATE)：在服务器更新资源（客户端提供改变后的完整资源，数据输入必须与由 GET 接收的数据表示保持一致)。 PATCH（UPDATE)：在服务器更新资源（客户端提供改变的属性)。 DELETE（DELETE)：从服务器删除资源。 HEAD：获取资源的元数据。 OPTIONS：1、获取服务器支持的HTTP请求方法 2、用来检查服务器的性能 5.34 响应内容 响应协议的格式如下： 响应首行；由HTTP协议版本号， 状态码， 状态消息 三部分组成。 响应头信息； 空行； 响应体。 响应内容是由服务器发送给浏览器的内容，浏览器会根据响应内容来显示。 HTTP/1.1 200 OK Server: Apache-Coyote/1.1 Content-Type: text/html;charset=UTF-8 Content-Length: 724 Set-Cookie: JSESSIONID=C97E2B4C55553EAB46079A4F263435A4; Path=/hello Date: Wed, 25 Sep 2012 04:15:03 GMT My JSP 'index.jsp' starting page --> 关键字： - HTTP/1.1 200 OK：响应协议为HTTP1.1，状态码为200，表示请求成功，OK是对状态码的解释； - Server: Apache-Coyote/1.1：服务器的版本信息； - Content-Type: text/html;charset=UTF-8：响应体使用的编码为UTF-8； - Content-Length: 724：响应体为724字节； - Set-Cookie: JSESSIONID=C97E2B4C55553EAB46079A4F263435A4; Path=/hello：响应给客户端的Cookie； - Date: Wed, 25 Sep 2012 04:15:03 GMT：响应的时间，这可能会有8小时的时区差； 5.35 响应码 响应头对浏览器来说很重要，它说明了响应的真正含义。例如200表示响应成功了，302表示重定向，这说明浏览器需要再发一个新的请求。 2xx表示成功，3xx表示重定向，4xx表示客户端出错，5xx表示服务器出错。 - 200：请求成功，浏览器会把响应体内容（通常是html)显示在浏览器中； - 404：请求的资源没有找到，说明客户端错误的请求了不存在的资源； - 500：请求资源找到了，但服务器内部出现了错误； - 302：重定向，当响应码为302时，表示服务器要求浏览器重新再发一个请求，服务器会发送一个响应头Location，它指定了新请求的URL地址； - 304：（缓存)301&302 301 Move Permanently 302 Found 301是永久性重定向。当网站需要改版时，多域名指向同一个页面时，为了不让网站被降低和分散权重，就需要使用301重定向来实现，同时在搜索引擎索引库中彻底废弃掉原先的老地址。 302是临时性重定向，搜索引擎会抓取新的内容而保留旧的网址。因为服务器返回302代码，搜索引擎认为新的网址只是暂时的，不会传递权重。5.36 其他响应头 告诉浏览器不要缓存的响应头： - Expires: -1；（过期时间，-1表示马上过期) - Cache-Control: no-cache；（不缓存) - Pragma: no-cache；（不缓存) 自动刷新响应头，浏览器会在3秒之后请求http://www.baidu.com： - Refresh: 3;url=http://www.baidu.com 5.37 HTML中指定响应头 在HTMl页面中可以使用来指定响应头，例如在index.html页面中给出，表示浏览器只会显示index.html页面3秒，然后自动跳转到http://www.itcast.cn。 缓存 5.38 强缓存与协商缓存 浏览器HTTP缓存可以分为强缓存和协商缓存。强缓存和协商缓存最大也是最根本的区别是：强缓存命中的话不会发请求到服务器（比如chrome中的200 from memory cache)，协商缓存一定会发请求到服务器，通过资源的请求首部字段验证资源是否命中协商缓存，如果协商缓存命中，服务器会将这个请求返回，但是不会返回这个资源的实体，而是通知客户端可以从缓存中加载这个资源（304 not modified)。 5.39 控制强缓存的字段按优先级介绍 1.- Pragma Pragma是HTTP/1.1之前版本遗留的通用首部字段，仅作为于HTTP/1.0的向后兼容而使用。虽然它是一个通用首部，但是它在响应报文中时的行为没有规范，依赖于浏览器的实现。RFC中该字段只有no-cache一个可选值，会通知浏览器不直接使用缓存，要求向服务器发请求校验新鲜度。因为它优先级最高，当存在时一定不会命中强缓存。 2.- Cache-Control Cache-Control是一个通用首部字段，也是HTTP/1.1控制浏览器缓存的主流字段。和浏览器缓存相关的是如下几个响应指令： 指令 参数 说明 private 无 表明响应只能被单个用户缓存，不能作为共享缓存（即代理服务器不能缓存它) public 可省略 表明响应可以被任何对象（包括：发送请求的客户端，代理服务器，等等)缓存 no-cache 可省略 缓存前必需确认其有效性 no-store 无 不缓存请求或响应的任何内容 max-age=[s] 必需 响应的最大值 - max-age（单位为s)设置缓存的存在时间，相对于发送请求的时间。只有响应报文首部设置Cache-Control为非0的max-age或者设置了大于请求日期的Expires（下文会讲)才有可能命中强缓存。当满足这个条件，同时响应报文首部中Cache-Control不存在no-cache、no-store且请求报文首部不存在Pragma字段，才会真正命中强缓存。 - no-cache 表示请求必须先与服务器确认缓存的有效性，如果有效才能使用缓存（协商缓存)，无论是响应报文首部还是请求报文首部出现这个字段均一定不会命中强缓存。Chrome硬性重新加载（Command+shift+R)会在请求的首部加上Pragma：no-cache和Cache-Control：no-cache。 - no-store 表示禁止浏览器以及所有中间缓存存储任何版本的返回响应，一定不会出现强缓存和协商缓存，适合个人隐私数据或者经济类数据。  public 表明响应可以被浏览器、CDN等等缓存。  private 响应只作为私有的缓存，不能被CDN等缓存。如果要求HTTP认证，响应会自动设置为private。 3.- Expires Expires是一个响应首部字段，它指定了一个日期/时间，在这个时间/日期之前，HTTP缓存被认为是有效的。无效的日期比如0，表示这个资源已经过期了。如果同时设置了Cache-Control响应首部字段的max-age，则Expires会被忽略。它也是HTTP/1.1之前版本遗留的通用首部字段，仅作为于HTTP/1.0的向后兼容而使用。 5.40 控制协商缓存的字段 1.- Last-Modified/If-Modified-Since If-Modified-Since是一个请求首部字段，并且只能用在GET或者HEAD请求中。Last-Modified是一个响应首部字段，包含服务器认定的资源作出修改的日期及时间。当带着If-Modified-Since头访问服务器请求资源时，服务器会检查Last-Modified，如果Last-Modified的时间早于或等于If-Modified-Since则会返回一个不带主体的304响应，否则将重新返回资源。 If-Modified-Since: , :: GMT Last-Modified: , :: GMT 2.- ETag/If-None-Match ETag是一个响应首部字段，它是根据实体内容生成的一段hash字符串，标识资源的状态，由服务端产生。If-None-Match是一个条件式的请求首部。如果请求资源时在请求首部加上这个字段，值为之前服务器端返回的资源上的ETag，则当且仅当服务器上没有任何资源的ETag属性值与这个首部中列出的时候，服务器才会返回带有所请求资源实体的200响应，否则服务器会返回不带实体的304响应。ETag优先级比Last-Modified高，同时存在时会以ETag为准。 因为ETag的特性，所以相较于Last-Modified有一些优势： 某些情况下服务器无法获取资源的最后修改时间 资源的最后修改时间变了但是内容没变，使用ETag可以正确缓存 如果资源修改非常频繁，在秒以下的时间进行修改，Last-Modified只能精确到秒 5.41 协商缓存细节 当用户第一次请求index.html时，服务器会添加一个名为Last-Modified响应头，这个头说明了index.html的最后修改时间，浏览器会把index.html内容，以及最后响应时间缓存下来。当用户第二次请求index.html时，在请求中包含一个名为If-Modified-Since请求头，它的值就是第一次请求时服务器通过Last-Modified响应头发送给浏览器的值，即index.html最后的修改时间，If-Modified-Since请求头就是在告诉服务器，我这里浏览器缓存的index.html最后修改时间是这个，您看看现在的index.html最后修改时间是不是这个，如果还是，那么您就不用再响应这个index.html内容了，我会把缓存的内容直接显示出来。而服务器端会获取If-Modified-Since值，与index.html的当前最后修改时间比对，如果相同，服务器会发响应码304，表示index.html与浏览器上次缓存的相同，无需再次发送，浏览器可以显示自己的缓存页面，如果比对不同，那么说明index.html已经做了修改，服务器会响应200。（只有html等静态资源可以做缓存，动态资源不做缓存) 响应头： - Last-Modified：最后的修改时间； 请求头： - If-Modified-Since：把上次请求的index.html的最后修改时间还给服务器； 状态码：304，比较If-Modified-Since的时间与文件真实的时间一样时，服务器会响应304，而且不会有响应正文，表示浏览器缓存的就是最新版本！ 幂等性（并非是HTTP的问题，而是服务器API设计问题) 幂等性是http层面的问题吗，还是服务器要处理和解决的内容？ 对HTTP协议的使用实际上存在着两种不同的方式：一种是RESTful的，它把HTTP当成应用层协议，比较忠实地遵守了HTTP协议的各种规定；另一种是SOA的，它并没有完全把HTTP当成应用层协议，而是把HTTP协议作为了传输层协议，然后在HTTP之上建立了自己的应用层协议。这里所讨论的HTTP幂等性主要针对RESTful风格的，但幂等性并不属于特定的协议，它是分布式系统的一种特性；所以，不论是SOA还是RESTful的Web API设计都应该考虑幂等性。下面将介绍HTTP GET、DELETE、PUT、POST四种主要方法的语义和幂等性。 HTTP GET方法用于获取资源，不应有副作用，所以是幂等的。 比如：GET http://www.bank.com/account/123456，不会改变资源的状态，不论调用一次还是N次都没有副作用。请注意，这里强调的是一次和N次具有相同的副作用，而不是每次GET的结果相同。GET http://www.news.com/latest-news这个HTTP请求可能会每次得到不同的结果，但它本身并没有产生任何副作用，因而是满足幂等性的。 HTTP DELETE方法用于删除资源，有副作用，但它应该满足幂等性。 比如：DELETE http://www.forum.com/article/4231，调用一次和N次对系统产生的副作用是相同的，即删掉id为4231的帖子；因此，调用者可以多次调用或刷新页面而不必担心引起错误。 比较容易混淆的是HTTP POST和PUT。POST和PUT的区别容易被简单地误认为“POST表示创建资源，PUT表示更新资源”；而实际上，二者均可用于创建资源，更为本质的差别是在幂等性方面。在HTTP规范中对POST和PUT是这样定义的：POST所对应的URI并非创建的资源本身，而是资源的接收者。比如：POST http://www.forum.com/articles的语义是在http://www.forum.com/articles下创建一篇帖子，HTTP响应中应包含帖子的创建状态以及帖子的URI。两次相同的POST请求会在服务器端创建两份资源，它们具有不同的URI；所以，POST方法不具备幂等性。而PUT所对应的URI是要创建或更新的资源本身。比如：PUT http://www.forum/articles/4231的语义是创建或更新ID为4231的帖子。对同一URI进行多次PUT的副作用和一次PUT是相同的；因此，PUT方法具有幂等性。 无状态 客户端和服务器在某次会话中产生的数据，从而【无状态】就意味着，这些数据不会被保留；协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。但是通过增加cookie和session机制，现在的网络请求其实是有状态的。在没有状态的http协议下，服务器也一定会保留你每次网络请求对数据的修改，但这跟保留每次访问的数据是不一样的，保留的只是会话产生的结果，而没有保留会话。 与之相对的是TCP，TCP是有状态的，因为每一条消息的seq和ack（还有一堆滑动窗口，拥塞的控制参数，等)都和前面消息相关。 HTTP并不会在内存里保留前次请求相关的任何状态，仅仅以协议逻辑（打包解包)存在，所以是它无状态的。 无状态的设计会加强透明度(visibility)，稳定度(reliability)和伸缩度(scalability)。提高透明度是因为系统无需通过请求内容以外的信息判断请求的完整内容；提高稳定度是指在部分失败的情况下，减轻了恢复的难度；提高伸缩度的原因是无需储存请求间的状态使服务器端可以很快释放资源并简化实现。 优点在于解放了服务器，每一次请求“点到为止”不会造成不必要连接占用，缺点在于每次请求会传输大量重复的内容信息。跨域 CORS 跨域资源共享 之所以会跨域，是因为受到了同源策略的限制，同源策略要求源相同才能正常进行通信，即协议、域名、端口号都完全一致。 同源策略具体限制些什么呢？ 不能向工作在不同源的的服务请求数据（client to server)这里有个问题之前也困扰了我很久，就是为什么home.com加载的cdn.home.com/index.js可以向home.com发请求而不会跨域呢？其实home.com加载的JS是工作在home.com的，它的源不是提供JS的cdn，所以这个时候是没有跨域的问题的，并且script标签能够加载非同源的资源，不受同源策略的影响。 无法获取不同源的document/cookie等BOM和DOM，可以说任何有关另外一个源的信息都无法得到 （client to client)。 跨域最常用的方法，应当属CORS，如下图所示： 只要浏览器检测到响应头带上了CORS，并且允许的源包括了本网站，那么就不会拦截请求响应。 CORS把请求分为两种，一种是简单请求，另一种是需要触发预检请求，这两者是相对的，怎样才算“不简单”？只要属于下面的其中一种就不是简单请求： （1)使用了除GET/POST/HEAD之外的请求方式，如PUT/DELETE （2)使用了除Content-Type/Accept等几个常用的http头这个时候就认为需要先发个预检请求5.42 简单请求 对于简单请求，浏览器直接发出CORS请求。具体来说，就是在头信息之中，增加一个Origin字段。 下面是一个例子，浏览器发现这次跨源AJAX请求是简单请求，就自动在头信息之中，添加一个Origin字段。 GET /cors HTTP/1.1 Origin: http://api.bob.com Host: api.alice.com Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... 上面的头信息中，Origin字段用来说明，本次请求来自哪个源（协议 + 域名 + 端口)。服务器根据这个值，决定是否同意这次请求。 如果Origin指定的源，不在许可范围内，服务器会返回一个正常的HTTP回应。浏览器发现，这个回应的头信息没有包含Access-Control-Allow-Origin字段（详见下文)，就知道出错了，从而抛出一个错误，被XMLHttpRequest的onerror回调函数捕获。注意，这种错误无法通过状态码识别，因为HTTP回应的状态码有可能是200。 如果Origin指定的域名在许可范围内，服务器返回的响应，会多出几个头信息字段。 Access-Control-Allow-Origin: http://api.bob.com Access-Control-Allow-Credentials: true Access-Control-Expose-Headers: FooBar Content-Type: text/html; charset=utf-8 上面的头信息之中，有三个与CORS请求相关的字段，都以Access-Control-开头。 （1)Access-Control-Allow-Origin 该字段是必须的。它的值要么是请求时Origin字段的值，要么是一个*，表示接受任意域名的请求。 （2)Access-Control-Allow-Credentials 该字段可选。它的值是一个布尔值，表示是否允许发送Cookie。默认情况下，Cookie不包括在CORS请求之中。设为true，即表示服务器明确许可，Cookie可以包含在请求中，一起发给服务器。这个值也只能设为true，如果服务器不要浏览器发送Cookie，删除该字段即可。 （3)Access-Control-Expose-Headers 该字段可选。CORS请求时，XMLHttpRequest对象的getResponseHeader()方法只能拿到6个基本字段：Cache-Control、Content-Language、Content-Type、Expires、Last-Modified、Pragma。如果想拿到其他字段，就必须在Access-Control-Expose-Headers里面指定。上面的例子指定，getResponseHeader('FooBar')可以返回FooBar字段的值。5.43 非简单请求 简单请求是那种对服务器有特殊要求的请求，比如请求方法是PUT或DELETE，或者Content-Type字段的类型是application/json。 非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为\"预检\"请求（preflight)。 浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复，浏览器才会发出正式XMLHttpRequest请求，否则就报错。 \"预检\"请求用的请求方法是OPTIONS，表示这个请求是用来询问的。头信息里面，关键字段是Origin，表示请求来自哪个源。 除了Origin字段，\"预检\"请求的头信息包括两个特殊字段。 （1)Access-Control-Request-Method 该字段是必须的，用来列出浏览器的CORS请求会用到哪些HTTP方法 （2)Access-Control-Request-Headers 该字段是一个逗号分隔的字符串，指定浏览器CORS请求会额外发送的头信息字段 服务器收到\"预检\"请求以后，检查了Origin、Access-Control-Request-Method和Access-Control-Request-Headers字段以后，确认允许跨源请求，就可以做出回应。 如果浏览器否定了\"预检\"请求，会返回一个正常的HTTP回应，但是没有任何CORS相关的头信息字段。这时，浏览器就会认定，服务器不同意预检请求，因此触发一个错误，被XMLHttpRequest对象的onerror回调函数捕获。 一旦服务器通过了\"预检\"请求，以后每次浏览器正常的CORS请求，就都跟简单请求一样，会有一个Origin头信息字段。服务器的回应，也都会有一个Access-Control-Allow-Origin头信息字段。 CORS与JSONP的使用目的相同，但是比JSONP更强大。 JSONP只支持GET请求，CORS支持所有类型的HTTP请求。JSONP的优势在于支持老式浏览器，以及可以向不支持CORS的网站请求数据。 长轮询与短轮询 短轮询相信大家都不难理解，比如你现在要做一个电商中商品详情的页面，这个详情界面中有一个字段是库存量（相信这个大家都不陌生，随便打开淘宝或者京东都能找到这种页面)。而这个库存量需要实时的变化，保持和服务器里实际的库存一致。 这个时候，你会怎么做？ 最简单的一种方式，就是你用JS写个死循环，不停的去请求服务器中的库存量是多少，然后刷新到这个页面当中，这其实就是所谓的短轮询。 这种方式有明显的坏处，那就是你很浪费服务器和客户端的资源。客户端还好点，现在PC机配置高了，你不停的请求还不至于把用户的电脑整死，但是服务器就很蛋疼了。如果有1000个人停留在某个商品详情页面，那就是说会有1000个客户端不停的去请求服务器获取库存量，这显然是不合理的。 那怎么办呢？ 长轮询这个时候就出现了，其实长轮询和短轮询最大的区别是，短轮询去服务端查询的时候，不管库存量有没有变化，服务器就立即返回结果了。而长轮询则不是，在长轮询中，服务器如果检测到库存量没有变化的话，将会把当前请求挂起一段时间（这个时间也叫作超时时间，一般是几十秒,Object.wait)。在这个时间里，服务器会去检测库存量有没有变化，检测到变化就立即返回（Object.notify)，否则就一直等到超时为止。 而对于客户端来说，不管是长轮询还是短轮询，客户端的动作都是一样的，就是不停的去请求，不同的是服务端，短轮询情况下服务端每次请求不管有没有变化都会立即返回结果，而长轮询情况下，如果有变化才会立即返回结果，而没有变化的话，则不会再立即给客户端返回结果，直到超时为止。 这样一来，客户端的请求次数将会大量减少（这也就意味着节省了网络流量，毕竟每次发请求，都会占用客户端的上传流量和服务端的下载流量)，而且也解决了服务端一直疲于接受请求的窘境。 但是长轮询也是有坏处的，因为把请求挂起同样会导致资源的浪费，假设还是1000个人停留在某个商品详情页面，那就很有可能服务器这边挂着1000个线程，在不停检测库存量，这依然是有问题的。 因此，从这里可以看出，不管是长轮询还是短轮询，都不太适用于客户端数量太多的情况，因为每个服务器所能承载的TCP连接数是有上限的，这种轮询很容易把连接数顶满。 长连接与短连接 HTTP的短连接和长连接；长连接与短连接的区别（LVS是通过长连接作负载均衡) HTTP的长连接和短连接本质上是TCP长连接和短连接。 在HTTP/1.0中，默认使用的是短连接。也就是说，浏览器和服务器每进行一次HTTP操作，就建立一次连接，但任务结束就中断连接。 但从 HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头有加入这行代码：Connection:keep-alive。 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的 TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache)中设定这个时间。实现长连接要客户端和服务端都支持长连接。 长连接可以省去较多的TCP建立和关闭的操作，减少浪费，节约时间。对于频繁请求资源的客户来说，较适用长连接。不过这里存在一个问题，存活功能的探测周期太长，还有就是它只是探测TCP连接的存活，属于比较斯文的做法，遇到恶意的连接时，保活功能就不够使了。在长连接的应用场景下，client端一般不会主动关闭它们之间的连接，Client与server之间的连接如果一直不关闭的话，会存在一个问题，随着客户端连接越来越多，server早晚有扛不住的时候，这时候server端需要采取一些策略，如关闭一些长时间没有读写事件发生的连接，这样可以避免一些恶意连接导致server端服务受损。 短连接对于服务器来说管理较为简单，存在的连接都是有用的连接，不需要额外的控制手段。但如果客户请求频繁，将在TCP的建立和关闭操作上浪费时间和带宽。 URL url有最大长度限制，就问长度有限制是get的原因还是url的原因，为什么长度会有限制，是http数据包的头的字段原因还是内容字段的原因 是GET的原因，长度受到服务器和客户端的限制。 URL编解码 Url的编码格式采用的是ASCII码，而不是Unicode，这也就是说你不能在Url中包含任何非ASCII字符，例如中文。 Url中只允许包含英文字母（a-zA-Z)、数字（0-9)、-_.~4个特殊字符以及所有保留字符； RFC3986中指定了以下字符为保留字符：! * ' ( ) ; : @ & = + $ , / ? # [ ] Url编码通常也被称为百分号编码（Url Encoding，also known as percent-encoding)，是因为它的编码方式非常简单，使用%百分号加上两位的十六进制字符。 URI&URL URL（Uniform ResourceLocator)统一资源定位符，是专门为标识网络上的资源位置而设计的一种编址方式。URL一般由3个部分组成： 应用层协议 主机IP地址或域名 资源所在路径/文件名 统一资源标识符（Uniform Resource Identifier，或URI)是一个用于标识某一互联网资源名称的字符串。 URI ：Uniform Resource Identifier，统一资源标识符； URL：Uniform Resource Locator，统一资源定位符； URN：Uniform ResourceName，统一资源名称。 其中，URL,URN是URI的子集。 URL是一种具体的URI，它不仅唯一标识资源，而且还提供了定位该资源的信息。URI是一种语义上的抽象概念，可以是绝对的，也可以是相对的，而URL则必须提供足够的信息来定位。HTTPS 5.44 HTTP缺点 - 明文传输，内容可能会被窃听 - 不验证通信方的身份，因此有可能遭遇伪装 - 无法证明报文的完整性，所以有可能已遭篡改 HTTP+加密+认证+完整性保护=HTTPS 用SSL将通信的报文主体内容进行加密，使用SSL建立http的安全通信线路，SSL处于http与TCP通信之间，这样的SSL与HTTP组合被称为HTTPS。 HTTPS 采用对称加密和非对称加密两者并用的混合加密机制 HTTPS 公钥能用公钥解吗？在客户端抓包，看到的是加密的还是没加密 是没加密的 https ssl tcp三者关系，其中哪些用到了对称加密，哪些用到了非对称加密，非对称加密密钥是如何实现的 加密的私钥和公钥各自如何分配（客户端拿公钥，服务器拿私钥) 客户端是如何认证服务器的真实身份，详细说明一下过程，包括公钥如何申请，哪一层加密哪一层解密 怎么攻击https TLS改进，如果session ticket被偷听到会怎样，如何防止中间人攻击 5.45 SSL/TLS SSL（Secure Socket Layer安全套接字层) TLS（Transport Layer Security) SSL发展到3.0版本后改成了TLS。 TLS主要提供三个基本服务 - 加密 - 身份验证 - 消息完整性校验 通常，HTTP 直接和 TCP 通信。当使用 SSL 时，则演变成先和 SSL 通信，再由 SSL 和 TCP 通信了。用 SSL 建立安全通信线路之后，就可以在这条线路上进行 HTTP 通信了。 SSL 是独立于 HTTP 的协议，所以不光是 HTTP 协议，其他运行在应用层的 SMTP 和 Telnet 等协议均可配合 SSL 协议使用。可以说 SSL 是当今世界上应用最为广泛的网络安全技术。 虽然使用 HTTP 协议无法确定通信方，但如果使用 SSL 则可以。SSL 不仅提供加密处理，而且还使用了一种被称为证书的手段，可用于确定双方身份。 证书由值得信任的第三方机构颁发，用以证明服务器和客户端是实际存在的。另外，伪造证书从技术角度来说是异常困难的一件事。所以只要能够确认通信方(服务器或客户端)持有的证书，即可判断通信方的真实意图。5.46 中间人攻击 mim 就是man in the middle，中间人攻击正常情况下浏览器与服务器在TLS连接下内容是加密的，第三方即使可以嗅探到所有的数据，也不能解密。中间人可以与你建立连接，然后中间人再与服务器建立连接，转发你们之间的内容。这时候中间人就获得了明文的信息。 有什么危害？你与服务器的通信被第三方解密、查看、修改。如何防范？如果确定是否被攻击？在访问https连接的时候，查看一下服务器提供的证书是不是正确的。除非入侵并取得服务器的证书私钥，否则中间人是不能完全伪装成服务器的样子的。 数字证书可以保证服务器发来的公钥是真的来自服务器的5.47 服务器保证其提供的公钥的正确性——数字证书 公钥是由数字证书认证机构(CA，Certificate Authority)和其相关机关颁发的公开密钥证书。 数字证书认证机构处于客户端与服务器双方都可信赖的第三方机构的立场上。服务器会将这份由数字证书认证机构颁发的公钥证书发送给客户端，以进行公开密钥加密方式通信。公钥证书也可叫做数字证书或直接称为证书。 接到证书的客户端可使用数字证书认证机构的公开密钥，对那张证书上的数字签名进行验证，一旦验证通过，客户端便可明确两件事： 认证服务器的公开密钥的是真实有效的数字证书认证机构 服务器的公开密钥是值得信赖的 此处认证机关的公开密钥必须安全地转交给客户端。使用通信方式时，如何安全转交是一件很困难的事，因此，多数浏览器开发商发布版本时，会事先在内部植入常用认证机关的公开密钥。 5.48 过程  客户端发起HTTPS请求 这个没什么好说的，就是用户在浏览器里输入一个HTTPS网址，然后连接到服务端的443端口。  服务端的配置 采用HTTPS协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请。区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面。这套证书其实就是一对公钥和私钥。  传送证书 这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间等等。  客户端解析证书 这部分工作是由客户端的SSL/TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警示框，提示证书存在的问题。如果证书没有问题，那么就生成一个随机值。然后用证书（也就是公钥)对这个随机值进行加密。  传送加密信息 这部分传送的是用证书加密后的随机值，目的是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。  服务端解密信息 服务端用私钥解密后，得到了客户端传过来的随机值，然后把内容通过该随机值（密钥)进行对称加密，将信息和私钥通过某种算法混合在一起，这样除非知道私钥，不然无法获取内容，而正好客户端和服务端都知道这个私钥，所以只要加密算法够复杂，私钥够复杂，数据就够安全。  传输加密后的信息 这部分信息就是服务端用私钥加密后的信息，可以在客户端用随机值解密还原。  客户端解密信息 客户端用之前生产的私钥解密服务端传过来的信息，于是获取了解密后的内容。整个过程第三方即使监听到了数据，也束手无策。 客户端获得服务器的公钥的过程是基于非对称加密实现的（数字证书) 而之后客户端和服务器之间的数据交换是基于对称加密实现的。5.49 更具体的过程  客户端通过发送 Client Hello 报文开始 SSL 通信。报文中包含客户端支持的 SSL 的指定版本、加密组件(Cipher Suite)列表(所使用的加密算法及密钥长度等)  服务器可进行 SSL 通信时，会以 Server Hello 报文作为应答。和客户端一样，在报文中包含 SSL 版本以及加密组件。服务器的加密组件内容是从接收 到的客户端加密组件内筛选出来的。  之后服务器发送 Certificate 报文。报文中包含公开密钥证书。  最后服务器发送 Server Hello Done 报文通知客户端，最初阶段的 SSL 握手协商部分结束。  SSL 第一次握手结束之后，客户端以 Client Key Exchange 报文作为回应。报文中包含通信加密中使用的一种被称为 Pre-master secret 的随机密码串。该 报文已用步骤 3 中的公开密钥进行加密。  接着客户端继续发送 Change Cipher Spec 报文。该报文会提示服务器，在此报文之后的通信会采用 Pre-master secret 密钥加密。  客户端发送 Finished 报文。该报文包含连接至今全部报文的整体校验值。这次握手协商是否能够成功，要以服务器是否能够正确解密该报文作为判定标准。  服务器同样发送 Change Cipher Spec 报文。  服务器同样发送 Finished 报文。  服务器和客户端的 Finished 报文交换完毕之后，SSL 连接就算建立完成。当然，通信会受到 SSL 的保护。从此处开始进行应用层协议的通信，即发 送 HTTP 请求。  应用层协议通信，即发送 HTTP 响应。  最后由客户端断开连接。断开连接时，发送 close_notify 报文。 WebSocket web浏览器和web服务器之间全双工通信标准。 优点是，直接发送数据，不用等待客户端请求，一直保持连接状态，且首部信息量少，通信量减少。 HTTP 2.0 5.50 二进制分帧 在应用层(HTTP2.0)和传输层(TCP or UDP)之间增加一个二进制分帧层。 在二进制分帧层上， HTTP 2.0 会将所有传输的信息分割为更小的消息和帧,并对它们采用二进制格式的编码。Frame 由 Frame Header 和 Frame Payload 两部分组成。不论是原来的 HTTP Header 还是 HTTP Body，在 HTTP/2 中，都将这些数据存储到 Frame Payload，组成一个个 Frame，再发送响应/请求。通过 Frame Header 中的 Type 区分这个 Frame 的类型。由此可见语义并没有太大变化，而是数据的格式变成二进制的 Frame。 HTTP 2.0 通信都在一个连接上完成，这个连接可以承载任意数量的双向数据流。相应地，每个数据流以消息的形式发送，而消息由一或多个帧组成，这些帧可以乱序发送，然后再根据每个帧首部的流标识符重新组装。 5.51 首部压缩 HTTP 2.0 在客户端和服务器端使用“首部表”来跟踪和存储之前发送的键-值对，对于相同的数据，不再通过每次请求和响应发送;通信期间几乎不会改变的通用键-值对(用户代理、可接受的媒体类型,等等)只需发送一次。事实上,如果请求中不包含首部(例如对同一资源的轮询请求),那么 首部开销就是零字节。此时所有首部都自动使用之前请求发送的首部。 如果首部发生变化了，那么只需要发送变化了数据在Headers帧里面，新增或修改的首部帧会被追加到“首部表”。首部表在 HTTP 2.0 的连接存续期内始终存在,由客户端和服务器共同渐进地更新 。 HTTP/2 使用了专门为首部压缩而设计的 HPACK 算法。 5.52 服务器推送 HTTP 2.0 新增的一个强大的新功能，就是服务器可以对一个客户端请求发送多个响应。换句话说，服务器除了对最初请求的响应外，还可以额外向客户端推送资源，而无需客户端明确地请求。 当浏览器请求一个html，服务器其实大概知道你是接下来要请求资源了，而不需要等待浏览器得到html后解析页面再发送资源请求。我们常用的内嵌图片也可以理解为一种强制的服务器推送：我请求html，却内嵌了张图。 有了HTTP2.0的服务器推送，HTTP1.x时代的内嵌资源的优化手段也变得没有意义了。而且使用服务器推送的资源的方式更加高效，因为客户端还可以缓存起来，甚至可以由不同的页面共享（依旧遵循同源策略)。当然，浏览器是可以决绝服务器推送的资源的。 5.53 多路复用 多路复用允许同时通过单一的HTTP/2连接发起多重的请求-响应信息。 每个 Frame Header 都有一个 Stream ID 就是被用于实现该特性。每次请求/响应使用不同的 Stream ID。就像同一个 TCP 链接上的数据包通过 IP:PORT来区分出数据包去往哪里一样。通过 Stream ID 标识，所有的请求和响应都可以欢快的同时跑在一条 TCP 链接上了。 SOA Web Service也叫XML Web Service WebService是一种可以接收从Internet或者Intranet上的其它系统中传递过来的请求，轻量级的独立的通讯技术。是通过SOAP在Web上提供的软件服务，使用WSDL文件进行说明，并通过UDDI进行注册。 SOA是一种架构风格，包括两个方面的内容： 1)抽象出服务，这些服务满足离散、松耦合、可复用、自治、无状态等特征； 2)服务可以灵活地组装和编排，满足流程整合和业务变化的需要 WebService是SOA的一种实现技术，跨语言，跨平台，提供了标准的服务定义、服务注册、服务接入和访问的方式。使用了XML、SOAP、WSDL、UDDI等技术。 SOA三角操作模型 - 1)三种角色 服务提供者：发布自己的服务，并且对服务请求进行响应 服务请求者：利用服务注册中心查找所需要的服务，然后使用该服务 服务注册中心：注册已经发布的服务，对其进行分类，并提供搜索服务 2)三个操作： 发布：为了使服务可访问，需要发布服务描述以使服务使用者可以发现它 查找：服务请求者查询服务注册中心来找到满足其要求的服务 绑定：检索到服务描述后，服务请求者继续根据服务描述中的信息调用服务XML XML：(Extensible Markup Language)扩展型可标记语言。面向短期的临时数据处理、面向万维网络，是Soap的基础。SOAP SOAP：(Simple Object Access Protocol)简单对象传输协议。是XML Web Service 的通信协议。当用户通过UDDI找到你的WSDL描述文档后，他通过可以SOAP调用你建立的Web服务中的一个或多个操作。SOAP是XML文档形式的调用方法的规范，它可以支持不同的底层接口，像HTTP(S)或者SMTP。 SOAP=RPC+HTTP+XML：采用HTTP作为底层通讯协议；RPC作为一致性的调用途径，ＸＭＬ作为数据传送的格式，允许服务提供者和服务客户经过防火墙在INTERNET进行通讯交互。 简单对象传输协议，是轻量级的、简单的、基于XML的用于交换数据的协议。 SOAP本质上是一个 XML文档，包含以下元素： 1)Envelope元素：必需元素，根元素，标识此XML文档为一条SOAP消息 可以包含命名空间和声明额外的属性 2)Header元素：可选元素，有关SOAP消息的应用程序专用消息 3)Body元素：必需元素，包含所有的请求和响应信息 4)Fault元素：可选元素，提供有关在处理此消息所发生错误的信息 SOAP处理模型： 1)用XML打包请求 2)将请求发送给服务器 3)服务器接收到请求，解码XML，处理请求，以XML格式返回响应 SOAP并不假定传输数据的下层协议，因此必须设计为能在各种协议上运行。即使绝大多数SOAP是运行在HTTP上，使用URI标识服务，SOAP也仅仅使用POST方法发送请求，用一个唯一的URI标识服务的入口。 使用 HTTP 协议的 SOAP，由于其设计原则上并不像 REST 那样强调与 Web 的工作方式相一致，所以，基于 SOAP 应用很难充分发挥 HTTP 本身的缓存能力。 HTTP是其通信协议/传输协议，SOAP是其应用协议 WSDL WSDL：(Web Services Description Language) WSDL 文件是一个 XML 文档，用于说明一组 SOAP 消息以及如何交换这些消息。大多数情况下由软件自动生成和使用。 网络服务描述语言，是基于XML的，用于描述网络服务、服务定位和服务提供的操作的协议。 UDDI UDDI (Universal Description, Discovery, and Integration) 是一个主要针对Web服务供应商和使用者的新项目。在用户能够调用Web服务之前，必须确定这个服务内包含哪些商务方法，找到被调用的接口定义，还要在服务端来编制软件，UDDI是一种根据描述文档来引导系统查找相应服务的机制。UDDI利用SOAP消息机制（标准的XML/HTTP)来发布，编辑，浏览以及查找注册信息。它采用XML格式来封装各种不同类型的数据，并且发送到注册中心或者由注册中心来返回需要的数据。 统一描述、发现、集成协议，提供基于网络服务的注册和发现机制 REST SOAP协议属于复杂的、重量级的协议，当前随着Web2.0的兴起，表述性状态转移（Representational State Transfer，REST)逐步成为一个流行的架构风格。REST是一种轻量级的Web Service架构风格，其实现和操作比SOAP和XML-RPC更为简洁，可以完全通过HTTP协议实现，还可以利用缓存Cache来提高响应速度，性能、效率和易用性上都优于SOAP协议。REST架构对资源的操作包括获取、创建、修改和删除资源的操作正好对应HTTP协议提供的GET、POST、PUT和DELETE方法，这种针对网络应用的设计和开发方式，可以降低开发的复杂性，提高系统的可伸缩性。REST架构尤其适用于完全无状态的CRUD（Create、Read、Update、Delete，创建、读取、更新、删除)操作。 REST简单而直观，把HTTP协议利用到了极限，在这种思想指导下，它甚至用HTTP请求的头信息来指明资源的表示形式（如果一个资源有多种形式的话，例如人类友善的页面还是机器可读的数据？)，用HTTP的错误机制来返回访问资源的错误。由此带来的直接好处是构建的成本减少了，例如用URI定位每一个资源可以利用通用成熟的技术，而不用再在服务器端开发一套资源访问机制。又如只需简单配置服务器就能规定资源的访问权限，例如通过禁止非GET访问把资源设成只读。 1．面向资源的接口设计 所有的接口设计都是针对资源来设计的，也就很类似于我们的面向对象和面向过程的设计区别，只不过现在将网络上的操作实体都作为资源来看待，同时URI的设计也是体现了对于资源的定位设计。后面会提到有一些网站的API设计说是REST设计，其实是RPC-REST的混合体，并非是REST的思想。 2．抽象操作为基础的CRUD 这点很简单，Http中的get,put,post,delete分别对应了read,update,create,delete四种操作，如果仅仅是作为对于资源的操作，抽象成为这四种已经足够了，但是对于现在的一些复杂的业务服务接口设计，可能这样的抽象未必能够满足。其实这也在后面的几个网站的API设计中暴露了这样的问题，如果要完全按照REST的思想来设计，那么适用的环境将会有限制，而非放之四海皆准的。 3．Http是应用协议而非传输协议 这点在后面各大网站的API分析中有很明显的体现，其实有些网站已经走到了SOAP的老路上，说是REST的理念设计，其实是作了一套私有的SOAP协议，因此称之为REST风格的自定义SOAP协议。 4．无状态，自包含 这点其实不仅仅是对于REST来说的，作为接口设计都需要能够做到这点，也是作为可扩展和高效性的最基本的保证，就算是使用SOAP的WebService也是一样。 Git git init 在本地新建一个repo,进入一个项目目录,执行git init,会初始化一个repo,并在当前文件夹下创建一个.git文件夹.git clone 获取一个url对应的远程Git repo, 创建一个local copy. 一般的格式是git clone [url]. clone下来的repo会以url最后一个斜线后面的名称命名,创建一个文件夹,如果想要指定特定的名称,可以git clone [url] newname指定.git status 查询repo的状态. git status -s: -s表示short, -s的输出标记会有两列,第一列是对staging区域而言,第二列是对working目录而言. git log show commit history of a branch. git log --oneline --number: 每条log只显示一行,显示number条. git log --oneline --graph:可以图形化地表示出分支合并历史. git log branchname可以显示特定分支的log. git log --oneline branch1 ^branch2,可以查看在分支1,却不在分支2中的提交.^表示排除这个分支(Window下可能要给^branch2加上引号). git log --decorate会显示出tag信息. git log --author=[author name] 可以指定作者的提交历史. git log --since --before --until --after 根据提交时间筛选log. --no-merges可以将merge的commits排除在外. git log --grep 根据commit信息过滤log: git log --grep=keywords 默认情况下, git log --grep --author是OR的关系,即满足一条即被返回,如果你想让它们是AND的关系,可以加上--all-match的option. git log -S: filter by introduced diff. 比如: git log -SmethodName (注意S和后面的词之间没有等号分隔). git log -p: show patch introduced at each commit. 每一个提交都是一个快照(snapshot),Git会把每次提交的diff计算出来,作为一个patch显示给你看. 另一种方法是git show [SHA]. git log --stat: show diffstat of changes introduced at each commit. 同样是用来看改动的相对信息的,--stat比-p的输出更简单一些. git add 在提交之前,Git有一个暂存区(staging area),可以放入新添加的文件或者加入新的改动. commit时提交的改动是上一次加入到staging area中的改动,而不是我们disk上的改动. git add . 会递归地添加当前工作目录中的所有文件. git diff 不加参数的git diff: show diff of unstaged changes. 此命令比较的是工作目录中当前文件和暂存区域快照之间的差异,也就是修改之后还没有暂存起来的变化内容. 若要看已经暂存起来的文件和上次提交时的快照之间的差异,可以用: git diff --cached 命令. show diff of staged changes. (Git 1.6.1 及更高版本还允许使用 git diff --staged，效果是相同的). git diff HEAD show diff of all staged or unstated changes. 也即比较woking directory和上次提交之间所有的改动. 如果想看自从某个版本之后都改动了什么,可以用: git diff [version tag] 跟log命令一样,diff也可以加上--stat参数来简化输出. git diff [branchA] [branchB]可以用来比较两个分支. 它实际上会返回一个由A到B的patch,不是我们想要的结果. 一般我们想要的结果是两个分支分开以后各自的改动都是什么,是由命令: git diff [branchA]…[branchB]给出的. 实际上它是:git diff $(git merge-base [branchA] [branchB]) [branchB]的结果. git commit 提交已经被add进来的改动. git commit -m “the commit message\" git commit -a 会先把所有已经track的文件的改动add进来,然后提交(有点像svn的一次提交,不用先暂存). 对于没有track的文件,还是需要git add一下. git commit --amend 增补提交. 会使用与当前提交节点相同的父节点进行一次新的提交,旧的提交将会被取消. git reset undo changes and commits. 这里的HEAD关键字指的是当前分支最末梢最新的一个提交.也就是版本库中该分支上的最新版本. git reset HEAD: unstage files from index and reset pointer to HEAD 这个命令用来把不小心add进去的文件从staged状态取出来,可以单独针对某一个文件操作: git reset HEAD - - filename, 这个- - 也可以不加. git reset --soft move HEAD to specific commit reference, index and staging are untouched. git reset --hard unstage files AND undo any changes in the working directory since last commit. 使用git reset —hard HEAD进行reset,即上次提交之后,所有staged的改动和工作目录的改动都会消失,还原到上次提交的状态. 这里的HEAD可以被写成任何一次提交的SHA-1. 不带soft和hard参数的git reset,实际上带的是默认参数mixed. 总结: git reset --mixed id,是将git的HEAD变了(也就是提交记录变了),但文件并没有改变，(也就是working tree并没有改变). 取消了commit和add的内容. git reset --soft id. 实际上，是git reset –mixed id 后,又做了一次git add.即取消了commit的内容. git reset --hard id.是将git的HEAD变了,文件也变了. 按改动范围排序如下: soft (commit) git revert 反转撤销提交.只要把出错的提交(commit)的名字(reference)作为参数传给命令就可以了. git revert HEAD: 撤销最近的一个提交. git revert会创建一个反向的新提交,可以通过参数-n来告诉Git先不要提交. git rm git rm file: 从staging区移除文件,同时也移除出工作目录. git rm --cached: 从staging区移除文件,但留在工作目录中. git rm --cached从功能上等同于git reset HEAD,清除了缓存区,但不动工作目录树. git clean git clean是从工作目录中移除没有track的文件. 通常的参数是git clean -df: -d表示同时移除目录,-f表示force,因为在git的配置文件中, clean.requireForce=true,如果不加-f,clean将会拒绝执行. git mv git rm - - cached orig; mv orig new; git add new git stash 把当前的改动压入一个栈. git stash将会把当前目录和index中的所有改动(但不包括未track的文件)压入一个栈,然后留给你一个clean的工作状态,即处于上一次最新提交处. git stash list会显示这个栈的list. git stash apply:取出stash中的上一个项目(stash@{0}),并且应用于当前的工作目录. 也可以指定别的项目,比如git stash apply stash@{1}. 如果你在应用stash中项目的同时想要删除它,可以用git stash pop 删除stash中的项目: git stash drop: 删除上一个,也可指定参数删除指定的一个项目. git stash clear: 删除所有项目. git branch git branch可以用来列出分支,创建分支和删除分支. git branch -v可以看见每一个分支的最后一次提交. git branch: 列出本地所有分支,当前分支会被星号标示出. git branch (branchname): 创建一个新的分支(当你用这种方式创建分支的时候,分支是基于你的上一次提交建立的). git branch -d (branchname): 删除一个分支. 删除remote的分支: git push (remote-name) :(branch-name): delete a remote branch. 这个是因为完整的命令形式是: git push remote-name local-branch:remote-branch 而这里local-branch的部分为空,就意味着删除了remote-branch git checkout 　　git checkout (branchname) 切换到一个分支. git checkout -b (branchname): 创建并切换到新的分支. 这个命令是将git branch newbranch和git checkout newbranch合在一起的结果. checkout还有另一个作用:替换本地改动: git checkout -- 此命令会使用HEAD中的最新内容替换掉你的工作目录中的文件.已添加到暂存区的改动以及新文件都不会受到影响. 注意:git checkout filename会删除该文件中所有没有暂存和提交的改动,这个操作是不可逆的. git merge 把一个分支merge进当前的分支. git merge [alias]/[branch] 把远程分支merge到当前分支. 如果出现冲突,需要手动修改,可以用git mergetool. 解决冲突的时候可以用到git diff,解决完之后用git add添加,即表示冲突已经被resolved. git tag tag a point in history as import. 会在一个提交上建立永久性的书签,通常是发布一个release版本或者ship了什么东西之后加tag. 比如: git tag v1.0 git tag -a v1.0, -a参数会允许你添加一些信息,即make an annotated tag. 当你运行git tag -a命令的时候,Git会打开一个编辑器让你输入tag信息. 我们可以利用commit SHA来给一个过去的提交打tag: git tag -a v0.9 XXXX push的时候是不包含tag的,如果想包含,可以在push时加上--tags参数. fetch的时候,branch HEAD可以reach的tags是自动被fetch下来的, tags that aren’t reachable from branch heads will be skipped.如果想确保所有的tags都被包含进来,需要加上--tags选项. git remote list, add and delete remote repository aliases. 因为不需要每次都用完整的url,所以Git为每一个remote repo的url都建立一个别名,然后用git remote来管理这个list. git remote: 列出remote aliases. 如果你clone一个project,Git会自动将原来的url添加进来,别名就叫做:origin. git remote -v:可以看见每一个别名对应的实际url. git remote add [alias] [url]: 添加一个新的remote repo. git remote rm [alias]: 删除一个存在的remote alias. git remote rename [old-alias] [new-alias]: 重命名. git remote set-url [alias] [url]:更新url. 可以加上—push和fetch参数,为同一个别名set不同的存取地址. git fetch download new branches and data from a remote repository. 可以git fetch [alias]取某一个远程repo,也可以git fetch --all取到全部repo fetch将会取到所有你本地没有的数据,所有取下来的分支可以被叫做remote branches,它们和本地分支一样(可以看diff,log等,也可以merge到其他分支),但是Git不允许你checkout到它们. git pull fetch from a remote repo and try to merge into the current branch. pull == fetch + merge FETCH_HEAD git pull会首先执行git fetch,然后执行git merge,把取来的分支的head merge到当前分支.这个merge操作会产生一个新的commit. 如果使用--rebase参数,它会执行git rebase来取代原来的git merge. git rebase --rebase不会产生合并的提交,它会将本地的所有提交临时保存为补丁(patch),放在”.git/rebase”目录中,然后将当前分支更新到最新的分支,最后把保存的补丁应用到分支上。本地的所有提交记录会被丢弃。 rebase的过程中,也许会出现冲突,Git会停止rebase并让你解决冲突,在解决完冲突之后,用git add去更新这些内容,然后无需执行commit,只需要: git rebase --continue就会继续打余下的补丁. git rebase --abort将会终止rebase,当前分支将会回到rebase之前的状态. git push push your new branches and data to a remote repository. git push [alias] [branch] 将会把当前分支merge到alias上的[branch]分支.如果分支已经存在,将会更新,如果不存在,将会添加这个分支. 如果有多个人向同一个remote repo push代码, Git会首先在你试图push的分支上运行git log,检查它的历史中是否能看到server上的branch现在的tip,如果本地历史中不能看到server的tip,说明本地的代码不是最新的,Git会拒绝你的push,让你先fetch,merge,之后再push,这样就保证了所有人的改动都会被考虑进来. git reflog git reflog是对reflog进行管理的命令,reflog是git用来记录引用变化的一种机制,比如记录分支的变化或者是HEAD引用的变化. 当git reflog不指定引用的时候,默认列出HEAD的reflog. HEAD@{0}代表HEAD当前的值,HEAD@{3}代表HEAD在3次变化之前的值. git会将变化记录到HEAD对应的reflog文件中,其路径为.git/logs/HEAD, 分支的reflog文件都放在.git/logs/refs目录下的子目录中. AJAX 1、AJAX概述 5.54 1.1 什么是AJAX AJAX（Asynchronous Javascript And XML)翻译成中文就是“异步Javascript和XML”。即使用Javascript语言与服务器进行异步交互，传输的数据为XML（当然，传输的数据不只是XML)。 AJAX还有一个最大的特点就是，当服务器响应时，不用刷新整个浏览器页面，而是可以局部刷新。这一特点给用户的感受是在不知不觉中完成请求和响应过程。 - 与服务器异步交互； - 浏览器页面局部刷新；5.55 1.2.　同步交互与异步交互 - 同步交互：客户端发出一个请求后，需要等待服务器响应结束后，才能发出第二个请求； - 异步交互：客户端发出一个请求后，无需等待服务器响应结束，就可以发出第二个请求。 5.56 1.3.　AJAX常见应用情景 当我们在百度中输入一个“传”字后，会马上出现一个下拉列表！列表中显示的是包含“传”字的10个关键字。 其实这里就使用了AJAX技术！当文件框发生了输入变化时，浏览器会使用AJAX技术向服务器发送一个请求，查询包含“传”字的前10个关键字，然后服务器会把查询到的结果响应给浏览器，最后浏览器把这10个关键字显示在下拉列表中。 - 整个过程中页面没有刷新，只是刷新页面中的局部位置而已！ - 当请求发出后，浏览器还可以进行其他操作，无需等待服务器的响应！ 当输入用户名后，把光标移动到其他表单项上时，浏览器会使用AJAX技术向服务器发出请求，服务器会查询名为zhangSan的用户是否存在，最终服务器返回true表示名为zhangSan的用户已经存在了，浏览器在得到结果后显示“用户名已被注册！”。 - 整个过程中页面没有刷新，只是局部刷新了； - 在请求发出后，浏览器不用等待服务器响应结果就可以进行其他操作；5.57 1.4　AJAX的优缺点 优点： - AJAX使用Javascript技术向服务器发送异步请求； - AJAX无须刷新整个页面； - 因为服务器响应内容不再是整个页面，而是页面中的局部，所以AJAX性能高； 缺点： - AJAX并不适合所有场景，很多时候还是要使用同步交互； - AJAX虽然提高了用户体验，但无形中向服务器发送的请求次数增多了，导致服务器压力增大； - 因为AJAX是在浏览器中使用Javascript技术完成的，所以还需要处理浏览器兼容性问题； 2、AJAX技术 5.58 2.1 AJAX第一例（发送GET请求) 2.1.1 准备工作 因为AJAX也需要请求服务器，异步请求也是请求服务器，所以我们需要先写好服务器端代码，即编写一个Servlet！ 这里，Servlet很简单，只需要输出“Hello AJAX!”。 public class AServlet extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { System.out.println(\"Hello AJAX!\"); response.getWriter().print(\"Hello AJAX!\"); } } 2.1.2　AJAX核心（XMLHttpRequest) 其实AJAX就是在Javascript中多添加了一个对象：XMLHttpRequest对象。所有的异步交互都是使用XMLHttpRequest对象完成的。也就是说，我们只需要学习一个Javascript的新对象即可。 注意，各个浏览器对XMLHttpRequest的支持也是不同的！大多数浏览器都支持DOM2规范，都可以使用：var xmlHttp = new XMLHttpRequest()来创建对象 为了处理浏览器兼容问题，给出下面方法来创建XMLHttpRequest对象： function createXMLHttpRequest() { var xmlHttp; // 适用于大多数浏览器，以及IE7和IE更高版本 try{ xmlHttp = new XMLHttpRequest(); } catch (e) { // 适用于IE6 try { xmlHttp = new ActiveXObject(\"Msxml2.XMLHTTP\"); } catch (e) { // 适用于IE5.5，以及IE更早版本 try{ xmlHttp = new ActiveXObject(\"Microsoft.XMLHTTP\"); } catch (e){} } } return xmlHttp; } 2.1.3　打开与服务器的连接（open方法) 当得到XMLHttpRequest对象后，就可以调用该对象的open()方法打开与服务器的连接了。open()方法的参数如下： open(method, url, async)： - method：请求方式，通常为GET或POST； - url：请求的服务器地址，例如：/ajaxdemo1/AServlet，若为GET请求，还可以在URL后追加参数； - async：这个参数可以不给，默认值为true，表示异步请求； var xmlHttp = createXMLHttpRequest(); xmlHttp.open(\"GET\", \"/ajaxdemo1/AServlet\", true); 2.1.4　发送请求 当使用open打开连接后，就可以调用XMLHttpRequest对象的send()方法发送请求了。send()方法的参数为POST请求参数，即对应HTTP协议的请求体内容，若是GET请求，需要在URL后连接参数。 注意：若没有参数，需要给出null为参数！若不给出null为参数，可能会导致FireFox浏览器不能正常发送请求！ xmlHttp.send(null); 2.1.5　接收服务器响应 当请求发送出去后，服务器端Servlet就开始执行了，但服务器端的响应还没有接收到。接下来我们来接收服务器的响应。 XMLHttpRequest对象有一个onreadystatechange事件，它会在XMLHttpRequest对象的状态发生变化时被调用。下面介绍一下XMLHttpRequest对象的5种状态： - 0：初始化未完成状态，只是创建了XMLHttpRequest对象，还未调用open()方法； - 1：请求已开始，open()方法已调用，但还没调用send()方法； - 2：请求发送完成状态，send()方法已调用； - 3：开始读取服务器响应； - 4：读取服务器响应结束。 onreadystatechange事件会在状态为1、2、3、4时引发。 　　下面代码会被执行四次！对应XMLHttpRequest的四种状态！ xmlHttp.onreadystatechange = function() { alert('hello'); }; 但通常我们只关心最后一种状态，即读取服务器响应结束时，客户端才会做出改变。我们可以通过XMLHttpRequest对象的readyState属性来得到XMLHttpRequest对象的状态。 xmlHttp.onreadystatechange = function() { if(xmlHttp.readyState == 4) { alert('hello'); } }; 其实我们还要关心服务器响应的状态码是否为200，其服务器响应为404，或500，那么就表示请求失败了。我们可以通过XMLHttpRequest对象的status属性得到服务器的状态码。 最后，我们还需要获取到服务器响应的内容，可以通过XMLHttpRequest对象的responseText得到服务器响应内容。 responsXML是xml格式的文本，是document对象 xmlHttp.onreadystatechange = function() { if(xmlHttp.readyState == 4 && xmlHttp.status == 200) { alert(xmlHttp.responseText); } }; 2.1.6　AJAX第一例小结 - 创建XMLHttpRequest对象； - 调用open()方法打开与服务器的连接； - 调用send()方法发送请求； - 为XMLHttpRequest对象指定onreadystatechange事件函数，这个函数会在XMLHttpRequest的1、2、3、4，四种状态时被调用； XMLHttpRequest对象的5种状态： - 0：初始化未完成状态，只是创建了XMLHttpRequest对象，还未调用open()方法； - 1：请求已开始，open()方法已调用，但还没调用send()方法； - 2：请求发送完成状态，send()方法已调用； - 3：开始读取服务器响应； - 4：读取服务器响应结束。 通常我们只关心4状态。 XMLHttpRequest对象的status属性表示服务器状态码，它只有在readyState为4时才能获取到。 XMLHttpRequest对象的responseText属性表示服务器响应内容，它只有在readyState为4时才能获取到！ var xmlHttp = new XMLHttpRequest(); xmlHttp.open(\"GET\", \"/AJAX/AServlet\"); xmlHttp.send(null); xmlHttp.onreadystatechange = function(){ if(xmlHttp.readyState == 4 && xmlHttp.status == 200){ var h1 = document.getElementById(\"h1\"); h1.innerHTML = xmlHttp.responseText; } } public class AServlet extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { response.setContentType(\"text/html;charset=utf-8\"); response.getWriter().print(\"hehe\"); } } 5.59 2.2　AJAX第二例（发送POST请求) 2.2.1　发送POST请求注意事项 POST请求必须设置ContentType请求头的值为application/x-www.form-encoded。表单的enctype默认值就是为application/x-www.form-encoded！因为默认值就是这个，所以大家可能会忽略这个值！当设置了的enctype=” application/x-www.form-encoded”时，等同与设置了Cotnent-Type请求头。 但在使用AJAX发送请求时，就没有默认值了，这需要我们自己来设置请求头： xmlHttp.setRequestHeader(\"Content-Type\", \"application/x-www-form-urlencoded\"); 当没有设置Content-Type请求头为application/x-www-form-urlencoded时，Web容器会忽略请求体的内容。所以，在使用AJAX发送POST请求时，需要设置这一请求头，然后使用send()方法来设置请求体内容。 xmlHttp.send(\"b=B\"); 　　这时Servlet就可以获取到这个参数！！！ AServlet public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { System.out.println(request.getParameter(\"b\")); System.out.println(\"Hello AJAX!\"); response.getWriter().print(\"Hello AJAX!\"); } ajax2.jsp function createXMLHttpRequest() { try { return new XMLHttpRequest();//大多数浏览器 } catch (e) { try { return new ActiveXObject(\"Msxml2.XMLHTTP\"); } catch (e) { return new ActiveXObject(\"Microsoft.XMLHTTP\"); } } } function send() { var xmlHttp = createXMLHttpRequest(); xmlHttp.onreadystatechange = function() { if(xmlHttp.readyState == 4 && xmlHttp.status == 200) { var div = document.getElementById(\"div1\"); div.innerHTML = xmlHttp.responseText; } }; xmlHttp.open(\"POST\", \"/ajaxdemo1/AServlet?\", true); xmlHttp.setRequestHeader(\"Content-Type\", \"application/x-www-form-urlencoded\"); xmlHttp.send(\"b=B\"); } AJAX2 测试 5.60 2.3　AJAX第三例（用户名是否已被注册) 2.3.1　功能介绍 在注册表单中，当用户填写了用户名后，把光标移开后，会自动向服务器发送异步请求。服务器返回true或false，返回true表示这个用户名已经被注册过，返回false表示没有注册过。 客户端得到服务器返回的结果后，确定是否在用户名文本框后显示“用户名已被注册”的错误信息！ 2.3.2　案例分析 - regist.jsp页面中给出注册表单； - 在username表单字段中添加onblur事件，调用send()方法； - send()方法获取username表单字段的内容，向服务器发送异步请求，参数为username； - RegistServlet：获取username参数，判断是否为“itcast”，如果是响应true，否则响应false； 2.3.3　代码 regist.jsp function ajax(){ var userText = document.getElementById(\"username\"); var username = userText.value; var xmlHttp = new XMLHttpRequest(); xmlHttp.open(\"GET\", \"?username=\"+username ); xmlHttp.send(null); xmlHttp.onreadystatechange = function(){ if(xmlHttp.readyState == 4 && xmlHttp.status == 200){ var span = document.getElementById(\"span1\"); if(xmlHttp.responseText == \"true\"){ span.innerHTML = \"用户名可用\"; }else{ span.innerHTML = \"用户名已被注册\"; } } } } 用户名： RegistServlet.java public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { String username = request.getParameter(\"username\"); System.out.println(username); if(username.equals(\"admin\")){ response.getWriter().print(false); }else{ response.getWriter().print(true); } } 前端 HTML的DOM对象说几个，Document的对象和方法 document.body 返回元素 1 document.cookie 返回或设置与当前文档相关的cookie 1 document.domain 返回当前文档的服务器域名 1 document.referrer 返回连接至当前文档的文档连接 1 document.title 返回当前文档的元素 1 document.URL 返回当前文档的完整URL 1 "},"zother5-Java-Interview/三、Java 集合.html":{"url":"zother5-Java-Interview/三、Java 集合.html","title":"三、Java 集合","keywords":"","body":" 本github最初的版本是一份word文档，目前只是把word刚刚搬上来了，但是有些图片、排版还没来得急整理，看起来可能还是有点困难 所以可以先关注一下我的公众号，在我的公众号后台回复 888 获取这个github仓库的PDF版本，左侧有导航栏，方便大家阅读。 集合框架 3.1 接口 常见接口 Map 接口和 Collection 接口是所有集合框架的父接口； Collection 接口的子接口包括：Set 接口、List 接口和Queue接口； Map 接口的实现类主要有：HashMap、TreeMap、LinkedHashMap、Hashtable、ConcurrentHashMap 以及 Properties 等； Set 接口的实现类主要有：HashSet、TreeSet、LinkedHashSet 等； List 接口的实现类主要有：ArrayList、LinkedList、Stack 、Vector以及CopyOnWriteArrayList 等； Queue接口的主要实现类有：ArrayDeque、ArrayBlockingQueue、LinkedBlockingQueue、PriorityQueue等； List接口和Set接口的区别 List 元素是有序的，可以重复；Set 元素是无序的，不可以重复。队列、Set、Map 区别 List 有序列表 Set无序集合 Map键值对的集合 Queue队列FlFO 3.2 List 有顺序，可重复ArrayList 基于数组实现，无容量的限制。 在执行插入元素时可能要扩容，在删除元素时并不会减小数组的容量，在查找元素时要遍历数组，对于非null的元素采取equals的方式寻找。 是非线程安全的。 注意点： （1)ArrayList随机元素时间复杂度O(1)，插入删除操作需大量移动元素，效率较低 （2)为了节约内存，当新建容器为空时，会共享Object[] EMPTY_ELEMENTDATA = {}和 Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}空数组 （3)容器底层采用数组存储，每次扩容为1.5倍 （4)ArrayList的实现中大量地调用了Arrays.copyof()和System.arraycopy()方法，其实Arrays.copyof()内部也是调用System.arraycopy()。System.arraycopy()为Native方法 （5)两个ToArray方法 Object[] toArray()方法。该方法有可能会抛出java.lang.ClassCastException异常 T[] toArray(T[] a)方法。该方法可以直接将ArrayList转换得到的Array进行整体向下转型 （6)ArrayList可以存储null值 （7)ArrayList每次修改（增加、删除)容器时，都是修改自身的modCount；在生成迭代器时，迭代器会保存该modCount值，迭代器每次获取元素时，会比较自身的modCount与ArrayList的modCount是否相等，来判断容器是否已经被修改，如果被修改了则抛出异常（fast-fail机制)。 成员变量 /** * Default initial capacity. */ private static final int DEFAULT_CAPACITY = 10; /** * Shared empty array instance used for empty instances. */ private static final Object[] EMPTY_ELEMENTDATA = {}; /** * Shared empty array instance used for default sized empty instances. We * distinguish this from EMPTY_ELEMENTDATA to know how much to inflate when * first element is added. */ private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; /** * The array buffer into which the elements of the ArrayList are stored. * The capacity of the ArrayList is the length of this array buffer. Any * empty ArrayList with elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA * will be expanded to DEFAULT_CAPACITY when the first element is added. */ transient Object[] elementData; // non-private to simplify nested class access /** * The size of the ArrayList (the number of elements it contains). * * @serial */ private int size; 构造方法 public ArrayList(int initialCapacity) { if (initialCapacity > 0) { this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { this.elementData = EMPTY_ELEMENTDATA; } else { throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); } } 添加 add(e) public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; } 即使初始化时指定大小 小于10个，添加元素时会调整大小，保证capacity不会少于10个。 private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length > 0) grow(minCapacity); } 扩容 private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity >> 1); if (newCapacity - minCapacity 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } Arrays.copyOf底层是System.arrayCopy public static T[] copyOf(U[] original, int newLength, Class newType) { @SuppressWarnings(\"unchecked\") T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy; } public static native void arraycopy(Object src, int srcPos, Object dest, int destPos, int length); 添加 add(index,e) public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; } private void rangeCheckForAdd(int index) { if (index > size || index 删除 remove(o) public boolean remove(Object o) { if (o == null) { for (int index = 0; index private void fastRemove(int index) { modCount++; int numMoved = size - index - 1; if (numMoved > 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work } 删除 remove(index) public E remove(int index) { rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved > 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue; } 获取 public E get(int index) { rangeCheck(index); return elementData(index); } private void rangeCheck(int index) { if (index >= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); } E elementData(int index) { return (E) elementData[index]; }更新 public E set(int index, E element) { rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; return oldValue; } 遍历 public Iterator iterator() { return new Itr(); } /** * An optimized version of AbstractList.Itr */ private class Itr implements Iterator { int cursor; // index of next element to return int lastRet = -1; // index of last element returned; -1 if no such int expectedModCount = modCount; public boolean hasNext() { return cursor != size; } @SuppressWarnings(\"unchecked\") public E next() { checkForComodification(); int i = cursor; if (i >= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i >= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i]; } public void remove() { if (lastRet consumer) { Objects.requireNonNull(consumer); final int size = ArrayList.this.size; int i = cursor; if (i >= size) { return; } final Object[] elementData = ArrayList.this.elementData; if (i >= elementData.length) { throw new ConcurrentModificationException(); } while (i != size && modCount == expectedModCount) { consumer.accept((E) elementData[i++]); } // update once at end of iteration to reduce heap write traffic cursor = i; lastRet = i - 1; checkForComodification(); } final void checkForComodification() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); } } 包含 public boolean contains(Object o) { return indexOf(o) >= 0; } public int indexOf(Object o) { if (o == null) { for (int i = 0; i LinkedList 基于双向链表机制 在插入元素时，须创建一个新的Entry对象，并切换相应元素的前后元素的引用；在查找元素时，须遍历链表；在删除元素时，须遍历链表，找到要删除的元素，然后从链表上将此元素删除即可。 是非线程安全的。 注意： （1)LinkedList有两个构造参数，一个为无參构造，只是新建一个空对象，第二个为有参构造，新建一个空对象，然后把所有元素添加进去。 （2)LinkedList的存储单元为一个名为Node的内部类，包含pre指针，next指针，和item元素，实现为双向链表 （3)LinkedList的删除、添加操作时间复杂度为O(1)，查找时间复杂度为O(n)，查找函数有一定优化，容器会先判断查找的元素是离头部较近，还是尾部较近，来决定从头部开始遍历还是尾部开始遍历 （4)LinkedList实现了Deque接口，因此也可以作为栈、队列和双端队列来使用。 （5)LinkedList可以存储null值成员变量 transient int size = 0; transient Node first; transient Node last;构造方法 public LinkedList() { } 添加 add(e) public boolean add(E e) { linkLast(e); return true; } 把一个元素添加到最后一个位置 void linkLast(E e) { final Node l = last; final Node newNode = new Node<>(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++; } 添加 add(index,e) public void add(int index, E element) { checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index)); } - Node node(int index) { // assert isElementIndex(index); if (index > 1)) { Node x = first; for (int i = 0; i x = last; for (int i = size - 1; i > index; i--) x = x.prev; return x; } } void linkBefore(E e, Node succ) { // assert succ != null; final Node pred = succ.prev; final Node newNode = new Node<>(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++; } 删除 remove(o) public boolean remove(Object o) { if (o == null) { for (Node x = first; x != null; x = x.next) { if (x.item == null) { unlink(x); return true; } } } else { for (Node x = first; x != null; x = x.next) { if (o.equals(x.item)) { unlink(x); return true; } } } return false; } E unlink(Node x) { // assert x != null; final E element = x.item; final Node next = x.next; final Node prev = x.prev; if (prev == null) { first = next; } else { prev.next = next; x.prev = null; } if (next == null) { last = prev; } else { next.prev = prev; x.next = null; } x.item = null; size--; modCount++; return element; } 删除 remove(index) public E remove(int index) { checkElementIndex(index); return unlink(node(index)); } 获取 public E get(int index) { checkElementIndex(index); return node(index).item; } 更新 public E set(int index, E element) { checkElementIndex(index); Node x = node(index); E oldVal = x.item; x.item = element; return oldVal; } 遍历 public ListIterator listIterator(int index) { checkPositionIndex(index); return new ListItr(index); } private class ListItr implements ListIterator { private Node lastReturned; private Node next; private int nextIndex; private int expectedModCount = modCount; ListItr(int index) { // assert isPositionIndex(index); next = (index == size) ? null : node(index); nextIndex = index; } public boolean hasNext() { return nextIndex 0; } public E previous() { checkForComodification(); if (!hasPrevious()) throw new NoSuchElementException(); lastReturned = next = (next == null) ? last : next.prev; nextIndex--; return lastReturned.item; } public int nextIndex() { return nextIndex; } public int previousIndex() { return nextIndex - 1; } public void remove() { checkForComodification(); if (lastReturned == null) throw new IllegalStateException(); Node lastNext = lastReturned.next; unlink(lastReturned); if (next == lastReturned) next = lastNext; else nextIndex--; lastReturned = null; expectedModCount++; } public void set(E e) { if (lastReturned == null) throw new IllegalStateException(); checkForComodification(); lastReturned.item = e; } public void add(E e) { checkForComodification(); lastReturned = null; if (next == null) linkLast(e); else linkBefore(e, next); nextIndex++; expectedModCount++; } public void forEachRemaining(Consumer action) { Objects.requireNonNull(action); while (modCount == expectedModCount && nextIndex 包含 public boolean contains(Object o) { return indexOf(o) != -1; } public int indexOf(Object o) { int index = 0; if (o == null) { for (Node x = first; x != null; x = x.next) { if (x.item == null) return index; index++; } } else { for (Node x = first; x != null; x = x.next) { if (o.equals(x.item)) return index; index++; } } return -1; } Vector 基于synchronized实现的线程安全的ArrayList，但在插入元素时容量扩充的机制和ArrayList稍有不同，并可通过传入capacityIncrement来控制容量的扩充。成员变量 protected Object[] elementData; protected int elementCount; protected int capacityIncrement; 构造方法 public Vector(int initialCapacity) { this(initialCapacity, 0); } public Vector() { this(10); } public Vector(int initialCapacity, int capacityIncrement) { super(); if (initialCapacity 添加 public synchronized boolean add(E e) { modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true; } 删除 public boolean remove(Object o) { return removeElement(o); } public synchronized boolean removeElement(Object obj) { modCount++; int i = indexOf(obj); if (i >= 0) { removeElementAt(i); return true; } return false; } 扩容 private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement > 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity); } 获取 public synchronized E get(int index) { if (index >= elementCount) throw new ArrayIndexOutOfBoundsException(index); return elementData(index); } 更新 public synchronized E set(int index, E element) { if (index >= elementCount) throw new ArrayIndexOutOfBoundsException(index); E oldValue = elementData(index); elementData[index] = element; return oldValue; } 包含 public boolean contains(Object o) { return indexOf(o, 0) >= 0; } public synchronized int indexOf(Object o, int index) { if (o == null) { for (int i = index ; i Stack 基于Vector实现，支持LIFO。类声明 public class Stack extends Vector {} push public E push(E item) { addElement(item); return item; } pop public synchronized E pop() { E obj; int len = size(); obj = peek(); removeElementAt(len - 1); return obj; } peek public synchronized E peek() { int len = size(); if (len == 0) throw new EmptyStackException(); return elementAt(len - 1); } CopyOnWriteArrayList 是一个线程安全、并且在读操作时无锁的ArrayList。 很多时候，我们的系统应对的都是读多写少的并发场景。CopyOnWriteArrayList容器允许并发读，读操作是无锁的，性能较高。至于写操作，比如向容器中添加一个元素，则首先将当前容器复制一份，然后在新副本上执行写操作，结束之后再将原容器的引用指向新容器。 优点 1)采用读写分离方式，读的效率非常高 2)CopyOnWriteArrayList的迭代器是基于创建时的数据快照的，故数组的增删改不会影响到迭代器 缺点 1)内存占用高，每次执行写操作都要将原容器拷贝一份，数据量大时，对内存压力较大，可能会引起频繁GC 2)只能保证数据的最终一致性，不能保证数据的实时一致性。写和读分别作用在新老不同容器上，在写操作执行过程中，读不会阻塞但读取到的却是老容器的数据。 成员变量 /** The lock protecting all mutators */ final transient ReentrantLock lock = new ReentrantLock(); /** The array, accessed only via getArray/setArray. */ private transient volatile Object[] array; 构造方法 public CopyOnWriteArrayList() { setArray(new Object[0]); } final void setArray(Object[] a) { array = a; } 添加（有锁，锁内重新创建数组) final Object[] getArray() { return array; } public boolean add(E e) { final ReentrantLock lock = this.lock; lock.lock(); try { Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; } finally { lock.unlock(); } } 存在则添加（有锁，锁内重新创建数组) 先保存一份数组snapshot，如果snapshot中存在，则直接返回。 如果不存在，那么加锁，获取当前数组current，比较snapshot与current，遍历它们共同长度内的元素，如果发现current中某一个元素等于e，那么直接返回（当然current与snapshot相同就不必看了)； 之后再遍历current单独的部分，如果发现current中某一个元素等于e，那么直接返回； 此时可以去创建一个长度+1的新数组，将e加入。 public boolean addIfAbsent(E e) { Object[] snapshot = getArray(); return indexOf(e, snapshot, 0, snapshot.length) >= 0 ? false : addIfAbsent(e, snapshot); } private boolean addIfAbsent(E e, Object[] snapshot) { final ReentrantLock lock = this.lock; lock.lock(); try { Object[] current = getArray(); int len = current.length; if (snapshot != current) { // Optimize for lost race to another addXXX operation int common = Math.min(snapshot.length, len); for (int i = 0; i //如果snapshot与current元素不同但current与e相同，那么直接返回（扫描0到common) if (current[i] != snapshot[i] && eq(e, current[i])) return false; // 如果current中存在e，那么直接返回（扫描commen到len) if (indexOf(e, current, common, len) >= 0) return false; } Object[] newElements = Arrays.copyOf(current, len + 1); newElements[len] = e; setArray(newElements); return true; } finally { lock.unlock(); } } 删除（有锁，锁内重新创建数组) public E remove(int index) { final ReentrantLock lock = this.lock; lock.lock(); try { Object[] elements = getArray(); int len = elements.length; E oldValue = get(elements, index); int numMoved = len - index - 1; if (numMoved == 0) setArray(Arrays.copyOf(elements, len - 1)); else { Object[] newElements = new Object[len - 1]; System.arraycopy(elements, 0, newElements, 0, index); System.arraycopy(elements, index + 1, newElements, index, numMoved); setArray(newElements); } return oldValue; } finally { lock.unlock(); } } 获取（无锁) public E get(int index) { return get(getArray(), index); } private E get(Object[] a, int index) { return (E) a[index]; } 更新（有锁，锁内重新创建数组) public E set(int index, E element) { final ReentrantLock lock = this.lock; lock.lock(); try { Object[] elements = getArray(); E oldValue = get(elements, index); if (oldValue != element) { int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len); newElements[index] = element; setArray(newElements); } else { // 为了保持“volatile”的语义，任何一个读操作都应该是一个写操作的结果， 也就是读操作看到的数据一定是某个写操作的结果（尽管写操作没有改变数据本身)。 所以这里即使不设置也没有问题，仅仅是为了一个语义上的补充（就如源码中的注释所言) // Not quite a no-op; ensures volatile write semantics setArray(elements); } return oldValue; } finally { lock.unlock(); } } 包含（无锁) public boolean contains(Object o) { Object[] elements = getArray(); return indexOf(o, elements, 0, elements.length) >= 0; } private static int indexOf(Object o, Object[] elements, int index, int fence) { if (o == null) { for (int i = index; i 遍历（遍历的是获取iterator时的数组快照) public Iterator iterator() { return new COWIterator(getArray(), 0); } static final class COWIterator implements ListIterator { /** Snapshot of the array */ private final Object[] snapshot; /** Index of element to be returned by subsequent call to next. */ private int cursor; private COWIterator(Object[] elements, int initialCursor) { cursor = initialCursor; snapshot = elements; } public boolean hasNext() { return cursor 0; } @SuppressWarnings(\"unchecked\") public E next() { if (! hasNext()) throw new NoSuchElementException(); return (E) snapshot[cursor++]; } @SuppressWarnings(\"unchecked\") public E previous() { if (! hasPrevious()) throw new NoSuchElementException(); return (E) snapshot[--cursor]; } public int nextIndex() { return cursor; } public int previousIndex() { return cursor-1; } /** * Not supported. Always throws UnsupportedOperationException. * @throws UnsupportedOperationException always; {@code remove} * is not supported by this iterator. */ public void remove() { throw new UnsupportedOperationException(); } /** * Not supported. Always throws UnsupportedOperationException. * @throws UnsupportedOperationException always; {@code set} * is not supported by this iterator. */ public void set(E e) { throw new UnsupportedOperationException(); } /** * Not supported. Always throws UnsupportedOperationException. * @throws UnsupportedOperationException always; {@code add} * is not supported by this iterator. */ public void add(E e) { throw new UnsupportedOperationException(); } @Override public void forEachRemaining(Consumer action) { Objects.requireNonNull(action); Object[] elements = snapshot; final int size = elements.length; for (int i = cursor; i List实现类之间的区别 (1) 对于需要快速插入，删除元素，应该使用LinkedList。 (2) 对于需要快速随机访问元素，应该使用ArrayList。 (3) 对于“单线程环境” 或者 “多线程环境，但List仅仅只会被单个线程操作”，此时应该使用非同步的类(如ArrayList)。 对于“多线程环境，且List可能同时被多个线程操作”，此时，应该使用同步的类(如Vector、CopyOnWriteArrayList)。 3.3 Set 没有顺序，不可重复HashSet（底层是HashMap) Set不允许元素重复。 基于HashMap实现，无容量限制。 是非线程安全的。 成员变量 private transient HashMap map; // Dummy value to associate with an Object in the backing Map private static final Object PRESENT = new Object(); 构造方法 /** * Constructs a new, empty set; the backing HashMap instance has * default initial capacity (16) and load factor (0.75). */ public HashSet() { map = new HashMap<>(); } public HashSet(int initialCapacity) { map = new HashMap<>(initialCapacity); } public HashSet(int initialCapacity, float loadFactor) { map = new HashMap<>(initialCapacity, loadFactor); } 添加 public boolean add(E e) { return map.put(e, PRESENT)==null; } 删除 public boolean remove(Object o) { return map.remove(o)==PRESENT; } 遍历 public Iterator iterator() { return map.keySet().iterator(); } 包含 public boolean contains(Object o) { return map.containsKey(o); } TreeSet（底层是TreeMap) 基于TreeMap实现，支持排序（自然排序 或者 根据创建TreeSet 时提供的 Comparator 进行排序)。 是非线程安全的。 成员变量 /** * The backing map. */ private transient NavigableMap m; // Dummy value to associate with an Object in the backing Map private static final Object PRESENT = new Object(); 构造方法 public TreeSet() { this(new TreeMap()); } public TreeSet(Comparator comparator) { this(new TreeMap<>(comparator)); } 添加 public boolean add(E e) { return m.put(e, PRESENT)==null; } 删除 public boolean remove(Object o) { return m.remove(o)==PRESENT; } 遍历 public Iterator iterator() { return m.navigableKeySet().iterator(); } 包含 public boolean contains(Object o) { return m.containsKey(o); } 获取开头 public E first() { return m.firstKey(); } 获取结尾 public E last() { return m.lastKey(); } 子集 public NavigableSet subSet(E fromElement, boolean fromInclusive, E toElement, boolean toInclusive) { return new TreeSet<>(m.subMap(fromElement, fromInclusive, toElement, toInclusive)); } 默认是含头不含尾 public SortedSet subSet(E fromElement, E toElement) { return subSet(fromElement, true, toElement, false); } LinkedHashSet（继承自HashSet，底层是LinkedHashMap) LinkedHashSet继承自HashSet，源码更少、更简单，唯一的区别是LinkedHashSet内部使用的是LinkHashMap。这样做的意义或者好处就是LinkedHashSet中的元素顺序是可以保证的，也就是说遍历序和插入序是一致的。类声明 public class LinkedHashSet extends HashSet implements Set, Cloneable, java.io.Serializable {} 构造方法 public LinkedHashSet(int initialCapacity, float loadFactor) { super(initialCapacity, loadFactor, true); } /** * Constructs a new, empty linked hash set with the specified initial * capacity and the default load factor (0.75). * * @param initialCapacity the initial capacity of the LinkedHashSet * @throws IllegalArgumentException if the initial capacity is less * than zero */ public LinkedHashSet(int initialCapacity) { super(initialCapacity, .75f, true); } /** * Constructs a new, empty linked hash set with the default initial * capacity (16) and load factor (0.75). */ public LinkedHashSet() { super(16, .75f, true); } super指的是HashSet的default访问级别的构造方法 /** * Constructs a new, empty linked hash set. (This package private * constructor is only used by LinkedHashSet.) The backing * HashMap instance is a LinkedHashMap with the specified initial * capacity and the specified load factor. * * @param initialCapacity the initial capacity of the hash map * @param loadFactor the load factor of the hash map * @param dummy ignored (distinguishes this * constructor from other int, float constructor.) * @throws IllegalArgumentException if the initial capacity is less * than zero, or if the load factor is nonpositive */ HashSet(int initialCapacity, float loadFactor, boolean dummy) { map = new LinkedHashMap<>(initialCapacity, loadFactor); } BitSet（位集，底层是long数组，用于替代List) BitSet是位操作的对象，值只有0或1即false和true，内部维护了一个long数组，初始只有一个long，所以BitSet最小的size是64（8个字节64个位，可以存储64个数字)，当随着存储的元素越来越多，BitSet内部会动态扩充，最终内部是由N个long来存储，这些针对操作都是透明的。 默认情况下，BitSet的所有位都是false即0。 不是线程安全的。 用1位来表示一个数据是否出现过，0为没有出现过，1表示出现过。使用的时候既可根据某一个是否为0表示，此数是否出现过。 一个1GB的空间，有8102410241024 = 8.5810^9bit，也就是1GB的空间可以表示85亿多个数。 常见的应用是那些需要对海量数据进行一些统计工作的时候，比如日志分析、用户数统计等等，如统计40亿个数据中没有出现的数据，将40亿个不同数据进行排序，海量数据去重等等。 JDK选择long数组作为BitSet的内部存储结构是出于性能的考虑，因为BitSet提供and和or这种操作，需要对两个BitSet中的所有bit位做and或者or，实现的时候需要遍历所有的数组元素。使用long能够使得循环的次数降到最低，所以Java选择使用long数组作为BitSet的内部存储结构。 BitSet() 创建一个新的位 set。 BitSet(int nbits) 创建一个位 set，它的初始大小足以显式表示索引范围在 0 到 nbits-1 的位。 void and(BitSet set) 对此目标位 set 和参数位 set 执行逻辑与操作。 void andNot(BitSet set) 清除此 BitSet 中所有的位，其相应的位在指定的 BitSet 中已设置。 int cardinality() 返回此 BitSet 中设置为 true 的位数。 void clear() 将此 BitSet 中的所有位设置为 false。 void clear(int bitIndex) 将索引指定处的位设置为 false。 void clear(int fromIndex, int toIndex) 将指定的 fromIndex（包括)到指定的 toIndex（不包括)范围内的位设置为 false。 Object clone() 复制此 BitSet，生成一个与之相等的新 BitSet。 boolean equals(Object obj) 将此对象与指定的对象进行比较。 void flip(int bitIndex) 将指定索引处的位设置为其当前值的补码。 void flip(int fromIndex, int toIndex) 将指定的 fromIndex（包括)到指定的 toIndex（不包括)范围内的每个位设置为其当前值的补码。 boolean get(int bitIndex) 返回指定索引处的位值。 BitSet get(int fromIndex, int toIndex) 返回一个新的 BitSet，它由此 BitSet 中从 fromIndex（包括)到 toIndex（不包括)范围内的位组成。 int hashCode() 返回此位 set 的哈希码值。 boolean intersects(BitSet set) 如果指定的 BitSet 中有设置为 true 的位，并且在此 BitSet 中也将其设置为true，则返回 ture。 boolean isEmpty() 如果此 BitSet 中没有包含任何设置为 true 的位，则返回 ture。 int length() 返回此 BitSet 的“逻辑大小”：BitSet 中最高设置位的索引加 1。 int nextClearBit(int fromIndex) 返回第一个设置为 false 的位的索引，这发生在指定的起始索引或之后的索引上。 int nextSetBit(int fromIndex) 返回第一个设置为 true 的位的索引，这发生在指定的起始索引或之后的索引上。 void or(BitSet set) 对此位 set 和位 set 参数执行逻辑或操作。 void set(int bitIndex) 将指定索引处的位设置为 true。 void set(int bitIndex, boolean value) 将指定索引处的位设置为指定的值。 void set(int fromIndex, int toIndex) 将指定的 fromIndex（包括)到指定的 toIndex（不包括)范围内的位设置为 true。 void set(int fromIndex, int toIndex, boolean value) 将指定的 fromIndex（包括)到指定的 toIndex（不包括)范围内的位设置为指定的值。 int size() 返回此 BitSet 表示位值时实际使用空间的位数。 String toString() 返回此位 set 的字符串表示形式。 void xor(BitSet set) 对此位 set 和位 set 参数执行逻辑异或操作。 去重示例 public static void containChars(String str) { BitSet used = new BitSet(); for (int i = 0; i [abcdf] 排序示例 public static void sortArray(int[] array) { BitSet bitSet = new BitSet(2 = 0; i = bitSet.nextSetBit(i + 1)) { orderedArray[k++] = i; } System.out.println(\"After ordering: \"); for (int i = 0; i BitSet size: 16384 After ordering: 1 2 3 356 423 700 2323 6400 9999 CopyOnWriteArraySet（底层是CopyOnWriteArrayList) 基于CopyOnWriteArrayList实现，其唯一的不同是在add时调用的是CopyOnWriteArrayList的addIfAbsent方法。 在每次add的时候都要进行数组的遍历，因此其性能会略低于CopyOnWriteArrayList。成员变量 private final CopyOnWriteArrayList al; 构造方法 public CopyOnWriteArraySet() { al = new CopyOnWriteArrayList(); } 添加 public boolean add(E e) { return al.addIfAbsent(e); } 删除 public boolean remove(Object o) { return al.remove(o); } 遍历 public Iterator iterator() { return al.iterator(); } 包含 public boolean contains(Object o) { return al.contains(o); } 3.4 Queue 先进先出”（FIFO—first in first out)的线性表 LinkedList类实现了Queue接口，因此我们可以把LinkedList当成Queue来用。 Java里有一个叫做Stack的类，却没有叫做Queue的类（它是个接口名字)。当需要使用栈时，Java已不推荐使用Stack，而是推荐使用更高效的ArrayDeque；既然Queue只是一个接口，当需要使用队列时也就首选ArrayDeque了（次选是LinkedList)。 Deque既可以作为栈使用，也可以作为队列使用。 Queue Method Equivalent Deque Method 说明 add(e) addLast(e) 向队尾插入元素，失败则抛出异常 remove() removeFirst() 获取并删除队首元素，失败则抛出异常 element() getFirst() 获取但不删除队首元素，失败则抛出异常 offer(e) offerLast(e) 向队尾插入元素，失败则返回false poll() pollFirst() 获取并删除队首元素，失败则返回null peek() peekFirst() 获取但不删除队首元素，失败则返回null Stack Method Equivalent Deque Method 说明 push(e) addFirst(e) 向栈顶插入元素，失败则抛出异常 无 offerFirst(e) 向栈顶插入元素，失败则返回false pop() removeFirst() 获取并删除栈顶元素，失败则抛出异常 无 pollFirst() 获取并删除栈顶元素，失败则返回null peek() peekFirst() 获取但不删除栈顶元素，失败则抛出异常 无 peekFirst() 获取但不删除栈顶元素，失败则返回null ArrayDeque和LinkedList是Deque的两个通用实现。 1)ArrayDeque（底层是循环数组，有界队列) head指向首端第一个有效元素，tail指向尾端第一个可以插入元素的空位。因为是循环数组，所以head不一定总等于0，tail也不一定总是比head大。成员变量 transient Object[] elements; // non-private to simplify nested class access transient int head; transient int tail; private static final int MIN_INITIAL_CAPACITY = 8; 构造方法 public ArrayDeque() { elements = new Object[16]; } public ArrayDeque(int numElements) { allocateElements(numElements); } /** * Allocates empty array to hold the given number of elements. * * @param numElements the number of elements to hold */ private void allocateElements(int numElements) { int initialCapacity = MIN_INITIAL_CAPACITY; // Find the best power of two to hold elements. // Tests \"= initialCapacity) { initialCapacity = numElements; initialCapacity |= (initialCapacity >>> 1); initialCapacity |= (initialCapacity >>> 2); initialCapacity |= (initialCapacity >>> 4); initialCapacity |= (initialCapacity >>> 8); initialCapacity |= (initialCapacity >>> 16); initialCapacity++; if (initialCapacity >>= 1;// Good luck allocating 2 ^ 30 elements } elements = new Object[initialCapacity]; } 扩容 /** * Doubles the capacity of this deque. Call only when full, i.e., * when head and tail have wrapped around to become equal. */ private void doubleCapacity() { assert head == tail; int p = head; int n = elements.length; int r = n - p; // number of elements to the right of p int newCapacity = n offer public boolean offer(E e) { return offerLast(e); } public boolean offerLast(E e) { addLast(e); return true; } public void addLast(E e) { if (e == null) throw new NullPointerException(); elements[tail] = e; if ( (tail = (tail + 1) & (elements.length - 1)) == head) doubleCapacity(); } poll public E poll() { return pollFirst(); } public E pollFirst() { int h = head; @SuppressWarnings(\"unchecked\") E result = (E) elements[h]; // Element is null if deque empty if (result == null) return null; elements[h] = null; // Must null out slot head = (h + 1) & (elements.length - 1); return result; } peek public E peek() { return peekFirst(); } public E peekFirst() { // elements[head] is null if deque empty return (E) elements[head]; } ConcurrentLinkedQueue（底层是链表，基于CAS的非阻塞队列，无界队列) ConcurrentLinkedQueue是一个基于链接节点的无界线程安全队列，它采用先进先出的规则对节点进行排序，当我们添加一个元素的时候，它会添加到队列的尾部，当我们获取一个元素时，它会返回队列头部的元素。它采用了“wait－free”算法（非阻塞)来实现。 1 . 使用 CAS 原子指令来处理对数据的并发访问，这是非阻塞算法得以实现的基础。 head/tail 并非总是指向队列的头 / 尾节点，也就是说允许队列处于不一致状态。 这个特性把入队 / 出队时，原本需要一起原子化执行的两个步骤分离开来，从而缩小了入队 / 出队时需要原子化更新值的范围到唯一变量。这是非阻塞算法得以实现的关键。 以批处理方式来更新head/tail，从整体上减少入队 / 出队操作的开销。 ConcurrentLinkedQueue的迭代器是弱一致性的，这在并发容器中是比较普遍的现象，主要是指在一个线程在遍历队列结点而另一个线程尝试对某个队列结点进行修改的话不会抛出ConcurrentModificationException，这也就造成在遍历某个尚未被修改的结点时，在next方法返回时可以看到该结点的修改，但在遍历后再对该结点修改时就看不到这种变化。 在入队时最后一个结点中的next域为null 队列中的所有未删除结点的item域不能为null且从head都可以在O(N)时间内遍历到 对于要删除的结点，不是将其引用直接置为空，而是将其的item域先置为null(迭代器在遍历是会跳过item为null的结点) 允许head和tail滞后更新，也就是上文提到的head/tail并非总是指向队列的头 / 尾节点（这主要是为了减少CAS指令执行的次数，但同时会增加volatile读的次数，但是这种消耗较小)。具体而言就是，当在队列中插入一个元素是，会检测tail和最后一个结点之间的距离是否在两个结点及以上(内部称之为hop)；而在出队时，对head的检测就是与队列的第一个结点的距离是否达到两个，有则将head指向第一个结点并将head原来指向的结点的next域指向自己，这样就能断开与队列的联系从而帮助GC head节点并不是总指向第一个结点，tail也并不是总指向最后一个节点。 源码过于复杂，可以先跳过。 成员变量 private transient volatile Node head; private transient volatile Node tail; 构造方法 public ConcurrentLinkedQueue() { head = tail = new Node(null); } Node#CAS操作 在obj的offset位置比较object field和期望的值，如果相同则更新。这个方法的操作应该是原子的，因此提供了一种不可中断的方式更新object field。 如果node的next值为cmp，则将其更新为val boolean casNext(Node cmp, Node val) { return UNSAFE.compareAndSwapObject(this, nextOffset, cmp, val); } boolean casItem(E cmp, E val) { return UNSAFE.compareAndSwapObject(this, itemOffset, cmp, val); } private boolean casHead(Node cmp, Node val) { return UNSAFE.compareAndSwapObject(this, headOffset, cmp, val); } void lazySetNext(Node val) { UNSAFE.putOrderedObject(this, nextOffset, val); } offer（无锁) /** * Inserts the specified element at the tail of this queue. * As the queue is unbounded, this method will never return {@code false}. * * @return {@code true} (as specified by {@link Queue#offer}) * @throws NullPointerException if the specified element is null */ public boolean offer(E e) { checkNotNull(e); final Node newNode = new Node(e); for (Node t = tail, p = t;;) { Node q = p.next; // q/p.next/tail.next为null，则说明p是尾节点，则插入 if (q == null) { // CAS插入 p.next = newNode，多线程环境下只有一个线程可以设置成功 // 此时 tail.next = newNode if (p.casNext(null, newNode)) { // CAS成功说明新节点已经放入链表 // 如果p不为t，说明当前线程是之前CAS失败后又重试CAS成功的，tail = newNode if (p != t) // hop two nodes at a time casTail(t, newNode); // Failure is OK. return true; } // Lost CAS race to another thread; re-read next } else if (p == q) //多线程操作时候，由于poll时候会把老的head变为自引用，然后head的next变为新head，所以这里需要重新找新的head，因为新的head后面的节点才是激活的节点 // p = head , t = tail p = (t != (t = tail)) ? t : head; else // 对上一次CAS失败的线程而言，t.next/p.next/tail.next/q 不是null了 // 副作用是p = q，p和q都指向了尾节点，进入第三次循环 p = (p != t && t != (t = tail)) ? t : q; } } poll（无锁) public E poll() { restartFromHead: for (;;) { for (Node h = head, p = h, q;;) { // 保存当前节点的值 E item = p.item; // 当前节点有值则CAS置为null, p.item = null if (item != null && p.casItem(item, null)) { // CAS成功代表当前节点已经从链表中移除 if (p != h) // hop two nodes at a time updateHead(h, ((q = p.next) != null) ? q : p); return item; } // 当前队列为空时则返回null else if ((q = p.next) == null) { updateHead(h, p); return null; } // 自引用了，则重新找新的队列头节点 else if (p == q) continue restartFromHead; else p = q; } } } final void updateHead(Node h, Node p) { if (h != p && casHead(h, p)) h.lazySetNext(h); } peek（无锁) public E peek() { restartFromHead: for (;;) { for (Node h = head, p = h, q;;) { E item = p.item; if (item != null || (q = p.next) == null) { updateHead(h, p); return item; } else if (p == q) continue restartFromHead; else p = q; } } } size（遍历计算大小，效率低) public int size() { int count = 0; for (Node p = first(); p != null; p = succ(p)) if (p.item != null) // Collection.size() spec says to max out if (++count == Integer.MAX_VALUE) break; return count; } ConcurrentLinkedDeque（底层是双向链表，基于CAS的非阻塞队列，无界队列) 2)PriorityQueue（底层是数组，逻辑上是小顶堆，无界队列) PriorityQueue底层实现的数据结构是“堆”，堆具有以下两个性质： 任意一个节点的值总是不大于（最大堆)或者不小于（最小堆)其父节点的值；堆是一棵完全二叉树 基于数组实现的二叉堆，对于数组中任意位置的n上元素，其左孩子在[2n+1]位置上，右孩子[2(n+1)]位置，它的父亲则在[(n-1)/2]上，而根的位置则是[0]。 1)时间复杂度：remove()方法和add()方法时间复杂度为O(logn)，remove(Object obj)和contains()方法需要O(n)时间复杂度，取队头则需要O(1)时间 2)在初始化阶段会执行建堆函数，最终建立的是最小堆，每次出队和入队操作不能保证队列元素的有序性，只能保证队头元素和新插入元素的有序性，如果需要有序输出队列中的元素，则只要调用Arrays.sort()方法即可 3)可以使用Iterator的迭代器方法输出队列中元素 4)PriorityQueue是非同步的，要实现同步需要调用java.util.concurrent包下的PriorityBlockingQueue类来实现同步 5)在队列中不允许使用null元素 6)PriorityQueue默认是一个小顶堆，然而可以通过传入自定义的Comparator函数来实现大顶堆 替代：用TreeMap复杂度太高，有没有更好的方法。hash方法，但是队列不是定长的，如果改变了大小要rehash代价太大，还有什么方法？用堆实现，那每次get put复杂度是多少（lgN) 成员变量 transient Object[] queue; // non-private to simplify nested class access /** * The number of elements in the priority queue. */ private int size = 0; /** * The comparator, or null if priority queue uses elements' * natural ordering. */ private final Comparator comparator; /** * The number of times this priority queue has been * structurally modified. See AbstractList for gory details. */ transient int modCount = 0; // non-private to simplify nested class access 构造方法 public PriorityQueue() { this(DEFAULT_INITIAL_CAPACITY, null); } /** * Creates a {@code PriorityQueue} with the specified initial * capacity that orders its elements according to their * {@linkplain Comparable natural ordering}. * * @param initialCapacity the initial capacity for this priority queue * @throws IllegalArgumentException if {@code initialCapacity} is less * than 1 */ public PriorityQueue(int initialCapacity) { this(initialCapacity, null); } /** * Creates a {@code PriorityQueue} with the default initial capacity and * whose elements are ordered according to the specified comparator. * * @param comparator the comparator that will be used to order this * priority queue. If {@code null}, the {@linkplain Comparable * natural ordering} of the elements will be used. * @since 1.8 */ public PriorityQueue(Comparator comparator) { this(DEFAULT_INITIAL_CAPACITY, comparator); } /** * Creates a {@code PriorityQueue} with the specified initial capacity * that orders its elements according to the specified comparator. * * @param initialCapacity the initial capacity for this priority queue * @param comparator the comparator that will be used to order this * priority queue. If {@code null}, the {@linkplain Comparable * natural ordering} of the elements will be used. * @throws IllegalArgumentException if {@code initialCapacity} is * less than 1 */ public PriorityQueue(int initialCapacity, Comparator comparator) { // Note: This restriction of at least one is not actually needed, // but continues for 1.5 compatibility if (initialCapacity 扩容 Double size if small; else grow by 50% private void grow(int minCapacity) { int oldCapacity = queue.length; // Double size if small; else grow by 50% int newCapacity = oldCapacity + ((oldCapacity > 1)); // overflow-conscious code if (newCapacity - MAX_ARRAY_SIZE > 0) newCapacity = hugeCapacity(minCapacity); queue = Arrays.copyOf(queue, newCapacity); } private static int hugeCapacity(int minCapacity) { if (minCapacity MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } offer public boolean offer(E e) { if (e == null) throw new NullPointerException(); modCount++; int i = size; if (i >= queue.length) grow(i + 1); size = i + 1; if (i == 0) queue[0] = e; else siftUp(i, e); return true; } private void siftUp(int k, E x) { if (comparator != null) siftUpUsingComparator(k, x); else siftUpComparable(k, x); } private void siftUpUsingComparator(int k, E x) { while (k > 0) { int parent = (k - 1) >>> 1; Object e = queue[parent]; if (comparator.compare(x, (E) e) >= 0) break; queue[k] = e; k = parent; } queue[k] = x; } private void siftUpComparable(int k, E x) { Comparable key = (Comparable) x; while (k > 0) { int parent = (k - 1) >>> 1; Object e = queue[parent]; if (key.compareTo((E) e) >= 0) break; queue[k] = e; k = parent; } queue[k] = key; } poll public E poll() { if (size == 0) return null; int s = --size; modCount++; E result = (E) queue[0]; E x = (E) queue[s]; queue[s] = null; if (s != 0) siftDown(0, x); return result; } private void siftDown(int k, E x) { if (comparator != null) siftDownUsingComparator(k, x); else siftDownComparable(k, x); } private void siftDownUsingComparator(int k, E x) { int half = size >>> 1; while (k 0) c = queue[child = right]; if (comparator.compare(x, (E) c) private void siftDownComparable(int k, E x) { Comparable key = (Comparable)x; int half = size >>> 1; // loop while a non-leaf while (k ) c).compareTo((E) queue[right]) > 0) c = queue[child = right]; if (key.compareTo((E) c) peek public E peek() { return (size == 0) ? null : (E) queue[0]; } 3)BlockingQueue 对于许多多线程问题，都可以通过使用一个或多个队列以优雅的方式将其形式化 生产者线程向队列插入元素，消费者线程则取出它们。使用队列，可以安全地从一个线程向另一个线程传递数据。 比如转账 一个线程将转账指令放入队列 一个线程从队列中取出指令执行转账，只有这个线程可以访问银行对象的内部。因此不需要同步 当试图向队列中添加元素而队列已满，或是想从队列移出元素而队列为空的时候，阻塞队列导致线程阻塞 在协调多个线程之间的合作时，阻塞队列是很有用的。 工作者线程可以周期性地将中间结果放入阻塞队列，其他工作者线程取出中间结果并进一步修改。队列会自动平衡负载，大概第一个线程集比第二个运行的慢，那么第二个线程集在等待结果时会阻塞，反之亦然 1)LinkedBlockingQueue的容量是没有上边界的，是一个双向队列 2)ArrayBlockingQueue在构造时需要指定容量，并且有一个参数来指定是否需要公平策略 3)PriorityBlockingQueue是一个带优先级的队列，元素按照它们的优先级顺序被移走。该队列没有容量上限。 4)DelayQueue包含实现了Delayed接口的对象 5)TransferQueue接口允许生产者线程等待，直到消费者准备就绪可以接收一个元素。如果生产者调用transfer方法，那么这个调用会阻塞，直到插入的元素被消费者取出之后才停止阻塞。 LinkedTransferQueue类实现了这个接口 ArrayBlockingQueue（底层是数组，阻塞队列，一把锁两个Condition，有界同步队列) 基于数组、先进先出、线程安全的集合类，特点是可实现指定时间的阻塞读写，并且容量是可限制的。成员变量 /** The queued items */ final Object[] items; /** items index for next take, poll, peek or remove */ int takeIndex; /** items index for next put, offer, or add */ int putIndex; /** Number of elements in the queue */ int count; /* * Concurrency control uses the classic two-condition algorithm * found in any textbook. */ /** Main lock guarding all access */ final ReentrantLock lock; /** Condition for waiting takes */ private final Condition notEmpty; /** Condition for waiting puts */ private final Condition notFull; /** * Shared state for currently active iterators, or null if there * are known not to be any. Allows queue operations to update * iterator state. */ transient Itrs itrs = null; 构造方法 public ArrayBlockingQueue(int capacity) { this(capacity, false); } /** * Creates an {@code ArrayBlockingQueue} with the given (fixed) * capacity and the specified access policy. * * @param capacity the capacity of this queue * @param fair if {@code true} then queue accesses for threads blocked * on insertion or removal, are processed in FIFO order; * if {@code false} the access order is unspecified. * @throws IllegalArgumentException if {@code capacity put（有锁，队列满则阻塞) public void put(E e) throws InterruptedException { checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { while (count == items.length) notFull.await(); enqueue(e); } finally { lock.unlock(); } } private void enqueue(E x) { // assert lock.getHoldCount() == 1; // assert items[putIndex] == null; final Object[] items = this.items; items[putIndex] = x; if (++putIndex == items.length) putIndex = 0; count++; notEmpty.signal(); } take（有锁，队列空则阻塞) public E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { while (count == 0) notEmpty.await(); return dequeue(); } finally { lock.unlock(); } } private E dequeue() { // assert lock.getHoldCount() == 1; // assert items[takeIndex] != null; final Object[] items = this.items; @SuppressWarnings(\"unchecked\") E x = (E) items[takeIndex]; items[takeIndex] = null; if (++takeIndex == items.length) takeIndex = 0; count--; if (itrs != null) itrs.elementDequeued(); notFull.signal(); return x; } offer（有锁，最多阻塞一段时间) public boolean offer(E e, long timeout, TimeUnit unit) throws InterruptedException { checkNotNull(e); long nanos = unit.toNanos(timeout); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { while (count == items.length) { if (nanos poll（有锁，最多阻塞一段时间) public E poll(long timeout, TimeUnit unit) throws InterruptedException { long nanos = unit.toNanos(timeout); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { while (count == 0) { if (nanos peek（有锁) public E peek() { final ReentrantLock lock = this.lock; lock.lock(); try { return itemAt(takeIndex); // null when queue is empty } finally { lock.unlock(); } final E itemAt(int i) { return (E) items[i]; } 遍历（构造迭代器加锁，遍历迭代器也加锁) public Iterator iterator() { return new Itr(); } private class Itr implements Iterator { /** Index to look for new nextItem; NONE at end */ private int cursor; /** Element to be returned by next call to next(); null if none */ private E nextItem; /** Index of nextItem; NONE if none, REMOVED if removed elsewhere */ private int nextIndex; /** Last element returned; null if none or not detached. */ private E lastItem; /** Index of lastItem, NONE if none, REMOVED if removed elsewhere */ private int lastRet; /** Previous value of takeIndex, or DETACHED when detached */ private int prevTakeIndex; /** Previous value of iters.cycles */ private int prevCycles; /** Special index value indicating \"not available\" or \"undefined\" */ private static final int NONE = -1; /** * Special index value indicating \"removed elsewhere\", that is, * removed by some operation other than a call to this.remove(). */ private static final int REMOVED = -2; /** Special value for prevTakeIndex indicating \"detached mode\" */ private static final int DETACHED = -3; Itr() { // assert lock.getHoldCount() == 0; lastRet = NONE; final ReentrantLock lock = ArrayBlockingQueue.this.lock; lock.lock(); try { if (count == 0) { // assert itrs == null; cursor = NONE; nextIndex = NONE; prevTakeIndex = DETACHED; } else { final int takeIndex = ArrayBlockingQueue.this.takeIndex; prevTakeIndex = takeIndex; nextItem = itemAt(nextIndex = takeIndex); cursor = incCursor(takeIndex); if (itrs == null) { itrs = new Itrs(this); } else { itrs.register(this); // in this order itrs.doSomeSweeping(false); } prevCycles = itrs.cycles; // assert takeIndex >= 0; // assert prevTakeIndex == takeIndex; // assert nextIndex >= 0; // assert nextItem != null; } } finally { lock.unlock(); } } } LinkedBlockingQueue（底层是链表，阻塞队列，两把锁，各自对应一个Condition，无界同步队列) 另一种BlockingQueue的实现，基于链表，没有容量限制。 由于出队只操作队头，入队只操作队尾，这里巧妙地使用了两把锁，对于put和offer入队操作使用一把锁，对于take和poll出队操作使用一把锁，避免了出队、入队时互相竞争锁的现象，因此LinkedBlockingQueue在高并发读写都多的情况下，性能会较ArrayBlockingQueue好很多，在遍历以及删除的情况下则要两把锁都要锁住。 多CPU情况下可以在同一时刻既消费又生产。成员变量 /** The capacity bound, or Integer.MAX_VALUE if none */ private final int capacity; /** Current number of elements */ private final AtomicInteger count = new AtomicInteger(); /** * Head of linked list. * Invariant: head.item == null */ transient Node head; /** * Tail of linked list. * Invariant: last.next == null */ private transient Node last; /** Lock held by take, poll, etc */ private final ReentrantLock takeLock = new ReentrantLock(); /** Wait queue for waiting takes */ private final Condition notEmpty = takeLock.newCondition(); /** Lock held by put, offer, etc */ private final ReentrantLock putLock = new ReentrantLock(); /** Wait queue for waiting puts */ private final Condition notFull = putLock.newCondition(); 构造方法 public LinkedBlockingQueue() { this(Integer.MAX_VALUE); } /** * Creates a {@code LinkedBlockingQueue} with the given (fixed) capacity. * * @param capacity the capacity of this queue * @throws IllegalArgumentException if {@code capacity} is not greater * than zero */ public LinkedBlockingQueue(int capacity) { if (capacity (null); } put（加putLock锁，队列满则阻塞) /** * Inserts the specified element at the tail of this queue, waiting if * necessary for space to become available. * * @throws InterruptedException {@inheritDoc} * @throws NullPointerException {@inheritDoc} */ public void put(E e) throws InterruptedException { if (e == null) throw new NullPointerException(); // Note: convention in all put/take/etc is to preset local var // holding count negative to indicate failure unless set. int c = -1; Node node = new Node(e); final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; putLock.lockInterruptibly(); try { /* * Note that count is used in wait guard even though it is * not protected by lock. This works because count can * only decrease at this point (all other puts are shut * out by lock), and we (or some other waiting put) are * signalled if it ever changes from capacity. Similarly * for all other uses of count in other wait guards. */ while (count.get() == capacity) { // 阻塞，直至有剩余空间 notFull.await(); } enqueue(node); c = count.getAndIncrement(); if (c + 1 // 还有剩余空间时，唤醒其他生产者 notFull.signal(); } finally { putLock.unlock(); } if (c == 0) // c是放入当前元素之前队列的容量，现在新添加一个元素，那么唤醒消费者进行消费 signalNotEmpty(); } private void enqueue(Node node) { // assert putLock.isHeldByCurrentThread(); // assert last.next == null; last = last.next = node; } private void signalNotEmpty() { final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try { // 唤醒消费线程 notEmpty.signal(); } finally { takeLock.unlock(); } } take（加takeLock锁，队列空则阻塞) public E take() throws InterruptedException { E x; int c = -1; final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly(); try { while (count.get() == 0) { - // 队列空则阻塞 notEmpty.await(); } x = dequeue(); c = count.getAndDecrement(); if (c > 1) // 还有元素则唤醒其他消费者 notEmpty.signal(); } finally { takeLock.unlock(); } if (c == capacity) // c是消费当前元素之前队列的容量，现在的容量是c-1，可以继续放入元素，唤醒生产者进行生产 signalNotFull(); return x; } private E dequeue() { // assert takeLock.isHeldByCurrentThread(); // assert head.item == null; Node h = head; Node first = h.next; h.next = h; // help GC head = first; E x = first.item; first.item = null; return x; } private void signalNotFull() { final ReentrantLock putLock = this.putLock; putLock.lock(); try { // 唤醒生产者 notFull.signal(); } finally { putLock.unlock(); } } peek（加takeLock锁) public E peek() { if (count.get() == 0) return null; final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try { Node first = head.next; if (first == null) return null; else return first.item; } finally { takeLock.unlock(); } } remove（加两把锁) /** * Locks to prevent both puts and takes. */ void fullyLock() { putLock.lock(); takeLock.lock(); } /** * Unlocks to allow both puts and takes. */ void fullyUnlock() { takeLock.unlock(); putLock.unlock(); } public boolean remove(Object o) { if (o == null) return false; fullyLock(); try { for (Node trail = head, p = trail.next; p != null; trail = p, p = p.next) { if (o.equals(p.item)) { unlink(p, trail); return true; } } return false; } finally { fullyUnlock(); } } 遍历（加两把锁) public Iterator iterator() { return new Itr(); } private class Itr implements Iterator { /* * Basic weakly-consistent iterator. At all times hold the next * item to hand out so that if hasNext() reports true, we will * still have it to return even if lost race with a take etc. */ private Node current; private Node lastRet; private E currentElement; Itr() { fullyLock(); try { current = head.next; if (current != null) currentElement = current.item; } finally { fullyUnlock(); } } public boolean hasNext() { return current != null; } /** * Returns the next live successor of p, or null if no such. * * Unlike other traversal methods, iterators need to handle both: * - dequeued nodes (p.next == p) * - (possibly multiple) interior removed nodes (p.item == null) */ private Node nextNode(Node p) { for (;;) { Node s = p.next; if (s == p) return head.next; if (s == null || s.item != null) return s; p = s; } } public E next() { fullyLock(); try { if (current == null) throw new NoSuchElementException(); E x = currentElement; lastRet = current; current = nextNode(current); currentElement = (current == null) ? null : current.item; return x; } finally { fullyUnlock(); } } public void remove() { if (lastRet == null) throw new IllegalStateException(); fullyLock(); try { Node node = lastRet; lastRet = null; for (Node trail = head, p = trail.next; p != null; trail = p, p = p.next) { if (p == node) { unlink(p, trail); break; } } } finally { fullyUnlock(); } } } LinkedBlockingDeque（底层是双向链表，阻塞队列，一把锁两个Condition，无界同步队列) LinkedBlockingDeque是一个基于链表的双端阻塞队列。和LinkedBlockingQueue类似，区别在于该类实现了Deque接口，而LinkedBlockingQueue实现了Queue接口。 LinkedBlockingDeque内部只有一把锁以及该锁上关联的两个条件，所以可以推断同一时刻只有一个线程可以在队头或者队尾执行入队或出队操作（类似于ArrayBlockingQueue)。可以发现这点和LinkedBlockingQueue不同，LinkedBlockingQueue可以同时有两个线程在两端执行操作。 LinkedBlockingDeque和LinkedBlockingQueue的相同点在于： 基于链表 容量可选，不设置的话，就是Int的最大值 和LinkedBlockingQueue的不同点在于： 双端链表和单链表 不存在哨兵节点 一把锁+两个条件 LinkedBlockingDeque和ArrayBlockingQueue的相同点在于：使用一把锁+两个条件维持队列的同步。 PriorityBlockingQueue（底层是数组，出队时队空则阻塞；无界队列，不存在队满情况，一把锁一个Condition) 支持优先级的无界阻塞队列。默认情况下元素采用自然顺序升序排序，当然我们也可以通过构造函数来指定Comparator来对元素进行排序。需要注意的是PriorityBlockingQueue不能保证同优先级元素的顺序。成员变量 private static final int DEFAULT_INITIAL_CAPACITY = 11; /** * The maximum size of array to allocate. * Some VMs reserve some header words in an array. * Attempts to allocate larger arrays may result in * OutOfMemoryError: Requested array size exceeds VM limit */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * Priority queue represented as a balanced binary heap: the two * children of queue[n] are queue[2*n+1] and queue[2*(n+1)]. The * priority queue is ordered by comparator, or by the elements' * natural ordering, if comparator is null: For each node n in the * heap and each descendant d of n, n comparator; /** * Lock used for all public operations */ private final ReentrantLock lock; /** * Condition for blocking when empty */ private final Condition notEmpty; /** * Spinlock for allocation, acquired via CAS. */ private transient volatile int allocationSpinLock; /** * A plain PriorityQueue used only for serialization, * to maintain compatibility with previous versions * of this class. Non-null only during serialization/deserialization. */ private PriorityQueue q; 构造方法 public PriorityBlockingQueue() { this(DEFAULT_INITIAL_CAPACITY, null); } /** * Creates a {@code PriorityBlockingQueue} with the specified * initial capacity that orders its elements according to their * {@linkplain Comparable natural ordering}. * * @param initialCapacity the initial capacity for this priority queue * @throws IllegalArgumentException if {@code initialCapacity} is less * than 1 */ public PriorityBlockingQueue(int initialCapacity) { this(initialCapacity, null); } /** * Creates a {@code PriorityBlockingQueue} with the specified initial * capacity that orders its elements according to the specified * comparator. * * @param initialCapacity the initial capacity for this priority queue * @param comparator the comparator that will be used to order this * priority queue. If {@code null}, the {@linkplain Comparable * natural ordering} of the elements will be used. * @throws IllegalArgumentException if {@code initialCapacity} is less * than 1 */ public PriorityBlockingQueue(int initialCapacity, Comparator comparator) { if (initialCapacity 扩容（基于CAS+Lock，CAS控制创建新的数组原子执行，Lock控制数组替换原子执行) private void tryGrow(Object[] array, int oldCap) { lock.unlock(); // must release and then re-acquire main lock Object[] newArray = null; if (allocationSpinLock == 0 && UNSAFE.compareAndSwapInt(this, allocationSpinLockOffset, 0, 1)) { try { int newCap = oldCap + ((oldCap > 1)); if (newCap - MAX_ARRAY_SIZE > 0) { // possible overflow int minCap = oldCap + 1; if (minCap MAX_ARRAY_SIZE) throw new OutOfMemoryError(); newCap = MAX_ARRAY_SIZE; } if (newCap > oldCap && queue == array) newArray = new Object[newCap]; } finally { allocationSpinLock = 0; } } if (newArray == null) // back off if another thread is allocating Thread.yield(); lock.lock(); if (newArray != null && queue == array) { queue = newArray; System.arraycopy(array, 0, newArray, 0, oldCap); } } put（有锁) public void put(E e) { offer(e); // never need to block } public boolean offer(E e) { if (e == null) throw new NullPointerException(); final ReentrantLock lock = this.lock; lock.lock(); int n, cap; Object[] array; while ((n = size) >= (cap = (array = queue).length)) tryGrow(array, cap); try { Comparator cmp = comparator; if (cmp == null) siftUpComparable(n, e, array); else siftUpUsingComparator(n, e, array, cmp); size = n + 1; notEmpty.signal(); } finally { lock.unlock(); } return true; } take（有锁) public E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly(); E result; try { while ( (result = dequeue()) == null) notEmpty.await(); } finally { lock.unlock(); } return result; } private E dequeue() { int n = size - 1; if (n cmp = comparator; if (cmp == null) siftDownComparable(0, x, array, n); else siftDownUsingComparator(0, x, array, n, cmp); size = n; return result; } } peek（有锁) public E peek() { final ReentrantLock lock = this.lock; lock.lock(); try { return (size == 0) ? null : (E) queue[0]; } finally { lock.unlock(); } } DelayQueue（底层是PriorityQueue，无界阻塞队列，过期元素方可移除，基于Lock) public class DelayQueue extends AbstractQueue implements BlockingQueue { private final transient ReentrantLock lock = new ReentrantLock(); private final PriorityQueue q = new PriorityQueue(); DelayQueue队列中每个元素都有个过期时间，并且队列是个优先级队列，当从队列获取元素时候，只有过期元素才会出队列。 每个元素都必须实现Delayed接口 public interface Delayed extends Comparable { /** * Returns the remaining delay associated with this object, in the * given time unit. * * @param unit the time unit * @return the remaining delay; zero or negative values indicate * that the delay has already elapsed */ long getDelay(TimeUnit unit); } getDelay方法返回对象的残留延迟，负值表示延迟结束 元素只有在延迟用完的时候才能从DelayQueue移出。还必须实现Comparable接口。 一个典型场景是重试机制的实现，比如当调用接口失败后，把当前调用信息放入delay=10s的元素，然后把元素放入队列，那么这个队列就是一个重试队列，一个线程通过take方法获取需要重试的接口，take返回则接口进行重试，失败则再次放入队列，同时也可以在元素加上重试次数。 成员变量 private final transient ReentrantLock lock = new ReentrantLock(); private final PriorityQueue q = new PriorityQueue(); private Thread leader = null; private final Condition available = lock.newCondition(); 构造方法 public DelayQueue() {} put public void put(E e) { offer(e); } public boolean offer(E e) { final ReentrantLock lock = this.lock; lock.lock(); try { q.offer(e); if (q.peek() == e) { leader = null; // 通知最先等待的线程 available.signal(); } return true; } finally { lock.unlock(); } } take 获取并移除队列首元素，如果队列没有过期元素则等待。 第一次调用take时候由于队列空，所以调用（2)把当前线程放入available的条件队列等待，当执行offer并且添加的元素就是队首元素时候就会通知最先等待的线程激活，循环重新获取队首元素，这时候first假如不空，则调用getdelay方法看该元素海剩下多少时间就过期了，如果delay （6)说明当前take返回了元素，如果当前队列还有元素则调用singal激活条件队列里面可能有的等待线程。leader那么为null，那么是第一次调用take获取过期元素的线程，第一次调用的线程调用设置等待时间的await方法等待数据过期，后面调用take的线程则调用await直到signal。 public E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { for (;;) { - // 1)获取但不移除队首元素 E first = q.peek(); if (first == null) - // 2)无元素，则阻塞 available.await(); else { long delay = first.getDelay(NANOSECONDS); - // 3)有元素，且已经过期，则移除 if (delay // 继续阻塞延迟的时间 available.awaitNanos(delay); } finally { if (leader == thisThread) leader = null; } } } } } finally { if (leader == null && q.peek() != null) available.signal(); lock.unlock(); } } peek SynchronousQueue（只存储一个元素，阻塞队列，基于CAS) 实现了BlockingQueue，是一个阻塞队列。 一个只存储一个元素的的阻塞队列，每个插入操作必须等到另一个线程调用移除操作，否则插入一直处于阻塞状态，吞吐量高于LinkedBlockingQueue。 SynchronousQueue内部并没有数据缓存空间，你不能调用peek()方法来看队列中是否有数据元素，因为数据元素只有当你试着取走的时候才可能存在，不取走而只想偷窥一下是不行的，当然遍历这个队列的操作也是不允许的。队列头元素是第一个排队要插入数据的线程，而不是要交换的数据。数据是在配对的生产者和消费者线程之间直接传递的，并不会将数据缓冲数据到队列中。可以这样来理解：生产者和消费者互相等待对方，握手，然后一起离开。 // 如果为 true，则等待线程以 FIFO 的顺序竞争访问；否则顺序是未指定的。 // SynchronousQueue sc =new SynchronousQueue<>(true);//fair - SynchronousQueue sc = new SynchronousQueue<>(); // 默认不指定的话是false，不公平的 4)TransferQueue（特殊的BlockingQueue) 生产者会一直阻塞直到所添加到队列的元素被某一个消费者所消费（不仅仅是添加到队列里就完事) 当我们不想生产者过度生产消息时，TransferQueue可能非常有用，可避免发生OutOfMemory错误。在这样的设计中，消费者的消费能力将决定生产者产生消息的速度。 public interface TransferQueue extends BlockingQueue { /** 立即转交一个元素给消费者，如果此时队列没有消费者，那就false */ boolean tryTransfer(E e); /** 转交一个元素给消费者，如果此时队列没有消费者，那就阻塞 */ void transfer(E e) throws InterruptedException; /** 带超时的tryTransfer */ boolean tryTransfer(E e, long timeout, TimeUnit unit) throws InterruptedException; /** 是否有消费者等待接收数据，瞬时状态，不一定准 */ boolean hasWaitingConsumer(); /** 返回还有多少个等待的消费者，跟上面那个一样，都是一种瞬时状态，不一定准 */ int getWaitingConsumerCount(); } LinkedTransferQueue（底层是链表，阻塞队列，无界同步队列) LinkedTransferQueue实现了TransferQueue接口，这个接口继承了BlockingQueue。之前BlockingQueue是队列满时再入队会阻塞，而这个接口实现的功能是队列不满时也可以阻塞，实现一种有阻塞的入队功能。 LinkedTransferQueue实际上是ConcurrentLinkedQueue、SynchronousQueue（公平模式)和LinkedBlockingQueue的超集。而且LinkedTransferQueue更好用，因为它不仅仅综合了这几个类的功能，同时也提供了更高效的实现。 5)Queue实现类之间的区别 非线程安全的：ArrayDeque、LinkedList、PriorityQueue 线程安全的：ConcurrentLinkedQueue、ConcurrentLinkedDeque、ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue 线程安全的又分为阻塞队列和非阻塞队列，阻塞队列提供了put、take等会阻塞当前线程的方法，比如ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue，也有offer、poll等阻塞一段时间候返回的方法； 非阻塞队列是使用CAS机制保证offer、poll等可以线程安全地入队出队，并且不需要加锁，不会阻塞当前线程，比如ConcurrentLinkedQueue、ConcurrentLinkedDeque。 ArrayBlockingQueue和LinkedBlockingQueue 区别 队列中锁的实现不同 ArrayBlockingQueue实现的队列中的锁是没有分离的，即生产和消费用的是同一个锁； LinkedBlockingQueue实现的队列中的锁是分离的，即生产用的是putLock，消费是takeLock 底层实现不同 前者基于数组，后者基于链表 队列边界不同 ArrayBlockingQueue实现的队列中必须指定队列的大小，是有界队列 LinkedBlockingQueue实现的队列中可以不指定队列的大小，但是默认是Integer.MAX_VALUE，是无界队列 3.5 Map HashMap（底层是数组+链表/红黑树，无序键值对集合，非线程安全) 基于哈希表实现，链地址法。 loadFactor默认为0.75，threshold（阈)为12，并创建一个大小为16的Entry数组。 在遍历时是无序的，如需有序，建议使用TreeMap。 采用数组方式存储key、value构成的Entry对象，无容量限制。 基于key hash寻找Entry对象存放在数组中的位置，对于hash冲突采用链表/红黑树的方式来解决。 HashMap在插入元素时可能会扩大数组的容量，在扩大容量时需要重新计算hash，并复制对象到新的数组中。 是非线程安全的。 // 1. 哈希冲突时采用链表法的类，一个哈希桶多于8个元素改为TreeNode static class Node implements Map.Entry // 2. 哈希冲突时采用红黑树存储的类，一个哈希桶少于6个元素改为Node static final class TreeNode extends LinkedHashMap.Entry 某个桶对应的链表过长的话搜索效率低，改为红黑树效率会提高。 为何按位与而不是取摸 hashmap的iterator读取时是否会读到另一个线程put的数据 红黑树；hashmap报ConcurrentModificationException的情况 Hash冲突中链表结构的数量大于8个，则调用树化转为红黑树结构，红黑树查找稍微快些；红黑树结构的数量小于6个时，则转为链表结构 如果加载因子越大，对空间的利用更充分，但是查找效率会降低（链表长度会越来越长)；如果加载因子太小，那么表中的数据将过于稀疏（很多空间还没用，就开始扩容了)，对空间造成严重浪费。如果我们在构造方法中不指定，则系统默认加载因子为0.75，这是一个比较理想的值，一般情况下我们是无需修改的。 一般对哈希表的散列很自然地会想到用hash值对length取模（即除法散列法)，Hashtable中也是这样实现的，这种方法基本能保证元素在哈希表中散列的比较均匀，但取模会用到除法运算，效率很低，HashMap中则通过h&(length-1)的方法来代替取模，同样实现了均匀的散列，但效率要高很多，这也是HashMap对Hashtable的一个改进。 哈希表的容量一定要是2的整数次幂。首先，length为2的整数次幂的话，h&(length-1)就相当于对length取模，这样便保证了散列的均匀，同时也提升了效率；其次，length为2的整数次幂的话，为偶数，这样length-1为奇数，奇数的最后一位是1，这样便保证了h&(length-1)的最后一位可能为0，也可能为1（这取决于h的值)，即与后的结果可能为偶数，也可能为奇数，这样便可以保证散列的均匀性，而如果length为奇数的话，很明显length-1为偶数，它的最后一位是0，这样h&(length-1)的最后一位肯定为0，即只能为偶数，这样任何hash值都只会被散列到数组的偶数下标位置上，这便浪费了近一半的空间，因此，length取2的整数次幂，是为了使不同hash值发生碰撞的概率较小，这样就能使元素在哈希表中均匀地散列。 Map#Entry（接口) interface Entry { K getKey(); V getValue(); V setValue(V value); boolean equals(Object o); int hashCode(); public static , V> Comparator> comparingByKey() { return (Comparator> & Serializable) (c1, c2) -> c1.getKey().compareTo(c2.getKey()); } public static > Comparator> comparingByValue() { return (Comparator> & Serializable) (c1, c2) -> c1.getValue().compareTo(c2.getValue()); } public static Comparator> comparingByKey(Comparator cmp) { Objects.requireNonNull(cmp); return (Comparator> & Serializable) (c1, c2) -> cmp.compare(c1.getKey(), c2.getKey()); } public static Comparator> comparingByValue(Comparator cmp) { Objects.requireNonNull(cmp); return (Comparator> & Serializable) (c1, c2) -> cmp.compare(c1.getValue(), c2.getValue()); } } HashMap#Node（Map.Entry的实现，链表的基本元素) static class Node implements Map.Entry { final int hash; final K key; V value; Node next; Node(int hash, K key, V value, Node next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + \"=\" + value; } public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry e = (Map.Entry)o; if (Objects.equals(key, e.getKey()) && Objects.equals(value, e.getValue())) return true; } return false; } } HashMap#TreeNode（Map.Entry的实现，红黑树的基本元素) static final class TreeNode extends LinkedHashMap.Entry { TreeNode parent; // red-black tree links TreeNode left; TreeNode right; TreeNode prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node next) { super(hash, key, val, next); } //... } LinkedHashMap#Entry static class Entry extends HashMap.Node { Entry before, after; Entry(int hash, K key, V value, Node next) { super(hash, key, value, next); } } 成员变量 /** * The default initial capacity - MUST be a power of two. */ static final int DEFAULT_INITIAL_CAPACITY = 1 /** * The table, initialized on first use, and resized as * necessary. When allocated, length is always a power of two. * (We also tolerate length zero in some operations to allow * bootstrapping mechanics that are currently not needed.) */ transient Node[] table; /** * Holds cached entrySet(). Note that AbstractMap fields are used * for keySet() and values(). */ transient Set> entrySet; /** * The number of key-value mappings contained in this map. */ transient int size; /** * The number of times this HashMap has been structurally modified * Structural modifications are those that change the number of mappings in * the HashMap or otherwise modify its internal structure (e.g., * rehash). This field is used to make iterators on Collection-views of * the HashMap fail-fast. (See ConcurrentModificationException). */ transient int modCount; /** * The next size value at which to resize (capacity * load factor). * * @serial */ // (The javadoc description is true upon serialization. // Additionally, if the table array has not been allocated, this // field holds the initial array capacity, or zero signifying // DEFAULT_INITIAL_CAPACITY.) // HashMap的阈值，用于判断是否需要调整HashMap的容量（threshold = 容量*装载因子) int threshold; /** * The load factor for the hash table. * * @serial */ final float loadFactor; AbstractMap transient Set keySet; transient Collection values; 构造方法 注意哪怕是指定了初始容量，也不会直接初始化table，而是在第一次put时调用resize来初始化table，resize里会将threshold视为初始容量。 public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor // 阈值为不小于容量的2的幂次 this.threshold = tableSizeFor(initialCapacity); } public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } /** * Constructs an empty HashMap with the default initial capacity * (16) and the default load factor (0.75). */ public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted } tableSizeFor（找到大于等于initialCapacity的最小的2的幂次以及原因) /** * Returns a power of two size for the given target capacity. */ static final int tableSizeFor(int cap) { int n = cap - 1; n |= n >>> 1; n |= n >>> 2; n |= n >>> 4; n |= n >>> 8; n |= n >>> 16; return (n = MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } hash（hash算法，算法比较高效、均匀) static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); } key的hash值高16位不变，低16位与高16位异或作为key的最终hash值。（h >>> 16，表示无符号右移16位，高位补0，任何数跟0异或都是其本身，因此key的hash值高16位不变。) 保证了对象的hashCode的高16位的变化能反应到低16位中， hash to index 如何根据hash值计算index？（put和get中的代码) n = table.length; index = (n-1)& hash; 当n总是2的n次方时，hash & (n-1)运算等价于h%n，但是&比%具有更高的效率。 put public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } // onlyIfAbsent如果为true，只有在hashmap没有该key的时候才添加 // evict如果为false，hashmap为创建模式；只有在使用Map集合作为构造器创建LinkedHashMap或HashMap时才会为false。 // 这两个参数均为实现java8的新接口而设置 Node newNode(int hash, K key, V value, Node next) { return new Node<>(hash, key, value, next); } final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node[] tab; // table Node p; // node pointer int n, i; // n 为length, i 为 node index if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // index处没有元素，则直接放入新节点 if ((p = tab[i = (n - 1) & hash]) == null) tab[i] = newNode(hash, key, value, null); else { // index处有元素 Node e; K k; if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k)))) // 假如key是相同的，那么替换value即可 e = p; else if (p instanceof TreeNode) // key不同，但如果p是红黑树根节点，那么将新节点放入红黑树 e = ((TreeNode)p).putTreeVal(this, tab, hash, key, value); else { // key不同，但如果p是链表头节点，那么判断链表中是否有该节点，如没有，则将新节点插入到链表尾部 for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); // 插入后如果发现已经链表长度已经适合转为红黑树了，则转换 if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } // 链表中某元素key和key相同，则替换value即可 if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size > threshold) resize(); afterNodeInsertion(evict); return null; } 扩容 resize // 扩容函数，如果hash桶为空，初始化默认大小，否则双倍扩容 // 注意！！因为扩容为2的倍数，根据hash桶的计算方法，元素哈希值不变 // 所以元素在新的hash桶的下标，要不跟旧的hash桶下标一致，要不增加1倍。 cap：capacity thr：threshold final Node[] resize() { Node[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap > 0) { if (oldCap >= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } else if ((newCap = oldCap oldCap >= DEFAULT_INITIAL_CAPACITY) newThr = oldThr } else if (oldThr > 0) // initial capacity was placed in threshold newCap = oldThr; else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR DEFAULT_INITIAL_CAPACITY); } if (newThr == 0) { float ft = (float)newCap loadFactor; newThr = (newCap (int)ft : Integer.MAX_VALUE); } threshold = newThr; Node[] newTab = (Node[])new Node[newCap]; table = newTab; if (oldTab != null) { for (int j = 0; j Node e; if ((e = oldTab[j]) != null) { // j位置原本元素存在 oldTab[j] = null; if (e.next == null) // 如果该位置没有形成链表，则再次计算index，放入新table // 假设扩容前的table大小为2的N次方，有上述put方法解析可知，元素的table索引为其hash值的后N位确定 那么扩容后的table大小即为2的N+1次方，则其中元素的table索引为其hash值的后N+1位确定，比原来多了一位 因此，table中的元素只有两种情况： 元素hash值第N+1位为0：不需要进行位置调整 元素hash值第N+1位为1：调整至原索引的两倍位置 newTab[e.hash & (newCap - 1)] = e; else if (e instanceof TreeNode) // 如果该位置形成了红黑树，则split ((TreeNode)e).split(this, newTab, j, oldCap); else { // preserve order // 如果该位置形成了链表，则分成两个链表，分别放在0~oldCap,oldCap~oldCap*2位置处 Node loHead = null, loTail = null; Node hiHead = null, hiTail = null; Node next; do { next = e.next; // 用于确定元素hash值第N+1位是否为0： 若为0，则使用loHead与loTail，将元素移至新table的原索引处 若不为0，则使用hiHead与hiHead，将元素移至新table的两倍索引处 if ((e.hash & oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); if (loTail != null) { loTail.next = null; newTab[j] = loHead; } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; }get（O(logn)) public V get(Object key) { Node e; return (e = getNode(hash(key), key)) == null ? null : e.value; } - final Node getNode(int hash, Object key) { Node[] tab; Node first, e; int n; K k; if ((tab = table) != null && (n = tab.length) > 0 && (first = tab[(n - 1) & hash]) != null) { // table不为空，且hash对应index元素不为空 // 如果index位置就是我们要找的key，则直接返回 if (first.hash == hash && // always check first node ((k = first.key) == key || (key != null && key.equals(k)))) return first; // 如果不是，则从链表或红黑树的角度继续找 if ((e = first.next) != null) { if (first instanceof TreeNode) return ((TreeNode)first).getTreeNode(hash, key); do { if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; }remove public V remove(Object key) { Node e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value; } value=null,matchValue=false,movable=true final Node removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) { Node[] tab; Node p; int n, index; if ((tab = table) != null && (n = tab.length) > 0 && (p = tab[index = (n - 1) & hash]) != null) { Node node = null, e; K k; V v; // 1) 如果hash 对应index即为我们要找的key，则找到 if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k)))) node = p; // 2) 从链表或红黑树的角度继续找 else if ((e = p.next) != null) { if (p instanceof TreeNode) node = ((TreeNode)p).getTreeNode(hash, key); else { do { if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) { node = e; break; } p = e; } while ((e = e.next) != null); } } // 找到后，根据找到的位置不同 相应地进行删除 if (node != null && (!matchValue || (v = node.value) == value || (value != null && value.equals(v)))) { if (node instanceof TreeNode) ((TreeNode)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; } } return null; } containsKey public boolean containsKey(Object key) { return getNode(hash(key), key) != null; } containsValue public boolean containsValue(Object value) { Node[] tab; V v; if ((tab = table) != null && size > 0) { for (int i = 0; i e = tab[i]; e != null; e = e.next) { if ((v = e.value) == value || (value != null && value.equals(v))) return true; } } } return false; } a)链表转红黑树 treeifyBin /** * Replaces all linked nodes in bin at index for given hash unless * table is too small, in which case resizes instead. */ final void treeifyBin(Node[] tab, int hash) { int n, index; Node e; if (tab == null || (n = tab.length) hd = null, tl = null; do { TreeNode p = replacementTreeNode(e, null); if (tl == null) hd = p; else { p.prev = tl; tl.next = p; } tl = p; } while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); } } b)红黑树转链表 TreeNode#untreeify final Node untreeify(HashMap map) { Node hd = null, tl = null; for (Node q = this; q != null; q = q.next) { Node p = map.replacementNode(q, null); if (tl == null) hd = p; else tl.next = p; tl = p; } return hd; } c)红黑树 查找 final TreeNode getTreeNode(int h, Object k) { return ((parent != null) ? root() : this).find(h, k, null); } /** * Finds the node starting at root p with the given hash and key. * The kc argument caches comparableClassFor(key) upon first use * comparing keys. */ final TreeNode find(int h, Object k, Class kc) { TreeNode p = this; do { int ph, dir; K pk; TreeNode pl = p.left, pr = p.right, q; if ((ph = p.hash) > h) p = pl; else if (ph d)红黑树 添加 final TreeNode putTreeVal(HashMap map, Node[] tab, int h, K k, V v) { Class kc = null; boolean searched = false; TreeNode root = (parent != null) ? root() : this; for (TreeNode p = root;;) { int dir, ph; K pk; if ((ph = p.hash) > h) dir = -1; else if (ph q, ch; searched = true; if (((ch = p.left) != null && (q = ch.find(h, k, kc)) != null) || ((ch = p.right) != null && (q = ch.find(h, k, kc)) != null)) return q; } dir = tieBreakOrder(k, pk); } TreeNode xp = p; if ((p = (dir xpn = xp.next; TreeNode x = map.newTreeNode(h, k, v, xpn); if (dir )xpn).prev = x; moveRootToFront(tab, balanceInsertion(root, x)); return null; } } } e)红黑树 删除 /** * Removes the given node, that must be present before this call. * This is messier than typical red-black deletion code because we * cannot swap the contents of an interior node with a leaf * successor that is pinned by \"next\" pointers that are accessible * independently during traversal. So instead we swap the tree * linkages. If the current tree appears to have too few nodes, * the bin is converted back to a plain bin. (The test triggers * somewhere between 2 and 6 nodes, depending on tree structure). */ final void removeTreeNode(HashMap map, Node[] tab, boolean movable) { int n; if (tab == null || (n = tab.length) == 0) return; int index = (n - 1) & hash; TreeNode first = (TreeNode)tab[index], root = first, rl; TreeNode succ = (TreeNode)next, pred = prev; if (pred == null) tab[index] = first = succ; else pred.next = succ; if (succ != null) succ.prev = pred; if (first == null) return; if (root.parent != null) root = root.root(); if (root == null || root.right == null || (rl = root.left) == null || rl.left == null) { tab[index] = first.untreeify(map); // too small return; } TreeNode p = this, pl = left, pr = right, replacement; if (pl != null && pr != null) { TreeNode s = pr, sl; while ((sl = s.left) != null) // find successor s = sl; boolean c = s.red; s.red = p.red; p.red = c; // swap colors TreeNode sr = s.right; TreeNode pp = p.parent; if (s == pr) { // p was s's direct parent p.parent = s; s.right = p; } else { TreeNode sp = s.parent; if ((p.parent = sp) != null) { if (s == sp.left) sp.left = p; else sp.right = p; } if ((s.right = pr) != null) pr.parent = s; } p.left = null; if ((p.right = sr) != null) sr.parent = p; if ((s.left = pl) != null) pl.parent = s; if ((s.parent = pp) == null) root = s; else if (p == pp.left) pp.left = s; else pp.right = s; if (sr != null) replacement = sr; else replacement = p; } else if (pl != null) replacement = pl; else if (pr != null) replacement = pr; else replacement = p; if (replacement != p) { TreeNode pp = replacement.parent = p.parent; if (pp == null) root = replacement; else if (p == pp.left) pp.left = replacement; else pp.right = replacement; p.left = p.right = p.parent = null; } TreeNode r = p.red ? root : balanceDeletion(root, replacement); if (replacement == p) { // detach TreeNode pp = p.parent; p.parent = null; if (pp != null) { if (p == pp.left) pp.left = null; else if (p == pp.right) pp.right = null; } } if (movable) moveRootToFront(tab, r); } f)红黑树 遍历 使用next指针，类似链表方式，便可遍历红黑树。 遍历（先迭代table，再迭代bucket->链表/红黑树) keySet keySet().iterator() public Set keySet() { Set ks = keySet; if (ks == null) { ks = new KeySet(); keySet = ks; } return ks; } final class KeySet extends AbstractSet { public final Iterator iterator() { return new KeyIterator(); } } KeyIterator实现了Iterator接口，并继承了HashIterator。前者仅适用于KeySet的迭代，后者适合所有基于HashMap的迭代。 HashMap#HashIterator abstract class HashIterator { Node next; // next entry to return Node current; // current entry int expectedModCount; // for fast-fail int index; // current slot HashIterator() { expectedModCount = modCount; Node[] t = table; current = next = null; index = 0; if (t != null && size > 0) { // advance to first entry do {} while (index nextNode() { Node[] t; Node e = next; if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (e == null) throw new NoSuchElementException(); // next的next为空的话，则继续遍历table，否则就返回next的next（链表或红黑树的下一个节点) if ((next = (current = e).next) == null && (t = table) != null) { do {} while (index p = current; if (p == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); current = null; K key = p.key; removeNode(hash(key), key, null, false, false); expectedModCount = modCount; } } HashMap#KeyIterator final class KeyIterator extends HashIterator implements Iterator { public final K next() { return nextNode().key; } } entrySet public Set> entrySet() { Set> es; return (es = entrySet) == null ? (entrySet = new EntrySet()) : es; } 使用的是该迭代器： final class EntryIterator extends HashIterator implements Iterator> { public final Map.Entry next() { return nextNode(); } } 多线程环境下的问题 1.8中hashmap的确不会因为多线程put导致死循环（1.7代码中会这样子)，但是依然有其他的弊端，比如数据丢失等等。因此多线程情况下还是建议使用ConcurrentHashMap。 数据丢失：当多线程put的时候，当index相同而又同时达到链表的末尾时，另一个线程put的数据会把之前线程put的数据覆盖掉，就会产生数据丢失。 if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); } Hashtable Hashtable同样是基于哈希表实现的，同样每个元素是一个key-value对，其内部也是通过单链表解决冲突问题，容量不足（超过了阈值)时，同样会自动增长。 Hashtable也是JDK1.0引入的类，是线程安全的，能用于多线程环境中。 Hashtable同样实现了Serializable接口，它支持序列化，实现了Cloneable接口，能被克隆。 Hashtable#Entry private static class Entry implements Map.Entry { final int hash; final K key; V value; Entry next; protected Entry(int hash, K key, V value, Entry next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } @SuppressWarnings(\"unchecked\") protected Object clone() { return new Entry<>(hash, key, value, (next==null ? null : (Entry) next.clone())); } // Map.Entry Ops public K getKey() { return key; } public V getValue() { return value; } public V setValue(V value) { if (value == null) throw new NullPointerException(); V oldValue = this.value; this.value = value; return oldValue; } public boolean equals(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; return (key==null ? e.getKey()==null : key.equals(e.getKey())) && (value==null ? e.getValue()==null : value.equals(e.getValue())); } public int hashCode() { return hash ^ Objects.hashCode(value); } public String toString() { return key.toString()+\"=\"+value.toString(); } } 成员变量 /** * The hash table data. */ private transient Entry[] table; /** * The total number of entries in the hash table. */ private transient int count; /** * The table is rehashed when its size exceeds this threshold. (The * value of this field is (int)(capacity * loadFactor).) * * @serial */ private int threshold; /** * The load factor for the hashtable. * * @serial */ private float loadFactor; /** * The number of times this Hashtable has been structurally modified * Structural modifications are those that change the number of entries in * the Hashtable or otherwise modify its internal structure (e.g., * rehash). This field is used to make iterators on Collection-views of * the Hashtable fail-fast. (See ConcurrentModificationException). */ private transient int modCount = 0; 构造方法 public Hashtable(int initialCapacity, float loadFactor) { if (initialCapacity [initialCapacity]; threshold = (int)Math.min(initialCapacity * loadFactor, MAX_ARRAY_SIZE + 1); } /** * Constructs a new, empty hashtable with the specified initial capacity * and default load factor (0.75). * * @param initialCapacity the initial capacity of the hashtable. * @exception IllegalArgumentException if the initial capacity is less * than zero. */ public Hashtable(int initialCapacity) { this(initialCapacity, 0.75f); } /** * Constructs a new, empty hashtable with a default initial capacity (11) * and load factor (0.75). */ public Hashtable() { this(11, 0.75f); } 11？ Hashtable 的容量增加逻辑是乘2+1，保证奇数。 在应用数据分布在等差数据集合(如偶数)上时，如果公差与桶容量有公约数n，则至少有(n-1)/n数量的桶是利用不到的。hash to index int hash = key.hashCode(); int index = (hash & 0x7FFFFFFF) % tab.length; 取与之后一定是一个非负数 0x7FFFFFFF is 0111 1111 1111 1111 1111 1111 1111 1111 : all 1 except the sign bit. (hash & 0x7FFFFFFF) will result in a positive integer. (hash & 0x7FFFFFFF) % tab.length will be in the range of the tab length.put（有锁) public synchronized V put(K key, V value) { // Make sure the value is not null if (value == null) { throw new NullPointerException(); } // Makes sure the key is not already in the hashtable. Entry tab[] = table; int hash = key.hashCode(); int index = (hash & 0x7FFFFFFF) % tab.length; @SuppressWarnings(\"unchecked\") Entry entry = (Entry)tab[index]; for(; entry != null ; entry = entry.next) { if ((entry.hash == hash) && entry.key.equals(key)) { V old = entry.value; entry.value = value; return old; } } addEntry(hash, key, value, index); return null; } private void addEntry(int hash, K key, V value, int index) { modCount++; Entry tab[] = table; if (count >= threshold) { // Rehash the table if the threshold is exceeded rehash(); tab = table; hash = key.hashCode(); index = (hash & 0x7FFFFFFF) % tab.length; } // Creates the new entry. @SuppressWarnings(\"unchecked\") Entry e = (Entry) tab[index]; tab[index] = new Entry<>(hash, key, value, e); count++; } 扩容 rehash - protected void rehash() { int oldCapacity = table.length; Entry[] oldMap = table; // overflow-conscious code int newCapacity = (oldCapacity 0) { if (oldCapacity == MAX_ARRAY_SIZE) // Keep running with MAX_ARRAY_SIZE buckets return; newCapacity = MAX_ARRAY_SIZE; } Entry[] newMap = new Entry[newCapacity]; modCount++; threshold = (int)Math.min(newCapacity * loadFactor, MAX_ARRAY_SIZE + 1); table = newMap; for (int i = oldCapacity ; i-- > 0 ;) { for (Entry old = (Entry)oldMap[i] ; old != null ; ) { Entry e = old; old = old.next; // 所有元素重新散列 int index = (e.hash & 0x7FFFFFFF) % newCapacity; e.next = (Entry)newMap[index]; newMap[index] = e; } } } get（有锁) public synchronized V get(Object key) { Entry tab[] = table; int hash = key.hashCode(); int index = (hash & 0x7FFFFFFF) % tab.length; for (Entry e = tab[index] ; e != null ; e = e.next) { if ((e.hash == hash) && e.key.equals(key)) { return (V)e.value; } } return null; } remove（有锁) public synchronized V remove(Object key) { Entry tab[] = table; int hash = key.hashCode(); int index = (hash & 0x7FFFFFFF) % tab.length; @SuppressWarnings(\"unchecked\") Entry e = (Entry)tab[index]; for(Entry prev = null ; e != null ; prev = e, e = e.next) { if ((e.hash == hash) && e.key.equals(key)) { modCount++; if (prev != null) { prev.next = e.next; } else { tab[index] = e.next; } count--; V oldValue = e.value; e.value = null; return oldValue; } } return null; } LinkedHashMap（底层是(数组+链表/红黑树)+环形双向链表，继承自HashMap) LinkedHashMap是key键有序的HashMap的一种实现。它除了使用哈希表这个数据结构，使用环形双向链表来保证key的顺序。 HashMap是无序的，也就是说，迭代HashMap所得到的元素顺序并不是它们最初放置到HashMap的顺序。HashMap的这一缺点往往会造成诸多不便，因为在有些场景中，我们确需要用到一个可以保持插入顺序的Map。庆幸的是，JDK为我们解决了这个问题，它为HashMap提供了一个子类 —— LinkedHashMap。虽然LinkedHashMap增加了时间和空间上的开销，但是它通过维护一个额外的双向链表保证了迭代顺序。特别地，该迭代顺序可以是插入顺序，也可以是访问顺序。因此，根据链表中元素的顺序可以将LinkedHashMap分为：保持插入顺序的LinkedHashMap 和 保持访问顺序（LRU，get后调整链表序，最新获取的放在最后)的LinkedHashMap，其中LinkedHashMap的默认实现是按插入顺序排序的。 特点： 一般来说，如果需要使用的Map中的key无序，选择HashMap；如果要求key有序，则选择TreeMap。 但是选择TreeMap就会有性能问题，因为TreeMap的get操作的时间复杂度是O(log(n))的，相比于HashMap的O(1)还是差不少的，LinkedHashMap的出现就是为了平衡这些因素，使得能够以O(1)时间复杂度增加查找元素，又能够保证key的有序性 实现原理： 将所有Entry节点链入一个双向链表的HashMap。在LinkedHashMap中，所有put进来的Entry都保存在哈希表中，但由于它又额外定义了一个以head为头结点的双向链表，因此对于每次put进来Entry，除了将其保存到哈希表上外，还会将其插入到双向链表的尾部。 LinkedHashMap#Entry static class Entry extends HashMap.Node { Entry before, after; Entry(int hash, K key, V value, Node next) { super(hash, key, value, next); } } 成员变量 /** * The head (eldest) of the doubly linked list. */ transient LinkedHashMap.Entry head; /** * The tail (youngest) of the doubly linked list. */ transient LinkedHashMap.Entry tail; /** * The iteration ordering method for this linked hash map: true * for access-order, false for insertion-order. * * @serial */ final boolean accessOrder; 构造方法 public LinkedHashMap(int initialCapacity, float loadFactor) { super(initialCapacity, loadFactor); accessOrder = false; } /** * Constructs an empty insertion-ordered LinkedHashMap instance * with the specified initial capacity and a default load factor (0.75). * * @param initialCapacity the initial capacity * @throws IllegalArgumentException if the initial capacity is negative */ public LinkedHashMap(int initialCapacity) { super(initialCapacity); accessOrder = false; } /** * Constructs an empty insertion-ordered LinkedHashMap instance * with the default initial capacity (16) and load factor (0.75). */ public LinkedHashMap() { super(); accessOrder = false; } /** * Constructs an empty LinkedHashMap instance with the * specified initial capacity, load factor and ordering mode. * * @param initialCapacity the initial capacity * @param loadFactor the load factor * @param accessOrder the ordering mode - true for * access-order, false for insertion-order * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) { super(initialCapacity, loadFactor); this.accessOrder = accessOrder; } put 同HashMap，但重写了afterNodeInsertion。 void afterNodeInsertion(boolean evict) { // possibly remove eldest LinkedHashMap.Entry first; if (evict && (first = head) != null && removeEldestEntry(first)) { K key = first.key; removeNode(hash(key), key, null, false, true); } } //可以自行重写该方法 protected boolean removeEldestEntry(Map.Entry eldest) { return false; } public class LRUHashMap extends LinkedHashMap{ private final int MAX_CACHE_SIZE; public BaseLRUCache(int cacheSize) { super(cacheSize, 0.75f, true); MAX_CACHE_SIZE = cacheSize; } @Override protected boolean removeEldestEntry(Map.Entry eldest) { return size() > MAX_CACHE_SIZE; } }get public V get(Object key) { Node e; if ((e = getNode(hash(key), key)) == null) return null; if (accessOrder) afterNodeAccess(e); return e.value; } void afterNodeAccess(Node e) { // move node to last LinkedHashMap.Entry last; if (accessOrder && (last = tail) != e) { LinkedHashMap.Entry p = (LinkedHashMap.Entry)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else { p.before = last; last.after = p; } tail = p; ++modCount; } }remove 同HashMap，但重写了afterNodeRemoval。 void afterNodeRemoval(Node e) { // unlink LinkedHashMap.Entry p = (LinkedHashMap.Entry)e, b = p.before, a = p.after; p.before = p.after = null; if (b == null) head = a; else b.after = a; if (a == null) tail = b; else a.before = b; } 遍历（迭代环形双向链表) entrySet public Set> entrySet() { Set> es; return (es = entrySet) == null ? (entrySet = new LinkedEntrySet()) : es; } 它使用的是该迭代器： abstract class LinkedHashIterator { LinkedHashMap.Entry next; LinkedHashMap.Entry current; int expectedModCount; LinkedHashIterator() { next = head; expectedModCount = modCount; current = null; } public final boolean hasNext() { return next != null; } final LinkedHashMap.Entry nextNode() { LinkedHashMap.Entry e = next; if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (e == null) throw new NoSuchElementException(); current = e; next = e.after; return e; } public final void remove() { Node p = current; if (p == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); current = null; K key = p.key; removeNode(hash(key), key, null, false, false); expectedModCount = modCount; } } final class LinkedEntryIterator extends LinkedHashIterator implements Iterator> { public final Map.Entry next() { return nextNode(); } } TreeMap（底层是红黑树) 支持排序的Map实现。 基于红黑树实现，无容量限制。 是非线程安全的。 TreeMap是根据key进行排序的，它的排序和定位需要依赖比较器或覆写Comparable接口，也因此不需要key覆写hashCode方法和equals方法，就可以排除掉重复的key，而HashMap的key则需要通过覆写hashCode方法和equals方法来确保没有重复的key TreeMap的查询、插入、删除效率均没有HashMap高，一般只有要对key排序时才使用TreeMap。 TreeMap的key不能为null，而HashMap的key可以为null。 TreeMap#Entry static final class Entry implements Map.Entry { K key; V value; Entry left; Entry right; Entry parent; boolean color = BLACK; /** * Make a new cell with given key, value, and parent, and with * {@code null} child links, and BLACK color. */ Entry(K key, V value, Entry parent) { this.key = key; this.value = value; this.parent = parent; } /** * Returns the key. * * @return the key */ public K getKey() { return key; } /** * Returns the value associated with the key. * * @return the value associated with the key */ public V getValue() { return value; } /** * Replaces the value currently associated with the key with the given * value. * * @return the value associated with the key before this method was * called */ public V setValue(V value) { V oldValue = this.value; this.value = value; return oldValue; } public boolean equals(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; return valEquals(key,e.getKey()) && valEquals(value,e.getValue()); } public int hashCode() { int keyHash = (key==null ? 0 : key.hashCode()); int valueHash = (value==null ? 0 : value.hashCode()); return keyHash ^ valueHash; } public String toString() { return key + \"=\" + value; } } 成员变量 private final Comparator comparator; private transient Entry root; /** * The number of entries in the tree */ private transient int size = 0; /** * The number of structural modifications to the tree. */ private transient int modCount = 0; 构造方法 public TreeMap() { comparator = null; } public TreeMap(Comparator comparator) { this.comparator = comparator; } put public V put(K key, V value) { Entry t = root; if (t == null) { compare(key, key); // type (and possibly null) check root = new Entry<>(key, value, null); size = 1; modCount++; return null; } int cmp; Entry parent; // split comparator and comparable paths Comparator cpr = comparator; if (cpr != null) { do { parent = t; cmp = cpr.compare(key, t.key); if (cmp 0) t = t.right; else return t.setValue(value); } while (t != null); } else { if (key == null) throw new NullPointerException(); @SuppressWarnings(\"unchecked\") Comparable k = (Comparable) key; do { parent = t; cmp = k.compareTo(t.key); if (cmp 0) t = t.right; else return t.setValue(value); } while (t != null); } Entry e = new Entry<>(key, value, parent); if (cmp get public V get(Object key) { Entry p = getEntry(key); return (p==null ? null : p.value); } final Entry getEntry(Object key) { // Offload comparator-based version for sake of performance if (comparator != null) return getEntryUsingComparator(key); if (key == null) throw new NullPointerException(); @SuppressWarnings(\"unchecked\") Comparable k = (Comparable) key; Entry p = root; while (p != null) { int cmp = k.compareTo(p.key); if (cmp 0) p = p.right; else return p; } return null; } final Entry getEntryUsingComparator(Object key) { @SuppressWarnings(\"unchecked\") K k = (K) key; Comparator cpr = comparator; if (cpr != null) { Entry p = root; while (p != null) { int cmp = cpr.compare(k, p.key); if (cmp 0) p = p.right; else return p; } } return null; } remove public V remove(Object key) { Entry p = getEntry(key); if (p == null) return null; V oldValue = p.value; deleteEntry(p); return oldValue; } private void deleteEntry(Entry p) { modCount++; size--; // If strictly internal, copy successor's element to p and then make p // point to successor. if (p.left != null && p.right != null) { Entry s = successor(p); p.key = s.key; p.value = s.value; p = s; } // p has 2 children // Start fixup at replacement node, if it exists. Entry replacement = (p.left != null ? p.left : p.right); if (replacement != null) { // Link replacement to parent replacement.parent = p.parent; if (p.parent == null) root = replacement; else if (p == p.parent.left) p.parent.left = replacement; else p.parent.right = replacement; // Null out links so they are OK to use by fixAfterDeletion. p.left = p.right = p.parent = null; // Fix replacement if (p.color == BLACK) fixAfterDeletion(replacement); } else if (p.parent == null) { // return if we are the only node. root = null; } else { // No children. Use self as phantom replacement and unlink. if (p.color == BLACK) fixAfterDeletion(p); if (p.parent != null) { if (p == p.parent.left) p.parent.left = null; else if (p == p.parent.right) p.parent.right = null; p.parent = null; } } } containsKey public boolean containsKey(Object key) { return getEntry(key) != null; } containsValue public boolean containsValue(Object value) { for (Entry e = getFirstEntry(); e != null; e = successor(e)) if (valEquals(value, e.value)) return true; return false; } final Entry getFirstEntry() { Entry p = root; if (p != null) while (p.left != null) p = p.left; return p; } static TreeMap.Entry successor(Entry t) { if (t == null) return null; else if (t.right != null) { Entry p = t.right; while (p.left != null) p = p.left; return p; } else { Entry p = t.parent; Entry ch = t; while (p != null && ch == p.right) { ch = p; p = p.parent; } return p; } } static final boolean valEquals(Object o1, Object o2) { return (o1==null ? o2==null : o1.equals(o2)); } 遍历 public Set> entrySet() { EntrySet es = entrySet; return (es != null) ? es : (entrySet = new EntrySet()); } class EntrySet extends AbstractSet> { public Iterator> iterator() { return new EntryIterator(getFirstEntry()); } } abstract class PrivateEntryIterator implements Iterator { Entry next; Entry lastReturned; int expectedModCount; PrivateEntryIterator(Entry first) { expectedModCount = modCount; lastReturned = null; next = first; } public final boolean hasNext() { return next != null; } final Entry nextEntry() { Entry e = next; if (e == null) throw new NoSuchElementException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); next = successor(e); lastReturned = e; return e; } final Entry prevEntry() { Entry e = next; if (e == null) throw new NoSuchElementException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); next = predecessor(e); lastReturned = e; return e; } public void remove() { if (lastReturned == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); // deleted entries are replaced by their successors if (lastReturned.left != null && lastReturned.right != null) next = lastReturned; deleteEntry(lastReturned); expectedModCount = modCount; lastReturned = null; } } final class EntryIterator extends PrivateEntryIterator> { EntryIterator(Entry first) { super(first); } public Map.Entry next() { return nextEntry(); } } ConcurrentHashMap（底层是数组+链表/红黑树，基于CAS+synchronized) JDK1.7前：分段锁 基于currentLevel划分出了多个Segment来对key-value进行存储，从而避免每次put操作都得锁住整个数组。在默认的情况下，最佳情况下可以允许16个线程并发无阻塞地操作集合对象，尽可能地减少并发时的阻塞现象。 put、remove会加锁。get和containsKey不会加锁。 计算size：在不加锁的情况下遍历所有的段，读取其count以及modCount，这两个属性都是volatile类型的，并进行统计，再遍历一次所有的段，比较modCount是否有改变。如有改变，则再尝试两次机上动作。 如执行了三次上述动作，仍然有问题，则遍历所有段，分别进行加锁，然后进行计算，计算完毕后释放所有锁，从而完成计算动作。 JDK1.8后：CAS+synchronized bin是桶 bucket的意思 ConcurrentHashMap是延迟初始化的，只有在插入数据时，整个HashMap才被初始化为2的次方大小个桶（bin)，每个bin包含哈希值相同的一系列Node（一般含有0或1个Node)。每个bin的第一个Node作为这个bin的锁，Hash值为零或者负的将被忽略； 每个bin的第一个Node插入用到CAS原理，这是在ConcurrentHashMap中最常发生的操作，其余的插入、删除、替换操作对bin中的第一个Node加锁，进行操作 ConcurrentHashMap的size()函数一般比较少用，同时为了提高增删查改的效率，容器并未在内部保存一个size值，而且采用每次调用size()函数时累加各个bin中Node的个数计算得到，而且这一过程不加锁，即得到的size值不一定是最新的。 ConcurrentHashMap#Node Node是最核心的内部类，它包装了key-value键值对，所有插入ConcurrentHashMap的数据都包装在这里面。它与HashMap中的定义很相似，但是但是有一些差别：它对value和next属性设置了volatile属性；’它不允许调用setValue方法直接改变Node的value域；它增加了find方法辅助map.get()方法。 static class Node implements Map.Entry { final int hash; final K key; volatile V val; // value和next是volatile的 volatile Node next; Node(int hash, K key, V val, Node next) { this.hash = hash; this.key = key; this.val = val; this.next = next; } public final K getKey() { return key; } public final V getValue() { return val; } public final int hashCode() { return key.hashCode() ^ val.hashCode(); } public final String toString(){ return key + \"=\" + val; } public final V setValue(V value) { throw new UnsupportedOperationException(); } public final boolean equals(Object o) { Object k, v, u; Map.Entry e; return ((o instanceof Map.Entry) && (k = (e = (Map.Entry)o).getKey()) != null && (v = e.getValue()) != null && (k == key || k.equals(key)) && (v == (u = val) || v.equals(u))); } /** * Virtualized support for map.get(); overridden in subclasses. */ Node find(int h, Object k) { Node e = this; if (k != null) { do { K ek; if (e.hash == h && ((ek = e.key) == k || (ek != null && k.equals(ek)))) return e; } while ((e = e.next) != null); } return null; } } ConcurrentHashMap#TreeNode 当链表长度过长的时候，会转换为TreeNode。但是与HashMap不相同的是，它并不是直接转换为红黑树，而是把这些结点包装成TreeNode放在TreeBin对象中，由TreeBin完成对红黑树的包装。而且TreeNode在ConcurrentHashMap继承自Node类，而并非HashMap中的继承自LinkedHashMap.Entry类，也就是说TreeNode带有next指针，这样做的目的是方便基于TreeBin的访问。 static final class TreeNode extends Node { TreeNode parent; // red-black tree links TreeNode left; TreeNode right; TreeNode prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node next, TreeNode parent) { super(hash, key, val, next); this.parent = parent; } Node find(int h, Object k) { return findTreeNode(h, k, null); } /** * Returns the TreeNode (or null if not found) for the given key * starting at given root. */ final TreeNode findTreeNode(int h, Object k, Class kc) { if (k != null) { TreeNode p = this; do { int ph, dir; K pk; TreeNode q; TreeNode pl = p.left, pr = p.right; if ((ph = p.hash) > h) p = pl; else if (ph ConcurrentHashMap#TreeBin 这个类并不负责包装用户的key、value信息，而是包装的很多TreeNode节点。它代替了TreeNode的根节点，也就是说在实际的ConcurrentHashMap“数组”中，存放的是TreeBin对象，而不是TreeNode对象，这是与HashMap的区别。另外这个类还带有了读写锁。 可以看到在构造TreeBin节点时，仅仅指定了它的hash值为TREEBIN常量，这也就是个标识位；同时也看到我们熟悉的红黑树构造方法。 /** * TreeNodes used at the heads of bins. TreeBins do not hold user * keys or values, but instead point to list of TreeNodes and * their root. They also maintain a parasitic read-write lock * forcing writers (who hold bin lock) to wait for readers (who do * not) to complete before tree restructuring operations. */ static final class TreeBin extends Node { TreeNode root; volatile TreeNode first; volatile Thread waiter; volatile int lockState; // values for lockState static final int WRITER = 1; // set while holding write lock static final int WAITER = 2; // set when waiting for write lock static final int READER = 4; // increment value for setting read lock /** * Tie-breaking utility for ordering insertions when equal * hashCodes and non-comparable. We don't require a total * order, just a consistent insertion rule to maintain * equivalence across rebalancings. Tie-breaking further than * necessary simplifies testing a bit. */ static int tieBreakOrder(Object a, Object b) { int d; if (a == null || b == null || (d = a.getClass().getName(). compareTo(b.getClass().getName())) == 0) d = (System.identityHashCode(a) b) { super(TREEBIN, null, null, null); this.first = b; TreeNode r = null; for (TreeNode x = b, next; x != null; x = next) { next = (TreeNode)x.next; x.left = x.right = null; if (r == null) { x.parent = null; x.red = false; r = x; } else { K k = x.key; int h = x.hash; Class kc = null; for (TreeNode p = r;;) { int dir, ph; K pk = p.key; if ((ph = p.hash) > h) dir = -1; else if (ph xp = p; if ((p = (dir } ConcurrentHashMap#ForwardingNode /** * A node inserted at head of bins during transfer operations. */ static final class ForwardingNode extends Node { final Node[] nextTable; ForwardingNode(Node[] tab) { super(MOVED, null, null, null); this.nextTable = tab; } Node find(int h, Object k) { // loop to avoid arbitrarily deep recursion on forwarding nodes outer: for (Node[] tab = nextTable;;) { Node e; int n; if (k == null || tab == null || (n = tab.length) == 0 || (e = tabAt(tab, (n - 1) & h)) == null) return null; for (;;) { int eh; K ek; if ((eh = e.hash) == h && ((ek = e.key) == k || (ek != null && k.equals(ek)))) return e; if (eh )e).nextTable; continue outer; } else return e.find(h, k); } if ((e = e.next) == null) return null; } } } } ConcurrentHashMap#ReservationNode /** * A place-holder node used in computeIfAbsent and compute */ static final class ReservationNode extends Node { ReservationNode() { super(RESERVED, null, null, null); } Node find(int h, Object k) { return null; } } 节点类型 hash值大于等于0，则是链表节点，Node hash值为-1 MOVED，则是forwarding nodes，存储nextTable的引用。只有table发生扩容的时候，ForwardingNode才会发挥作用，作为一个占位符放在table中表示当前节点为null或则已经被移动。 hash值为-2 TREEBIN，则是红黑树根，TreeBin类型 hash值为-3 RESERVED，则是reservation nodes， static final int MOVED = -1; // hash for forwarding nodes static final int TREEBIN = -2; // hash for roots of trees static final int RESERVED = -3; // hash for transient reservations 成员变量 /** * The largest possible table capacity. This value must be * exactly 1>> 2)} for * the associated resizing threshold. */ private static final float LOAD_FACTOR = 0.75f; /** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2, and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */ static final int TREEIFY_THRESHOLD = 8; /** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */ static final int UNTREEIFY_THRESHOLD = 6; /** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * The value should be at least 4 * TREEIFY_THRESHOLD to avoid * conflicts between resizing and treeification thresholds. */ static final int MIN_TREEIFY_CAPACITY = 64; /** * Minimum number of rebinnings per transfer step. Ranges are * subdivided to allow multiple resizer threads. This value * serves as a lower bound to avoid resizers encountering * excessive memory contention. The value should be at least * DEFAULT_CAPACITY. */ private static final int MIN_TRANSFER_STRIDE = 16; /** * The number of bits used for generation stamp in sizeCtl. * Must be at least 6 for 32bit arrays. */ private static int RESIZE_STAMP_BITS = 16; /** * The maximum number of threads that can help resize. * Must fit in 32 - RESIZE_STAMP_BITS bits. */ private static final int MAX_RESIZERS = (1 /** * The array of bins. Lazily initialized upon first insertion. * Size is always a power of two. Accessed directly by iterators. */ transient volatile Node[] table; /** * The next table to use; non-null only while resizing. */ private transient volatile Node[] nextTable; /** * Base counter value, used mainly when there is no contention, * but also as a fallback during table initialization * races. Updated via CAS. */ private transient volatile long baseCount; /** * Table initialization and resizing control. When negative, the * table is being initialized or resized: -1 for initialization, * else -(1 + the number of active resizing threads). Otherwise, * when table is null, holds the initial table size to use upon * creation, or 0 for default. After initialization, holds the * next element count value upon which to resize the table. 负数代表正在进行初始化或扩容操作 -1代表正在初始化 -N 表示有N-1个线程正在进行扩容操作 正数或0代表hash表还没有被初始化，这个数值表示初始化或下一次进行扩容的大小，这一点类似于扩容阈值的概念。还后面可以看到，它的值始终是当前ConcurrentHashMap容量的0.75倍，这与loadfactor是对应的。 */ private transient volatile int sizeCtl; /** * The next table index (plus one) to split while resizing. */ private transient volatile int transferIndex; /** * Spinlock (locked via CAS) used when resizing and/or creating CounterCells. */ private transient volatile int cellsBusy; /** * Table of counter cells. When non-null, size is a power of 2. */ private transient volatile CounterCell[] counterCells; // views private transient KeySetView keySet; private transient ValuesView values; private transient EntrySetView entrySet; 构造方法 public ConcurrentHashMap() { } /** * Creates a new, empty map with an initial table size * accommodating the specified number of elements without the need * to dynamically resize. * * @param initialCapacity The implementation performs internal * sizing to accommodate this many elements. * @throws IllegalArgumentException if the initial capacity of * elements is negative */ public ConcurrentHashMap(int initialCapacity) { if (initialCapacity = (MAXIMUM_CAPACITY >>> 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity >>> 1) + 1)); this.sizeCtl = cap; } /** * Creates a new map with the same mappings as the given map. * * @param m the map */ public ConcurrentHashMap(Map m) { this.sizeCtl = DEFAULT_CAPACITY; putAll(m); } /** * Creates a new, empty map with an initial table size based on * the given number of elements ({@code initialCapacity}) and * initial table density ({@code loadFactor}). * * @param initialCapacity the initial capacity. The implementation * performs internal sizing to accommodate this many elements, * given the specified load factor. * @param loadFactor the load factor (table density) for * establishing the initial table size * @throws IllegalArgumentException if the initial capacity of * elements is negative or the load factor is nonpositive * * @since 1.6 */ public ConcurrentHashMap(int initialCapacity, float loadFactor) { this(initialCapacity, loadFactor, 1); } /** * Creates a new, empty map with an initial table size based on * the given number of elements ({@code initialCapacity}), table * density ({@code loadFactor}), and number of concurrently * updating threads ({@code concurrencyLevel}). * * @param initialCapacity the initial capacity. The implementation * performs internal sizing to accommodate this many elements, * given the specified load factor. * @param loadFactor the load factor (table density) for * establishing the initial table size * @param concurrencyLevel the estimated number of concurrently * updating threads. The implementation may use this value as * a sizing hint. * @throws IllegalArgumentException if the initial capacity is * negative or the load factor or concurrencyLevel are * nonpositive */ public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) { if (!(loadFactor > 0.0f) || initialCapacity = (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); this.sizeCtl = cap; } CAS private static final sun.misc.Unsafe U; Unsafe类的几个CAS方法，可以原子性地修改对象的某个属性值 /** * Atomically update Java variable to x if it is currently * holding expected. * @return true if successful */ public final native boolean compareAndSwapObject(Object o, long offset, Object expected, Object x); /** * Atomically update Java variable to x if it is currently * holding expected. * @return true if successful */ public final native boolean compareAndSwapInt(Object o, long offset, int expected, int x); /** * Atomically update Java variable to x if it is currently * holding expected. * @return true if successful */ public final native boolean compareAndSwapLong(Object o, long offset, long expected, long x); /** * Fetches a reference value from a given Java variable, with volatile * load semantics. Otherwise identical to {@link #getObject(Object, long)} */ public native Object getObjectVolatile(Object o, long offset); /** * Stores a reference value into a given Java variable, with * volatile store semantics. Otherwise identical to {@link #putObject(Object, long, Object)} */ public native void putObjectVolatile(Object o, long offset, Object x); Unsafe.getObjectVolatile可以直接获取指定内存的数据，保证了每次拿到数据都是最新的。三个核心方法 ConcurrentHashMap定义了三个原子操作，用于对指定位置的节点进行操作。正是这些原子操作保证了ConcurrentHashMap的线程安全。 static final Node tabAt(Node[] tab, int i) { return (Node)U.getObjectVolatile(tab, ((long)i static final boolean casTabAt(Node[] tab, int i, Node c, Node v) { return U.compareAndSwapObject(tab, ((long)i static final void setTabAt(Node[] tab, int i, Node v) { U.putObjectVolatile(tab, ((long)i 初始化 对于ConcurrentHashMap来说，调用它的构造方法仅仅是设置了一些参数而已。而整个table的初始化是在向ConcurrentHashMap中插入元素的时候发生的。如调用put、computeIfAbsent、compute、merge等方法的时候，调用时机是检查table==null。 初始化方法主要应用了关键属性sizeCtl 如果这个值 private final Node[] initTable() { Node[] tab; int sc; while ((tab = table) == null || tab.length == 0) { if ((sc = sizeCtl) - // 利用CAS方法把sizectl的值置为-1 表示本线程正在进行初始化 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if ((tab = table) == null || tab.length == 0) { int n = (sc > 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(\"unchecked\") Node[] nt = (Node[])new Node[n]; table = tab = nt; - // 相当于0.75*n 设置一个扩容的阈值 sc = n - (n >>> 2); } } finally { sizeCtl = sc; } break; } } return tab; } spread（hash) h是某个对象的hashCode返回值 static final int spread(int h) { return (h ^ (h >>> 16)) & HASH_BITS; } static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash 类似于Hashtable+HashMap的hash实现，Hashtable中也是和一个魔法值取与，保证结果一定为正数；HashMap中也是将hashCode与其移动低n位的结果再取异或，保证了对象的hashCode的高16位的变化能反应到低16位中， size相关 成员变量 @sun.misc.Contended static final class CounterCell { volatile long value; CounterCell(long x) { value = x; } } /** * Base counter value, used mainly when there is no contention, * but also as a fallback during table initialization * races. Updated via CAS. */ private transient volatile long baseCount; /** * Spinlock (locked via CAS) used when resizing and/or creating CounterCells. */ private transient volatile int cellsBusy; /** * Table of counter cells. When non-null, size is a power of 2. */ private transient volatile CounterCell[] counterCells; 每个CounterCell都对应一个bucket，CounterCell中的long值就是对应bucket的binCount。 计算总大小就是将所有bucket的binCount求和，而每个binCount都存储在CounterCell#value中，每当put或者remove时都会更新节点所在bucket对应的CounterCell#value。 size() 没有直接返回baseCount 而是统计一次这个值，而这个值其实也是一个大概的数值，因此可能在统计的时候有其他线程正在执行插入或删除操作。 public int size() { long n = sumCount(); return ((n (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); } 在baseCount基础上再加上所有counterCell的值求和。 而在addCount时，会先尝试CAS更新baseCount，如果有冲突，则再尝试CAS更新随机的一个counterCell中的value，这样求和就是正确的size了。 final long sumCount() { CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) { for (int i = 0; i // 所有counter的值求和 sum += a.value; } } return sum; } put（若bucket第一个结点插入则使用CAS，否则加锁) public V put(K key, V value) { return putVal(key, value, false); } 整体流程就是首先定义不允许key或value为null的情况放入 。对于每一个放入的值，首先利用spread方法对key的hashcode进行一次hash计算，由此来确定这个值在table中的位置。 1)如果这个位置是空的，那么直接放入，而且不需要加锁操作。 2)如果这个位置存在结点，说明发生了hash碰撞，首先判断这个节点的类型。 a)如果是MOVED节点，则表示正在扩容，帮助进行扩容 b)如果是链表节点(hash >=0),则得到的结点就是hash值相同的节点组成的链表的头节点。需要依次向后遍历确定这个新加入的值所在位置。如果遇到hash值与key值都与新加入节点是一致的情况，则只需要更新value值即可。否则依次向后遍历，直到链表尾插入这个结点。 如果加入这个节点以后链表长度大于8，就把这个链表转换成红黑树。 c)如果这个节点的类型已经是树节点的话，直接调用树节点的插入方法进行插入新的值。 3)addCount 增加计数值 /** Implementation for put and putIfAbsent */ final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; // 死循环，只有插入成功时才会跳出 for (Node[] tab = table;;) { Node f; int n, i, fh; if (tab == null || (n = tab.length) == 0) // table为空则初始化（延迟初始化) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) { // hash to index后正好为空，则CAS放入；如果失败那么进入下次循环继续尝试 if (casTabAt(tab, i, null, new Node(hash, key, value, null))) break; // no lock when adding to empty bin } // 如果index处非空，且hash为MOVED（表示该节点是ForwardingNode)，则表示有其它线程正在扩容，则一起进行扩容操作。 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); // 如果index处非空，且为链表节点或树节点 else { V oldVal = null; // 对某个bucket上执行添加操作仅需要锁住第一个Node即可（可以保证不会多线程同时对某个bucket进行写入) synchronized (f) { if (tabAt(tab, i) == f) { // 1) 如果是链表节点，那么插入到链表中 if (fh >= 0) { // binCount是该bucket中元素个数 binCount = 1; for (Node e = f;; ++binCount) { K ek; if (e.hash == hash && ((ek = e.key) == key || (ek != null && key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } Node pred = e; if ((e = e.next) == null) { pred.next = new Node(hash, key, value, null); break; } } } // 2)如果是红黑树树根，那么插入到红黑树中 else if (f instanceof TreeBin) { Node p; binCount = 2; if ((p = ((TreeBin)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } // 插入节点/释放锁之后，如果大小合适调整为红黑树，那么将链表转为红黑树 if (binCount != 0) { if (binCount >= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } // 将当前ConcurrentHashMap的元素数量+1 ，如果超过阈值，那么进行扩容 addCount(1L, binCount); return null; }treeifyBin（有锁，数组较小则扩容，较大则转为红黑树) private final void treeifyBin(Node[] tab, int index) { Node b; int n, sc; if (tab != null) { - // 如果数组长度n小于阈值MIN_TREEIFY_CAPACITY，默认是64，则会调用tryPresize方法把数组长度扩大到原来的两倍，并触发transfer方法，重新调整节点的位置 if ((n = tab.length) = 0) { synchronized (b) { if (tabAt(tab, index) == b) { TreeNode hd = null, tl = null; for (Node e = b; e != null; e = e.next) { TreeNode p = new TreeNode(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; } setTabAt(tab, index, new TreeBin(hd)); } } } } } tryPreSize /** * Tries to presize table to accommodate the given number of elements. * * @param size number of elements (doesn't need to be perfectly accurate) */ private final void tryPresize(int size) { int c = (size >= (MAXIMUM_CAPACITY >>> 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size >>> 1) + 1); int sc; while ((sc = sizeCtl) >= 0) { Node[] tab = table; int n; if (tab == null || (n = tab.length) == 0) { n = (sc > c) ? sc : c; if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if (table == tab) { @SuppressWarnings(\"unchecked\") Node[] nt = (Node[])new Node[n]; table = nt; sc = n - (n >>> 2); } } finally { sizeCtl = sc; } } } else if (c = MAXIMUM_CAPACITY) break; else if (tab == table) { int rs = resizeStamp(n); if (sc [] nt; if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex 扩容 tryPresize tryPresize在putAll以及treeifyBin中调用 private final void tryPresize(int size) { - // c是扩容之后预计表的大小 int c = (size >= (MAXIMUM_CAPACITY >>> 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size >>> 1) + 1); int sc; // 没有正在初始化或扩容 while ((sc = sizeCtl) >= 0) { Node[] tab = table; int n; if (tab == null || (n = tab.length) == 0) { n = (sc > c) ? sc : c; // 期间没有其他线程对表操作，则CAS将SIZECTL状态置为-1，表示正在进行初始化 if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if (table == tab) { @SuppressWarnings(\"unchecked\") Node[] nt = (Node[])new Node[n]; table = nt; // 即0.75*n sc = n - (n >>> 2); } } finally { sizeCtl = sc; } } } // 若欲扩容值不大于原阈值，或现有容量>=最值，则do nothing else if (c = MAXIMUM_CAPACITY) break; // table不为空，且在此期间其他线程未修改table else if (tab == table) { int rs = resizeStamp(n); if (sc [] nt; if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex // 协助扩容 transfer(tab, nt); } else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs // 发起扩容 transfer(tab, null); } } } addCount x=1，check=bucketCount private final void addCount(long x, int check) { // 计数值加x // 利用CAS方法更新baseCount的值 CounterCell[] as; long b, s; if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) { // 如果CAS更新baseCount失败或者counterCells不为空，那么尝试CAS更新当前线程的hashCode对应的bucket的value CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) (a = as[ThreadLocalRandom.getProbe() & m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) { // 如果两次CAS都失败了，那么调用fullAddCount方法 fullAddCount(x, uncontended); return; } if (check return; s = sumCount(); } // 以上与扩容无关，如果check值大于等于0 则需要检查是否需要进行扩容操作 if (check >= 0) { Node[] tab, nt; int n, sc; while (s >= (long)(sc = sizeCtl) && (tab = table) != null && (n = tab.length) // 如果sizeCtl是小于0的，说明有其他线程正在执行扩容操作，nextTable一定不为空 if (sc >> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex // 协助扩容 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); } // 当前线程是唯一的或是第一个发起扩容的线程 此时nextTable=null else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs // 发起扩容 transfer(tab, null); s = sumCount(); } } } private final void fullAddCount(long x, boolean wasUncontended) { int h; if ((h = ThreadLocalRandom.getProbe()) == 0) { ThreadLocalRandom.localInit(); // force initialization h = ThreadLocalRandom.getProbe(); wasUncontended = true; } boolean collide = false; // True if last slot nonempty for (;;) { CounterCell[] as; CounterCell a; int n; long v; if ((as = counterCells) != null && (n = as.length) > 0) { if ((a = as[(n - 1) & h]) == null) { if (cellsBusy == 0) { // Try to attach new Cell CounterCell r = new CounterCell(x); // Optimistic create if (cellsBusy == 0 && U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) { boolean created = false; try { // Recheck under lock CounterCell[] rs; int m, j; if ((rs = counterCells) != null && (m = rs.length) > 0 && rs[j = (m - 1) & h] == null) { rs[j] = r; created = true; } } finally { cellsBusy = 0; } if (created) break; continue; // Slot is now non-empty } } collide = false; } else if (!wasUncontended) // CAS already known to fail wasUncontended = true; // Continue after rehash else if (U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x)) break; else if (counterCells != as || n >= NCPU) collide = false; // At max size or stale else if (!collide) collide = true; else if (cellsBusy == 0 && U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) { try { if (counterCells == as) {// Expand table unless stale CounterCell[] rs = new CounterCell[n transfer 当table容量不足的时候，即table的元素数量达到容量阈值sizeCtl，需要对table进行扩容。 整个扩容分为两部分： 1)构建一个nextTable，大小为table的两倍。 2)把table的数据复制到nextTable中。 这两个过程在单线程下实现很简单，但是ConcurrentHashMap是支持并发插入的，扩容操作自然也会有并发的出现，这种情况下，第二步可以支持节点的并发复制，这样性能自然提升不少，但实现的复杂度也上升了一个台阶。 先看第一步，构建nextTable，毫无疑问，这个过程只能有单个线程进行nextTable的初始化。 通过Unsafe.compareAndSwapInt修改sizeCtl值，保证只有一个线程能够初始化nextTable，扩容后的数组长度为原来的两倍。 节点从table移动到nextTable，大体思想是遍历、复制的过程。 1)首先根据运算得到需要遍历的次数i，然后利用tabAt方法获得i位置的元素f，初始化一个ForwardingNode实例fwd。 2)如果f==null，则在table中的i位置放入fwd，这个过程是采用 Unsafe.compareAndSwapObjectf方法实现的，很巧妙的实现了节点的并发移动。 3)如果f是链表的头节点，就构造一个反序链表，把他们分别放在nextTable的i和i+n的位置上，移动完成，采用Unsafe.putObjectVolatile方法给table原位置赋值fwd。 4)如果f是TreeBin节点，也做一个反序处理，并判断是否需要untreeify，把处理的结果分别放在nextTable的i和i+n的位置上，移动完成，同样采用Unsafe.putObjectVolatile方法给table原位置赋值fwd。 5)遍历过所有的节点以后就完成了复制工作，把table指向nextTable，并更新sizeCtl为新数组大小的0.75倍 ，扩容完成。 在多线程环境下，ConcurrentHashMap用两点来保证正确性：ForwardingNode和synchronized。当一个线程遍历到的节点如果是ForwardingNode，则继续往后遍历，如果不是，则将该节点加锁，防止其他线程进入，完成后设置ForwardingNode节点，以便要其他线程可以看到该节点已经处理过了，如此交叉进行，高效而又安全。 /** * Moves and/or copies the nodes in each bin to new table. See * above for explanation. */ private final void transfer(Node[] tab, Node[] nextTab) { int n = tab.length, stride; if ((stride = (NCPU > 1) ? (n >>> 3) / NCPU : n) // 扩容第一步，创建两倍长的数组nextTable，单线程执行 // initiating只能有一个线程进行构造nextTable，如果别的线程进入发现不为空就不用构造nextTable了 if (nextTab == null) { // initiating try { @SuppressWarnings(\"unchecked\") Node[] nt = (Node[])new Node[n } // 扩容第二步，把table的数据复制到nextTable中，多线程可以同时进行 int nextn = nextTab.length; // 构造一个ForwardingNode用于多线程之间的共同扩容情况 ForwardingNode fwd = new ForwardingNode(nextTab); boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab // 遍历每个节点 for (int i = 0, bound = 0;;) { Node f; int fh; while (advance) { int nextIndex, nextBound; if (--i >= bound || finishing) advance = false; else if ((nextIndex = transferIndex) stride ? nextIndex - stride : 0))) { bound = nextBound; i = nextIndex - 1; advance = false; } } // 得到一个i，i指向table中某一个尚未拷贝的bucket，下面的代码是对i对应的bucket进行拷贝，拷贝完后将bucket赋值为fwd（ForwadingNode) //**************************************************************// if (i = n || i + n >= nextn) { int sc; // 如果原table已经复制结束 if (finishing) { nextTable = null; table = nextTab; // 修改扩容后的阀值，应该是现在容量的0.75倍 sizeCtl = (n >> 1); return; } // 采用CAS算法更新SizeCtl，减一，表示有一个新的线程参与到扩容操作 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { if ((sc - 2) != resizeStamp(n) } } // CAS算法获取某一个数组的节点，为空就设为ForwordingNode else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // 如果这个节点的hash值是MOVED，就表示这个节点是ForwordingNode节点，就表示这个节点已经被处理过了，直接跳过 else if ((fh = f.hash) == MOVED) advance = true; // already processed else { synchronized (f) { if (tabAt(tab, i) == f) { Node ln, hn; if (fh >= 0) { //如果这个节点的确是链表节点，则拆分为两个子链表，存储到nextTable相应的两个位置 int runBit = fh & n; Node lastRun = f; for (Node p = f.next; p != null; p = p.next) { int b = p.hash & n; if (b != runBit) { runBit = b; lastRun = p; } } if (runBit == 0) { ln = lastRun; hn = null; } else { hn = lastRun; ln = null; } // for (Node p = f; p != lastRun; p = p.next) { int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph & n) == 0) ln = new Node(ph, pk, pv, ln); else hn = new Node(ph, pk, pv, hn); } // CAS存储在nextTable的i位置上 setTabAt(nextTab, i, ln); // CAS存储在nextTable的i+n位置上 setTabAt(nextTab, i + n, hn); // CAS在原table的i处设置forwordingNode节点，表示这个这个节点已经处理完毕 setTabAt(tab, i, fwd); advance = true; } // 如果这个节点是红黑树，则拆分为两颗子树，保存到nextTable相应的两个位置 else if (f instanceof TreeBin) { TreeBin t = (TreeBin)f; TreeNode lo = null, loTail = null; TreeNode hi = null, hiTail = null; int lc = 0, hc = 0; for (Node e = t.first; e != null; e = e.next) { int h = e.hash; TreeNode p = new TreeNode (h, e.key, e.val, null, null); if ((h & n) == 0) { if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; } else { if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; } } // 如果拆分后的树的节点数量已经少于6个就需要重新转化为链表 ln = (lc (lo) : t; hn = (hc (hi) : t; // CAS存储在nextTable的i位置上 setTabAt(nextTab, i, ln); // CAS存储在nextTable的i+n位置上 setTabAt(nextTab, i + n, hn); // CAS在原table的i处设置forwordingNode节点，表示这个这个节点已经处理完毕 setTabAt(tab, i, fwd); advance = true; } } } } } } helpTransfer - final Node[] helpTransfer(Node[] tab, Node f) { Node[] nextTab; int sc; if (tab != null && (f instanceof ForwardingNode) && (nextTab = ((ForwardingNode)f).nextTable) != null) { int rs = resizeStamp(tab.length); while (nextTab == nextTable && table == tab && (sc = sizeCtl) >> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex // 调用扩容方法 transfer(tab, nextTab); break; } } return nextTab; } return table; }putIfAbsent public V putIfAbsent(K key, V value) { return putVal(key, value, true); } get（无锁) public V get(Object key) { Node[] tab; Node e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null && (n = tab.length) > 0 && (e = tabAt(tab, (n - 1) & h)) != null) { if ((eh = e.hash) == h) { // bucket中第一个结点就是我们要找的，直接返回 if ((ek = e.key) == key || (ek != null && key.equals(ek))) return e.val; } else if (eh // bucket中第一个结点是红黑树根，则调用find方法去找 return (p = e.find(h, key)) != null ? p.val : null; // bucket中第一个结点是链表，则遍历链表查找 while ((e = e.next) != null) { if (e.hash == h && ((ek = e.key) == key || (ek != null && key.equals(ek)))) return e.val; } } return null; } untreeify（无锁) static Node untreeify(Node b) { Node hd = null, tl = null; for (Node q = b; q != null; q = q.next) { Node p = new Node(q.hash, q.key, q.val, null); if (tl == null) hd = p; else tl.next = p; tl = p; } return hd; } remove（有锁) public boolean remove(Object key, Object value) { if (key == null) throw new NullPointerException(); return value != null && replaceNode(key, null, value) != null; } /** * Implementation for the four public remove/replace methods: * Replaces node value with v, conditional upon match of cv if * non-null. If resulting value is null, delete. */ final V replaceNode(Object key, V value, Object cv) { int hash = spread(key.hashCode()); for (Node[] tab = table;;) { Node f; int n, i, fh; if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) & hash)) == null) break; else if ((fh = f.hash) == MOVED) // 如果已经被移动，那么就帮助进行扩容 tab = helpTransfer(tab, f); else { V oldVal = null; boolean validated = false; synchronized (f) { if (tabAt(tab, i) == f) { // 如果是链表，则删除链表中的节点 if (fh >= 0) { validated = true; for (Node e = f, pred = null;;) { K ek; if (e.hash == hash && ((ek = e.key) == key || (ek != null && key.equals(ek)))) { V ev = e.val; if (cv == null || cv == ev || (ev != null && cv.equals(ev))) { oldVal = ev; if (value != null) e.val = value; else if (pred != null) pred.next = e.next; else setTabAt(tab, i, e.next); } break; } pred = e; if ((e = e.next) == null) break; } } // 如果是红黑树，则从红黑树中删除结点 else if (f instanceof TreeBin) { validated = true; TreeBin t = (TreeBin)f; TreeNode r, p; if ((r = t.root) != null && (p = r.findTreeNode(hash, key, null)) != null) { V pv = p.val; if (cv == null || cv == pv || (pv != null && cv.equals(pv))) { oldVal = pv; if (value != null) p.val = value; else if (t.removeTreeNode(p)) setTabAt(tab, i, untreeify(t.first)); } } } } } if (validated) { if (oldVal != null) { if (value == null) addCount(-1L, -1); return oldVal; } break; } } } return null; } containsKey public boolean containsKey(Object key) { return get(key) != null; } containsValue public boolean containsValue(Object value) { if (value == null) throw new NullPointerException(); Node[] t; if ((t = table) != null) { Traverser it = new Traverser(t, t.length, 0, t.length); for (Node p; (p = it.advance()) != null; ) { V v; if ((v = p.val) == value || (v != null && value.equals(v))) return true; } } return false; } static class Traverser { Node[] tab; // current table; updated if resized Node next; // the next entry to use TableStack stack, spare; // to save/restore on ForwardingNodes int index; // index of bin to use next int baseIndex; // current index of initial table int baseLimit; // index bound for initial table final int baseSize; // initial table size Traverser(Node[] tab, int size, int index, int limit) { this.tab = tab; this.baseSize = size; this.baseIndex = this.index = index; this.baseLimit = limit; this.next = null; } /** * Advances if possible, returning next valid node, or null if none. */ final Node advance() { Node e; if ((e = next) != null) e = e.next; for (;;) { Node[] t; int i, n; // must use locals in checks if (e != null) return next = e; if (baseIndex >= baseLimit || (t = tab) == null || (n = t.length) )e).nextTable; e = null; pushState(t, i, n); continue; } else if (e instanceof TreeBin) e = ((TreeBin)e).first; else e = null; } if (stack != null) recoverState(n); else if ((index = i + baseSize) >= n) index = ++baseIndex; // visit upper slots if present } } /** * Saves traversal state upon encountering a forwarding node. */ private void pushState(Node[] t, int i, int n) { TableStack s = spare; // reuse if possible if (s != null) spare = s.next; else s = new TableStack(); s.tab = t; s.length = n; s.index = i; s.next = stack; stack = s; } /** * Possibly pops traversal state. * * @param n length of current table */ private void recoverState(int n) { TableStack s; int len; while ((s = stack) != null && (index += (len = s.length)) >= n) { n = len; index = s.index; tab = s.tab; s.tab = null; TableStack next = s.next; s.next = spare; // save for reuse stack = next; spare = s; } if (s == null && (index += baseSize) >= n) index = ++baseIndex; } } 遍历 public Set> entrySet() { EntrySetView es; return (es = entrySet) != null ? es : (entrySet = new EntrySetView(this)); } public Iterator> iterator() { ConcurrentHashMap m = map; Node[] t; int f = (t = m.table) == null ? 0 : t.length; return new EntryIterator(t, f, 0, f, m); } static class BaseIterator extends Traverser { final ConcurrentHashMap map; Node lastReturned; BaseIterator(Node[] tab, int size, int index, int limit, ConcurrentHashMap map) { super(tab, size, index, limit); this.map = map; advance(); } public final boolean hasNext() { return next != null; } public final boolean hasMoreElements() { return next != null; } public final void remove() { Node p; if ((p = lastReturned) == null) throw new IllegalStateException(); lastReturned = null; map.replaceNode(p.key, null, null); } } static final class EntryIterator extends BaseIterator implements Iterator> { EntryIterator(Node[] tab, int index, int size, int limit, ConcurrentHashMap map) { super(tab, index, size, limit, map); } public final Map.Entry next() { Node p; if ((p = next) == null) throw new NoSuchElementException(); K k = p.key; V v = p.val; lastReturned = p; advance(); return new MapEntry(k, v, map); } } 1.7 分段锁实现 采用 Segment + HashEntry的方式进行实现 put 当执行 put方法插入数据时，根据key的hash值，在 Segment数组中找到相应的位置，如果相应位置的 Segment还未初始化，则通过CAS进行赋值，接着执行 Segment对象的 put方法通过加锁机制插入数据，实现如下： 场景：线程A和线程B同时执行相同 Segment对象的 put方法 1、线程A执行 tryLock()方法成功获取锁，则把 HashEntry对象插入到相应的位置； 2、线程B获取锁失败，则执行 scanAndLockForPut()方法，在 scanAndLockForPut方法中，会通过重复执行 tryLock()方法尝试获取锁，在多处理器环境下，重复次数为64，单处理器重复次数为1，当执行 tryLock()方法的次数超过上限时，则执行 lock()方法挂起线程B； 3、当线程A执行完插入操作时，会通过 unlock()方法释放锁，接着唤醒线程B继续执行； size 因为 ConcurrentHashMap是可以并发插入数据的，所以在准确计算元素时存在一定的难度，一般的思路是统计每个 Segment对象中的元素个数，然后进行累加，但是这种方式计算出来的结果并不一样的准确的，因为在计算后面几个 Segment的元素个数时，已经计算过的 Segment同时可能有数据的插入或则删除。 先采用不加锁的方式，连续计算元素的个数，最多计算3次： 1、如果前后两次计算结果相同，则说明计算出来的元素个数是准确的； 2、如果前后两次计算结果都不同，则给每个 Segment进行加锁，再计算一次元素的个数； ConcurrentSkipListMap ConcurrentSkipListMap有几个ConcurrentHashMap 不能比拟的优点： 　　1、ConcurrentSkipListMap 的key是有序的。 2、ConcurrentSkipListMap 支持更高的并发。ConcurrentSkipListMap 的存取时间是log（N)，和线程数几乎无关。也就是说在数据量一定的情况下，并发的线程越多，ConcurrentSkipListMap越能体现出他的优势。 SkipList 跳表： 跳表是平衡树的一种替代的数据结构，但是和红黑树不相同的是，跳表对于树的平衡的实现是基于一种随机化的算法的，这样也就是说跳表的插入和删除的工作是比较简单的。 下面来研究一下跳表的核心思想： 先从链表开始，如果是一个简单的链表，那么我们知道在链表中查找一个元素I的话，需要将整个链表遍历一次。 如果是说链表是排序的，并且节点中还存储了指向前面第二个节点的指针的话，那么在查找一个节点时，仅仅需要遍历N/2个节点即可。 这基本上就是跳表的核心思想，其实也是一种通过“空间来换取时间”的一个算法，通过在每个节点中增加了向前的指针，从而提升查找的效率。 Map实现类之间的区别 HashMap与ConcurrentHashMap区别 - 1)前者允许key或value为null，后者不允许 - 2)前者不是线程安全的，后者是 HashMap、TreeMap与LinkedHashMap区别 - 1)HashMap遍历时，取得数据的顺序是完全随机的； TreeMap可以按照自然顺序或Comparator排序； LinkedHashMap可以按照插入顺序或访问顺序排序，且get的效率（O(1))比TreeMap（O(logn))更高。 2)HashMap底层基于哈希表，数组+链表/红黑树； TreeMap底层基于红黑树 LinkedHashMap底层基于HashMap与环形双向链表 3)就get和put效率而言，HashMap是最高的，LinkedHashMap次之，TreeMap最次。HashMap与Hashtable区别 扩容策略：Hashtable在不指定容量的情况下的默认容量为11，而HashMap为16，Hashtable不要求底层数组的容量一定要为2的整数次幂（2+1)，而HashMap则要求一定为2的整数次幂（2)。 允许null：Hashtable中key和value都不允许为null，而HashMap中key和value都允许为null（key只能有一个为null，而value则可以有多个为null)。 线程安全：前者不是线程安全的，后者是；ConcurrentHashMap、Collections.synchronizedMap与Hashtable的异同 它们都是同步Map，但三者实现同步的机制不同；后两者都是简单地在方法上加synchronized实现的，锁的粒度较大；前者是基于CAS和synchronized实现的，锁的粒度较小，大部分都是lock-free无锁实现同步的。 ConcurrentHashMap还提供了putIfAbsent同步方法。 3.6 Collections 同步集合包装 public static Map synchronizedMap(Map m) { return new SynchronizedMap<>(m); } private static class SynchronizedMap implements Map, Serializable { private static final long serialVersionUID = 1978198479659022715L; private final Map m; // Backing Map final Object mutex; // Object on which to synchronize SynchronizedMap(Map m) { this.m = Objects.requireNonNull(m); mutex = this; } SynchronizedMap(Map m, Object mutex) { this.m = m; this.mutex = mutex; } public int size() { synchronized (mutex) {return m.size();} } public boolean isEmpty() { synchronized (mutex) {return m.isEmpty();} } public boolean containsKey(Object key) { synchronized (mutex) {return m.containsKey(key);} } public boolean containsValue(Object value) { synchronized (mutex) {return m.containsValue(value);} } public V get(Object key) { synchronized (mutex) {return m.get(key);} } public V put(K key, V value) { synchronized (mutex) {return m.put(key, value);} } public V remove(Object key) { synchronized (mutex) {return m.remove(key);} } public void putAll(Map map) { synchronized (mutex) {m.putAll(map);} } public void clear() { synchronized (mutex) {m.clear();} } private transient Set keySet; private transient Set> entrySet; private transient Collection values; public Set keySet() { synchronized (mutex) { if (keySet==null) keySet = new SynchronizedSet<>(m.keySet(), mutex); return keySet; } } public Set> entrySet() { synchronized (mutex) { if (entrySet==null) entrySet = new SynchronizedSet<>(m.entrySet(), mutex); return entrySet; } } public Collection values() { synchronized (mutex) { if (values==null) values = new SynchronizedCollection<>(m.values(), mutex); return values; } } public boolean equals(Object o) { if (this == o) return true; synchronized (mutex) {return m.equals(o);} } public int hashCode() { synchronized (mutex) {return m.hashCode();} } public String toString() { synchronized (mutex) {return m.toString();} } // Override default methods in Map @Override public V getOrDefault(Object k, V defaultValue) { synchronized (mutex) {return m.getOrDefault(k, defaultValue);} } @Override public void forEach(BiConsumer action) { synchronized (mutex) {m.forEach(action);} } @Override public void replaceAll(BiFunction function) { synchronized (mutex) {m.replaceAll(function);} } @Override public V putIfAbsent(K key, V value) { synchronized (mutex) {return m.putIfAbsent(key, value);} } @Override public boolean remove(Object key, Object value) { synchronized (mutex) {return m.remove(key, value);} } @Override public boolean replace(K key, V oldValue, V newValue) { synchronized (mutex) {return m.replace(key, oldValue, newValue);} } @Override public V replace(K key, V value) { synchronized (mutex) {return m.replace(key, value);} } @Override public V computeIfAbsent(K key, Function mappingFunction) { synchronized (mutex) {return m.computeIfAbsent(key, mappingFunction);} } @Override public V computeIfPresent(K key, BiFunction remappingFunction) { synchronized (mutex) {return m.computeIfPresent(key, remappingFunction);} } @Override public V compute(K key, BiFunction remappingFunction) { synchronized (mutex) {return m.compute(key, remappingFunction);} } @Override public V merge(K key, V value, BiFunction remappingFunction) { synchronized (mutex) {return m.merge(key, value, remappingFunction);} } private void writeObject(ObjectOutputStream s) throws IOException { synchronized (mutex) {s.defaultWriteObject();} } } 不可变集合包装 public static Map unmodifiableMap(Map m) { return new UnmodifiableMap<>(m); } private static class UnmodifiableMap implements Map, Serializable { private static final long serialVersionUID = -1034234728574286014L; private final Map m; UnmodifiableMap(Map m) { if (m==null) throw new NullPointerException(); this.m = m; } public int size() {return m.size();} public boolean isEmpty() {return m.isEmpty();} public boolean containsKey(Object key) {return m.containsKey(key);} public boolean containsValue(Object val) {return m.containsValue(val);} public V get(Object key) {return m.get(key);} public V put(K key, V value) { throw new UnsupportedOperationException(); } public V remove(Object key) { throw new UnsupportedOperationException(); } public void putAll(Map m) { throw new UnsupportedOperationException(); } public void clear() { throw new UnsupportedOperationException(); } private transient Set keySet; private transient Set> entrySet; private transient Collection values; public Set keySet() { if (keySet==null) keySet = unmodifiableSet(m.keySet()); return keySet; } public Set> entrySet() { if (entrySet==null) entrySet = new UnmodifiableEntrySet<>(m.entrySet()); return entrySet; } public Collection values() { if (values==null) values = unmodifiableCollection(m.values()); return values; } public boolean equals(Object o) {return o == this || m.equals(o);} public int hashCode() {return m.hashCode();} public String toString() {return m.toString();} // Override default methods in Map @Override @SuppressWarnings(\"unchecked\") public V getOrDefault(Object k, V defaultValue) { // Safe cast as we don't change the value return ((Map)m).getOrDefault(k, defaultValue); } @Override public void forEach(BiConsumer action) { m.forEach(action); } @Override public void replaceAll(BiFunction function) { throw new UnsupportedOperationException(); } @Override public V putIfAbsent(K key, V value) { throw new UnsupportedOperationException(); } @Override public boolean remove(Object key, Object value) { throw new UnsupportedOperationException(); } @Override public boolean replace(K key, V oldValue, V newValue) { throw new UnsupportedOperationException(); } @Override public V replace(K key, V value) { throw new UnsupportedOperationException(); } @Override public V computeIfAbsent(K key, Function mappingFunction) { throw new UnsupportedOperationException(); } @Override public V computeIfPresent(K key, BiFunction remappingFunction) { throw new UnsupportedOperationException(); } @Override public V compute(K key, BiFunction remappingFunction) { throw new UnsupportedOperationException(); } @Override public V merge(K key, V value, BiFunction remappingFunction) { throw new UnsupportedOperationException(); } 空集合包装 public static final Map emptyMap() { return (Map) EMPTY_MAP; } public static final Map EMPTY_MAP = new EmptyMap<>(); private static class EmptyMap extends AbstractMap implements Serializable { private static final long serialVersionUID = 6428348081105594320L; public int size() {return 0;} public boolean isEmpty() {return true;} public boolean containsKey(Object key) {return false;} public boolean containsValue(Object value) {return false;} public V get(Object key) {return null;} public Set keySet() {return emptySet();} public Collection values() {return emptySet();} public Set> entrySet() {return emptySet();} public boolean equals(Object o) { return (o instanceof Map) && ((Map)o).isEmpty(); } public int hashCode() {return 0;} // Override default methods in Map @Override @SuppressWarnings(\"unchecked\") public V getOrDefault(Object k, V defaultValue) { return defaultValue; } @Override public void forEach(BiConsumer action) { Objects.requireNonNull(action); } @Override public void replaceAll(BiFunction function) { Objects.requireNonNull(function); } @Override public V putIfAbsent(K key, V value) { throw new UnsupportedOperationException(); } @Override public boolean remove(Object key, Object value) { throw new UnsupportedOperationException(); } @Override public boolean replace(K key, V oldValue, V newValue) { throw new UnsupportedOperationException(); } @Override public V replace(K key, V value) { throw new UnsupportedOperationException(); } @Override public V computeIfAbsent(K key, Function mappingFunction) { throw new UnsupportedOperationException(); } @Override public V computeIfPresent(K key, BiFunction remappingFunction) { throw new UnsupportedOperationException(); } @Override public V compute(K key, BiFunction remappingFunction) { throw new UnsupportedOperationException(); } @Override public V merge(K key, V value, BiFunction remappingFunction) { throw new UnsupportedOperationException(); } // Preserves singleton property private Object readResolve() { return EMPTY_MAP; } } Collections.sort public static > void sort(List list) { list.sort(null); } List#sort default void sort(Comparator c) { Object[] a = this.toArray(); Arrays.sort(a, (Comparator) c); ListIterator i = this.listIterator(); for (Object e : a) { i.next(); i.set((E) e); } } public static void sort(T[] a, Comparator c) { if (c == null) { sort(a); } else { if (LegacyMergeSort.userRequested) legacyMergeSort(a, c); else TimSort.sort(a, 0, a.length, c, null, 0, 0); } } public static void sort(Object[] a) { if (LegacyMergeSort.userRequested) legacyMergeSort(a); else ComparableTimSort.sort(a, 0, a.length, null, 0, 0); } 1.7（TimSort) 结合了归并排序和插入排序的混合算法，它基于一个简单的事实，实际中大部分数据都是部分有序（升序或降序)的。 TimSort 算法为了减少对升序部分的回溯和对降序部分的性能倒退，将输入按其升序和降序特点进行了分区。排序的输入的单位不是一个个单独的数字，而是一个个的块-分区。其中每一个分区叫一个run。针对这些 run 序列，每次拿一个 run 出来按规则进行合并。每次合并会将两个 run合并成一个 run。合并的结果保存到栈中。合并直到消耗掉所有的 run，这时将栈上剩余的 run合并到只剩一个 run 为止。这时这个仅剩的 run 便是排好序的结果。 综上述过程，Timsort算法的过程包括 （0)如果数组长度小于某个值，直接用二分插入排序算法 （1)找到各个run，并入栈 （2)按规则合并run /** * Sorts the given range, using the given workspace array slice * for temp storage when possible. This method is designed to be * invoked from public methods (in class Arrays) after performing * any necessary array bounds checks and expanding parameters into * the required forms. * * @param a the array to be sorted * @param lo the index of the first element, inclusive, to be sorted * @param hi the index of the last element, exclusive, to be sorted * @param work a workspace array (slice) * @param workBase origin of usable space in work array * @param workLen usable size of work array * @since 1.8 */ static void sort(Object[] a, int lo, int hi, Object[] work, int workBase, int workLen) { assert a != null && lo >= 0 && lo /** * Creates a TimSort instance to maintain the state of an ongoing sort. * * @param a the array to be sorted * @param work a workspace array (slice) * @param workBase origin of usable space in work array * @param workLen usable size of work array */ private ComparableTimSort(Object[] a, Object[] work, int workBase, int workLen) { this.a = a; // Allocate temp storage (which may be increased later if necessary) int len = a.length; int tlen = (len >> 1 : INITIAL_TMP_STORAGE_LENGTH; if (work == null || workLen work.length) { tmp = new Object[tlen]; tmpBase = 0; tmpLen = tlen; } else { tmp = work; tmpBase = workBase; tmpLen = workLen; } /* * Allocate runs-to-be-merged stack (which cannot be expanded). The * stack length requirements are described in listsort.txt. The C * version always uses the same stack length (85), but this was * measured to be too expensive when sorting \"mid-sized\" arrays (e.g., * 100 elements) in Java. Therefore, we use smaller (but sufficiently * large) stack lengths for smaller arrays. The \"magic numbers\" in the * computation below must be changed if MIN_MERGE is decreased. See * the MIN_MERGE declaration above for more information. * The maximum value of 49 allows for an array up to length * Integer.MAX_VALUE-4, if array is filled by the worst case stack size * increasing scenario. More explanations are given in section 4 of: * http://envisage-project.eu/wp-content/uploads/2015/02/sorting.pdf */ int stackLen = (len 1.6 （MergeSort) private static void legacyMergeSort(Object[] a) { Object[] aux = a.clone(); mergeSort(aux, a, 0, a.length, 0); } private static void mergeSort(Object[] src, Object[] dest, int low, int high, int off) { int length = high - low; // 7 // Insertion sort on smallest arrays if (length low && ((Comparable) dest[j-1]).compareTo(dest[j])>0; j--) swap(dest, j, j-1); return; } // Recursively sort halves of dest into src int destLow = low; int destHigh = high; low += off; high += off; int mid = (low + high) >>> 1; mergeSort(dest, src, low, mid, -off); mergeSort(dest, src, mid, high, -off); // If list is already sorted, just copy from src to dest. This is an // optimization that results in faster sorts for nearly ordered lists. if (((Comparable)src[mid-1]).compareTo(src[mid]) = high || p 3.7 Fail-Fast 在ArrayList,LinkedList,HashMap等等的内部实现增，删，改中我们总能看到modCount的身影，modCount字面意思就是修改次数，但为什么要记录modCount的修改次数呢？ 所有使用modCount属性集合的都是线程不安全的。 在一个迭代器初始的时候会赋予它调用这个迭代器的对象的modCount，在迭代器遍历的过程中，一旦发现这个对象的modCount和迭代器中存储的modCount不一样那就抛异常。 它是 java 集合的一种错误检测机制，当多个线程对集合进行结构上的改变的操作时，有可能会产生 fail-fast。 例如 ：假设存在两个线程（线程 1、线程 2)，线程 1 通过 Iterator 在遍历集合 A 中的元素，在某个时候线程 2 修改了集合 A 的结构（是结构上面的修改，而不是简单的修改集合元素的内容)，那么这个时候程序就会抛出 ConcurrentModificationException 异常，从而产生 fail-fast 机制。 原因： 迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变 modCount 的值。 每当迭代器使用 hashNext()/next() 遍历下一个元素之前，都会检测 modCount 变量是否为 expectedmodCount 值，是的话就返回遍历；否则抛出异常，终止遍历。 解决办法：使用线程安全的集合 "},"zother5-Java-Interview/九、操作系统.html":{"url":"zother5-Java-Interview/九、操作系统.html","title":"九、操作系统","keywords":"","body":"第 6 章 操作系统 第 6.1 节 计算机操作系统 概述 基本特征 1.并发 并发是指宏观上在一段时间内能同时运行多个程序，而并行则指同一时刻能运行多个指令。 并行需要硬件支持，如多流水线、多核处理器或者分布式计算系统。 操作系统通过引入进程和线程，使得程序能够并发运行。 2.共享 共享是指系统中的资源可以被多个并发进程共同使用。有两种共享方式：互斥共享和同时共享。 互斥共享的资源称为临界资源，例如打印机等，在同一时刻只允许一个进程访问，需要用同步机制来实现互斥访问。 3.虚拟 虚拟技术把一个物理实体转换为多个逻辑实体。 主要有两种虚拟技术：时（时间）分复用技术和空（空间）分复用技术。 多个进程能在同一个处理器上并发执行使用了时分复用技术，让每个进程轮流占用处理器，每次只执行一小个时间片 并快速切换。 虚拟内存使用了空分复用技术，它将物理内存抽象为地址空间，每个进程都有各自的地址空间。地址空间的页被映射 到物理内存，地址空间的页并不需要全部在物理内存中，当使用到一个没有在物理内存的页时，执行页面置换算法， 将该页置换到内存中。 4.异步 异步指进程不是一次性执行完毕，而是走走停停，以不可知的速度向前推进。 基本功能 1.进程管理 进程控制、进程同步、进程通信、死锁处理、处理机调度等。 2.内存管理 内存分配、地址映射、内存保护与共享、虚拟内存等。 3.文件管理 文件存储空间的管理、目录管理、文件读写管理和保护等。 4.设备管理 完成用户的 I/O 请求，方便用户使用各种设备，并提高设备的利用率。主要包括缓冲管理、设备分配、设备处理、虛拟设备等。 系统调用 如果一个进程在用户态需要使用内核态的功能，就进行系统调用从而陷入内核，由操作系统代为完成。 Linux 的系统调用主要有以下这些： Task Commands 进程控制 fork(); exit(); wait(); 进程通信 pipe(); shmget(); mmap(); 文件操作 open(); read(); write(); 设备操作 ioctl(); read(); write(); 信息维护 getpid(); alarm(); sleep(); 安全 chmod(); umask(); chown(); 大内核和微内核 1.大内核 大内核是将操作系统功能作为一个紧密结合的整体放到内核。由于各模块共享信息，因此有很高的性能。 2.微内核 由于操作系统不断复杂，因此将一部分操作系统功能移出内核，从而降低内核的复杂性。移出的部分根据分层的原则 划分成若干服务，相互独立。 在微内核结构下，操作系统被划分成小的、定义良好的模块，只有微内核这一个模块运行在内核态，其余模块运行在 用户态。 因为需要频繁地在用户态和核心态之间进行切换，所以会有一定的性能损失。 中断分类 1.外中断 由 CPU 执行指令以外的事件引起，如 I/O 完成中断，表示设备输入/输出处理已经完成，处理器能够发送下一个输入/输出请求。此外还有时钟中断、控制台中断等。 2.异常 由 CPU 执行指令的内部事件引起，如非法操作码、地址越界、算术溢出等。 3.陷入 在用户程序中使用系统调用。 微信公众号 更多精彩内容将发布在微信公众号 Python看世界 上，公众号也提供了一份技术面试复习大纲，不仅系统整理了面试知识点，而且标注了各个知识点的重要程度，从而帮你理清多而杂的面试知识点，后台回复 \"大纲\"即可领取。可以根据大纲上列的知识点来进行复习，就不用看很多不重要的内容，也可以知道哪些内容很重要从而多安排一些 复习时间。 进程管理 进程与线程 1.进程 进程是资源分配的基本单位。 进程控制块 (Process Control Block, PCB) 描述进程的基本信息和运行状态，所谓的创建进程和撤销进程，都是指对 PCB 的操作。 下图显示了 4 个程序创建了 4 个进程，这 4 个进程可以并发地执行。 2.线程 线程是独立调度的基本单位。 一个进程中可以有多个线程，它们共享进程资源。 QQ 和浏览器是两个进程，浏览器进程里面有很多线程，例如 HTTP 请求线程、事件响应线程、渲染线程等等，线程的并发执行使得在浏览器中点击一个新链接从而发起 HTTP 请求时，浏览器还可以响应用户的其它事件。 3.区别 Ⅰ 拥有资源 进程是资源分配的基本单位，但是线程不拥有资源，线程可以访问隶属进程的资源。 Ⅱ 调度 线程是独立调度的基本单位，在同一进程中，线程的切换不会引起进程切换，从一个进程中的线程切换到另一个进程 中的线程时，会引起进程切换。 Ⅲ 系统开销 由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O 设备等，所付出的开销远大于创建或撤销线程时的开销。类似地，在进行进程切换时，涉及当前执行进程 CPU 环境的保存及新调度进程 CPU 环境的设置，而线程切换时只需保存和设置少量寄存器内容，开销很小。 Ⅳ 通信方面 线程间可以通过直接读写同一进程中的数据进行通信，但是进程通信需要借助 IPC。 进程状态的切换 就绪状态（ready）：等待被调度运行状态（running） 阻塞状态（waiting）：等待资源 应该注意以下内容： 只有就绪态和运行态可以相互转换，其它的都是单向转换。就绪状态的进程通过调度算法从而获得 CPU 时间， 转为运行状态；而运行状态的进程，在分配给它的 CPU 时间片用完之后就会转为就绪状态，等待下一次调度。阻塞状态是缺少需要的资源从而由运行状态转换而来，但是该资源不包括 CPU 时间，缺少 CPU 时间会从运行态转换为就绪态。 进程调度算法 不同环境的调度算法目标不同，因此需要针对不同环境来讨论调度算法。 1.批处理系统 批处理系统没有太多的用户操作，在该系统中，调度算法目标是保证吞吐量和周转时间（从提交到终止的时间）。 1.1先来先服务 ﬁrst-come ﬁrst-serverd（FCFS） 非抢占式的调度算法，按照请求的顺序进行调度。 有利于长作业，但不利于短作业，因为短作业必须一直等待前面的长作业执行完毕才能执行，而长作业又需要执行很 长时间，造成了短作业等待时间过长。 1.2短作业优先 shortest job ﬁrst（SJF） 非抢占式的调度算法，按估计运行时间最短的顺序进行调度。 长作业有可能会饿死，处于一直等待短作业执行完毕的状态。因为如果一直有短作业到来，那么长作业永远得不到调 度。 1.3最短剩余时间优先 shortest remaining time next（SRTN） 最短作业优先的抢占式版本，按剩余运行时间的顺序进行调度。 当一个新的作业到达时，其整个运行时间与当前进程的剩余时间作比较。如果新的进程需要的时间更少，则挂起当前进程，运行新的进程。否则新的进程等待。 2.交互式系统 交互式系统有大量的用户交互操作，在该系统中调度算法的目标是快速地进行响应。 2.1时间片轮转 将所有就绪进程按 FCFS 的原则排成一个队列，每次调度时，把 CPU 时间分配给队首进程，该进程可以执行一个时间片。当时间片用完时，由计时器发出时钟中断，调度程序便停止该进程的执行，并将它送往就绪队列的末尾，同时 继续把 CPU 时间分配给队首的进程。 时间片轮转算法的效率和时间片的大小有很大关系： 因为进程切换都要保存进程的信息并且载入新进程的信息，如果时间片太小，会导致进程切换得太频繁，在进 程切换上就会花过多时间。 而如果时间片过长，那么实时性就不能得到保证。 2.2优先级调度 为每个进程分配一个优先级，按优先级进行调度。 为了防止低优先级的进程永远等不到调度，可以随着时间的推移增加等待进程的优先级。 2.3多级反馈队列 一个进程需要执行 100 个时间片，如果采用时间片轮转调度算法，那么需要交换 100 次。 多级队列是为这种需要连续执行多个时间片的进程考虑，它设置了多个队列，每个队列时间片大小都不同，例如 1,2,4,8,..。进程在第一个队列没执行完，就会被移到下一个队列。这种方式下，之前的进程只需要交换 7 次。 每个队列优先权也不同，最上面的优先权最高。因此只有上一个队列没有进程在排队，才能调度当前队列上的进程。 可以将这种调度算法看成是时间片轮转调度算法和优先级调度算法的结合。 3.实时系统 实时系统要求一个请求在一个确定时间内得到响应。 分为硬实时和软实时，前者必须满足绝对的截止时间，后者可以容忍一定的超时。 进程同步 1.临界区 对临界资源进行访问的那段代码称为临界区。 为了互斥访问临界资源，每个进程在进入临界区之前，需要先进行检查。 2.同步与互斥 同步：多个进程因为合作产生的直接制约关系，使得进程有一定的先后执行关系。 互斥：多个进程在同一时刻只有一个进程能进入临界区。 3.信号量 信号量（Semaphore）是一个整型变量，可以对其执行 down 和 up 操作，也就是常见的 P 和 V 操作。 down : 如果信号量大于 0 ，执行 -1 操作；如果信号量等于 0，进程睡眠，等待信号量大于 0； up ：对信号量执行 +1 操作，唤醒睡眠的进程让其完成 down 操作。 down 和 up 操作需要被设计成原语，不可分割，通常的做法是在执行这些操作的时候屏蔽中断。 如果信号量的取值只能为 0 或者 1，那么就成为了 互斥量（Mutex） ，0 表示临界区已经加锁，1 表示临界区解锁。 使用信号量实现生产者-消费者问题 问题描述：使用一个缓冲区来保存物品，只有缓冲区没有满，生产者才可以放入物品；只有缓冲区不为空，消费者才 可以拿走物品。 因为缓冲区属于临界资源，因此需要使用一个互斥量 mutex 来控制对缓冲区的互斥访问。 为了同步生产者和消费者的行为，需要记录缓冲区中物品的数量。数量可以使用信号量来进行统计，这里需要使用两 个信号量：empty 记录空缓冲区的数量，full 记录满缓冲区的数量。其中，empty 信号量是在生产者进程中使用， 当 empty 不为 0 时，生产者才可以放入物品；full 信号量是在消费者进程中使用，当 full 信号量不为 0 时，消费者才可以取走物品。 注意，不能先对缓冲区进行加锁，再测试信号量。也就是说，不能先执行 down(mutex) 再执行 down(empty)。如果这么做了，那么可能会出现这种情况：生产者对缓冲区加锁后，执行 down(empty) 操作，发现 empty = 0，此时生产者睡眠。消费者不能进入临界区，因为生产者对缓冲区加锁了，消费者就无法执行 up(empty) 操作，empty 永远都为 0，导致生产者永远等待下，不会释放锁，消费者因此也会永远等待下去。 4.管程 使用信号量机制实现的生产者消费者问题需要客户端代码做很多控制，而管程把控制的代码独立出来，不仅不容易出 错，也使得客户端代码调用更容易。 c 语言不支持管程，下面的示例代码使用了类 Pascal 语言来描述管程。示例代码的管程提供了 insert() 和 remove() 方法，客户端代码通过调用这两个方法来解决生产者-消费者问题。 管程有一个重要特性：在一个时刻只能有一个进程使用管程。进程在无法继续执行的时候不能一直占用管程，否则其 它进程永远不能使用管程。 管程引入了 条件变量 以及相关的操作：wait() 和 signal() 来实现同步操作。对条件变量执行 wait() 操作会导致调用进程阻塞，把管程让出来给另一个进程持有。signal() 操作用于唤醒被阻塞的进程。 使用管程实现生产者-消费者问题 经典同步问题 生产者和消费者问题前面已经讨论过了。 1.读者-写者问题 允许多个进程同时对数据进行读操作，但是不允许读和写以及写和写操作同时发生。 一个整型变量 count 记录在对数据进行读操作的进程数量，一个互斥量 count_mutex 用于对 count 加锁，一个互斥量 data_mutex 用于对读写的数据加锁。 以下内容由 @Bandi Yugandhar 提供。 The ﬁrst case may result Writer to starve. This case favous Writers i.e no writer, once added to the queue, shall be kept waiting longer than absolutely necessary(only when there are readers that entered the queue before the writer). We can observe that every reader is forced to acquire ReadLock. On the otherhand, writers doesn’t need to lock individually. Once the ﬁrst writer locks the ReadLock, it will be released only when there is no writer left in the queue. From the both cases we observed that either reader or writer has to starve. Below solutionadds the constraint that no thread shall be allowed to starve; that is, the operation of obtaining a lock on the shared data will always terminate in a bounded amount of time. 2.哲学家进餐问题 五个哲学家围着一张圆桌，每个哲学家面前放着食物。哲学家的生活有两种交替活动：吃饭以及思考。当一个哲学家 吃饭时，需要先拿起自己左右两边的两根筷子，并且一次只能拿起一根筷子。 下面是一种错误的解法，考虑到如果所有哲学家同时拿起左手边的筷子，那么就无法拿起右手边的筷子，造成死锁。 为了防止死锁的发生，可以设置两个条件： 必须同时拿起左右两根筷子； 只有在两个邻居都没有进餐的情况下才允许进餐。 进程通信 进程同步与进程通信很容易混淆，它们的区别在于： 进程同步：控制多个进程按一定顺序执行； 进程通信：进程间传输信息。 进程通信是一种手段，而进程同步是一种目的。也可以说，为了能够达到进程同步的目的，需要让进程进行通信，传 输一些进程同步所需要的信息。 1.管道 管道是通过调用 pipe 函数创建的，fd[0] 用于读，fd[1] 用于写。 它具有以下限制： 只支持半双工通信（单向交替传输）； 只能在父子进程或者兄弟进程中使用。 2.FIFO 也称为命名管道，去除了管道只能在父子进程中使用的限制。 FIFO 常用于客户-服务器应用程序中，FIFO 用作汇聚点，在客户进程和服务器进程之间传递数据。 3.消息队列 相比于 FIFO，消息队列具有以下优点： 消息队列可以独立于读写进程存在，从而避免了 FIFO 中同步管道的打开和关闭时可能产生的困难； 避免了 FIFO 的同步阻塞问题，不需要进程自己提供同步方法； 读进程可以根据消息类型有选择地接收消息，而不像 FIFO 那样只能默认地接收。 4.信号量 它是一个计数器，用于为多个进程提供对共享数据对象的访问。 5.共享存储 允许多个进程共享一个给定的存储区。因为数据不需要在进程之间复制，所以这是最快的一种 IPC。需要使用信号量用来同步对共享存储的访问。 多个进程可以将同一个文件映射到它们的地址空间从而实现共享内存。另外 XSI 共享内存不是使用文件，而是使用内存的匿名段。 6.套接字 与其它通信机制不同的是，它可用于不同机器间的进程通信。 微信公众号 更多精彩内容将发布在微信公众号 Python看世界 上，公众号也提供了一份技术面试复习大纲，不仅系统整理了面试知识点，而且标注了各个知识点的重要程度，从而帮你理清多而杂的面试知识点，后台回复 \"大纲\"即可领取。可以根据大纲上列的知识点来进行复习，就不用看很多不重要的内容，也可以知道哪些内容很重要从而多安排一些 复习时间。 死锁 必要条件 互斥：每个资源要么已经分配给了一个进程，要么就是可用的。占有和等待：已经得到了某个资源的进程可以再请求新的资源。 不可抢占：已经分配给一个进程的资源不能强制性地被抢占，它只能被占有它的进程显式地释放。 环路等待：有两个或者两个以上的进程组成一条环路，该环路中的每个进程都在等待下一个进程所占有的资 源。 处理方法 主要有以下四种方法： 鸵鸟策略 死锁检测与死锁恢复死锁预防 死锁避免 鸵鸟策略 把头埋在沙子里，假装根本没发生问题。 因为解决死锁问题的代价很高，因此鸵鸟策略这种不采取任务措施的方案会获得更高的性能。 当发生死锁时不会对用户造成多大影响，或发生死锁的概率很低，可以采用鸵鸟策略。 大多数操作系统，包括 Unix，Linux 和 Windows，处理死锁问题的办法仅仅是忽略它。 死锁检测与死锁恢复 不试图阻止死锁，而是当检测到死锁发生时，采取措施进行恢复。 1.每种类型一个资源的死锁检测 上图为资源分配图，其中方框表示资源，圆圈表示进程。资源指向进程表示该资源已经分配给该进程，进程指向资源 表示进程请求获取该资源。 图 a 可以抽取出环，如图 b，它满足了环路等待条件，因此会发生死锁。 每种类型一个资源的死锁检测算法是通过检测有向图是否存在环来实现，从一个节点出发进行深度优先搜索，对访问 过的节点进行标记，如果访问了已经标记的节点，就表示有向图存在环，也就是检测到死锁的发生。 2.每种类型多个资源的死锁检测 上图中，有三个进程四个资源，每个数据代表的含义如下： E 向量：资源总量 A 向量：资源剩余量 C 矩阵：每个进程所拥有的资源数量，每一行都代表一个进程拥有资源的数量 R 矩阵：每个进程请求的资源数量 进程 P1 和 P2 所请求的资源都得不到满足，只有进程 P3 可以，让 P3 执行，之后释放 P3 拥有的资源，此时 A = (2 2 2 0)。P2 可以执行，执行后释放 P2 拥有的资源，A = (4 2 2 1) 。P1 也可以执行。所有进程都可以顺利执行，没有死锁。 算法总结如下： 每个进程最开始时都不被标记，执行过程有可能被标记。当算法结束时，任何没有被标记的进程都是死锁进程。 1.寻找一个没有标记的进程 Pi，它所请求的资源小于等于 A。 2.如果找到了这样一个进程，那么将 C 矩阵的第 i 行向量加到 A 中，标记该进程，并转回 1。 3.如果没有这样一个进程，算法终止。 3.死锁恢复 利用抢占恢复利用回滚恢复 通过杀死进程恢复 死锁预防 在程序运行之前预防发生死锁。 1.破坏互斥条件 例如假脱机打印机技术允许若干个进程同时输出，唯一真正请求物理打印机的进程是打印机守护进程。 2.破坏占有和等待条件 一种实现方式是规定所有进程在开始执行前请求所需要的全部资源。 3.破坏不可抢占条件 4.破坏环路等待 给资源统一编号，进程只能按编号顺序来请求资源。 死锁避免 在程序运行时避免发生死锁。 1.安全状态 图 a 的第二列 Has 表示已拥有的资源数，第三列 Max 表示总共需要的资源数，Free 表示还有可以使用的资源数。从图 a 开始出发，先让 B 拥有所需的所有资源（图 b），运行结束后释放 B，此时 Free 变为 5（图 c）；接着以同样的方式运行 C 和 A，使得所有进程都能成功运行，因此可以称图 a 所示的状态时安全的。 定义：如果没有死锁发生，并且即使所有进程突然请求对资源的最大需求，也仍然存在某种调度次序能够使得每一个 进程运行完毕，则称该状态是安全的。 安全状态的检测与死锁的检测类似，因为安全状态必须要求不能发生死锁。下面的银行家算法与死锁检测算法非常类 似，可以结合着做参考对比。 2.单个资源的银行家算法 一个小城镇的银行家，他向一群客户分别承诺了一定的贷款额度，算法要做的是判断对请求的满足是否会进入不安全 状态，如果是，就拒绝请求；否则予以分配。 上图 c 为不安全状态，因此算法会拒绝之前的请求，从而避免进入图 c 中的状态。 3.多个资源的银行家算法 上图中有五个进程，四个资源。左边的图表示已经分配的资源，右边的图表示还需要分配的资源。最右边的 E、P 以及 A 分别表示：总资源、已分配资源以及可用资源，注意这三个为向量，而不是具体数值，例如 A=(1020)，表示 4 个资源分别还剩下 1/0/2/0。 检查一个状态是否安全的算法如下： 查找右边的矩阵是否存在一行小于等于向量 A。如果不存在这样的行，那么系统将会发生死锁，状态是不安全的。 假若找到这样一行，将该进程标记为终止，并将其已分配资源加到 A 中。重复以上两步，直到所有进程都标记为终止，则状态时安全的。 如果一个状态不是安全的，需要拒绝进入这个状态。 微信公众号 更多精彩内容将发布在微信公众号 Python看世界 上，公众号也提供了一份技术面试复习大纲，不仅系统整理了面试知识点，而且标注了各个知识点的重要程度，从而帮你理清多而杂的面试知识点，后台回复 \"大纲\"即可领取。可以根据大纲上列的知识点来进行复习，就不用看很多不重要的内容，也可以知道哪些内容很重要从而多安排一些 复习时间。 内存管理 虚拟内存 虚拟内存的目的是为了让物理内存扩充成更大的逻辑内存，从而让程序获得更多的可用内存。 为了更好的管理内存，操作系统将内存抽象成地址空间。每个程序拥有自己的地址空间，这个地址空间被分割成多个 块，每一块称为一页。这些页被映射到物理内存，但不需要映射到连续的物理内存，也不需要所有页都必须在物理内 存中。当程序引用到不在物理内存中的页时，由硬件执行必要的映射，将缺失的部分装入物理内存并重新执行失败的 指令。 从上面的描述中可以看出，虚拟内存允许程序不用将地址空间中的每一页都映射到物理内存，也就是说一个程序不需 要全部调入内存就可以运行，这使得有限的内存运行大程序成为可能。例如有一台计算机可以产生 16 位地址，那么一个程序的地址空间范围是 0~64K。该计算机只有 32KB 的物理内存，虚拟内存技术允许该计算机运行一个 64K 大小的程序。 分页系统地址映射 内存管理单元（MMU）管理着地址空间和物理内存的转换，其中的页表（Page table）存储着页（程序地址空间） 和页框（物理内存空间）的映射表。 一个虚拟地址分成两个部分，一部分存储页面号，一部分存储偏移量。 下图的页表存放着 16 个页，这 16 个页需要用 4 个比特位来进行索引定位。例如对于虚拟地址（0010 000000000100），前 4 位是存储页面号 2，读取表项内容为（110 1），页表项最后一位表示是否存在于内存中，1 表示存在。后 12 位存储偏移量。这个页对应的页框的地址为 （110 000000000100）。 页面置换算法 在程序运行过程中，如果要访问的页面不在内存中，就发生缺页中断从而将该页调入内存中。此时如果内存已无空闲 空间，系统必须从内存中调出一个页面到磁盘对换区中来腾出空间。 页面置换算法和缓存淘汰策略类似，可以将内存看成磁盘的缓存。在缓存系统中，缓存的大小有限，当有新的缓存到 达时，需要淘汰一部分已经存在的缓存，这样才有空间存放新的缓存数据。 页面置换算法的主要目标是使页面置换频率最低（也可以说缺页率最低）。 1.最佳 OPT, Optimal replacement algorithm 所选择的被换出的页面将是最长时间内不再被访问，通常可以保证获得最低的缺页率。 是一种理论上的算法，因为无法知道一个页面多长时间不再被访问。 举例：一个系统为某进程分配了三个物理块，并有如下页面引用序列： 开始运行时，先将 7, 0, 1 三个页面装入内存。当进程要访问页面 2 时，产生缺页中断，会将页面 7 换出，因为页面 7 再次被访问的时间最长。 2.最近最久未使用 LRU, Least Recently Used 虽然无法知道将来要使用的页面情况，但是可以知道过去使用页面的情况。LRU 将最近最久未使用的页面换出。 为了实现 LRU，需要在内存中维护一个所有页面的链表。当一个页面被访问时，将这个页面移到链表表头。这样就能保证链表表尾的页面是最近最久未访问的。 因为每次访问都需要更新链表，因此这种方式实现的 LRU 代价很高。 3.最近未使用 NRU, Not Recently Used 每个页面都有两个状态位：R 与 M，当页面被访问时设置页面的 R=1，当页面被修改时设置 M=1。其中 R 位会定时被清零。可以将页面分成以下四类： R=0，M=0 R=0，M=1 R=1，M=0 R=1，M=1 当发生缺页中断时，NRU 算法随机地从类编号最小的非空类中挑选一个页面将它换出。 NRU 优先换出已经被修改的脏页面（R=0，M=1），而不是被频繁使用的干净页面（R=1，M=0）。 4.先进先出 FIFO, First In First Out 选择换出的页面是最先进入的页面。 该算法会将那些经常被访问的页面也被换出，从而使缺页率升高。 5.第二次机会算法 FIFO 算法可能会把经常使用的页面置换出去，为了避免这一问题，对该算法做一个简单的修改： 当页面被访问 (读或写) 时设置该页面的 R 位为 1。需要替换的时候，检查最老页面的 R 位。如果 R 位是 0，那么这个页面既老又没有被使用，可以立刻置换掉；如果是 1，就将 R 位清 0，并把该页面放到链表的尾端，修改它的装入时间使它就像刚装入的一样，然后继续从链表的头部开始搜索。 6.时钟 Clock 第二次机会算法需要在链表中移动页面，降低了效率。时钟算法使用环形链表将页面连接起来，再使用一个指针指向 最老的页面。 分段 虚拟内存采用的是分页技术，也就是将地址空间划分成固定大小的页，每一页再与内存进行映射。 下图为一个编译器在编译过程中建立的多个表，有 4 个表是动态增长的，如果使用分页系统的一维地址空间，动态增长的特点会导致覆盖问题的出现。 分段的做法是把每个表分成段，一个段构成一个独立的地址空间。每个段的长度可以不同，并且可以动态增长。 段页式 程序的地址空间划分成多个拥有独立地址空间的段，每个段上的地址空间划分成大小相同的页。这样既拥有分段系统 的共享和保护，又拥有分页系统的虚拟内存功能。 分页与分段的比较 对程序员的透明性：分页透明，但是分段需要程序员显式划分每个段。地址空间的维度：分页是一维地址空间，分段是二维的。 大小是否可以改变：页的大小不可变，段的大小可以动态改变。 出现的原因：分页主要用于实现虚拟内存，从而获得更大的地址空间；分段主要是为了使程序和数据可以被划 分为逻辑上独立的地址空间并且有助于共享和保护。 微信公众号 更多精彩内容将发布在微信公众号 Python看世界 上，公众号也提供了一份技术面试复习大纲，不仅系统整理了面试知识点，而且标注了各个知识点的重要程度，从而帮你理清多而杂的面试知识点，后台回复 \"大纲\"即可领取。可以根据大纲上列的知识点来进行复习，就不用看很多不重要的内容，也可以知道哪些内容很重要从而多安排一些 复习时间。 设备管理 磁盘结构 盘面（Platter）：一个磁盘有多个盘面； 磁道（Track）：盘面上的圆形带状区域，一个盘面可以有多个磁道； 扇区（Track Sector）：磁道上的一个弧段，一个磁道可以有多个扇区，它是最小的物理储存单位，目前主要有512 bytes 与 4 K 两种大小； 磁头（Head）：与盘面非常接近，能够将盘面上的磁场转换为电信号（读），或者将电信号转换为盘面的磁场 （写）； 制动手臂（Actuator arm）：用于在磁道之间移动磁头； 主轴（Spindle）：使整个盘面转动。 磁盘调度算法 读写一个磁盘块的时间的影响因素有： 旋转时间（主轴转动盘面，使得磁头移动到适当的扇区上） 寻道时间（制动手臂移动，使得磁头移动到适当的磁道上） 实际的数据传输时间 其中，寻道时间最长，因此磁盘调度的主要目标是使磁盘的平均寻道时间最短。 1.先来先服务 FCFS, First Come First Served 按照磁盘请求的顺序进行调度。 优点是公平和简单。缺点也很明显，因为未对寻道做任何优化，使平均寻道时间可能较长。 2.最短寻道时间优先 SSTF, Shortest Seek Time First 优先调度与当前磁头所在磁道距离最近的磁道。 虽然平均寻道时间比较低，但是不够公平。如果新到达的磁道请求总是比一个在等待的磁道请求近，那么在等待的磁 道请求会一直等待下去，也就是出现饥饿现象。具体来说，两端的磁道请求更容易出现饥饿现象。 3.电梯算法 SCAN 电梯总是保持一个方向运行，直到该方向没有请求为止，然后改变运行方向。 电梯算法（扫描算法）和电梯的运行过程类似，总是按一个方向来进行磁盘调度，直到该方向上没有未完成的磁盘请 求，然后改变方向。 因为考虑了移动方向，因此所有的磁盘请求都会被满足，解决了 SSTF 的饥饿问题。 微信公众号 更多精彩内容将发布在微信公众号 Python看世界 上，公众号也提供了一份技术面试复习大纲，不仅系统整理了面试知识点，而且标注了各个知识点的重要程度，从而帮你理清多而杂的面试知识点，后台回复 \"大纲\"即可领取。可以根据大纲上列的知识点来进行复习，就不用看很多不重要的内容，也可以知道哪些内容很重要从而多安排一些 复习时间。 链接 编译系统 以下是一个 hello.c 程序： 在 Unix 系统上，由编译器把源文件转换为目标文件。 这个过程大致如下： 预处理阶段：处理以 # 开头的预处理命令； 编译阶段：翻译成汇编文件； 汇编阶段：将汇编文件翻译成可重定位目标文件； 链接阶段：将可重定位目标文件和 printf.o 等单独预编译好的目标文件进行合并，得到最终的可执行目标文件。 静态链接 静态链接器以一组可重定位目标文件为输入，生成一个完全链接的可执行目标文件作为输出。链接器主要完成以下两 个任务： 符号解析：每个符号对应于一个函数、一个全局变量或一个静态变量，符号解析的目的是将每个符号引用与一 个符号定义关联起来。 重定位：链接器通过把每个符号定义与一个内存位置关联起来，然后修改所有对这些符号的引用，使得它们指 向这个内存位置。 目标文件 可执行目标文件：可以直接在内存中执行； 可重定位目标文件：可与其它可重定位目标文件在链接阶段合并，创建一个可执行目标文件； 共享目标文件：这是一种特殊的可重定位目标文件，可以在运行时被动态加载进内存并链接； 动态链接 静态库有以下两个问题： 当静态库更新时那么整个程序都要重新进行链接； 对于 printf 这种标准函数库，如果每个程序都要有代码，这会极大浪费资源。 共享库是为了解决静态库的这两个问题而设计的，在 Linux 系统中通常用 .so 后缀来表示，Windows 系统上它们被称为 DLL。它具有以下特点： 在给定的文件系统中一个库只有一个文件，所有引用该库的可执行目标文件都共享这个文件，它不会被复制到 引用它的可执行文件中； 在内存中，一个共享库的 .text 节（已编译程序的机器代码）的一个副本可以被不同的正在运行的进程共享。 微信公众号 更多精彩内容将发布在微信公众号 Python看世界 上，公众号也提供了一份技术面试复习大纲，不仅系统整理了面试知识点，而且标注了各个知识点的重要程度，从而帮你理清多而杂的面试知识点，后台回复 \"大纲\"即可领取。可以根据大纲上列的知识点来进行复习，就不用看很多不重要的内容，也可以知道哪些内容很重要从而多安排一些 复习时间。 参考资料 Tanenbaum A S, Bos H. Modern operating systems[M]. Prentice Hall Press, 2014. 汤子瀛, 哲凤屏, 汤小丹. 计算机操作系统[M]. 西安电子科技大学出版社, 2001. Bryant, R. E., & O’Hallaron, D. R. (2004). 深入理解计算机系统. 史蒂文斯. UNIX 环境高级编程 [M]. 人民邮电出版社, 2014. Operating System Notes Operating-System Structures Processes Inter Process Communication Presentation[1] Decoding UCS Invicta – Part 1 "},"zother5-Java-Interview/二、设计模式.html":{"url":"zother5-Java-Interview/二、设计模式.html","title":"二、设计模式","keywords":"","body":"设计模式 2.1 设计原则 单一职责原则 不要存在多于一个导致类变更的原因。 总结：一个类只负责一项职责。里氏替换原则 1.子类可以实现父类的抽象方法，但不能覆盖父类的非抽象方法。 2.子类中可以增加自己特有的方法。 3.当子类的方法重载父类的方法时，方法的前置条件（即方法的形参)要比父类方法的输入参数更宽松。 4.当子类的方法实现父类的抽象方法时，方法的后置条件（即方法的返回值)要比父类更严格。 总结：所有引用父类的地方必须能透明地使用其子类对象依赖倒置原则/面向接口编程 高层模块不应该依赖低层模块，二者都应该依赖其抽象；抽象不应该依赖细节；细节应该依赖抽象。接口隔离原则 使用多个专门的接口来替代一个统一的接口； 一个类对另一个类的依赖应建立在最小的接口上迪米特法则 一个类对自己依赖的类知道的越少越好。也就是说，对于被依赖的类来说，无论逻辑多么复杂，都尽量地的将逻辑封装在类的内部开闭原则 对扩展开放，对修改关闭 用抽象构建框架，用实现扩展细节合成复用原则/组合优于继承 尽量多使用组合和聚合，尽量少使用甚至不使用继承关系 2.2 分类 创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代器模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 2.3 创建型设计模式 工厂方法模式 介绍 工厂模式分为简单（静态)工厂模式、工厂方法模式和抽象工厂模式 1) 工厂类角色：这是本模式的核心，含有一定的商业逻辑和判断逻辑。在java中它往往由一个具体类实现。 2) 抽象产品角色：它一般是具体产品继承的父类或者实现的接口。在java中由接口或者抽象类来实现。 3) 具体产品角色：工厂类所创建的对象就是此角色的实例。在java中由一个具体类实现。 简单工厂模式：一个工厂类处于对产品类实例化调用的中心位置上,它决定那一个产品类应当被实例化, 工厂方法模式：一个抽象产品类，可以派生出多个具体产品类。 　　 一个抽象工厂类，可以派生出多个具体工厂类。 　　 每个具体工厂类只能创建一个具体产品类的实例。UML 适用场景与优缺点 适用场景： 1)客户不需要知道要使用的对象的创建过程 2)客户使用的对象存在变动的可能，或者根本就不知道使用哪一个具体对象 缺点： 类的数量膨胀 抽象工厂模式 介绍 抽象工厂模式：多个抽象产品类，每个抽象产品类可以派生出多个具体产品类。 　　 一个抽象工厂类，可以派生出多个具体工厂类。 　　 每个具体工厂类可以创建多个具体产品类的实例。 区别： 工厂方法模式只有一个抽象产品类，而抽象工厂模式有多个。 　　 工厂方法模式的具体工厂类只能创建一个具体产品类的实例，而抽象工厂模式可以创建多个。UML 适用场景与优缺点 适用场景： 1)系统中有多个产品族，而系统一次只能消费其中一族产品 2)同属于同一个产品族的产品一起使用 单例模式 介绍 通过单例模式可以保证系统中，应用该模式的类一个类只有一个实例。即一个类只有一个对象实例UML 适用场景与优缺点 使用场景： 1)当类只有一个实例且客户可以从一个众所周知的访问点 访问它 2)当这个唯一实例应该是通过子类化可扩展的，且客户应该无序更改代码就能使用一个扩展的实例 优点： 1)对唯一实例的受控访问 2)缩小命名空间，避免命名污染 3)允许单例有子类 4)允许可变数目的实例 public class Car{ //懒汉式，线程不安全 private static Car instance; private Car() {} public static Car getInstance() { if(instance == null) { instance = new Car(); } return instance; } 这种写法lazy loading很明显，但是致命的是在多线程不能正常工作。 //懒汉式，线程安全 private static Car instance ; private Car() {} public static synchronized Car getInstance(){ if(instance == null) { instance = new Car(); } return instance; } 这种写法能够在多线程中很好的工作，而且看起来它也具备很好的lazy loading，但是，遗憾的是，效率很低，99%情况下不需要同步。 //饿汉式 private static Car instance = new Car(); private Car() {} public static Car getInstance() { return instance; } 这种方式基于classloder机制避免了多线程的同步问题，不过，instance在类装载时就实例化，虽然导致类装载的原因有很多种，在单例模式中大多数都是调用getInstance方法， 但是也不能确定有其他的方式（或者其他的静态方法)导致类装载，这时候初始化instance显然没有达到lazy loading的效果。 //饿汉式变种 private static Car instance; static { instance = new Car(); } private Car() {} public static Car getInstance() { return instance; } 表面上看起来差别挺大，其实更第三种方式差不多，都是在类初始化即实例化instance。 //静态内部类（类的加载是线程安全的) private static class CarHolder{ private static final Car instance = new Car(); } private Car() {} public static Car getInstance() { return CarHolder.instance; } 这种方式同样利用了classloder的机制来保证初始化instance时只有一个线程，它跟第三种和第四种方式不同的是（很细微的差别)：第三种和第四种方式是只要Singleton类被装载了，那么instance就会被实例化（没有达到lazy loading效果)，而这种方式是Singleton类被装载了，instance不一定被初始化。因为SingletonHolder类没有被主动使用，只有显示通过调用getInstance方法时，才会显示装载SingletonHolder类，从而实例化instance。想象一下，如果实例化instance很消耗资源，我想让他延迟加载，另外一方面，我不希望在Singleton类加载时就实例化，因为我不能确保Singleton类还可能在其他的地方被主动使用从而被加载，那么这个时候实例化instance显然是不合适的。这个时候，这种方式相比第三和第四种方式就显得很合理。 // 枚举 public enum Car { INSTANCE; } //双重校验锁 private volatile static Car instance; private Car() {} public static Car getInstance() { if(instance == null) { synchronized(Car.class) { if(instance == null) { instance = new Car(); } } } return instance; } 这个是第二种方式的升级版，俗称双重检查锁定。 所谓双重检查加锁机制，指的是：并不是每次进入getInstance方法都需要同步， 而是先不同步，进入方法过后，先检查实例是否存在，如果不存在才进入下面的同步块， 这是第一重检查。进入同步块后，再次检查实例是否存在，如果不存在，就在同步的情况下创建一个实例，这是第二重检查。这样一来，就只需要同步一次了，从而减少了多次在同步情况下进行判断所浪费的时间。 双重检查加锁机制的实现会使用一个关键字volatile，它的意思是：被volatile 修饰的变量的值，将不会被本地线程缓存，所有对该变量的读写都是直接操作共享内存，从而确保多个线程能正确的处理该变量。 说明：由于volatile关键字可能会屏蔽掉虚拟机中的一些必要的代码优化，所以运行效率并不是很高。因此一般建议，没有特别的需要，不要使用。也就是说，虽然可以使用”双重检查加锁“机制来实现线程安全的单例，但并不建议大量采用，可以根据情况来选用。 } 建造者模式 介绍 将一个复杂对象的创建和它的表示分离，使得同样的创建过程可以创建不同的表示。 Builder用于构建组件 Director负责装配 客户端通过Director来获得最终产品，Director与Builder打交道，持有一个Builder的引用。UML 适用场景与优缺点 适用场景： 1)当创建复杂对象的算法应该独立于该对象的组成部分以及它们的赚个屁方式时 2)当构造过程必须允许被构造的对象有不同的表示时 优点： 1)可以改变一个对象的内部表示：Builder对象提供给Director一个构造产品的抽象接口，该接口使得Buildewr可以隐藏这个产品的表示和内部结构，同时隐藏了该产品是如何装配的。 2)将构造代码与表示代码分离 3)可以对构造过程进行更精细化的控制 原型模式 介绍 UML 适用场景与优缺点 • 当要实例化的类是在运行时刻指定时，例如，通过动态装载； • 为了避免创建一个与产品类层次平行的工厂类层次时； • 当一个类的实例只能有几个不同状态组合中的一种时。建立相应数目的原型并克隆它们可能比每次用合适的状态手工实例化该类更方便一些。 优点： 性能优良。原型模式是在内存二进制流的拷贝，要比直接new一个对象性能好很多，特别是要在一个循环体内产生大量的对象时，原型模式可以更好地体现其优点。 缺点： 逃避构造函数的约束。这既是它的优点也是缺点，直接在内存中拷贝，构造函数是不会执行的。优点就是减少了约束，缺点也是减少了约束，需要大家在实际应用时考虑。 2.4 结构型设计模式 适配器模式 介绍 将一个类的接口转换成客户所期待的另一种接口 Adapter可以组合+实现（对象适配器方式)，也可以继承+实现（类适配器方式)。但是继承不如组合好，因此尽量使用组合+实现。 UML 适用场景与优缺点 适用场景： 1)想使用一个已存在的类，而它的接口不符合你的需求 2)想创建一个可以复用的类，该类可以与不相关的类或不可预见的类协同工作装饰器模式 介绍 UML 适用场景与优缺点 适用场景： 1)在不影响其他对象的情况下，以动态透明的方式给单个对象添加职责 2)处理那些可以撤销的职责 3)当不能通过生成子类的方法进行扩充时 优点： 1)比继承更加灵活，可以用添加和分离的方式，用装饰在运行时 增加和删除职责 2)避免在层次结构高的类有太多特征，用装饰器为其逐渐地添加功能 代理模式 介绍 代理可以分为静态代理和动态代理 为其他对象提供一种代理以控制对这个对象的访问。 为了一个对象提供一个替身或者占位符，以控制对这个对象的访问 远程代理能够控制访问远程对象(RMI) 虚拟代理控制访问创建开销大的资源（先创建一个资源消耗较小的对象表示，真实对象只在需要时才会被创建) 保护代理基于权限控制对资源的访问UML 适用场景与优缺点 使用场景： 按职责来划分，通常有以下使用场景： 1、远程代理。 2、虚拟代理。 3、Copy-on-Write 代理。 4、保护（Protect or Access)代理。 5、Cache代理。 6、防火墙（Firewall)代理。 7、同步化（Synchronization)代理。 8、智能引用（Smart Reference)代理。 优点： 1、职责清晰。 2、高扩展性。 3、智能化。 缺点： 1、由于在客户端和真实主题之间增加了代理对象，因此有些类型的代理模式可能会造成请求的处理速度变慢。 2、实现代理模式需要额外的工作，有些代理模式的实现非常复杂。外观模式 介绍 为子系统的一组接口提供一个一致的界面，定义了一个高层接口，这一接口使得子系统更加容易使用。 遵循了迪米特法则： 通俗的来讲，就是一个类对自己依赖的类知道的越少越好。也就是说，对于被依赖的类来说，无论逻辑多么复杂，都尽量地的将逻辑封装在类的内部，对外除了提供的public方法，不对外泄漏任何信息。 外观模式就是一种较好的封装 是整体和子组件之间的关系，外部类不应该与一个类的子组件过多的接触，应该尽可能与整体打交道。 UML 适用场景与优缺点 适用场景： 1)为一个复杂子系统提供一个简单接口 2)客户与抽象类的实现部分之间存在着很大的依赖性，引入Facade将子系统与客户解耦，提高了子系统的独立性和可移植性 优点： 1)对客户屏蔽了子系统组件，减少客户处理的对象数目，并使得子系统使用起来更加容易 2)实现了子系统与客户之间的松耦合 3)降低了大型软件中的编译依赖性 4)只是提供了一个访问子系统的统一入口，并不影响客户直接使用子系统桥接模式 介绍 处理多层继承结构，处理多维度变化的场景，将各个维度设计成独立的继承结构，使各个维度可以独立地扩展，在抽象层建立关联。 一个维度的父类持有另一个维度的接口的引用（使用组合代替了继承) 希望有一个Bridge类来将类型维度和品牌维度连接起来，这样增加类型和增加品牌不会影响对方。 两种变化以上的情况应该考虑桥接模式UML 适用场景与优缺点 适用场景： 1．如果一个系统需要在构件的抽象化角色和具体化角色之间增加更多的灵活性，避免在两个层次之间建立静态的联系。 2．设计要求实现化角色的任何改变不应当影响客户端，或者说实现化角色的改变对客户端是完全透明的。 3．一个构件有多于一个的抽象化角色和实现化角色，系统需要它们之间进行动态耦合。 4．虽然在系统中使用继承是没有问题的，但是由于抽象化角色和具体化角色需要独立变化，设计要求需要独立管理这两者。 组合模式（树形结构) 介绍 无子节点的是叶子，有子节点的是容器 叶子和容器的共同点抽象为Component组件 每个容器持有一个Component的List引用，包含它的所有子节点。UML 适用场景与优缺点 适用场景： 1)想表示对象的层次结构 2)希望客户忽略组合对象与单一对象的不同，用户将统一使用组合结构中的所有对象 优点： 1)定义了包含基本对象和组合对象的类层次结构 2)简化客户代码，客户可以一致地使用组合结构和单个对象 3)更容易添加新类型的组件享元模式 介绍 将相同部分放在一个类中，工厂持有一个Map，可以创建相同部分，如果已持有那么直接返回。 不同部分单独设计一个类，可以作为相同部分类的方法的参数传入 将一个对象拆成两部分（成员变量拆成两部分)：相同部分和不同部分。相同部分使用工厂创建，进行共享；不同部分作为参数传入UML 适用场景与优缺点 适用场景：池化 内存池 数据库连接池 线程池 优点： 1)极大减少内存中对象的数量 2)相同或相似对象内存中只存一份，节省内存 3)外部状态相对独立，不影响内部状态 缺点： 1)模式较复杂，使程序逻辑复杂化 2)读取外部状态使运行时间变长，用时间换取了空间 2.5 行为型设计模式 策略模式 介绍 策略模式定义了一系列的算法，并将每一个算法封装起来，而且使他们可以相互替换，让算法独立于使用它的客户而独立变化。 环境类(Context):用一个ConcreteStrategy对象来配置。维护一个对Strategy对象的引用。可定义一个接口来让Strategy访问它的数据。 抽象策略类(Strategy):定义所有支持的算法的公共接口。 Context使用这个接口来调用某ConcreteStrategy定义的算法。 具体策略类(ConcreteStrategy):以Strategy接口实现某具体算法。UML 适用场景与优缺点 适用场景： 实现某一个功能由多种算法或者策略，我们可以根据环境或者条件的不同选择不同的算法或者策略来完成该功能 优点： 1)Strategy类层次为Context定义了一系列的可供重用的算法或行为。继承有助于析取出这些算法中的公共功能。 2)提供了可以替换继承关系的方法 3)消除if-else模板方法模式 介绍 UML 适用场景与优缺点 适用场景： 1)一次性实现一个算法的不变部分，并将可变部分留给子类来实现 2)个子类中公共的行为提取出来并集中到一个公共父类中以避免重复 3)控制子类扩展，只允许在某些点进行扩展 优点： 1)在一个父类中形式化地定义算法，由它的子类实现细节的处理 2)是一个代码复用的基本技术 3)控制翻转（好莱坞原则)，父类调用子类的操作，通过对子类的扩展来增加新的行为，符合开闭原则观察者模式 介绍 也称为发布-订阅模式。 在此种模式中，一个目标物件管理所有相依于它的观察者物件，并且在它本身的状态改变时主动发出通知。这通常透过呼叫各观察者所提供的方法来实现。此种模式通常被用来实现事件处理系统。 与Reactor模式非常类似，不过，观察者模式与单个事件源关联，而反应器模式则与多个事件源关联。UML 适用场景与优缺点 适用场景： 1)当一个对象的改变需要通知其他对象，而且它不知道具体有多少个对象有待通知时 2)当一个抽象模型有两个方面，其中一个方面依赖于另一方面，将这二者封装在独立的对象中国以使它们可以独立地改变和服用 优点： 1)独立地改变目标和观察者，解耦 2)是吸纳表示层和数据逻辑层分离（表示层是观察者，逻辑层是主题)迭代器模式 介绍 找到一种不同容器的统一的遍历方式，定义一个接口，所有可以提供遍历方式的容器都实现这个接口，返回一个迭代器，然后所有的迭代器的接口是一致的。 所有的容器都可以通过iterator方法返回一个迭代器Iterator，这个迭代器对外暴露的接口是一致的，因此可以保证对所有的容器遍历方法是一致的，仅需得到这个容器的迭代器即可，而各个容器对迭代器的实现是不同的，即遍历方式是不同的。迭代器模式可以将各个容器的遍历方式的调用方式统一起来，隐藏了内部遍历的实现细节。 UML 适用场景与优缺点 适用场景： 1)访问一个聚合对象的内容而无需暴露它的内部表示 2)需要为聚合对象提供多种遍历方式 3)为遍历不同的聚合结构提供一个统一的接口 优点： 1)支持以不同的方式遍历一个聚合对象 2)简化聚合接口 3)方便添加新的聚合类和迭代器类责任链模式 介绍 使多个处理器对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系，将这些处理器对象连成一条链，并沿这条链传递请求，直到有一个处理器对象处理它为止UML 适用场景与优缺点 适用场景： 1)有多个处理器对象可以处理一个请求，哪个处理器对象处理该请求在运行时动态确定 2)在不明确指定接收者的情况下，向多个处理器对象中的一个提交请求 3)可以动态指定一组处理器对象处理请求 优点： 1)降低耦合，使得 请求发送者无需知道是哪个处理器对象处理请求 2)简化对象的相互连接 3)增强了给对象指派责任的灵活性 4)方便添加新的请求处理类命令模式 介绍 命令模式把一个请求或者操作封装到一个对象中，把发出命令的责任和执行命令的责任分割开，委派给不同的对象，可降低行为请求者与行为实现者之间耦合度。从使用角度来看就是请求者把接口实现类作为参数传给使用者，使用者直接调用这个接口的方法，而不用关心具体执行的那个命令。 Command模式将操作的执行逻辑封装到一个个Command对象中，解耦了操作发起者和操作执行逻辑之间的耦合关系：操作发起者要进行一个操作，不用关心具体的执行逻辑，只需创建一个相应的Command实例，调用它的执行接口即可。而在swing中，与界面交互的各种操作，比如插入，删除等被称之为Edit，实际上就是Command。 使用undo包很简单，主要操作步骤如下： 1、创建CommandManager实例（持有Command的undo栈和redo栈)； 2、创建各种实现Command的具体操作类； 3、调用某种操作时，创建一个具体操作类的实例，加入CommandManager； 4、在Undo/Redo时，直接调用CommandManager的undo/redo方法。 UML 黑色箭头表示持有，关联关系 Client持有Invoker 菱形箭头也是持有，聚合关系 Invoker持有Command 白色箭头是继承，ConcreteCommand继承了Command适用场景与优缺点 适用场景： 1)系统需要将请求调用者和请求接收者解耦，使得调用者和接收者不直接交互 2)系统需要在不同的时间制定请求，将请求排序和执行请求 3)系统需要支持undo和redo操作 4)系统需要将一组操作组合在一起 优点： 1)降低系统的耦合度，调用者和接收者解耦 2)Command是头等对象，可以被操纵和扩展 3)组合命令 4)方便实现undo和redo 备忘录模式 介绍 Originate是实体类，并负责创建和恢复Memento（比如JavaBean) Memento负责保存对象的状态 CareTaker 负责存储Memento（一个或一系列)（多条历史记录) Originate除了对象的属性和setter getter之外，还有创建和恢复Memento的方法 Memento也持有对象的所有属性和setter getter，它的构造方法是由Originate对象得到其内部状态。 CareTaker持有一个或一组Memento，并提供setter and getter UML 适用场景与优缺点 适用场景： 1、需要保存/恢复数据的相关状态场景。 2、提供一个可回滚的操作。 优点： 1、给用户提供了一种可以恢复状态的机制，可以使用户能够比较方便地回到某个历史的状态。 2、实现了信息的封装，使得用户不需要关心状态的保存细节。 缺点：消耗资源。如果类的成员变量过多，势必会占用比较大的资源，而且每一次保存都会消耗一定的内存。 状态模式 介绍 UML 适用场景与优缺点 适用场景： 1)一个对象的行为取决于它的状态 2)代码中包含大量的与对象状态有关的条件语句 优点： 1)将与特定状态相关的行为局部化，并且将不同状态的行为分割开来 2)使得状态转换显式化 3)State对象可被共享 访问者模式 介绍 UML 适用场景与优缺点 中介者模式 介绍 解耦多个同事对象之间的交互关系。 每个同事对象都持有中介者对象的引用，只跟中介者打交道。我们通过中介者统一管理这些交互关系。 每个同事类都持有一个中介者类的引用。 UML 将多对多的关系解耦后转为一对多的关系，每个对象和中介者打交道，不直接和其他对象打交道。 如果关系比较简单，那么没有必要使用中介者模式，反而会复杂化。适用场景与优缺点 MVC中的C就是中介者 适用场景： 1)系统中对象之间存在着复杂的引用关系 2)一组对象以定义良好但复杂的方式进行通信 3)一个对象引用其他很多对象并直接与这些对象通信，导致难以复用该对象 优点： 1)减少子类生成 2)简化同事类的设计和实现 3)简化对象协议（一对多代替多对多) 4)对对象如何协作进行了抽象 5)使控制集中化（将交互复杂性变为中介者的复杂性)解释器模式 介绍 UML 适用场景与优缺点 2.6 设计模式的区分 代理模式和装饰器区别 装饰器模式关注于在一个对象上动态的添加方法，然而代理模式关注于控制对对象的访问。换句话说，用代理模式，代理类（proxy class)可以对它的客户隐藏一个对象的具体信息。 因此，当使用代理模式的时候，我们常常在一个代理类中创建一个对象的实例。并且，当我们使用装饰器模式的时候，我们通常的做法是将原始对象作为一个参数传给装饰者的构造器。 相同点：都是为被代理(被装饰)的类扩充新的功能。 不同点：代理模式具有控制被代理类的访问等性质，而装饰模式紧紧是单纯的扩充被装饰的类。所以区别仅仅在是否对被代理/被装饰的类进行了控制而已。 适配器模式和代理模式的区别 适配器模式，一个适配允许通常因为接口不兼容而不能在一起工作的类工作在一起，做法是将类自己的接口包裹在一个已存在的类中。 装饰器模式，原有的不能满足现有的需求，对原有的进行增强。 代理模式，同一个类而去调用另一个类的方法，不对这个方法进行直接操作，控制访问。 抽象工厂和工厂方法模式的区别 工厂方法：创建某个具体产品 抽象工厂：创建某个产品族中的系列产品 工厂方法模式 抽象工厂模式 针对的是一个产品等级结构 针对的是面向多个产品等级结构 一个抽象产品类 多个抽象产品类 可以派生出多个具体产品类 每个抽象产品类可以派生出多个具体产品类 一个抽象工厂类，可以派生出多个具体工厂类 一个抽象工厂类，可以派生出多个具体工厂类 每个具体工厂类只能创建一个具体产品类的实例 每个具体工厂类可以创建多个具体产品类的实例 2.7 JDK中的设计模式（17) 创建型 1)工厂方法 Collection.iterator() 由具体的聚集类来确定使用哪一个Iterator 2)单例模式 Runtime.getRuntime() 3)建造者模式 StringBuilder 4)原型模式 Java中的Cloneable结构性 1)适配器模式 InputStreamReader OutputStreamWriter RunnableAdapter 2)装饰器模式 io包 FileInputStream BufferedInputStream 3)代理模式 动态代理；RMI 4)外观模式 java.util.logging 5)桥接模式 JDBC 6)组合模式 dom 7)享元模式 Integer.valueOf行为型 1)策略模式 线程池的四种拒绝策略 2)模板方法模式 AbstractList、AbstractMap等 InputStream、OutputStream AQS 3)观察者模式 Swing中的Listener 4)迭代器模式 集合类中的iterator 5)责任链模式 J2EE中的Filter 6)命令模式 Runnable、Callable，ThreadPoolExecutor 7)备忘录模式 8)状态模式 9)访问者模式 10)中介者模式 11)解释器模式 2.8 Spring中的设计模式（6) - 1)抽象工厂模式： BeanFactory 2)代理模式： AOP 3)模板方法模式： AbstractApplicationContext中定义了一系列的抽象方法，比如refreshBeanFactory、closeBeanFactory、getBeanFactory。 4)单例模式： Spring可以管理单例对象，控制对象为单例 5)原型模式： Spring可以管理多例对象，控制对象为prototype 6)适配器模式： Advice与Interceptor的适配 Adapter类接口：Target public interface AdvisorAdapter { boolean supportsAdvice(Advice advice); MethodInterceptor getInterceptor(Advisor advisor); } MethodBeforeAdviceAdapter类，Adapter class MethodBeforeAdviceAdapter implements AdvisorAdapter, Serializable { public boolean supportsAdvice(Advice advice) { return (advice instanceof MethodBeforeAdvice); } public MethodInterceptor getInterceptor(Advisor advisor) { MethodBeforeAdvice advice = (MethodBeforeAdvice) advisor.getAdvice(); return new MethodBeforeAdviceInterceptor(advice); } } "},"zother5-Java-Interview/二十、Spring源码解析.html":{"url":"zother5-Java-Interview/二十、Spring源码解析.html","title":"二十、Spring源码解析","keywords":"","body":" Spring Source 他人总结 Spring1)ioc 容器——BeanFactory 是最原始的 ioc 容器，有以下方法 1.getBean2.判断是否有 Bean，containsBean3.判断是否单例 isSingleton。BeanFactory 只是对 ioc 容器最基本行为作了定义，而不关心 Bean 是怎样定义和加载的。如果我们想要知道一个工厂具体产生对象的过程，则要看这个接口的实现类。在 spring 中实现这个接口有很多类，其中一个 是xmlBeanFactory。 xmlBeanFactory 的功能是建立在 DefaultListablexmlBeanFactory 这个基本容器的基础上的，并在这个基本容器的基础上实行了其他诸如 xml 读取的附加功能。 xmlBeanFactory（Resource resource)构造函数，resource 是 spring 中对与外部资源的抽象，最常见的是文件的抽象，特别是 xml 文件，而且 resource 里面通常是保存了 spring 使用者的 Bean 定义，eg.applicationContext.xml 在被加载时，就会被抽象为 resource 处理。 [我自己理解, resource 就是定义 Bean 的 xml 文件]。 loc 容器建立过程：1)创建 ioc 配置文件的抽象资源，这个抽象资源包含了 BeanDefinition 的定义信息。2)创建一个 BeanFactory，这里使用的是 DefaultListablexmlBeanFactory。3)创建一个载入 BeanDefinition 的读取器，这里使用 xmlBeanDefinitionReader 来载入 xml 文件形式的 BeanDefinition。4)然后将上面定义好的 resource 通过一个回调配置给 BeanFactory。 5)从资源里读入配置信息，具体解析过程由 xmlBeanDefinitionReader 完成。6)ioc 容器建立起来。 BeanDefinition 类似于 resource 接口的功能，起到的作用就是对所有的 Bean 进行一层抽象的统一，把形式各样的对象统一封装为一个便于 spring 内部进行协调管理和调度的数据结构。BeanDefinition 屏蔽了不同对象对于 spring 框架的差异。 Resource 里有 inputStream。 解析 xml，获得 document 对象，接下来只要再对 document 结构进行分析便可知道 Bean 在 xml 中是怎么定义的，也就可以将其转化为 BeanDefinition 对象。我们配置的 Bean 的信息经过解析，在 spring 内部已经转换为 BeanDefinition 这种统一的结构，但这些数据还不能供 ioc 容器直接使用，需要在 ioc 容器中对这些 BeanDefinition 数据进行注册，注册完成的 BeanDefinition，都会以 BeanName 为 Key，BeanDefinition 为 value，交由 map 管理。注册完之后，一个 ioc 容器就可以用了。 自己理解的，xml 文件抽象为 resource 对象，Bean 抽象为 BeanDefinition 对象。 2) 依赖注入——依赖注入发生在 getBean 方法中，getBean 又调用 dogetBean 方法。 getBean 是依赖注入的起点，之后调用 createBean 方法，创建过程又委托给了 docreateBean 方法。在 docreateBean 中有两个方法：1)createBeanInstance，生成 Bean 包含的 java 对象 2)populateBean 完成注入。在创建 Bean 的实例中，getInstantiationstrategy 方法挺重要，该方法作用是获得实例化策略对象，也就是指通过哪种方案进行实例化的过程。spring 当中提供两种方案进行实例化：BeanUtils 和 cglib。BeanUtils 实现机制是 java 反射，cglib 是一个第三方类库，采用的是一种字节码加强方式。Spring 中默认实例化策略为 cglib。populateBean 进行依赖注入，获得 BeanDefinition 中设置的 property 信息，简单理解依赖注入的过程就是对这些 property 进行赋值的过程，在配置 Bean 的属性时，属性可能有多种类型，我们在进行注入的时候，不同类型的属性我们不能一概而论地进行处理。集合类型属性和非集合类型属性差别很大，对不同的类型应该有不同的处理过程。所以要先判断 value 类型，再调用具体方法。 3) aop——将那些与业务无关，却为业务模块所公共调用的逻辑或责任封装起来，称其为 aspect，便于减少系统的重复代码。使用模块技术，aop 把软件系统分为两个部分：核心关注点和横切关注点。业务处理的主要流程是核心关注点，与之关系不大的部分是横切关注点。实现 aop 的两大技术：1)采用动态代理，利用截取消息的方式，对该消息进行装饰，以获取原有对象行为的执行。2)采用静态织入，引入特定的语法创建切面，从而可以使编译器可在编译期间织入有关切面的代码。 spring 提供两种方式生成代理对象，jdkProxy 和 cglib。默认的策略是，若目标类是接口则用 jdk 动态代理技术，否则使用 cglib 生成代理。在 jdk 动态代理中使用 Proxy.newProxyInstance()生成代理对象（ JdkDynamicAopProxy 类的 getProxy 方法)， JdkDynamicAopProxy 也实现了 invocationhandler 接口，有 invoke 方法，就是在该方法中实现了切片织入。主流程可以简述为：获取可应用到此方法上的通知链（Interceptor chain)，若有，则应用通知，并执行 joinpoint，若没有，则直接反射执行 joinpoint。 Introduction 是指给一个已有类添加方法或字段属性，Introduction 还可以在不改变现有类代码的情况下，让现有 java 类实现新的接口，或为其指定一个父类实现多继承，相对于 advice 可以动态改变程序的功能或流程来说，Introduction 用来改变类的静态结构。 拦截器，是对连接点进行拦截，从而在连接点前后加入自定义的切面模块功能。作用于同一个连接点的多个拦截器组成一个拦截器链，拦截器链上的每一个拦截器，通常会调用下一个拦截器。 连接点，程序执行过程中的行为，比如方法调用或特定异常被抛出。 切入点，指定一个 advice 将被引发的一系列的连接点的集合。aop 框架必须允许开发者指定切入点。 通知（advice)：在特定的连接点，aop 框架执行的动作。Spring 以拦截器作通知模型，维护一个围绕连接点的拦截器链。 拦截器（advisor)，自己理解，在 invoke 前后加入的方法就是通知。使用 spring 的 PointCutadvisor，只拦截特定的方法，一个 advisor 定义订一个 PointCut 和一个 advice，满足 PointCut（指定哪些方面需要拦截)，则执行相应的 advice（定义了增强的功能)。 PointCutadvisor 有 两 个 常 用 实 现 类 ： NameMatchMethodPointCutadvisor 和 regexMethodPointCutadvisor。前者需要注入 mappedname 和 advice 属性，后者需要注入 pattern 和 advice 属性。mappedname 指明要拦截的方法名，pattern 按照正则表达式的方法指明了要拦截的方法名，advice 定义一个增强，即要加入的操作（需要自己实现 MethodBeforeAdvice、MethodafterAdvice、throwAdvice、Methodinterceptor 接口之一)，然后在 ProxyBeanFactory 的拦截器中注入这个 PointCutadvisor。注：一个 ProxyFactoryBean 只能指定一个代理目标。 在 spring 中配置 aop 很麻烦，首先需要编写 xxxadvice 类（需要自己实现 MethodBeforeAdvice、MethodafterAdvice、throwAdvice、Methodinterceptor 接口之一)，然后在 xml 配置 advisor。还要在 advisor 中注入 advice，然后将 advisor 加入 ProxyFactoryBean 中。而在 spring2.x 以后引入了 aspect 注解，只需要定义一个 aspect 类，在 aspect 中声明 advice 类（可同时声明多个)，然后在 xml 配置这个 aspect 类，最后添加一行就可以搞定。 通知类型 接口 描述 前置通知 MethodBeforeAdvice 在目标方法调用前调用 后置通知 MethodafterAdvice 在目标方法调用后调用 异常通知 throwAdvice 在目标方法抛出异常时调用 环绕通知 Methodinterceptor 拦截对目标方法调用 还有一类是引入通知，用来定义切入点的。 Spring IOC IOC=ConfigReader+ReflectionUtil 容器继承体系 1、从接口BeanFactory到HierarchicalBeanFactory，再到ConfigurableBeanFactory,这是一条主要的BeanFactory设计路径。在这条接口设计路径中，BeanFactory，是一条主要的BeanFactory设计路径。在这条接口设计路径中，BeanFactory接口定义了基本的Ioc容器的规范。在这个接口定义中，包括了getBean()这样的Ioc容器的基本方法（通过这个方法可以从容器中取得Bean)。而HierarchicalBeanFactory接口在继承了BeanFactory的基本接口后，增加了getParentBeanFactory()的接口功能，使BeanFactory具备了双亲Ioc容器的管理功能。在接下来的ConfigurableBeanFactory接口中，主要定义了一些对BeanFactory的配置功能，比如通过setParentBeanFactory()设置双亲Ioc容器，通过addBeanPostProcessor()配置Bean后置处理器，等等。通过这些接口设计的叠加，定义了BeanFactory就是最简单的Ioc容器的基本功能。 2、第二条接口设计主线是，以ApplicationContext作为核心的接口设计，这里涉及的主要接口设计有，从BeanFactory到ListableBeanFactory，再到ApplicationContext，再到我们常用的WebApplicationContext或者ConfigurableApplicationContext接口。我们常用的应用基本都是org.framework.context 包里的WebApplicationContext或者ConfigurableApplicationContext实现。在这个接口体系中，ListableBeanFactory和HierarchicalBeanFactory两个接口，连接BeanFactory接口定义和ApplicationContext应用的接口定义。在ListableBeanFactory接口中，细化了许多BeanFactory的接口功能，比如定义了getBeanDefinitionNames()接口方法；对于ApplicationContext接口，它通过继承MessageSource、ResourceLoader、ApplicationEventPublisher接口，在BeanFactory简单Ioc容器的基础上添加了许多对高级容器的特性支持。 3、.这个接口系统是以BeanFactory和ApplicationContext为核心设计的，而BeanFactory是Ioc容器中最基本的接口，在ApplicationContext的设计中，一方面，可以看到它继承了BeanFactory接口体系中的ListableBeanFactory、AutowireCapableBeanFactory、HierarchicalBeanFactory等BeanFactory的接口，具备了BeanFactory Ioc容器的基本功能；另一方面，通过继承MessageSource、ResourceLoader、ApplicationEventPublisher这些接口，BeanFactory为ApplicationContext赋予了更高级的Ioc容器特性。对于ApplicationContext而言，为了在Web环境中使用它，还设计了WebApplicationContext接口，而这个接口通过继承ThemeSource接口来扩充功能。 BeanFactory（容器接口) public interface BeanFactory { //这里是对FactoryBean的转义定义，因为如果使用bean的名字检索FactoryBean得到的对象是工厂生成的对象 String FACTORY_BEAN_PREFIX = \"&\"; //这里根据bean的名字，在IOC容器中得到bean实例，这个IOC容器就是一个大的抽象工厂。 Object getBean(String name) throws BeansException; //这里根据bean的名字和Class类型来得到bean实例，和上面的方法不同在于它会抛出异常：如果根据名字取得的bean实例的Class类型和需要的不同的话。 T getBean(String name, Class requiredType); T getBean(Class requiredType) throws BeansException; Object getBean(String name, Object... args) throws BeansException; //这里提供对bean的检索，看看是否在IOC容器有这个名字的bean boolean containsBean(String name); //这里根据bean名字得到bean实例，并同时判断这个bean是不是单件 boolean isSingleton(String name) throws NoSuchBeanDefinitionException; //这里根据bean名字得到bean实例，并同时判断这个bean是不是原型 boolean isPrototype(String name) throws NoSuchBeanDefinitionException; //这里对得到bean实例的Class类型 Class getType(String name) throws NoSuchBeanDefinitionException; //这里得到bean的别名，如果根据别名检索，那么其原名也会被检索出来 String[] getAliases(String name); } XmlBeanFactory（基础容器实现) public class XmlBeanFactory extends DefaultListableBeanFactory { private final XmlBeanDefinitionReader reader = new XmlBeanDefinitionReader(this); /** * Create a new XmlBeanFactory with the given resource, * which must be parsable using DOM. * @param resource XML resource to load bean definitions from * @throws BeansException in case of loading or parsing errors */ public XmlBeanFactory(Resource resource) throws BeansException { this(resource, null); } /** * Create a new XmlBeanFactory with the given input stream, * which must be parsable using DOM. * @param resource XML resource to load bean definitions from * @param parentBeanFactory parent bean factory * @throws BeansException in case of loading or parsing errors */ public XmlBeanFactory(Resource resource, BeanFactory parentBeanFactory) throws BeansException { super(parentBeanFactory); this.reader.loadBeanDefinitions(resource); } } 作为简单ioc容器系列最底层的实现XmlBeanFactory是建立在DefaultListableBeanFactory容器的基础之上的，并在这个基本容器的基础上实现了其他诸如xml读取的附加功能。 Resource接口体系 仅仅使用 java 标准 java.net.URL 和针对不同 URL 前缀的标准处理器并不能满足我们对各种底层资源的访问，比如：我们就不能通过 URL 的标准实现来访问相对类路径或者相对 ServletContext 的各种资源。虽然我们可以针对特定的 url 前缀来注册一个新的 URLStreamHandler（和现有的针对各种特定前缀的处理器类似，比如 http：)，然而这往往会是一件比较麻烦的事情(要求了解 url 的实现机制等)，而且 url 接口也缺少了部分基本的方法，如检查当前资源是否存在的方法。 相对标准 url 访问机制，Spring 的 Resource 接口对抽象底层资源的访问提供了一套更好的机制。 Resource 是 Spring 中对外部资源的抽象，最常见的是文件的抽象，特别是 xml 文件，而且 Resource 里面通常是保存了 Spring 使用者的 Bean 定义。 其实现类有：ByteArrayResouece,BeanDefinitionResource,InputStreamResource, ClassPathResource等 ResourceLoader接口用于返回Resource对象；其实现可以看作是一个生产Resource的工厂类。Spring提供了一个适用于所有环境的DefaultResourceLoader实现，可以返回ClassPathResource、UrlResource。 ResourceLoader在进行加载资源时需要使用前缀来指定需要加载：“classpath:path”表示返回ClasspathResource，“http://path”和“file:path”表示返回UrlResource资源，如果不加前缀则需要根据当前上下文来决定，DefaultResourceLoader默认实现可以加载classpath资源。 对于目前所有ApplicationContext都实现了ResourceLoader，因此可以使用其来加载资源。 ClassPathXmlApplicationContext：不指定前缀将返回默认的ClassPathResource资源，否则将根据前缀来加载资源； WebApplicationContext：不指定前缀将返回ServletContextResource，否则将根据前缀来加载资源； ApplicationContext接口（高级容器接口) ApplicationContext是spring中较高级的容器。和BeanFactory类似，它可以加载配置文件中定义的bean，将所有的bean集中在一起，当有请求的时候分配bean。 另外，它增加了企业所需要的功能，比如，从属性文件中解析文本信息和将事件传递给所指定的监听器。这个容器在org.springframework.context.ApplicationContext接口中定义。ApplicationContext包含BeanFactory所有的功能，一般情况下，相对于BeanFactory，ApplicationContext会被推荐使用。 特点 1.支持不同的信息源。继承了MessageSource接口，这个接口为ApplicationContext提供了很多信息源的扩展功能，比如：国际化的实现为多语言版本的应用提供服务。 2.访问资源。这一特性主要体现在ResourcePatternResolver接口上，对Resource和ResourceLoader的支持，这样我们可以从不同地方得到Bean定义资源。这种抽象使用户程序可以灵活地定义Bean定义信息，尤其是从不同的IO途径得到Bean定义信息。这在接口上看不出来，不过一般来说，具体ApplicationContext都是继承了DefaultResourceLoader的子类。因为DefaultResourceLoader是AbstractApplicationContext的基类。 3.支持应用事件。继承了接口ApplicationEventPublisher，为应用环境引入了事件机制，这些事件和Bean的生命周期的结合为Bean的管理提供了便利。 4.附件服务。EnvironmentCapable里的服务让基本的Ioc功能更加丰富。 5.ListableBeanFactory和HierarchicalBeanFactory是继承的主要容器。 实现 最常被使用的ApplicationContext接口实现类： 1，FileSystemXmlApplicationContext：该容器从XML文件中加载已被定义的bean。在这里，你需要提供给构造器XML文件的完整路径。 2，ClassPathXmlApplicationContext：该容器从XML文件中加载已被定义的bean。在这里，你不需要提供XML文件的完整路径，只需正确配置CLASSPATH环境变量即可，因为，容器会从CLASSPATH中搜索bean配置文件。 3，WebXmlApplicationContext：该容器会在一个 web 应用程序的范围内加载在XML文件中 ClassPathXmlApplicationContext（高级容器实现) public ClassPathXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException { super(parent); setConfigLocations(configLocations); if (refresh) { refresh(); } } Bean的注册 AbstractApplicationContext#refresh（bean注册) refresh是AbstractApplicationContext中方法。 逻辑： 1)初始化前的准备工作，比如对系统属性或者环境变量进行准备及验证 2)初始化BeanFactory，并进行XML文件读取（component-scan->包括class文件) 3)对BeanFactory进行各种功能填充，比如@Qualifier和@Autowired 4)子类覆盖方法做额外的处理 5)激活各种BeanFactory处理器 6)注册拦截bean创建的bean处理器，这里只是注册，真正的调用是在getBean的时候 7)为上下文初始化Message源，即为不同语言的消息体进行国际化处理 8)初始化应用消息广播器，并放入applicationEventMulticaster bean中 9)留给子类来初始化其他的bean 10)在所有注册的bean中查找listener bean，注册到消息广播器中 11)初始化剩下的代理实例（非lazy-init) 12)完成刷新过程，通知生命周期处理器lifecycleProcessor刷新过程，同时发出ContextRefreshEvent通知别人。 public void refresh() throws BeansException, IllegalStateException { synchronized (this.startupShutdownMonitor) { // 准备 刷新的上下文环境 prepareRefresh(); // 初始化BeanFactory，并进行XML文件的读取 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 对BeanFactory进行各种功能填充 prepareBeanFactory(beanFactory); try { // 子类覆盖方法做额外的处理 postProcessBeanFactory(beanFactory); // 激活各种BeanFactory处理器 invokeBeanFactoryPostProcessors(beanFactory); // 注册拦截Bean创建的Bean处理器，这里只是注册，真正的调用是在getBean的时候 registerBeanPostProcessors(beanFactory); // 为上下文初始化Message源，即不同语言的消息体，国际化处理 initMessageSource(); // 初始化应用消息广播器，并放入applicationEventMulticaster bean中 initApplicationEventMulticaster(); // 留给子类来初始化其他bean onRefresh(); // 在所有注册的bean中查找Listener bean，注册到消息广播器中 registerListeners(); // 初始化剩下的单例实例（除了lazy-init) finishBeanFactoryInitialization(beanFactory); // 完成刷新过程，通知生命周期处理器lifecycleProcessor刷新过程，同时发出ContextRefreshEvent通知别人 finishRefresh(); } catch (BeansException ex) { if (logger.isWarnEnabled()) { logger.warn(\"Exception encountered during context initialization - \" + \"cancelling refresh attempt: \" + ex); } // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; } finally { // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); } } } 1) obtainFreshBeanFactory（创建beanFactory，解析XML) 看obtainFreshBeanFactory这个方法： protected ConfigurableListableBeanFactory obtainFreshBeanFactory() { refreshBeanFactory(); ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (logger.isDebugEnabled()) {logger.debug(\"Bean factory for \" + getDisplayName() + \": \" + beanFactory); } return beanFactory; } 1.1) refreshBeanFactory 看refreshBeanFactory这个方法，这个方法是抽象方法，有一种是在AbstractRefreshableApplicationContext这个类中实现的： @Override protected final void refreshBeanFactory() throws BeansException { if (hasBeanFactory()) {destroyBeans(); closeBeanFactory(); } try {DefaultListableBeanFactory beanFactory = createBeanFactory(); beanFactory.setSerializationId(getId()); customizeBeanFactory(beanFactory); //该方法最终调用XmlBeanDefinitionReader类中loadBeanDefinitions(EncodedResource) loadBeanDefinitions(beanFactory); synchronized (this.beanFactoryMonitor) { this.beanFactory = beanFactory; } } catch (IOException ex) { throw new ApplicationContextException(\"I/O error parsing bean definition source for \" + getDisplayName(), ex); } } 1.1.1) loadBeanDefinitions(beanFactory)（创建reader) 看loadBeanDefinitions(beanFactory)这个方法： @Override protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException { // Create a new XmlBeanDefinitionReader for the given BeanFactory. XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); // Configure the bean definition reader with this context's // resource loading environment. beanDefinitionReader.setEnvironment(this.getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); // Allow a subclass to provide custom initialization of the reader, // then proceed with actually loading the bean definitions. initBeanDefinitionReader(beanDefinitionReader); loadBeanDefinitions(beanDefinitionReader); } 1.1.1.1) loadBeanDefinitions(beanDefinitionReader)（调用reader的load) 看loadBeanDefinitions(beanDefinitionReader)的另一个重载： protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException { Resource[] configResources = getConfigResources(); if (configResources != null) {reader.loadBeanDefinitions(configResources); } String[] configLocations = getConfigLocations(); if (configLocations != null) {reader.loadBeanDefinitions(configLocations); } } 1.1.1.1.1) XmlBeanDefinitionReader.loadBeanDefinitions（解析标签) 看reader.loadBeanDefinitions方法： @Override public int loadBeanDefinitions(String... locations) throws BeanDefinitionStoreException { Assert.notNull(locations, \"Location array must not be null\"); int counter = 0; for (String location : locations) { counter += loadBeanDefinitions(location); } return counter; } loadBeanDefinitions(location,null) 它最终调用了loadBeanDefinitions(location,null)方法 public int loadBeanDefinitions(String location, Set actualResources) throws BeanDefinitionStoreException { ResourceLoader resourceLoader = getResourceLoader(); if (resourceLoader == null) { throw new BeanDefinitionStoreException( \"Cannot import bean definitions from location [\" + location + \"]: no ResourceLoader available\"); } if (resourceLoader instanceof ResourcePatternResolver) { // Resource pattern matching available. try { Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); int loadCount = loadBeanDefinitions(resources); if (actualResources != null) { for (Resource resource : resources) { actualResources.add(resource); } } if (logger.isDebugEnabled()) { logger.debug(\"Loaded \" + loadCount + \" bean definitions from location pattern [\" + location + \"]\"); } return loadCount; } catch (IOException ex) { throw new BeanDefinitionStoreException( \"Could not resolve bean definition resource pattern [\" + location + \"]\", ex); } } else { // Can only load single resources by absolute URL. Resource resource = resourceLoader.getResource(location); int loadCount = loadBeanDefinitions(resource); if (actualResources != null) { actualResources.add(resource); } if (logger.isDebugEnabled()) { logger.debug(\"Loaded \" + loadCount + \" bean definitions from location [\" + location + \"]\"); } return loadCount; } } public int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException { Assert.notNull(resources, \"Resource array must not be null\"); int counter = 0; for (Resource resource : resources) { counter += loadBeanDefinitions(resource); } return counter; } public int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException { return loadBeanDefinitions(new EncodedResource(resource)); } public int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException { Assert.notNull(encodedResource, \"EncodedResource must not be null\"); if (logger.isInfoEnabled()) { logger.info(\"Loading XML bean definitions from \" + encodedResource.getResource()); } Set currentResources = this.resourcesCurrentlyBeingLoaded.get(); if (currentResources == null) { currentResources = new HashSet(4); this.resourcesCurrentlyBeingLoaded.set(currentResources); } if (!currentResources.add(encodedResource)) { throw new BeanDefinitionStoreException( \"Detected cyclic loading of \" + encodedResource + \" - check your import definitions!\"); } try { InputStream inputStream = encodedResource.getResource().getInputStream(); try { InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) { inputSource.setEncoding(encodedResource.getEncoding()); } return doLoadBeanDefinitions(inputSource, encodedResource.getResource()); } finally { inputStream.close(); } } catch (IOException ex) { throw new BeanDefinitionStoreException( \"IOException parsing XML document from \" + encodedResource.getResource(), ex); } finally { currentResources.remove(encodedResource); if (currentResources.isEmpty()) { this.resourcesCurrentlyBeingLoaded.remove(); } } } doLoadBeanDefinitions 继续向下找，最终调用了reader的doLoadBeanDefinitions方法： protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource)throws BeanDefinitionStoreException { try {Document doc = doLoadDocument(inputSource, resource); return registerBeanDefinitions(doc, resource); } catch (BeanDefinitionStoreException ex) {throw ex; } ... } doLoadDocument方法中用到了documentLoader对象加载document，它最后又会用到domParser解析xml文件。 最终是使用jaxp的dom方式读取的XML配置文件。（JAXP是一种标准，SUN公司对其提供了实现) registerBeanDefinitions 将XML对应的Document对象转为BeanDefinitions： public int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException { BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); int countBefore = getRegistry().getBeanDefinitionCount(); documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); return getRegistry().getBeanDefinitionCount() - countBefore; } documentReader#registerBeanDefinitions 这是一个抽象方法，在DefaultBeanDefinitionDocumentReader类中得到实现： public void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) { this.readerContext = readerContext; logger.debug(\"Loading bean definitions\"); Element root = doc.getDocumentElement(); doRegisterBeanDefinitions(root); } doRegisterBeanDefinitions 实际解析文档的是doRegisterBeanDefinitions方法： protected void doRegisterBeanDefinitions(Element root) { // Any nested elements will cause recursion in this method. In // order to propagate and preserve default-* attributes correctly, // keep track of the current (parent) delegate, which may be null. Create // the new (child) delegate with a reference to the parent for fallback purposes, // then ultimately reset this.delegate back to its original (parent) reference. // this behavior emulates a stack of delegates without actually necessitating one. BeanDefinitionParserDelegate parent = this.delegate; this.delegate = createDelegate(getReaderContext(), root, parent); if (this.delegate.isDefaultNamespace(root)) { String profileSpec = root.getAttribute(PROFILE_ATTRIBUTE); if (StringUtils.hasText(profileSpec)) { String[] specifiedProfiles = StringUtils.tokenizeToStringArray( profileSpec, BeanDefinitionParserDelegate.MULTI_VALUE_ATTRIBUTE_DELIMITERS); if (!getReaderContext().getEnvironment().acceptsProfiles(specifiedProfiles)) { if (logger.isInfoEnabled()) { logger.info(\"Skipped XML bean definition file due to specified profiles [\" + profileSpec + \"] not matching: \" + getReaderContext().getResource()); } return; } } } preProcessXml(root); parseBeanDefinitions(root, this.delegate); postProcessXml(root); this.delegate = parent; } parseBeanDefinitions（解析标签) 实际解析方法parseBeanDefinitions： protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) { if (delegate.isDefaultNamespace(root)) {NodeList nl = root.getChildNodes(); for (int i = 0; i } else {delegate.parseCustomElement(root); } } 分开解析默认标签和自定义标签。 parseDefaultElement（解析默认标签) 就import、alias、bean和beans四种默认标签进行解析 private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) { if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) { importBeanDefinitionResource(ele); } else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) { processAliasRegistration(ele); } else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) { processBeanDefinition(ele, delegate); } else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) { // recurse doRegisterBeanDefinitions(ele); } } parseCustomElement（解析自定义标签) 调用的是parseCustomElement(ele,null) public BeanDefinition parseCustomElement(Element ele, BeanDefinition containingBd) { String namespaceUri = getNamespaceURI(ele); NamespaceHandler handler = this.readerContext.getNamespaceHandlerResolver().resolve(namespaceUri); if (handler == null) { error(\"Unable to locate Spring NamespaceHandler for XML schema namespace [\" + namespaceUri + \"]\", ele); return null; } return handler.parse(ele, new ParserContext(this.readerContext, this, containingBd)); } NamespaceHandlerSupport#parse public BeanDefinition parse(Element element, ParserContext parserContext) { return findParserForElement(element, parserContext).parse(element, parserContext); } a) findParserForElement private BeanDefinitionParser findParserForElement(Element element, ParserContext parserContext) { String localName = parserContext.getDelegate().getLocalName(element); BeanDefinitionParser parser = this.parsers.get(localName); if (parser == null) { parserContext.getReaderContext().fatal( \"Cannot locate BeanDefinitionParser for element [\" + localName + \"]\", element); } return parser; } parser是ComponentScanBeanDefinitionParser类型。 b) ComponentScanBeanDefinitionParser#parse public BeanDefinition parse(Element element, ParserContext parserContext) { String basePackage = element.getAttribute(BASE_PACKAGE_ATTRIBUTE); basePackage = parserContext.getReaderContext().getEnvironment().resolvePlaceholders(basePackage); String[] basePackages = StringUtils.tokenizeToStringArray(basePackage, ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS); // Actually scan for bean definitions and register them. ClassPathBeanDefinitionScanner scanner = configureScanner(parserContext, element); Set beanDefinitions = scanner.doScan(basePackages); registerComponents(parserContext.getReaderContext(), beanDefinitions, element); return null; } - 1) ClassPathBeanDefinitionScanner.doScan（解析注解定义的bean) 根据自定义标签component-scan来扫描包，批量注册bean。 protected Set doScan(String... basePackages) { Assert.notEmpty(basePackages, \"At least one base package must be specified\"); Set beanDefinitions = new LinkedHashSet(); for (String basePackage : basePackages) { Set candidates = findCandidateComponents(basePackage); for (BeanDefinition candidate : candidates) { ScopeMetadata scopeMetadata = this.scopeMetadataResolver.resolveScopeMetadata(candidate); candidate.setScope(scopeMetadata.getScopeName()); String beanName = this.beanNameGenerator.generateBeanName(candidate, this.registry); if (candidate instanceof AbstractBeanDefinition) { postProcessBeanDefinition((AbstractBeanDefinition) candidate, beanName); } if (candidate instanceof AnnotatedBeanDefinition) { AnnotationConfigUtils.processCommonDefinitionAnnotations((AnnotatedBeanDefinition) candidate); } if (checkCandidate(beanName, candidate)) { BeanDefinitionHolder definitionHolder = new BeanDefinitionHolder(candidate, beanName); definitionHolder = AnnotationConfigUtils.applyScopedProxyMode(scopeMetadata, definitionHolder, this.registry); beanDefinitions.add(definitionHolder); registerBeanDefinition(definitionHolder, this.registry); } } } return beanDefinitions; } 1.1) findCandidateComponents public Set findCandidateComponents(String basePackage) { Set candidates = new LinkedHashSet(); try { String packageSearchPath = ResourcePatternResolver.CLASSPATH_ALL_URL_PREFIX + resolveBasePackage(basePackage) + '/' + this.resourcePattern; // 这里是未加筛选的拿到了所有class文件Resource[] resources = this.resourcePatternResolver.getResources(packageSearchPath); boolean traceEnabled = logger.isTraceEnabled(); boolean debugEnabled = logger.isDebugEnabled(); for (Resource resource : resources) { if (traceEnabled) { logger.trace(\"Scanning \" + resource); } if (resource.isReadable()) { try { // 这里会把class文件读进来 MetadataReader metadataReader = this.metadataReaderFactory.getMetadataReader(resource); // 判断class文件是否是注册在Spring中的bean类型 if (isCandidateComponent(metadataReader)) { ScannedGenericBeanDefinition sbd = new ScannedGenericBeanDefinition(metadataReader); sbd.setResource(resource); sbd.setSource(resource); if (isCandidateComponent(sbd)) { if (debugEnabled) { logger.debug(\"Identified candidate component class: \" + resource); } candidates.add(sbd); } else { if (debugEnabled) { logger.debug(\"Ignored because not a concrete top-level class: \" + resource); } } } else { if (traceEnabled) { logger.trace(\"Ignored because not matching any filter: \" + resource); } } } catch (Throwable ex) { throw new BeanDefinitionStoreException( \"Failed to read candidate component class: \" + resource, ex); } } else { if (traceEnabled) { logger.trace(\"Ignored because not readable: \" + resource); } } } } catch (IOException ex) {throw new BeanDefinitionStoreException(\"I/O failure during classpath scanning\", ex); } return candidates; } 1.1.1) PathMatchingResourcePatternResolver.getResources public Resource[] getResources(String locationPattern) throws IOException { Assert.notNull(locationPattern, \"Location pattern must not be null\"); if (locationPattern.startsWith(CLASSPATH_ALL_URL_PREFIX)) { // a class path resource (multiple resources for same name possible) if (getPathMatcher().isPattern(locationPattern.substring(CLASSPATH_ALL_URL_PREFIX.length()))) { // a class path resource pattern return findPathMatchingResources(locationPattern); } else { // all class path resources with the given name return findAllClassPathResources(locationPattern.substring(CLASSPATH_ALL_URL_PREFIX.length())); } } else { // Only look for a pattern after a prefix here // (to not get fooled by a pattern symbol in a strange prefix). int prefixEnd = locationPattern.indexOf(\":\") + 1; if (getPathMatcher().isPattern(locationPattern.substring(prefixEnd))) { // a file pattern return findPathMatchingResources(locationPattern); } else { // a single resource with the given name return new Resource[] {getResourceLoader().getResource(locationPattern)}; } } } 1.1.1.1) findPathMatchingResources protected Resource[] findPathMatchingResources(String locationPattern) throws IOException { String rootDirPath = determineRootDir(locationPattern); String subPattern = locationPattern.substring(rootDirPath.length()); // 根路径，component-scan中配置的包名 Resource[] rootDirResources = getResources(rootDirPath); Set result = new LinkedHashSet(16); for (Resource rootDirResource : rootDirResources) { rootDirResource = resolveRootDirResource(rootDirResource); URL rootDirURL = rootDirResource.getURL(); if (equinoxResolveMethod != null) { if (rootDirURL.getProtocol().startsWith(\"bundle\")) { rootDirURL = (URL) ReflectionUtils.invokeMethod(equinoxResolveMethod, null, rootDirURL); rootDirResource = new UrlResource(rootDirURL); } } if (rootDirURL.getProtocol().startsWith(ResourceUtils.URL_PROTOCOL_VFS)) { result.addAll(VfsResourceMatchingDelegate.findMatchingResources(rootDirURL, subPattern, getPathMatcher())); } else if (ResourceUtils.isJarURL(rootDirURL) || isJarResource(rootDirResource)) { result.addAll(doFindPathMatchingJarResources(rootDirResource, rootDirURL, subPattern)); } else { result.addAll(doFindPathMatchingFileResources(rootDirResource, subPattern)); } } if (logger.isDebugEnabled()) { logger.debug(\"Resolved location pattern [\" + locationPattern + \"] to resources \" + result); } return result.toArray(new Resource[result.size()]); } 1.1.1.1.1) doFindPathMatchingFileResources protected Set doFindPathMatchingFileResources(Resource rootDirResource, String subPattern) throws IOException { File rootDir; try { rootDir = rootDirResource.getFile().getAbsoluteFile(); } catch (IOException ex) { if (logger.isWarnEnabled()) { logger.warn(\"Cannot search for matching files underneath \" + rootDirResource + \" because it does not correspond to a directory in the file system\", ex); } return Collections.emptySet(); } return doFindMatchingFileSystemResources(rootDir, subPattern); } doFindMatchingFileSystemResources protected Set doFindMatchingFileSystemResources(File rootDir, String subPattern) throws IOException { if (logger.isDebugEnabled()) { logger.debug(\"Looking for matching resources in directory tree [\" + rootDir.getPath() + \"]\"); } Set matchingFiles = retrieveMatchingFiles(rootDir, subPattern); Set result = new LinkedHashSet(matchingFiles.size()); for (File file : matchingFiles) { result.add(new FileSystemResource(file)); } return result; } 1.1.1.1.1.1) retrieveMatchingFiles protected Set retrieveMatchingFiles(File rootDir, String pattern) throws IOException { if (!rootDir.exists()) { // Silently skip non-existing directories. if (logger.isDebugEnabled()) { logger.debug(\"Skipping [\" + rootDir.getAbsolutePath() + \"] because it does not exist\"); } return Collections.emptySet(); } if (!rootDir.isDirectory()) { // Complain louder if it exists but is no directory. if (logger.isWarnEnabled()) { logger.warn(\"Skipping [\" + rootDir.getAbsolutePath() + \"] because it does not denote a directory\"); } return Collections.emptySet(); } if (!rootDir.canRead()) { if (logger.isWarnEnabled()) { logger.warn(\"Cannot search for matching files underneath directory [\" + rootDir.getAbsolutePath() + \"] because the application is not allowed to read the directory\"); } return Collections.emptySet(); } String fullPattern = StringUtils.replace(rootDir.getAbsolutePath(), File.separator, \"/\"); if (!pattern.startsWith(\"/\")) { fullPattern += \"/\"; } fullPattern = fullPattern + StringUtils.replace(pattern, File.separator, \"/\"); Set result = new LinkedHashSet(8); doRetrieveMatchingFiles(fullPattern, rootDir, result); return result; } 1.1.1.1.1.1.1) doRetrieveMatchingFiles（递归方法) protected void doRetrieveMatchingFiles(String fullPattern, File dir, Set result) throws IOException { if (logger.isDebugEnabled()) {logger.debug(\"Searching directory [\" + dir.getAbsolutePath() + \"] for files matching pattern [\" + fullPattern + \"]\"); } // 拿到component-scan目录下的所有class文件 File[] dirContents = dir.listFiles(); if (dirContents == null) { if (logger.isWarnEnabled()) { logger.warn(\"Could not retrieve contents of directory [\" + dir.getAbsolutePath() + \"]\"); } return; } Arrays.sort(dirContents); for (File content : dirContents) { String currPath = StringUtils.replace(content.getAbsolutePath(), File.separator, \"/\"); if (content.isDirectory() && getPathMatcher().matchStart(fullPattern, currPath + \"/\")) { if (!content.canRead()) { if (logger.isDebugEnabled()) { logger.debug(\"Skipping subdirectory [\" + dir.getAbsolutePath() + \"] because the application is not allowed to read the directory\"); } } else { doRetrieveMatchingFiles(fullPattern, content, result); } } if (getPathMatcher().match(fullPattern, currPath)) { result.add(content); } } } 1.1.2) CachingMetadataResourceFactory.getMetadataReader（读取class文件) public MetadataReader getMetadataReader(Resource resource) throws IOException { if (getCacheLimit() - 1.1.2.1) SimpleMetadataReader.getMetadataReader 该类封装了Class文件中的各种信息，保存在ClassMatadata和AnnotationMetadata中。 final class SimpleMetadataReader implements MetadataReader { private final Resource resource; private final ClassMetadata classMetadata; private final AnnotationMetadata annotationMetadata; } public MetadataReader getMetadataReader(Resource resource) throws IOException { return new SimpleMetadataReader(resource, this.resourceLoader.getClassLoader()); } SimpleMetadataReader(Resource resource, ClassLoader classLoader) throws IOException { InputStream is = new BufferedInputStream(resource.getInputStream()); ClassReader classReader; try { classReader = new ClassReader(is); } catch (IllegalArgumentException ex) { throw new NestedIOException(\"ASM ClassReader failed to parse class file - \" + \"probably due to a new Java class file version that isn't supported yet: \" + resource, ex); } finally { is.close(); } AnnotationMetadataReadingVisitor visitor = new AnnotationMetadataReadingVisitor(classLoader); classReader.accept(visitor, ClassReader.SKIP_DEBUG); this.annotationMetadata = visitor; // (since AnnotationMetadataReadingVisitor extends ClassMetadataReadingVisitor) this.classMetadata = visitor; this.resource = resource; } 1.1.2.1.1) ClassReader.accept 源码比较奇怪，大概是按照class文件的格式解析，并将结果封装到visitor里面。 1.1.3) isCandidateComponent protected boolean isCandidateComponent(MetadataReader metadataReader) throws IOException { for (TypeFilter tf : this.excludeFilters) { if (tf.match(metadataReader, this.metadataReaderFactory)) { return false; } } for (TypeFilter tf : this.includeFilters) { if (tf.match(metadataReader, this.metadataReaderFactory)) { return isConditionMatch(metadataReader); } } return false; } 这里的includeFilters有一个AnnotationTypeFilter。 1.1.3.1) AbstractTypeHierarchyTraversingFilter.match public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException { // This method optimizes avoiding unnecessary creation of ClassReaders // as well as visiting over those readers. if (matchSelf(metadataReader)) { return true; } ClassMetadata metadata = metadataReader.getClassMetadata(); if (matchClassName(metadata.getClassName())) { return true; } if (this.considerInherited) { if (metadata.hasSuperClass()) { // Optimization to avoid creating ClassReader for super class. Boolean superClassMatch = matchSuperClass(metadata.getSuperClassName()); if (superClassMatch != null) { if (superClassMatch.booleanValue()) { return true; } } else { // Need to read super class to determine a match... try { if (match(metadata.getSuperClassName(), metadataReaderFactory)) { return true; } } catch (IOException ex) { logger.debug(\"Could not read super class [\" + metadata.getSuperClassName() + \"] of type-filtered class [\" + metadata.getClassName() + \"]\"); } } } } if (this.considerInterfaces) { for (String ifc : metadata.getInterfaceNames()) { // Optimization to avoid creating ClassReader for super class Boolean interfaceMatch = matchInterface(ifc); if (interfaceMatch != null) { if (interfaceMatch.booleanValue()) { return true; } } else { // Need to read interface to determine a match... try { if (match(ifc, metadataReaderFactory)) { return true; } } catch (IOException ex) { logger.debug(\"Could not read interface [\" + ifc + \"] for type-filtered class [\" + metadata.getClassName() + \"]\"); } } } } return false; } protected boolean matchSelf(MetadataReader metadataReader) { AnnotationMetadata metadata = metadataReader.getAnnotationMetadata(); return metadata.hasAnnotation(this.annotationType.getName()) || (this.considerMetaAnnotations && metadata.hasMetaAnnotation(this.annotationType.getName())); } this.annotationType是@Component类型，所以 metadata.hasAnnotation(this.annotationType.getName())当类上注解了@Component时为true。 这里因为@Service等也注解了@Component了，所以@Service、@Controller等在这里都被视为@Component。 public boolean hasMetaAnnotation(String metaAnnotationType) { Collection> allMetaTypes = this.metaAnnotationMap.values(); for (Set metaTypes : allMetaTypes) { if (metaTypes.contains(metaAnnotationType)) { return true; } } return false; } - 1.2) registerBeanDefinition（将beanDefinition记录到BeanFactory) protected void registerBeanDefinition(BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry) { BeanDefinitionReaderUtils.registerBeanDefinition(definitionHolder, registry); } 1.2.1) DefaultListableBeanFactory.registerBeanDefinition（保存beanDefinition) public void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException { Assert.hasText(beanName, \"Bean name must not be empty\"); Assert.notNull(beanDefinition, \"BeanDefinition must not be null\"); if (beanDefinition instanceof AbstractBeanDefinition) { try { ((AbstractBeanDefinition) beanDefinition).validate(); } catch (BeanDefinitionValidationException ex) { throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName, \"Validation of bean definition failed\", ex); } } BeanDefinition oldBeanDefinition; oldBeanDefinition = this.beanDefinitionMap.get(beanName); if (oldBeanDefinition != null) { if (!isAllowBeanDefinitionOverriding()) { throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName, \"Cannot register bean definition [\" + beanDefinition + \"] for bean '\" + beanName + \"': There is already [\" + oldBeanDefinition + \"] bound.\"); } else if (oldBeanDefinition.getRole() updatedDefinitions = new ArrayList(this.beanDefinitionNames.size() + 1); updatedDefinitions.addAll(this.beanDefinitionNames); updatedDefinitions.add(beanName); this.beanDefinitionNames = updatedDefinitions; if (this.manualSingletonNames.contains(beanName)) { Set updatedSingletons = new LinkedHashSet(this.manualSingletonNames); updatedSingletons.remove(beanName); this.manualSingletonNames = updatedSingletons; } } } else { // Still in startup registration phase this.beanDefinitionMap.put(beanName, beanDefinition); this.beanDefinitionNames.add(beanName); this.manualSingletonNames.remove(beanName); } this.frozenBeanDefinitionNames = null; } if (oldBeanDefinition != null || containsSingleton(beanName)) { resetBeanDefinition(beanName); } } 标签解析完毕后会将beanName和beanDefinition作为key和value放入beanfactory的beanDefinitionMap中。 2) finishBeanFactoryInitialization（初始化非lazy-load且singleton的bean) protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) { // Initialize conversion service for this context. if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) && beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) { beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); } // Register a default embedded value resolver if no bean post-processor // (such as a PropertyPlaceholderConfigurer bean) registered any before: // at this point, primarily for resolution in annotation attribute values. if (!beanFactory.hasEmbeddedValueResolver()) { beanFactory.addEmbeddedValueResolver(new StringValueResolver() { @Override public String resolveStringValue(String strVal) { return getEnvironment().resolvePlaceholders(strVal); } }); } // Initialize LoadTimeWeaverAware beans early to allow for registering their transformers early. String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); for (String weaverAwareName : weaverAwareNames) { getBean(weaverAwareName); } // Stop using the temporary ClassLoader for type matching. beanFactory.setTempClassLoader(null); // Allow for caching all bean definition metadata, not expecting further changes. beanFactory.freezeConfiguration(); // Instantiate all remaining (non-lazy-init) singletons. beanFactory.preInstantiateSingletons(); } - 2.1) ConfigurableListableBeanFactory#preInstantiateSingletons DefaultListableBeanFactory.preInstantiateSingletons public void preInstantiateSingletons() throws BeansException { if (this.logger.isDebugEnabled()) { this.logger.debug(\"Pre-instantiating singletons in \" + this); } // Iterate over a copy to allow for init methods which in turn register new bean definitions. // While this may not be part of the regular factory bootstrap, it does otherwise work fine. List beanNames = new ArrayList(this.beanDefinitionNames); // Trigger initialization of all non-lazy singleton beans... for (String beanName : beanNames) { RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); if (!bd.isAbstract() && bd.isSingleton() && !bd.isLazyInit()) { if (isFactoryBean(beanName)) { final FactoryBean factory = (FactoryBean) getBean(FACTORY_BEAN_PREFIX + beanName); boolean isEagerInit; if (System.getSecurityManager() != null && factory instanceof SmartFactoryBean) { isEagerInit = AccessController.doPrivileged(new PrivilegedAction() { @Override public Boolean run() { return ((SmartFactoryBean) factory).isEagerInit(); } }, getAccessControlContext()); } else { isEagerInit = (factory instanceof SmartFactoryBean && ((SmartFactoryBean) factory).isEagerInit()); } if (isEagerInit) { getBean(beanName); } } else { getBean(beanName); } } } // Trigger post-initialization callback for all applicable beans... for (String beanName : beanNames) { Object singletonInstance = getSingleton(beanName); if (singletonInstance instanceof SmartInitializingSingleton) { final SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; if (System.getSecurityManager() != null) { AccessController.doPrivileged(new PrivilegedAction() { @Override public Object run() { smartSingleton.afterSingletonsInstantiated(); return null; } }, getAccessControlContext()); } else { smartSingleton.afterSingletonsInstantiated(); } } } } Bean的加载 FactoryBean（工厂Bean，用户定制) Spring通过反射机制利用bean的class属性指定实现类来实例化bean。 Spring提供了一个FactoryBean的工厂类接口，用户可以通过实现该接口定制实例化bean的逻辑。 FactoryBean public interface FactoryBean { // 返回bean实例，如果isSingleton()返回true，那么该实例会放到Spring容器中单例缓存池中 T getObject() throws Exception; Class getObjectType(); boolean isSingleton(); } FactoryBean： 这个接口使你可以提供一个复杂的逻辑来生成Bean。它本质是一个Bean，但这个Bean不是用来注入到其它地方像Service、Dao一样使用的，它是用来生成其它Bean使用的。实现了这个接口后，Spring在容器初始化时，把实现这个接口的Bean取出来，使用接口的getObject()方法来生成我们要想的Bean。当然，那些生成Bean的业务逻辑也要写getObject()方法中。 其返回的对象不是指定类的一个实例，其返回的是该工厂Bean的getObject方法所返回的对象。创建出来的对象是否属于单例由isSingleton中的返回决定。 使用场景：1、通过外部对类是否是单例进行控制，该类自己无法感知 2、在创建Object对象之前进行初始化的操作，在afterPropertiesSet()中完成。（一次性的初始化，保存在成员变量中，并不是每次getObject都会调用afterPropertiesSet，afterPropertiesSet只会被调用一次) 实例： public class CarFactoryBean implements FactoryBean { private String brand; private double price; @Override public Car getObject() throws Exception { return new Car(brand,price); } @Override public Class getObjectType() { return Car.class; } @Override public boolean isSingleton() { return true; } public String getBrand() { return brand; } public void setBrand(String brand) { this.brand = brand; } public double getPrice() { return price; } public void setPrice(double price) { this.price = price; } } public class Main { public static void main(String[] args) { ApplicationContext ctx = new ClassPathXmlApplicationContext(\"beans-factoryBean.xml\"); Car car = (Car) ctx.getBean(\"car\"); System.out.println(car); } } ObjectFactory（Spring使用) public interface ObjectFactory { T getObject() throws BeansException; } ObjectFactory： 它的目的也是作为一个工厂，来生成Object（这个接口只有一个方法getObject())。这个接口一般被用来，包装一个factory，通过个这工厂来返回一个新实例（prototype类型)。这个接口和FactoryBean有点像，但FactoryBean的实现是被当做一个SPI（Service Provider Interface)实例来使用在BeanFactory里面；ObjectFactory的实现一般被用来注入到其它Bean中，作为API来使用。就像ObjectFactoryCreatingFactoryBean的例子，它的返回值就是一个ObjectFactory，这个ObjectFactory被注入到了Bean中，在Bean通过这个接口的实例，来取得我们想要的Bean。 总的来说，FactoryBean和ObjectFactory都是用来取得Bean，但使用的方法和地方不同，FactoryBean被配置好后，Spring调用getObject()方法来取得Bean，ObjectFactory配置好后，在Bean里面可以取得ObjectFactory实例，需要我们手动来调用getObject()来取得Bean。InitializingBean InitializingBean接口为bean提供了初始化方法的方式，它只包括afterPropertiesSet方法，凡是继承该接口的类，在初始化bean的时候会执行该方法。 AbstractBeanFactory#getBean public Object getBean(String name) throws BeansException { return doGetBean(name, null, null, false); } doGetBean 有三个方法非常关键：getSingleton，createBean和getObjectForBeanInstance。 protected T doGetBean( final String name, final Class requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException { final String beanName = transformedBeanName(name); Object bean; // Eagerly check singleton cache for manually registered singletons. // 检查缓存中或者实例工厂中是否有对应的实例（解决循环依赖的问题) // Spring创建bean的原则是不等bean创建完成就会将创建bean的ObjectFactory提早曝光，也就是将ObjectFactory加入到缓存中，一旦下个bean创建时需要上个bean则直接使用ObjectFactory。 // 直接尝试从缓存获取或者从singletonFactories中的ObjectFactory中获取 Object sharedInstance = getSingleton(beanName); if (sharedInstance != null && args == null) { // 已经创建过了if (logger.isDebugEnabled()) { if (isSingletonCurrentlyInCreation(beanName)) { logger.debug(\"Returning eagerly cached instance of singleton bean '\" + beanName + \"' that is not fully initialized yet - a consequence of a circular reference\"); } else { logger.debug(\"Returning cached instance of singleton bean '\" + beanName + \"'\"); } } // 返回对应的实例（从缓存中只得到了bean的原始状态，还需要对bean进行实例化) bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); } else { // 没有创建，需要创建 // Fail if we're already creating this bean instance: // We're assumably within a circular reference. if (isPrototypeCurrentlyInCreation(beanName)) { throw new BeanCurrentlyInCreationException(beanName); } // Check if bean definition exists in this factory. BeanFactory parentBeanFactory = getParentBeanFactory(); // 如果beanDefinitionMap（已经加载了的类)中不包含beanName，则尝试从parentBeanFactory处理if (parentBeanFactory != null && !containsBeanDefinition(beanName)) { // Not found -> check parent. String nameToLookup = originalBeanName(name); if (args != null) { // Delegation to parent with explicit args. // 递归 return (T) parentBeanFactory.getBean(nameToLookup, args); } else { // No args -> delegate to standard getBean method. return parentBeanFactory.getBean(nameToLookup, requiredType); } } // 从这里开始创建bean，先进行记录 if (!typeCheckOnly) { markBeanAsCreated(beanName); } try { // 将存储XML配置文件的GenericBeanDefinition转换为RootBeanDefinition；转换的时候如果父类bean不为空的话，那么会合并父类的属性。 final RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); checkMergedBeanDefinition(mbd, beanName, args); // Guarantee initialization of beans that the current bean depends on. // 若存在依赖则需要递归实例化依赖的bean String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) { for (String dep : dependsOn) { if (isDependent(beanName, dep)) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Circular depends-on relationship between '\" + beanName + \"' and '\" + dep + \"'\"); } // 缓存依赖调用 registerDependentBean(dep, beanName); getBean(dep); } } // Create bean instance. // 真正的创建bean if (mbd.isSingleton()) { // 单例 sharedInstance = getSingleton(beanName, new ObjectFactory() { @Override public Object getObject() throws BeansException { try { return createBean(beanName, mbd, args); } catch (BeansException ex) { // Explicitly remove instance from singleton cache: It might have been put there // eagerly by the creation process, to allow for circular reference resolution. // Also remove any beans that received a temporary reference to the bean. destroySingleton(beanName); throw ex; } } }); bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); } else if (mbd.isPrototype()) { // It's a prototype -> create a new instance. Object prototypeInstance = null; try { beforePrototypeCreation(beanName); prototypeInstance = createBean(beanName, mbd, args); } finally { afterPrototypeCreation(beanName); } bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); } else { // 在指定的scope上实例化bean String scopeName = mbd.getScope(); final Scope scope = this.scopes.get(scopeName); if (scope == null) { throw new IllegalStateException(\"No Scope registered for scope name '\" + scopeName + \"'\"); } try { Object scopedInstance = scope.get(beanName, new ObjectFactory() { @Override public Object getObject() throws BeansException { beforePrototypeCreation(beanName); try { return createBean(beanName, mbd, args); } finally { afterPrototypeCreation(beanName); } } }); bean = getObjectForBeanInstance(scopedInstance, name, beanName, mbd); } catch (IllegalStateException ex) { throw new BeanCreationException(beanName, \"Scope '\" + scopeName + \"' is not active for the current thread; consider \" + \"defining a scoped proxy for this bean if you intend to refer to it from a singleton\", ex); } } } catch (BeansException ex) { cleanupAfterBeanCreationFailure(beanName); throw ex; } } // Check if required type matches the type of the actual bean instance. if (requiredType != null && bean != null && !requiredType.isAssignableFrom(bean.getClass())) { try { return getTypeConverter().convertIfNecessary(bean, requiredType); } catch (TypeMismatchException ex) { if (logger.isDebugEnabled()) { logger.debug(\"Failed to convert bean '\" + name + \"' to required type '\" + ClassUtils.getQualifiedName(requiredType) + \"'\", ex); } throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); } } return (T) bean; } - 1) getSingleton(beanName)（借助缓存或singletonFactories) getSingleton(beanName,true) - > true表示允许早期依赖 逻辑： 1)从singletonObjects中获取，它是一个真正的缓存，有就直接返回 2)获取不到再从earlySingletonObjects里面获取 3)还是获取不到，再尝试从singletonFactories里面获取beanName对应的ObjectFactory，然后调用这个ObjectFactory的getObject来获取之前创建的bean，并放到earlySingletonObjects里面去，并且从singletonFactories中remove掉这个ObjectFactory。 成员变量Map： 1)singletonObjects是beanName与beanInstance的Map，是真正的缓存，beanInstance是构造完毕的，凡是正常地构造完毕的单例bean都会放入缓存中。 2)earlySingletonObjects也是beanName与beanInstance的Map，beanInstance是已经调用了createBean方法，但是没有清除加载状态和加入至缓存的bean。仅在当前bean创建时存在，用于检测代理bean循环依赖。 3)singleFactories是beanName与ObjectFactory的Map，仅在当前bean创建时存在，是尚未调用createBean的bean。用于setter循环依赖时实现注入。 4)registeredSingletons：用来保存当前所有已注册的bean。 singletonFactories和earlySingletonObjects都是一个临时工。在所有的对象创建完毕之后，此两个对象的size都为0。 protected Object getSingleton(String beanName, boolean allowEarlyReference) { Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null && isSingletonCurrentlyInCreation(beanName)) {synchronized (this.singletonObjects) { singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null && allowEarlyReference) { ObjectFactory singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) { singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); } } } } return (singletonObject != NULL_OBJECT ? singletonObject : null); } - 2) getSingleton(beanName,ObjectFactory)（从头创建单例bean) 从头创建一个单例的bean需要经过getSingleton(beanName,ObjectFactory)和createBean两个关键方法。 逻辑： 1)检查缓存是否已经加载过 2)若没有加载，则记录beanName的正在加载状态 3)加载单例前 记录加载状态 4)通过ObjectFactory的getObject方法实例化bean 5)加载单例后 清除加载状态 6)将结果记录至缓存并删除加载bean过程中所记录的各种辅助状态 7)返回处理结果 public Object getSingleton(String beanName, ObjectFactory singletonFactory) { Assert.notNull(beanName, \"'beanName' must not be null\"); synchronized (this.singletonObjects) { // 检查对应的bean是否已经被加载过Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) { // 没有被加载过 if (this.singletonsCurrentlyInDestruction) { throw new BeanCreationNotAllowedException(beanName, \"Singleton bean creation not allowed while singletons of this factory are in destruction \" + \"(Do not request a bean from a BeanFactory in a destroy method implementation!)\"); } if (logger.isDebugEnabled()) { logger.debug(\"Creating shared instance of singleton bean '\" + beanName + \"'\"); } // 记录加载状态 beforeSingletonCreation(beanName); boolean newSingleton = false; boolean recordSuppressedExceptions = (this.suppressedExceptions == null); if (recordSuppressedExceptions) { this.suppressedExceptions = new LinkedHashSet(); } try { // 初始化bean，在这里调用了createBean方法 singletonObject = singletonFactory.getObject(); newSingleton = true; } catch (IllegalStateException ex) { // Has the singleton object implicitly appeared in the meantime -> // if yes, proceed with it since the exception indicates that state. singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) { throw ex; } } catch (BeanCreationException ex) { if (recordSuppressedExceptions) { for (Exception suppressedException : this.suppressedExceptions) { ex.addRelatedCause(suppressedException); } } throw ex; } finally { if (recordSuppressedExceptions) { this.suppressedExceptions = null; } // 清除加载状态 afterSingletonCreation(beanName); } if (newSingleton) { // 加入缓存 addSingleton(beanName, singletonObject); } } return (singletonObject != NULL_OBJECT ? singletonObject : null); } } 2.1) beforeSingletonCreation （记录加载状态) 记录加载状态，通过this.singletonsCurrentlyInCreation.add(beanName)将当前正在创建的bean记录在缓存中，这样便可以对循环依赖进行检测。 protected void beforeSingletonCreation(String beanName) { if (!this.inCreationCheckExclusions.contains(beanName) && !this.singletonsCurrentlyInCreation.add(beanName)) { throw new BeanCurrentlyInCreationException(beanName); } } 2.2) afterSingletonCreation（清除加载状态) 当bean加载结束后需要移除缓存中对该bean的正在加载状态的记录。 protected void afterSingletonCreation(String beanName) { if (!this.inCreationCheckExclusions.contains(beanName) && !this.singletonsCurrentlyInCreation.remove(beanName)) {throw new IllegalStateException(\"Singleton '\" + beanName + \"' isn't currently in creation\"); } } 2.3) addSingleton（结果记录至缓存) 将结果记录至缓存中并删除加载bean过程中所记录的各种辅助状态 protected void addSingleton(String beanName, Object singletonObject) { synchronized (this.singletonObjects) {this.singletonObjects.put(beanName, (singletonObject != null ? singletonObject : NULL_OBJECT)); this.singletonFactories.remove(beanName); this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); } } 3) createBean（创建单例或多例的bean，在3中有被调用) protected Object createBean(String beanName, RootBeanDefinition mbd, Object[] args) throws BeanCreationException { if (logger.isDebugEnabled()) { logger.debug(\"Creating instance of bean '\" + beanName + \"'\"); } RootBeanDefinition mbdToUse = mbd; // Make sure bean class is actually resolved at this point, and // clone the bean definition in case of a dynamically resolved Class // which cannot be stored in the shared merged bean definition. // 锁定class，根据设置的class属性或者根据className来解析Class Class resolvedClass = resolveBeanClass(mbd, beanName); if (resolvedClass != null && !mbd.hasBeanClass() && mbd.getBeanClassName() != null) { mbdToUse = new RootBeanDefinition(mbd); mbdToUse.setBeanClass(resolvedClass); } // Prepare method overrides. try { // 验证及准备覆盖的方法 mbdToUse.prepareMethodOverrides(); } catch (BeanDefinitionValidationException ex) { throw new BeanDefinitionStoreException(mbdToUse.getResourceDescription(), beanName, \"Validation of method overrides failed\", ex); } try { // Give BeanPostProcessors a chance to return a proxy instead of the target bean instance. // 给BeanPostProcessor一个机会来返回代理来替代真正的实例Object bean = resolveBeforeInstantiation(beanName, mbdToUse); if (bean != null) { return bean; } } catch (Throwable ex) {throw new BeanCreationException(mbdToUse.getResourceDescription(), beanName, \"BeanPostProcessor before instantiation of bean failed\", ex); } // 实际创建bean Object beanInstance = doCreateBean(beanName, mbdToUse, args); if (logger.isDebugEnabled()) {logger.debug(\"Finished creating instance of bean '\" + beanName + \"'\"); } return beanInstance; } 3.1) AbstractBeanDefinition#prepareMethodOverrides（决定实例化策略->反射 or CGLIB) 验证及准备覆盖的方法 在Spring配置中存在lookup-method和replace-method两个配置功能，而这两个配置的加载其实就是将配置统一存放在BeanDefinition中的methodOverrides属性里，这两个功能实现原理其实是在bean实例化的时候如果检测到存在methodOverrides属性，会动态地为当前bean生成代理并使用对应的拦截器为bean做增强处理，相关逻辑实现在bean的实例化部分详细介绍。 public void prepareMethodOverrides() throws BeanDefinitionValidationException { // Check that lookup methods exists. MethodOverrides methodOverrides = getMethodOverrides(); if (!methodOverrides.isEmpty()) { Set overrides = methodOverrides.getOverrides(); synchronized (overrides) { for (MethodOverride mo : overrides) { prepareMethodOverride(mo); } } } } - 3.1.1) prepareMethodOverride protected void prepareMethodOverride(MethodOverride mo) throws BeanDefinitionValidationException { //获取对应类中对应方法名的个数 int count = ClassUtils.getMethodCountForName(getBeanClass(), mo.getMethodName()); if (count == 0) { throw new BeanDefinitionValidationException( \"Invalid method override: no method with name '\" + mo.getMethodName() + \"' on class [\" + getBeanClassName() + \"]\"); } else if (count == 1) { // Mark override as not overloaded, to avoid the overhead of arg type checking. // 标记MethodOverride暂未被覆盖，避免参数类型检查的开销mo.setOverloaded(false); } } 3.2) resolveBeforeInstantiation（可能会创建代理过的bean) 如果该方法返回bean不为空，则跳过后续实际创建bean的过程，直接返回代理后的bean。 与AOP有关！ protected Object resolveBeforeInstantiation(String beanName, RootBeanDefinition mbd) { Object bean = null; if (!Boolean.FALSE.equals(mbd.beforeInstantiationResolved)) { // Make sure bean class is actually resolved at this point. if (!mbd.isSynthetic() && hasInstantiationAwareBeanPostProcessors()) { Class targetType = determineTargetType(beanName, mbd); if (targetType != null) { bean = applyBeanPostProcessorsBeforeInstantiation(targetType, beanName); if (bean != null) { bean = applyBeanPostProcessorsAfterInitialization(bean, beanName); } } } mbd.beforeInstantiationResolved = (bean != null); } return bean; } 3.2.1) applyBeanPostProcessorsBeforeInstantiation（实例化前的后处理器应用) protected Object applyBeanPostProcessorsBeforeInstantiation(Class beanClass, String beanName) { for (BeanPostProcessor bp : getBeanPostProcessors()) { if (bp instanceof InstantiationAwareBeanPostProcessor) { InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; Object result = ibp.postProcessBeforeInstantiation(beanClass, beanName); if (result != null) { return result; } } } return null; } 3.2.2) applyBeanPostProcessorsAfterInitialization（实例化后的后处理器应用) Spring中的规则是在bean的初始化后尽可能保证将注册的后处理器的postProcessAfterInitialization方法应用到该bean中，因为如果返回的bean不为空，那么便不会再次经历普通bean的创建过程。 public Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException { Object result = existingBean; for (BeanPostProcessor beanProcessor : getBeanPostProcessors()) { result = beanProcessor.postProcessAfterInitialization(result, beanName); if (result == null) { return result; } } return result; } 3.3) doCreateBean（创建常规bean) 逻辑： 1)如果是单例，则需要首先清除缓存factoryBeanInstanceCache 2)实例化bean，将BeanDefinition转换为BeanWrapper。 转换过程： 如果存在工厂方法，则使用工厂方法进行初始化 一个类有多个构造函数，每个构造函数都有不同的参数，所以需要根据参数锁定构造函数并进行初始化 如果既不存在工厂方法也不存在带有参数的构造函数，则使用默认的构造函数进行bean的初始化。 3)MergedBeanDefinitionPostProcessor的应用 bean合并后的处理，Autowired注解正是通过此方法实现诸如类型的预解析。 4)添加singletonFactories缓存 5)属性填充 6)代理bean循环依赖检查，对于已加载的bean，检测是否已经出现了循环依赖，并判断是否需要抛出异常。（仅针对于当前bean在initializeBean中被代理过的情况，正常的循环依赖在此之前就已经被检测出来的，只有代理后的bean的循环依赖是在这里检查的) 7)注册DisposableBean 如果配置了destroy-method，这里需要注册以便于在销毁时候调用。 8)完成创建并返回 protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) throws BeanCreationException { // Instantiate the bean. BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) { instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); } // 根据指定bean使用对应的策略创建新的实例，如：工厂方法；构造器注入；简单初始化 if (instanceWrapper == null) { instanceWrapper = createBeanInstance(beanName, mbd, args); } final Object bean = (instanceWrapper != null ? instanceWrapper.getWrappedInstance() : null); Class beanType = (instanceWrapper != null ? instanceWrapper.getWrappedClass() : null); mbd.resolvedTargetType = beanType; // Allow post-processors to modify the merged bean definition. synchronized (mbd.postProcessingLock) { if (!mbd.postProcessed) { try { applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); } catch (Throwable ex) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Post-processing of merged bean definition failed\", ex); } mbd.postProcessed = true; } } // Eagerly cache singletons to be able to resolve circular references // even when triggered by lifecycle interfaces like BeanFactoryAware. // 是否需要提早曝光：单例&&允许循环依赖&&当前bean正在创建中 // 检测循环依赖 boolean earlySingletonExposure = (mbd.isSingleton() && this.allowCircularReferences && isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) {if (logger.isDebugEnabled()) { logger.debug(\"Eagerly caching bean '\" + beanName + \"' to allow for resolving potential circular references\"); } // 放入singletonFactories，下次重新获取当前bean时可以直接返回 addSingletonFactory(beanName, new ObjectFactory() { @Override public Object getObject() throws BeansException { // 对bean再一次依赖引用，主要应用SmartInstantiationAware BeanPostProcessor ，其中AOP就是在这里将advice动态织入bean中，若没有则直接返回bean，不做任何处理 return getEarlyBeanReference(beanName, mbd, bean); } }); } // 填充bean，将属性值注入 // Initialize the bean instance. Object exposedObject = bean; try { // 3.3.3populateBean(beanName, mbd, instanceWrapper); if (exposedObject != null) { // 3.3.4 // 调用初始化方法，比如init-method // 注意这里是重新赋值，因为初始化会调用BeanPostProcessor，可能会返回代理对象 exposedObject = initializeBean(beanName, exposedObject, mbd); } } catch (Throwable ex) {if (ex instanceof BeanCreationException && beanName.equals(((BeanCreationException) ex).getBeanName())) { throw (BeanCreationException) ex; } else { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Initialization of bean failed\", ex); } } // 循环依赖检查 if (earlySingletonExposure) {Object earlySingletonReference = getSingleton(beanName, false); // 如果singletonObjects（缓存)中存在当前bean，earlySingletonReference不为空，它指向的是缓存中的当前bean，但未必和当前bean是同一个，因为上面initializeBean时可能会返回代理后的beanif (earlySingletonReference != null) { if (exposedObject == bean) { exposedObject = earlySingletonReference; } // bean不同，说明使用动态代理对其进行了增强，这是不被允许的 // 因为其他bean依赖于当前bean，注入的是从singletonFactories中取出来的，并不是当前bean（代理后的) else if (!this.allowRawInjectionDespiteWrapping && hasDependentBean(beanName)) { String[] dependentBeans = getDependentBeans(beanName); Set actualDependentBeans = new LinkedHashSet(dependentBeans.length); for (String dependentBean : dependentBeans) { if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) { actualDependentBeans.add(dependentBean); } } // 因为bean创建后它所依赖的bean一定是已经创建的，actualDependentBeans不为空表示当前bean创建后它依赖的bean却没有全部创建完，也就是说存在循环依赖 if (!actualDependentBeans.isEmpty()) { throw new BeanCurrentlyInCreationException(beanName, \"Bean with name '\" + beanName + \"' has been injected into other beans [\" + StringUtils.collectionToCommaDelimitedString(actualDependentBeans) + \"] in its raw version as part of a circular reference, but has eventually been \" + \"wrapped. This means that said other beans do not use the final version of the \" + \"bean. This is often the result of over-eager type matching - consider using \" + \"'getBeanNamesOfType' with the 'allowEagerInit' flag turned off, for example.\"); } } } } // 注册DisposableBean // Register bean as disposable. try { registerDisposableBeanIfNecessary(beanName, bean, mbd); } catch (BeanDefinitionValidationException ex) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Invalid destruction signature\", ex); } return exposedObject; } 3.3.1) createBeanInstance（实例化bean) 逻辑： 1)如果在RootBeanDefinition中存在factoryMethodName属性，或者说在配置文件中配置了factory-method，那么Spring会尝试使用instantiateUsingFactoryMethod方法根据RootBeanDefinition中的配置生成bean的实例。 2)解析构造方法并进行构造方法的实例化。因为一个bean对应的类中可能会有多个构造方法，而每个构造方法的参数不同，Spring再根据参数及类型去判断最终会使用哪个构造方法进行实例化。但是，判断的过程是个比较消耗性能的步骤，所以采用缓存机制，如果已经解析过，则不需要重复解析而是直接从RootBeanDefinition中的属性resolvedConstructorOrFactoryMethod缓存的值去取，否则需要再次解析，并将解析的结果添加至RootBeanDefinition中的属性resolvedConstructorOrFactoryMethod。 protected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, Object[] args) { // Make sure bean class is actually resolved at this point. // 解析class Class beanClass = resolveBeanClass(mbd, beanName); if (beanClass != null && !Modifier.isPublic(beanClass.getModifiers()) && !mbd.isNonPublicAccessAllowed()) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Bean class isn't public, and non-public access not allowed: \" + beanClass.getName()); } // 如果工厂方法不为空，则使用工厂方法初始化策略 if (mbd.getFactoryMethodName() != null) { return instantiateUsingFactoryMethod(beanName, mbd, args); } // Shortcut when re-creating the same bean... boolean resolved = false; boolean autowireNecessary = false; if (args == null) { synchronized (mbd.constructorArgumentLock) { if (mbd.resolvedConstructorOrFactoryMethod != null) { resolved = true; autowireNecessary = mbd.constructorArgumentsResolved; } } } // 如果解析过，那么直接创建；否则要获取构造方法 if (resolved) {if (autowireNecessary) { return autowireConstructor(beanName, mbd, null, null); } else { return instantiateBean(beanName, mbd); } } // 需要根据参数解析构造方法 // Need to determine the constructor... Constructor[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName); if (ctors != null || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_CONSTRUCTOR || mbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args)) { // 构造方法自动注入return autowireConstructor(beanName, mbd, ctors, args); } // 使用默认构造方法 // No special handling: simply use no-arg constructor. return instantiateBean(beanName, mbd); } - 3.3.1.1) instantiateUsingFactoryMethod（工厂方法的实例化) AbstractAutowireCapableBeanFactory. instantiateUsingFactoryMethod protected BeanWrapper instantiateUsingFactoryMethod( String beanName, RootBeanDefinition mbd, Object[] explicitArgs) { return new ConstructorResolver(this).instantiateUsingFactoryMethod(beanName, mbd, explicitArgs); } 3.3.1.2) autowireConstructor（有参数的构造方法的实例化) protected BeanWrapper autowireConstructor( String beanName, RootBeanDefinition mbd, Constructor[] ctors, Object[] explicitArgs) { return new ConstructorResolver(this).autowireConstructor(beanName, mbd, ctors, explicitArgs); } ConstructorResolver.autowireConstructor 逻辑： 1)确定 构造方法的参数 根据explicitArgs参数判断 如果传入的参数explicitArgs不为空，那么可以直接确定参数，因为explicitArgs参数是在getBean的时候用户指定的。——getBean(String name,Object ...args) 在获取bean的时候，用户不但可以指定bean的名称，还可以指定bean所对应类的构造函数或者工厂方法的方法参数，主要用于静态工厂方法的调用，而这里是需要给定完全匹配的参数的。 所以，如果传入参数explicitArgs不为空，则可以确定构造方法参数就是它。 从缓存中获取 如果确定参数的办法之前已经分析过，即构造方法参数已经记录在缓存中，那么可以直接拿来使用。而且，在缓存中缓存的可能是参数的最终类型，也可能是参数的初始类型。 从配置文件获取 即从头开始分析。 分析从获取配置文件中配置的构造方法信息开始，可以调用BeanDefinition.getConstructorArgumentValues()来获取配置的构造方法信息。 有了配置中的信息便可以获取对应的参数值信息了，获取参数值的信息包括直接指定值，而这一处理委托给resolveConstructorArguments方法，并返回能解析到的参数的个数。 2)确定 构造方法 根据构造方法参数在所有构造方法中锁定对应的构造方法，匹配的方法就是根据参数个数匹配，所以在匹配之前需要先对构造方法按照public构造方法优先参数数量降序、非public构造方法参数数量降序。这样可以在遍历的情况下迅速判断排在后面的构造方法参数个数是否符合条件。 由于在配置文件中并不是唯一限制使用参数位置索引的方法去创建，同样还支持指定参数名称进行设定参数值的情况，如，那么这种情况就需要首先确定构造方法中的参数名称。 获取参数名称可以有两种方式，一种是通过注解的方式直接获取，另一种就是使用Spring中提供的工具类ParameterNameDiscoverer来获取。构造方法、参数名称、参数类型、参数值都确定后就可以锁定构造方法以及转换对应的参数类型了。 3)根据确定的构造方法转换对应的参数类型 主要是使用Spring中提供的类型转换器或者用户提供的自定义类型转换器进行转换 4)构造方法不确定性的验证 有时候即使构造方法、参数名称、参数类型、参数值都确定后也不一定会直接锁定构造方法，所以Spring在最后又做了一次验证。 5)根据实例化策略以及得到的构造方法及构造方法参数实例化bean。 public BeanWrapper autowireConstructor(final String beanName, final RootBeanDefinition mbd, Constructor[] chosenCtors, final Object[] explicitArgs) { BeanWrapperImpl bw = new BeanWrapperImpl(); this.beanFactory.initBeanWrapper(bw); Constructor constructorToUse = null; ArgumentsHolder argsHolderToUse = null; Object[] argsToUse = null; // explicitArgs通过getBean方法传入 // 如果getBean方法调用的时候指定方法参数，那么直接使用 if (explicitArgs != null) {argsToUse = explicitArgs; } else { // 如果在getBean方法时候没有指定则尝试从配置文件中解析Object[] argsToResolve = null; // 尝试从缓存中获取synchronized (mbd.constructorArgumentLock) { constructorToUse = (Constructor) mbd.resolvedConstructorOrFactoryMethod; if (constructorToUse != null && mbd.constructorArgumentsResolved) { // Found a cached constructor... // 从缓存中取 argsToUse = mbd.resolvedConstructorArguments; if (argsToUse == null) { // 配置的构造方法参数 argsToResolve = mbd.preparedConstructorArguments; } } } // 如果缓存中存在if (argsToResolve != null) { // 解析参数类型，如给定方法的构造方法A(int,int)，则通过此方法后就会把配置中的(“1”,”1”)转换为(1,1) // 缓存中的值可能是原始值，也可能是最终值 argsToUse = resolvePreparedArguments(beanName, mbd, bw, constructorToUse, argsToResolve); } } //没有被缓存 if (constructorToUse == null) { // Need to resolve the constructor. boolean autowiring = (chosenCtors != null || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_CONSTRUCTOR); ConstructorArgumentValues resolvedValues = null; int minNrOfArgs; if (explicitArgs != null) { minNrOfArgs = explicitArgs.length; } else { // 提取配置文件中的配置的构造方法参数 ConstructorArgumentValues cargs = mbd.getConstructorArgumentValues(); // 用于承载解析后的构造方法参数的值 resolvedValues = new ConstructorArgumentValues(); // 能解析到的参数个数 minNrOfArgs = resolveConstructorArguments(beanName, mbd, bw, cargs, resolvedValues); } // Take specified constructors, if any. Constructor[] candidates = chosenCtors; if (candidates == null) { Class beanClass = mbd.getBeanClass(); try { candidates = (mbd.isNonPublicAccessAllowed() ? beanClass.getDeclaredConstructors() : beanClass.getConstructors()); } catch (Throwable ex) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Resolution of declared constructors on bean Class [\" + beanClass.getName() + \"] from ClassLoader [\" + beanClass.getClassLoader() + \"] failed\", ex); } } // 排序给定的构造方法，public构造方法优先参数数量降序、非public构造方法参数数量降序 AutowireUtils.sortConstructors(candidates); int minTypeDiffWeight = Integer.MAX_VALUE; Set> ambiguousConstructors = null; LinkedList causes = null; for (Constructor candidate : candidates) { Class[] paramTypes = candidate.getParameterTypes(); if (constructorToUse != null && argsToUse.length > paramTypes.length) { // 如果已经找到选用的构造方法或者需要的参数个数小于当前的构造方法参数个数，则终止 // Already found greedy constructor that can be satisfied -> // do not look any further, there are only less greedy constructors left. break; } if (paramTypes.length // 参数个数不相等 continue; } // 参数个数相等 ArgumentsHolder argsHolder; // 如果构造方法有参数，则根据值构造对应参数类型的参数 if (resolvedValues != null) { try { // 从注解上获取参数名称 String[] paramNames = ConstructorPropertiesChecker.evaluate(candidate, paramTypes.length); if (paramNames == null) { // 获取参数名称探索器 ParameterNameDiscoverer pnd = this.beanFactory.getParameterNameDiscoverer(); if (pnd != null) { // 获取指定构造方法的参数名称 paramNames = pnd.getParameterNames(candidate); } } // 根据名称和数据类型 创建参数持有者 argsHolder = createArgumentArray(beanName, mbd, resolvedValues, bw, paramTypes, paramNames, getUserDeclaredConstructor(candidate), autowiring); } catch (UnsatisfiedDependencyException ex) { if (this.beanFactory.logger.isTraceEnabled()) { this.beanFactory.logger.trace( \"Ignoring constructor [\" + candidate + \"] of bean '\" + beanName + \"': \" + ex); } // Swallow and try next constructor. if (causes == null) { causes = new LinkedList(); } causes.add(ex); continue; } } // 如果构造方法没有参数 else { // Explicit arguments given -> arguments length must match exactly. if (paramTypes.length != explicitArgs.length) { continue; } argsHolder = new ArgumentsHolder(explicitArgs); } // 探测是否有不确定性的构造方法存在，例如不同构造方法的参数为父子关系 int typeDiffWeight = (mbd.isLenientConstructorResolution() ? argsHolder.getTypeDifferenceWeight(paramTypes) : argsHolder.getAssignabilityWeight(paramTypes)); // Choose this constructor if it represents the closest match. // 如果它代表着当前最接近的匹配则选择作为构造方法 if (typeDiffWeight >(); ambiguousConstructors.add(constructorToUse); } ambiguousConstructors.add(candidate); } } if (constructorToUse == null) { if (causes != null) { UnsatisfiedDependencyException ex = causes.removeLast(); for (Exception cause : causes) { this.beanFactory.onSuppressedException(cause); } throw ex; } throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Could not resolve matching constructor \" + \"(hint: specify index/type/name arguments for simple parameters to avoid type ambiguities)\"); } else if (ambiguousConstructors != null && !mbd.isLenientConstructorResolution()) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Ambiguous constructor matches found in bean '\" + beanName + \"' \" + \"(hint: specify index/type/name arguments for simple parameters to avoid type ambiguities): \" + ambiguousConstructors); } if (explicitArgs == null) { // 将解析的构造方法加入缓存 argsHolderToUse.storeCache(mbd, constructorToUse); } } try { Object beanInstance; if (System.getSecurityManager() != null) { final Constructor ctorToUse = constructorToUse; final Object[] argumentsToUse = argsToUse; beanInstance = AccessController.doPrivileged(new PrivilegedAction() { @Override public Object run() { return beanFactory.getInstantiationStrategy().instantiate( mbd, beanName, beanFactory, ctorToUse, argumentsToUse); } }, beanFactory.getAccessControlContext()); } else { beanInstance = this.beanFactory.getInstantiationStrategy().instantiate( mbd, beanName, this.beanFactory, constructorToUse, argsToUse); } // 将构建的实例加入BeanWrapper中 bw.setBeanInstance(beanInstance); return bw; } catch (Throwable ex) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Bean instantiation via constructor failed\", ex); } } - 3.3.1.2.1) createArgumentArray（递归获取参数的bean) private ArgumentsHolder createArgumentArray( String beanName, RootBeanDefinition mbd, ConstructorArgumentValues resolvedValues, BeanWrapper bw, Class[] paramTypes, String[] paramNames, Object methodOrCtor, boolean autowiring) throws UnsatisfiedDependencyException { String methodType = (methodOrCtor instanceof Constructor ? \"constructor\" : \"factory method\"); TypeConverter converter = (this.beanFactory.getCustomTypeConverter() != null ? this.beanFactory.getCustomTypeConverter() : bw); ArgumentsHolder args = new ArgumentsHolder(paramTypes.length); Set usedValueHolders = new HashSet(paramTypes.length); Set autowiredBeanNames = new LinkedHashSet(4); for (int paramIndex = 0; paramIndex paramType = paramTypes[paramIndex]; String paramName = (paramNames != null ? paramNames[paramIndex] : null); // Try to find matching constructor argument value, either indexed or generic. ConstructorArgumentValues.ValueHolder valueHolder = resolvedValues.getArgumentValue(paramIndex, paramType, paramName, usedValueHolders); // If we couldn't find a direct match and are not supposed to autowire, // let's try the next generic, untyped argument value as fallback: // it could match after type conversion (for example, String -> int). if (valueHolder == null && !autowiring) { valueHolder = resolvedValues.getGenericArgumentValue(null, null, usedValueHolders); } if (valueHolder != null) { // We found a potential match - let's give it a try. // Do not consider the same value definition multiple times! usedValueHolders.add(valueHolder); Object originalValue = valueHolder.getValue(); Object convertedValue; if (valueHolder.isConverted()) { convertedValue = valueHolder.getConvertedValue(); args.preparedArguments[paramIndex] = convertedValue; } else { ConstructorArgumentValues.ValueHolder sourceHolder = (ConstructorArgumentValues.ValueHolder) valueHolder.getSource(); Object sourceValue = sourceHolder.getValue(); try { convertedValue = converter.convertIfNecessary(originalValue, paramType, MethodParameter.forMethodOrConstructor(methodOrCtor, paramIndex)); // TODO re-enable once race condition has been found (SPR-7423) /* if (originalValue == sourceValue || sourceValue instanceof TypedStringValue) { // Either a converted value or still the original one: store converted value. sourceHolder.setConvertedValue(convertedValue); args.preparedArguments[paramIndex] = convertedValue; } else { */ args.resolveNecessary = true; args.preparedArguments[paramIndex] = sourceValue; // } } catch (TypeMismatchException ex) { throw new UnsatisfiedDependencyException( mbd.getResourceDescription(), beanName, paramIndex, paramType, \"Could not convert \" + methodType + \" argument value of type [\" + ObjectUtils.nullSafeClassName(valueHolder.getValue()) + \"] to required type [\" + paramType.getName() + \"]: \" + ex.getMessage()); } } args.arguments[paramIndex] = convertedValue; args.rawArguments[paramIndex] = originalValue; } else { // No explicit match found: we're either supposed to autowire or // have to fail creating an argument array for the given constructor. if (!autowiring) { throw new UnsatisfiedDependencyException( mbd.getResourceDescription(), beanName, paramIndex, paramType, \"Ambiguous \" + methodType + \" argument types - \" + \"did you specify the correct bean references as \" + methodType + \" arguments?\"); } try { MethodParameter param = MethodParameter.forMethodOrConstructor(methodOrCtor, paramIndex); Object autowiredArgument = resolveAutowiredArgument(param, beanName, autowiredBeanNames, converter); args.rawArguments[paramIndex] = autowiredArgument; args.arguments[paramIndex] = autowiredArgument; args.preparedArguments[paramIndex] = new AutowiredArgumentMarker(); args.resolveNecessary = true; } catch (BeansException ex) { throw new UnsatisfiedDependencyException( mbd.getResourceDescription(), beanName, paramIndex, paramType, ex); } } } for (String autowiredBeanName : autowiredBeanNames) { this.beanFactory.registerDependentBean(autowiredBeanName, beanName); if (this.beanFactory.logger.isDebugEnabled()) { this.beanFactory.logger.debug(\"Autowiring by type from bean name '\" + beanName + \"' via \" + methodType + \" to bean named '\" + autowiredBeanName + \"'\"); } } return args; } - 3.3.1.2.1.1) resolveAutowiredArgument protected Object resolveAutowiredArgument( MethodParameter param, String beanName, Set autowiredBeanNames, TypeConverter typeConverter) { return this.beanFactory.resolveDependency( new DependencyDescriptor(param, true), beanName, autowiredBeanNames, typeConverter); } 3.3.1.2.2) InstantiationStrategy.instantiate 实例化的时候可以采用不同的策略进行实例化。 如果beanDefinition.getMethodOverrides()为空，即用户没有使用replace或lookup的配置方法，那么直接使用反射的方式；如果使用了，需要将这两个配置通过的功能切入进去，所以就必须要使用动态代理的方式将包含两个特性所对应的逻辑的拦截增强器设置进去，这样才可以保证在调用方法的时候会被相应的拦截器增强，返回值为包含拦截器的代理实例。 以SimpleInstantiationStrategy为例： public Object instantiate(RootBeanDefinition bd, String beanName, BeanFactory owner) { // Don't override the class with CGLIB if no overrides. // 如果有需要覆盖或者动态替换的方法，则使用cglib进行动态代理，因为可以在创建代理的同时将动态方法织入类中，但是如果没有需要动态改变的方法，为了方便直接反射就可以了 if (bd.getMethodOverrides().isEmpty()) { // 没有需要覆盖的方法 Constructor constructorToUse; synchronized (bd.constructorArgumentLock) { constructorToUse = (Constructor) bd.resolvedConstructorOrFactoryMethod; if (constructorToUse == null) { final Class clazz = bd.getBeanClass(); if (clazz.isInterface()) { throw new BeanInstantiationException(clazz, \"Specified class is an interface\"); } try { if (System.getSecurityManager() != null) { constructorToUse = AccessController.doPrivileged(new PrivilegedExceptionAction>() { @Override public Constructor run() throws Exception { return clazz.getDeclaredConstructor((Class[]) null); } }); } else { constructorToUse = clazz.getDeclaredConstructor((Class[]) null); } bd.resolvedConstructorOrFactoryMethod = constructorToUse; } catch (Throwable ex) { throw new BeanInstantiationException(clazz, \"No default constructor found\", ex); } } } // 反射实例化 return BeanUtils.instantiateClass(constructorToUse); } else { // Must generate CGLIB subclass. return instantiateWithMethodInjection(bd, beanName, owner); } } 以CglibSubclassingInstantiationStrategy为例： public Object instantiate(Constructor ctor, Object... args) { Class subclass = createEnhancedSubclass(this.beanDefinition); Object instance; if (ctor == null) { instance = BeanUtils.instantiateClass(subclass); } else { try { Constructor enhancedSubclassConstructor = subclass.getConstructor(ctor.getParameterTypes()); instance = enhancedSubclassConstructor.newInstance(args); } catch (Exception ex) { throw new BeanInstantiationException(this.beanDefinition.getBeanClass(), \"Failed to invoke constructor for CGLIB enhanced subclass [\" + subclass.getName() + \"]\", ex); } } // SPR-10785: set callbacks directly on the instance instead of in the // enhanced class (via the Enhancer) in order to avoid memory leaks. Factory factory = (Factory) instance; factory.setCallbacks(new Callback[] {NoOp.INSTANCE, new LookupOverrideMethodInterceptor(this.beanDefinition, this.owner), new ReplaceOverrideMethodInterceptor(this.beanDefinition, this.owner)}); return instance; } createEnhancedSubclass: private Class createEnhancedSubclass(RootBeanDefinition beanDefinition) { Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(beanDefinition.getBeanClass()); enhancer.setNamingPolicy(SpringNamingPolicy.INSTANCE); if (this.owner instanceof ConfigurableBeanFactory) { ClassLoader cl = ((ConfigurableBeanFactory) this.owner).getBeanClassLoader(); enhancer.setStrategy(new ClassLoaderAwareGeneratorStrategy(cl)); } enhancer.setCallbackFilter(new MethodOverrideCallbackFilter(beanDefinition)); enhancer.setCallbackTypes(CALLBACK_TYPES); return enhancer.createClass(); } 3.3.1.3) instantiateBean（无参数的构造方法的实例化) protected BeanWrapper instantiateBean(final String beanName, final RootBeanDefinition mbd) { try { Object beanInstance; final BeanFactory parent = this; if (System.getSecurityManager() != null) { beanInstance = AccessController.doPrivileged(new PrivilegedAction() { @Override public Object run() { return getInstantiationStrategy().instantiate(mbd, beanName, parent); } }, getAccessControlContext()); } else { // 实例化即可，见3.3.1.2.1 beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, parent); } BeanWrapper bw = new BeanWrapperImpl(beanInstance); initBeanWrapper(bw); return bw; } catch (Throwable ex) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Instantiation of bean failed\", ex); } } 3.3.2) addSingletonFactory（singletonFactories缓存) 执行到这里时可以肯定当前bean已经构造完成，只是尚未填充属性，但是内存地址已经确定了。将当前bean加入到singletonFactories中，下次getBean时会先检测当前bean是否已经被加入到singletonFactories，如果已经存在，则返回缓存，否则就正常创建。 protected void addSingletonFactory(String beanName, ObjectFactory singletonFactory) { Assert.notNull(singletonFactory, \"Singleton factory must not be null\"); synchronized (this.singletonObjects) {if (!this.singletonObjects.containsKey(beanName)) { this.singletonFactories.put(beanName, singletonFactory); this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); } } } 3.3.3) getEarlyBeanReference（从singletonFactories取出来时调用的getObject) protected Object getEarlyBeanReference(String beanName, RootBeanDefinition mbd, Object bean) { Object exposedObject = bean; if (bean != null && !mbd.isSynthetic() && hasInstantiationAwareBeanPostProcessors()) {for (BeanPostProcessor bp : getBeanPostProcessors()) { if (bp instanceof SmartInstantiationAwareBeanPostProcessor) { SmartInstantiationAwareBeanPostProcessor ibp = (SmartInstantiationAwareBeanPostProcessor) bp; exposedObject = ibp.getEarlyBeanReference(exposedObject, beanName); if (exposedObject == null) { return null; } } } } return exposedObject; } - 3.3.4) polulateBean（属性值注入) 逻辑： 1)InstantiationAwareBeanPostProcessor处理器的postProcessAfterInstantiation函数的应用，此函数可以控制程序是否继续进行属性填充。 2)根据注入类型（byName/byType)，提取依赖的bean，并统一存入PropertyValues中。 3)应用InstantiationAwareBeanPostProcessor处理器的postProcessPropertyValues方法，对属性获取完毕填充前 对属性的再次处理，典型应用是RequiredAnnotationBeanPostProcessor类中对属性的验证 4)将所有PropertyValues中的属性填充至BeanWrapper中。 protected void populateBean(String beanName, RootBeanDefinition mbd, BeanWrapper bw) { PropertyValues pvs = mbd.getPropertyValues(); if (bw == null) { if (!pvs.isEmpty()) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Cannot apply property values to null instance\"); } else { // 没有可填充的属性 // Skip property population phase for null instance. return; } } // 给InstantiationAwareBeanPostProcessor最后一次机会在属性设置前来改变bean // 如：可以用来支持属性注入的类型 // Give any InstantiationAwareBeanPostProcessors the opportunity to modify the // state of the bean before properties are set. This can be used, for example, // to support styles of field injection. boolean continueWithPropertyPopulation = true; if (!mbd.isSynthetic() && hasInstantiationAwareBeanPostProcessors()) { for (BeanPostProcessor bp : getBeanPostProcessors()) { if (bp instanceof InstantiationAwareBeanPostProcessor) { InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; // 返回值为是否继续填充bean if (!ibp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) { continueWithPropertyPopulation = false; break; } } } } // 如果后处理器发出停止填充命令则终止后续的执行 if (!continueWithPropertyPopulation) { return; } if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) { MutablePropertyValues newPvs = new MutablePropertyValues(pvs); // Add property values based on autowire by name if applicable. if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME) { // 根据名称自动注入，存入PropertyValues autowireByName(beanName, mbd, bw, newPvs); } // Add property values based on autowire by type if applicable. if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) { // 根据类型自动注入，存入PropertyValues autowireByType(beanName, mbd, bw, newPvs); } pvs = newPvs; } // 后处理器已经初始化 boolean hasInstAwareBpps = hasInstantiationAwareBeanPostProcessors(); // 需要依赖检查 boolean needsDepCheck = (mbd.getDependencyCheck() != RootBeanDefinition.DEPENDENCY_CHECK_NONE); if (hasInstAwareBpps || needsDepCheck) { PropertyDescriptor[] filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); if (hasInstAwareBpps) { // 对所有需要依赖检查的属性进行后处理 for (BeanPostProcessor bp : getBeanPostProcessors()) { if (bp instanceof InstantiationAwareBeanPostProcessor) { InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; pvs = ibp.postProcessPropertyValues(pvs, filteredPds, bw.getWrappedInstance(), beanName); if (pvs == null) { return; } } } } if (needsDepCheck) { checkDependencies(beanName, mbd, filteredPds, pvs); } } // 将属性应用到bean中 applyPropertyValues(beanName, mbd, bw, pvs); } 3.3.4.1) autowireByName（按名获取待注入的属性) protected void autowireByName(String beanName, AbstractBeanDefinition mbd, BeanWrapper bw, MutablePropertyValues pvs) { // 寻找bw中需要依赖注入的属性 String[] propertyNames = unsatisfiedNonSimpleProperties(mbd, bw); for (String propertyName : propertyNames) {if (containsBean(propertyName)) { // 递归初始化相关的bean Object bean = getBean(propertyName); pvs.add(propertyName, bean); // 注册依赖 registerDependentBean(propertyName, beanName); if (logger.isDebugEnabled()) { logger.debug(\"Added autowiring by name from bean name '\" + beanName + \"' via property '\" + propertyName + \"' to bean named '\" + propertyName + \"'\"); } } else { if (logger.isTraceEnabled()) { logger.trace(\"Not autowiring property '\" + propertyName + \"' of bean '\" + beanName + \"' by name: no matching bean found\"); } } } } 3.3.4.2) autowireByType（按类型获取待注入的属性) protected void autowireByType( String beanName, AbstractBeanDefinition mbd, BeanWrapper bw, MutablePropertyValues pvs) { TypeConverter converter = getCustomTypeConverter(); if (converter == null) { converter = bw; } Set autowiredBeanNames = new LinkedHashSet(4); // 寻找bw中需要依赖注入的属性 String[] propertyNames = unsatisfiedNonSimpleProperties(mbd, bw); for (String propertyName : propertyNames) {try { PropertyDescriptor pd = bw.getPropertyDescriptor(propertyName); // Don't try autowiring by type for type Object: never makes sense, // even if it technically is a unsatisfied, non-simple property. if (Object.class != pd.getPropertyType()) { // 探测指定属性的set方法 MethodParameter methodParam = BeanUtils.getWriteMethodParameter(pd); // Do not allow eager init for type matching in case of a prioritized post-processor. boolean eager = !PriorityOrdered.class.isAssignableFrom(bw.getWrappedClass()); DependencyDescriptor desc = new AutowireByTypeDependencyDescriptor(methodParam, eager); // 解析指定beanName的属性所匹配的值，并把解析到的属性名称存储在autowireBeanNames中 Object autowiredArgument = resolveDependency(desc, beanName, autowiredBeanNames, converter); if (autowiredArgument != null) { pvs.add(propertyName, autowiredArgument); } // 如@Autowired private List list; // 这时候会找到所有匹配A类型的bean并将其注入，所以每个属性可能会对应多个bean for (String autowiredBeanName : autowiredBeanNames) { // 注册依赖 registerDependentBean(autowiredBeanName, beanName); if (logger.isDebugEnabled()) { logger.debug(\"Autowiring by type from bean name '\" + beanName + \"' via property '\" + propertyName + \"' to bean named '\" + autowiredBeanName + \"'\"); } } autowiredBeanNames.clear(); } } catch (BeansException ex) { throw new UnsatisfiedDependencyException(mbd.getResourceDescription(), beanName, propertyName, ex); } } } 3.3.4.2.1) DefaultListableBeanFactory#resolveDependency（寻找类型匹配) public Object resolveDependency(DependencyDescriptor descriptor, String requestingBeanName, Set autowiredBeanNames, TypeConverter typeConverter) throws BeansException { descriptor.initParameterNameDiscovery(getParameterNameDiscoverer()); // 处理特定的类的注入 if (javaUtilOptionalClass == descriptor.getDependencyType()) {return new OptionalDependencyFactory().createOptionalDependency(descriptor, requestingBeanName); } else if (ObjectFactory.class == descriptor.getDependencyType() || ObjectProvider.class == descriptor.getDependencyType()) { return new DependencyObjectProvider(descriptor, requestingBeanName); } else if (javaxInjectProviderClass == descriptor.getDependencyType()) {return new Jsr330ProviderFactory().createDependencyProvider(descriptor, requestingBeanName); } else {Object result = getAutowireCandidateResolver().getLazyResolutionProxyIfNecessary( descriptor, requestingBeanName); if (result == null) { // 处理通用逻辑 result = doResolveDependency(descriptor, requestingBeanName, autowiredBeanNames, typeConverter); } return result; } } 3.3.4.2.1.1) doResolveDependency public Object doResolveDependency(DependencyDescriptor descriptor, String beanName, Set autowiredBeanNames, TypeConverter typeConverter) throws BeansException { InjectionPoint previousInjectionPoint = ConstructorResolver.setCurrentInjectionPoint(descriptor); try { Object shortcut = descriptor.resolveShortcut(this); if (shortcut != null) { return shortcut; } Class type = descriptor.getDependencyType(); Object value = getAutowireCandidateResolver().getSuggestedValue(descriptor); if (value != null) { if (value instanceof String) { String strVal = resolveEmbeddedValue((String) value); BeanDefinition bd = (beanName != null && containsBean(beanName) ? getMergedBeanDefinition(beanName) : null); value = evaluateBeanDefinitionString(strVal, bd); } TypeConverter converter = (typeConverter != null ? typeConverter : getTypeConverter()); return (descriptor.getField() != null ? converter.convertIfNecessary(value, type, descriptor.getField()) : converter.convertIfNecessary(value, type, descriptor.getMethodParameter())); } Object multipleBeans = resolveMultipleBeans(descriptor, beanName, autowiredBeanNames, typeConverter); if (multipleBeans != null) { return multipleBeans; } Map matchingBeans = findAutowireCandidates(beanName, type, descriptor); if (matchingBeans.isEmpty()) { if (descriptor.isRequired()) { raiseNoMatchingBeanFound(type, descriptor.getResolvableType(), descriptor); } return null; } String autowiredBeanName; Object instanceCandidate; if (matchingBeans.size() > 1) { autowiredBeanName = determineAutowireCandidate(matchingBeans, descriptor); if (autowiredBeanName == null) { if (descriptor.isRequired() || !indicatesMultipleBeans(type)) { return descriptor.resolveNotUnique(type, matchingBeans); } else { // In case of an optional Collection/Map, silently ignore a non-unique case: // possibly it was meant to be an empty collection of multiple regular beans // (before 4.3 in particular when we didn't even look for collection beans). return null; } } instanceCandidate = matchingBeans.get(autowiredBeanName); } else { // We have exactly one match. Map.Entry entry = matchingBeans.entrySet().iterator().next(); autowiredBeanName = entry.getKey(); instanceCandidate = entry.getValue(); } if (autowiredBeanNames != null) { autowiredBeanNames.add(autowiredBeanName); } return (instanceCandidate instanceof Class ? descriptor.resolveCandidate(autowiredBeanName, type, this) : instanceCandidate); } finally { ConstructorResolver.setCurrentInjectionPoint(previousInjectionPoint); } } - 3.3.4.2.1.1.1) findAutowireCandidates（获取setter注入的bean) protected Map findAutowireCandidates( String beanName, Class requiredType, DependencyDescriptor descriptor) { String[] candidateNames = BeanFactoryUtils.beanNamesForTypeIncludingAncestors( this, requiredType, true, descriptor.isEager()); Map result = new LinkedHashMap(candidateNames.length); for (Class autowiringType : this.resolvableDependencies.keySet()) { if (autowiringType.isAssignableFrom(requiredType)) { Object autowiringValue = this.resolvableDependencies.get(autowiringType); autowiringValue = AutowireUtils.resolveAutowiringValue(autowiringValue, requiredType); if (requiredType.isInstance(autowiringValue)) { result.put(ObjectUtils.identityToString(autowiringValue), autowiringValue); break; } } } for (String candidateName : candidateNames) { if (!isSelfReference(beanName, candidateName) && isAutowireCandidate(candidateName, descriptor)) { result.put(candidateName, getBean(candidateName)); } } if (result.isEmpty()) { DependencyDescriptor fallbackDescriptor = descriptor.forFallbackMatch(); for (String candidateName : candidateNames) { if (!candidateName.equals(beanName) && isAutowireCandidate(candidateName, fallbackDescriptor)) { result.put(candidateName, getBean(candidateName)); } } } return result; } 3.3.4.3) AutowiredAnnotationBeanPostProcesser#postProcessPropertyValues（@Autowired依赖注入) public PropertyValues postProcessPropertyValues( PropertyValues pvs, PropertyDescriptor[] pds, Object bean, String beanName) throws BeansException { InjectionMetadata metadata = findAutowiringMetadata(beanName, bean.getClass(), pvs); try { metadata.inject(bean, beanName, pvs); } catch (Throwable ex) { throw new BeanCreationException(beanName, \"Injection of autowired dependencies failed\", ex); } return pvs; } - 3.3.4.3.1) InjectionMetadata#inject public void inject(Object target, String beanName, PropertyValues pvs) throws Throwable { Collection elementsToIterate = (this.checkedElements != null ? this.checkedElements : this.injectedElements); if (!elementsToIterate.isEmpty()) { boolean debug = logger.isDebugEnabled(); for (InjectedElement element : elementsToIterate) { if (debug) { logger.debug(\"Processing injected element of bean '\" + beanName + \"': \" + element); } element.inject(target, beanName, pvs); } } } - 3.3.4.3.1.1) AutowiredFieldElement#inject - protected void inject(Object bean, String beanName, PropertyValues pvs) throws Throwable { Field field = (Field) this.member; try { Object value; if (this.cached) { value = resolvedCachedArgument(beanName, this.cachedFieldValue); } else { DependencyDescriptor desc = new DependencyDescriptor(field, this.required); desc.setContainingClass(bean.getClass()); Set autowiredBeanNames = new LinkedHashSet(1); TypeConverter typeConverter = beanFactory.getTypeConverter(); - // 3.3.4.2.1 value = beanFactory.resolveDependency(desc, beanName, autowiredBeanNames, typeConverter); synchronized (this) { if (!this.cached) { if (value != null || this.required) { this.cachedFieldValue = desc; registerDependentBeans(beanName, autowiredBeanNames); if (autowiredBeanNames.size() == 1) { String autowiredBeanName = autowiredBeanNames.iterator().next(); if (beanFactory.containsBean(autowiredBeanName)) { if (beanFactory.isTypeMatch(autowiredBeanName, field.getType())) { this.cachedFieldValue = new RuntimeBeanReference(autowiredBeanName); } } } } else { this.cachedFieldValue = null; } this.cached = true; } } } if (value != null) { ReflectionUtils.makeAccessible(field); field.set(bean, value); } } catch (Throwable ex) { throw new BeanCreationException(\"Could not autowire field: \" + field, ex); } } - 3.3.4.4) applyPropertyValues（注入属性值) AbstractAutowireCapableBeanFactory protected void applyPropertyValues(String beanName, BeanDefinition mbd, BeanWrapper bw, PropertyValues pvs) { if (pvs == null || pvs.isEmpty()) { return; } MutablePropertyValues mpvs = null; List original; if (System.getSecurityManager() != null) { if (bw instanceof BeanWrapperImpl) { ((BeanWrapperImpl) bw).setSecurityContext(getAccessControlContext()); } } if (pvs instanceof MutablePropertyValues) { mpvs = (MutablePropertyValues) pvs; // 如果mpvs中的值已经被转换为对应的类型，则可以直接设置到beanwrapper中if (mpvs.isConverted()) { // Shortcut: use the pre-converted values as-is. try { bw.setPropertyValues(mpvs); return; } catch (BeansException ex) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Error setting property values\", ex); } } original = mpvs.getPropertyValueList(); } else { // 如果pvs并不是使用MutablePropertyValues封装的类型，那么直接使用原始的属性获取方法 original = Arrays.asList(pvs.getPropertyValues()); } TypeConverter converter = getCustomTypeConverter(); if (converter == null) { converter = bw; } // 获取对应的解析器 BeanDefinitionValueResolver valueResolver = new BeanDefinitionValueResolver(this, beanName, mbd, converter); // Create a deep copy, resolving any references for values. List deepCopy = new ArrayList(original.size()); boolean resolveNecessary = false; // 遍历属性，将属性转换为对应类的对应属性的类型 for (PropertyValue pv : original) { if (pv.isConverted()) { deepCopy.add(pv); } else { String propertyName = pv.getName(); Object originalValue = pv.getValue(); Object resolvedValue = valueResolver.resolveValueIfNecessary(pv, originalValue); Object convertedValue = resolvedValue; boolean convertible = bw.isWritableProperty(propertyName) && !PropertyAccessorUtils.isNestedOrIndexedProperty(propertyName); if (convertible) { convertedValue = convertForProperty(resolvedValue, propertyName, bw, converter); } // Possibly store converted value in merged bean definition, // in order to avoid re-conversion for every created bean instance. if (resolvedValue == originalValue) { if (convertible) { pv.setConvertedValue(convertedValue); } deepCopy.add(pv); } else if (convertible && originalValue instanceof TypedStringValue && !((TypedStringValue) originalValue).isDynamic() && !(convertedValue instanceof Collection || ObjectUtils.isArray(convertedValue))) { pv.setConvertedValue(convertedValue); deepCopy.add(pv); } else { resolveNecessary = true; deepCopy.add(new PropertyValue(pv, convertedValue)); } } } if (mpvs != null && !resolveNecessary) { mpvs.setConverted(); } // Set our (possibly massaged) deep copy. try { bw.setPropertyValues(new MutablePropertyValues(deepCopy)); } catch (BeansException ex) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Error setting property values\", ex); } } 3.3.5) initializeBean（调用init-method方法) AbstractAutowireCapableBeanFactory 主要是调用用户设定的初始化方法，还有一些其他工作 protected Object initializeBean(final String beanName, final Object bean, RootBeanDefinition mbd) { if (System.getSecurityManager() != null) { AccessController.doPrivileged(new PrivilegedAction() { @Override public Object run() { // 激活Aware方法 invokeAwareMethods(beanName, bean); return null; } }, getAccessControlContext()); } else { invokeAwareMethods(beanName, bean); } Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) { // 应用后处理器 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); } try { // 激活用户自定义的init方法 invokeInitMethods(beanName, wrappedBean, mbd); } catch (Throwable ex) { throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, \"Invocation of init method failed\", ex); } if (mbd == null || !mbd.isSynthetic()) { wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); } return wrappedBean; } 3.3.5.1) invokeAwareMethods Aware Spring中提供一些Aware相关接口，比如BeanFactoryAware、ApplicationContextAware等，实现这些Aware接口的bean被初始化后，可以取得一些相对应的资源。 如实现BeanFactoryAware的bean在初始化后，Spring容器将会注入BeanFactory的实例。 private void invokeAwareMethods(final String beanName, final Object bean) { if (bean instanceof Aware) { if (bean instanceof BeanNameAware) { ((BeanNameAware) bean).setBeanName(beanName); } if (bean instanceof BeanClassLoaderAware) { ((BeanClassLoaderAware) bean).setBeanClassLoader(getBeanClassLoader()); } if (bean instanceof BeanFactoryAware) { ((BeanFactoryAware) bean).setBeanFactory(AbstractAutowireCapableBeanFactory.this); } } } - 3.3.5.2) BeanPostProcessor 调用用户自定义初始化方法之前和之后分别会调用BeanPostProcessor的postProcessBeforeInitialization和postProcessAfterInitialization方法，使 用户可以根据自己的业务需求进行响应的处理。 public Object applyBeanPostProcessorsBeforeInitialization(Object existingBean, String beanName) throws BeansException { Object result = existingBean; for (BeanPostProcessor beanProcessor : getBeanPostProcessors()) { result = beanProcessor.postProcessBeforeInitialization(result, beanName); if (result == null) { return result; } } return result; } public Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException { Object result = existingBean; for (BeanPostProcessor beanProcessor : getBeanPostProcessors()) { result = beanProcessor.postProcessAfterInitialization(result, beanName); if (result == null) { return result; } } return result; } - 3.3.5.2.1) @PostConstructor 当使用该注解时，Spring会去注册一个BeanPostProcessor：InitDestroyAnnotationBeanPostProcessor。该bean会同自定义的BeanPostProcessor一样，在自定义初始化方法之前和之后被调用（当然@PostConstrcut只会在之前被调用)。 InitDestroyAnnotationBeanPostProcessor#postProcessBeforeInitialization public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { LifecycleMetadata metadata = findLifecycleMetadata(bean.getClass()); try { metadata.invokeInitMethods(bean, beanName); } catch (InvocationTargetException ex) { throw new BeanCreationException(beanName, \"Invocation of init method failed\", ex.getTargetException()); } catch (Throwable ex) { throw new BeanCreationException(beanName, \"Couldn't invoke init method\", ex); } return bean; } - 3.3.5.2.1.1) findLifecycleMetadata private LifecycleMetadata findLifecycleMetadata(Class clazz) { if (this.lifecycleMetadataCache == null) { // Happens after deserialization, during destruction... return buildLifecycleMetadata(clazz); } // Quick check on the concurrent map first, with minimal locking. LifecycleMetadata metadata = this.lifecycleMetadataCache.get(clazz); if (metadata == null) { synchronized (this.lifecycleMetadataCache) { metadata = this.lifecycleMetadataCache.get(clazz); if (metadata == null) { metadata = buildLifecycleMetadata(clazz); this.lifecycleMetadataCache.put(clazz, metadata); } return metadata; } } return metadata; } private LifecycleMetadata buildLifecycleMetadata(final Class clazz) { final boolean debug = logger.isDebugEnabled(); LinkedList initMethods = new LinkedList(); LinkedList destroyMethods = new LinkedList(); Class targetClass = clazz; do { final LinkedList currInitMethods = new LinkedList(); final LinkedList currDestroyMethods = new LinkedList(); ReflectionUtils.doWithLocalMethods(targetClass, new ReflectionUtils.MethodCallback() { @Override public void doWith(Method method) throws IllegalArgumentException, IllegalAccessException { if (initAnnotationType != null) { if (method.getAnnotation(initAnnotationType) != null) { LifecycleElement element = new LifecycleElement(method); currInitMethods.add(element); if (debug) { logger.debug(\"Found init method on class [\" + clazz.getName() + \"]: \" + method); } } } if (destroyAnnotationType != null) { if (method.getAnnotation(destroyAnnotationType) != null) { currDestroyMethods.add(new LifecycleElement(method)); if (debug) { logger.debug(\"Found destroy method on class [\" + clazz.getName() + \"]: \" + method); } } } } }); initMethods.addAll(0, currInitMethods); destroyMethods.addAll(currDestroyMethods); targetClass = targetClass.getSuperclass(); } while (targetClass != null && targetClass != Object.class); return new LifecycleMetadata(clazz, initMethods, destroyMethods); } - 3.3.5.2.1.2) invokeInitMethods public void invokeInitMethods(Object target, String beanName) throws Throwable { Collection initMethodsToIterate = (this.checkedInitMethods != null ? this.checkedInitMethods : this.initMethods); if (!initMethodsToIterate.isEmpty()) { boolean debug = logger.isDebugEnabled(); for (LifecycleElement element : initMethodsToIterate) { if (debug) { logger.debug(\"Invoking init method on bean '\" + beanName + \"': \" + element.getMethod()); } element.invoke(target); } } } 3.3.5.3) invokeInitMethods（激活自定义的init方法) 客户定制的初始化方法除了使用配置init-method外，还可以使自定义的bean实现InitializingBean接口,并在afterPropertiesSet中实现自己的初始化业务逻辑。 init-method和afterPropertiesSet都是在初始化bean时执行，执行顺序是afterPropertiesSet先执行，init-method后执行。 该方法中实现了这两个步骤的初始化方法调用。 protected void invokeInitMethods(String beanName, final Object bean, RootBeanDefinition mbd) throws Throwable { // 首先检查是否是InitializingBean，如果是的话需要调用afterPropertiesSet方法 boolean isInitializingBean = (bean instanceof InitializingBean); if (isInitializingBean && (mbd == null || !mbd.isExternallyManagedInitMethod(\"afterPropertiesSet\"))) { if (logger.isDebugEnabled()) { logger.debug(\"Invoking afterPropertiesSet() on bean with name '\" + beanName + \"'\"); } if (System.getSecurityManager() != null) { try { AccessController.doPrivileged(new PrivilegedExceptionAction() { @Override public Object run() throws Exception { ((InitializingBean) bean).afterPropertiesSet(); return null; } }, getAccessControlContext()); } catch (PrivilegedActionException pae) { throw pae.getException(); } } else { ((InitializingBean) bean).afterPropertiesSet(); } } if (mbd != null) { String initMethodName = mbd.getInitMethodName(); if (initMethodName != null && !(isInitializingBean && \"afterPropertiesSet\".equals(initMethodName)) && !mbd.isExternallyManagedInitMethod(initMethodName)) { invokeCustomInitMethod(beanName, bean, mbd); } } } 3.3.5.3.1) invokeCustomInitMethod protected void invokeCustomInitMethod(String beanName, final Object bean, RootBeanDefinition mbd) throws Throwable { String initMethodName = mbd.getInitMethodName(); final Method initMethod = (mbd.isNonPublicAccessAllowed() ? BeanUtils.findMethod(bean.getClass(), initMethodName) : ClassUtils.getMethodIfAvailable(bean.getClass(), initMethodName)); if (initMethod == null) { if (mbd.isEnforceInitMethod()) { throw new BeanDefinitionValidationException(\"Couldn't find an init method named '\" + initMethodName + \"' on bean with name '\" + beanName + \"'\"); } else { if (logger.isDebugEnabled()) { logger.debug(\"No default init method named '\" + initMethodName + \"' found on bean with name '\" + beanName + \"'\"); } // Ignore non-existent default lifecycle methods. return; } } if (logger.isDebugEnabled()) { logger.debug(\"Invoking init method '\" + initMethodName + \"' on bean with name '\" + beanName + \"'\"); } if (System.getSecurityManager() != null) { AccessController.doPrivileged(new PrivilegedExceptionAction() { @Override public Object run() throws Exception { ReflectionUtils.makeAccessible(initMethod); return null; } }); try { AccessController.doPrivileged(new PrivilegedExceptionAction() { @Override public Object run() throws Exception { initMethod.invoke(bean); return null; } }, getAccessControlContext()); } catch (PrivilegedActionException pae) { InvocationTargetException ex = (InvocationTargetException) pae.getException(); throw ex.getTargetException(); } } else { try { ReflectionUtils.makeAccessible(initMethod); initMethod.invoke(bean); } catch (InvocationTargetException ex) { throw ex.getTargetException(); } } } 3.3.6) getSingleton(beanName,allowEarlyReference) protected Object getSingleton(String beanName, boolean allowEarlyReference) { Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null && isSingletonCurrentlyInCreation(beanName)) {synchronized (this.singletonObjects) { singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null && allowEarlyReference) { ObjectFactory singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) { singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); } } } } return (singletonObject != NULL_OBJECT ? singletonObject : null); } 3.3.7) registerDisposableBeanIfNecessary（注册DisposableBean) 对于销毁方法的扩展，除了配置属性destroy-method，用户还可以注册后处理器DestructionAwareBeanPostProcessor来统一处理bean的销毁方法。 protected void registerDisposableBeanIfNecessary(String beanName, Object bean, RootBeanDefinition mbd) { AccessControlContext acc = (System.getSecurityManager() != null ? getAccessControlContext() : null); if (!mbd.isPrototype() && requiresDestruction(bean, mbd)) {if (mbd.isSingleton()) { // 单例模式下，注册需要销毁的bean，此方法中会处理实现DisposableBean的bean，并且对所有的bean使用DestructionAwareBeanPostProcessors处理 // Register a DisposableBean implementation that performs all destruction // work for the given bean: DestructionAwareBeanPostProcessors, // DisposableBean interface, custom destroy method. registerDisposableBean(beanName, new DisposableBeanAdapter(bean, beanName, mbd, getBeanPostProcessors(), acc)); } else { // 自定义scope的处理 // A bean with a custom scope... Scope scope = this.scopes.get(mbd.getScope()); if (scope == null) { throw new IllegalStateException(\"No Scope registered for scope name '\" + mbd.getScope() + \"'\"); } scope.registerDestructionCallback(beanName, new DisposableBeanAdapter(bean, beanName, mbd, getBeanPostProcessors(), acc)); } } } 4) getObjectForBeanInstance（从bean 的实例中获取对象) 无论是从缓存中获取到的bean还是通过不同的scope策略加载的bean都只是最原始的bean状态，并不一定是我们最终想要的bean。 比如，我们需要对FactoryBean进行处理，那么这里得到的其实是FactoryBean的初始状态，但是我们真正需要的是FactoryBean中定义的factory-method（getObject方法)方法中返回的bean，而getObjectForBeanInstance就是完成这个工作的。 下面这个方法完成了以下任务： 1)对FactoryBean正确性的验证 2)对非FactoryBean不做任何处理 3)对bean进行转换 4)将从Factory解析bean的工作委托给getObjectFromFactoryBean。 protected Object getObjectForBeanInstance( Object beanInstance, String name, String beanName, RootBeanDefinition mbd) { // Don't let calling code try to dereference the factory if the bean isn't a factory. if (BeanFactoryUtils.isFactoryDereference(name) && !(beanInstance instanceof FactoryBean)) { throw new BeanIsNotAFactoryException(transformedBeanName(name), beanInstance.getClass()); } // Now we have the bean instance, which may be a normal bean or a FactoryBean. // If it's a FactoryBean, we use it to create a bean instance, unless the // caller actually wants a reference to the factory. if (!(beanInstance instanceof FactoryBean) || BeanFactoryUtils.isFactoryDereference(name)) { return beanInstance; } Object object = null; if (mbd == null) { object = getCachedObjectForFactoryBean(beanName); } if (object == null) { // Return bean instance from factory. FactoryBean factory = (FactoryBean) beanInstance; // Caches object obtained from FactoryBean if it is a singleton. if (mbd == null && containsBeanDefinition(beanName)) { mbd = getMergedLocalBeanDefinition(beanName); } boolean synthetic = (mbd != null && mbd.isSynthetic()); object = getObjectFromFactoryBean(factory, beanName, !synthetic); } return object; } 4.1) getObjectFromFactoryBean（从FactoryBean中解析bean) 返回的bean如果是单例的，那就必须要保证全局唯一，同时，也因为是单例的，所以不必重复创建，可以使用缓存来提高性能。 protected Object getObjectFromFactoryBean(FactoryBean factory, String beanName, boolean shouldPostProcess) { if (factory.isSingleton() && containsSingleton(beanName)) {synchronized (getSingletonMutex()) { Object object = this.factoryBeanObjectCache.get(beanName); if (object == null) { object = doGetObjectFromFactoryBean(factory, beanName); // Only post-process and store if not put there already during getObject() call above // (e.g. because of circular reference processing triggered by custom getBean calls) Object alreadyThere = this.factoryBeanObjectCache.get(beanName); if (alreadyThere != null) { object = alreadyThere; } else { if (object != null && shouldPostProcess) { try { object = postProcessObjectFromFactoryBean(object, beanName); } catch (Throwable ex) { throw new BeanCreationException(beanName, \"Post-processing of FactoryBean's singleton object failed\", ex); } } this.factoryBeanObjectCache.put(beanName, (object != null ? object : NULL_OBJECT)); } } return (object != NULL_OBJECT ? object : null); } } else {Object object = doGetObjectFromFactoryBean(factory, beanName); if (object != null && shouldPostProcess) { try { object = postProcessObjectFromFactoryBean(object, beanName); } catch (Throwable ex) { throw new BeanCreationException(beanName, \"Post-processing of FactoryBean's object failed\", ex); } } return object; } } 4.1.1) doGetObjectFromFactoryBean private Object doGetObjectFromFactoryBean(final FactoryBean factory, final String beanName) throws BeanCreationException { Object object; try { if (System.getSecurityManager() != null) { AccessControlContext acc = getAccessControlContext(); try { object = AccessController.doPrivileged(new PrivilegedExceptionAction() { @Override public Object run() throws Exception { return factory.getObject(); } }, acc); } catch (PrivilegedActionException pae) { throw pae.getException(); } } else { object = factory.getObject(); } } catch (FactoryBeanNotInitializedException ex) { throw new BeanCurrentlyInCreationException(beanName, ex.toString()); } catch (Throwable ex) { throw new BeanCreationException(beanName, \"FactoryBean threw exception on object creation\", ex); } // Do not accept a null value for a FactoryBean that's not fully // initialized yet: Many FactoryBeans just return null then. if (object == null && isSingletonCurrentlyInCreation(beanName)) { throw new BeanCurrentlyInCreationException( beanName, \"FactoryBean which is currently in creation returned null from getObject\"); } return object; } 循环依赖 Spring容器循环依赖包括构造器循环依赖和setter循环依赖。 核心就是singletonObjects、singletonFactories、earlySingletonObjects和singletonsCurrentlyInCreation四个集合。 1)singletonObjects是beanName与beanInstance的Map，是真正的缓存，beanInstance是构造完毕的，凡是正常地构造完毕的单例bean都会放入缓存中。 2)earlySingletonObjects也是beanName与beanInstance的Map，beanInstance是已经调用了createBean方法，但是没有清除加载状态和加入至缓存的bean。仅在当前bean创建时存在，用于检测代理bean循环依赖。 3)singleFactories是beanName与ObjectFactory的Map，仅在当前bean创建时存在，是尚未调用createBean的bean。用于setter循环依赖时实现注入。 4)singletonsCurrentlyInCreation是beanName的集合，用于检测构造器循环依赖。 getBean在循环依赖时所执行的步骤是这样的： 1)检测当前bean是否在singletonObjects中，在则直接返回缓存好的bean；不在则检测是否在singletonFactories中，在，则调用其getObject方法，返回，并从singletonFactories中移除，加入到earlySingletonObjects中。 2)正常创建，beforeSingletonCreation:检测当前bean是否在singletonsCurrentlyInCreation，如果存在，抛出异常。表示存在构造器循环依赖。如果不存在，则将当前bean加入。 3)bean初始化，分为构造方法初始化、工厂方法初始化和简单初始化。如果是构造方法初始化，那么递归地获取参数bean。其他情况不会递归获取bean。 4)addSingletonFactory:如果当前bean不在singletonObjects中，则将当前bean加入到singletonFactories中（getObject方法是getEarlyBeanReference)，并从earlySingletonObjects中移除。 5)填充属性，简单初始化的话会递归创建所依赖的bean。 6)调用用户初始化方法，比如BeanPostProcesser、InitializingBean、init-method，有可能返回代理后的bean。 6) 检测循环依赖，如果当前bean在singletonObjects中，则判断当前bean(current bean)与singletonObjects中的bean(cached bean)是否是同一个，如果不是，那么说明当前bean是被代理过的，由于依赖当前bean的bean持有的是对cached bean的引用，这是不被允许的，所以会抛出BeanCurrentlyInCreationException异常。 7)afterSingletonCreation:将当前bean从singletonsCurrentlyInCreation中删除 8)addSingleton:将当前bean加入到singletonObjects，然后从singletonFactories, earlySingletonObjects中移除，结束 构造器循环依赖 表示通过构造器注入构成的循环依赖，此依赖是无法解决的，只能抛出BeanCurrentlyInCreationException异常表示循环依赖。 1、Spring容器创建单例“A” Bean，首先检测singletonFactories是否包含A，发现没有，于是正常创建，然后检测A是否包含在singletonsCurrentlyInCreation中，没有，则将A放入。构造方法初始化时需要B实例（A尚未放入到singletonFactories中)，于是调用了getBean(B)方法、 2、Spring容器创建单例“B” Bean，首先检测singletonFactories是否包含B，发现没有，于是正常创建，然后检测B是否包含在singletonsCurrentlyInCreation中，没有，则将B放入。构造方法初始化时需要C实例（B尚未放入到singletonFactories中)，于是调用了getBean(C)方法、 3、Spring容器创建单例“C” Bean，首先检测singletonFactories是否包含C，发现没有，于是正常创建，然后检测C是否包含在singletonsCurrentlyInCreation中，没有，则将C放入。构造方法初始化时需要A实例（C尚未放入到singletonFactories中)，于是调用了getBean(A)方法、 4、Spring容器创建单例“A” Bean，首先检测singletonFactories是否包含A，发现没有于是正常创建，然后检测A是否包含在singletonsCurrentlyInCreation中，有，抛出BeanCurrentlyInCreationException异常。 setter循环依赖 表示通过setter注入方式构成的循环依赖。 对于setter注入造成的依赖是通过Spring容器提前暴露刚完成构造器注入但未完成其他步骤（如setter注入)的Bean来完成的，而且只能解决单例作用域的Bean循环依赖。 1、Spring容器创建单例“A” Bean，首先检测singletonFactories是否包含A，发现没有，于是正常创建，然后检测A是否包含在singletonsCurrentlyInCreation中，没有，则将A放入。注入属性时需要B实例，于是调用了getBean(B)方法、 2、Spring容器创建单例“B” Bean，首先检测singletonFactories是否包含B，发现没有，于是正常创建，然后检测B是否包含在singletonsCurrentlyInCreation中，没有，则将B放入。注入属性时需要C实例，于是调用了getBean(C)方法、 3、Spring容器创建单例“C” Bean，首先检测singletonFactories是否包含C，发现没有，于是正常创建，然后检测C是否包含在singletonsCurrentlyInCreation中，没有，则将C放入。注入属性时需要A实例，于是调用了getBean(A)方法、 4、Spring容器创建单例“A” Bean，首先检测singletonFactories是否包含A，发现有，于是返回缓存了的bean，并将A从singletonFactories删除，返回A实例。 5、C得到A实例。set进来，B、A也是这样。结束。 对于“prototype”作用域Bean，Spring容器无法完成依赖注入，因为“prototype”作用域的Bean，Spring容器不进行缓存，因此无法提前暴露一个创建中的Bean。 实例 public class Main { public static void main(String[] args) { ApplicationContext applicationContext = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); UserService userService = (UserService) applicationContext.getBean(\"userService\"); userService.login(); } } 实例——循环依赖 注解注入 public class A { @Autowired private B b; } public class B { @Autowired private C c; } public class C { @Autowired private A a; } - 1)创建A时调用beforeSingletonCreation - 2)调用doCreateBean时earlySingletonExposure为true，调用了addSingletonFactory 注意，并没有递归去初始化B，返回它所依赖的bean时得到的是null。 - 3)然后调用populateBean，在postProcesser处理时调用了 - AutowiredAnnotationBeanPostProcesser#postProcessPropertyValues，最终调用了3.3.4.2.1.1.1) findAutowireCandidates，它调用了A所依赖的B的getBean方法。 - 4)B的getBean时调用了beforeSingletonCreation， - 5)调用doCreateBean时earlySingletonExposure为true，调用了addSingletonFactory 注意，并没有递归去初始化C，返回它所依赖的bean时得到的是null。 6)然后调用populateBean，在postProcesser处理时调用了 AutowiredAnnotationBeanPostProcesser#postProcessPropertyValues，最终调用了3.3.4.2.1.1.1) findAutowireCandidates，它调用了B所依赖的C的getBean方法。 7)B的getBean时调用了beforeSingletonCreation， 8)调用doCreateBean时earlySingletonExposure为true，调用了addSingletonFactory 注意，并没有递归去初始化A，返回它所依赖的bean时得到的是null。 9)然后调用populateBean，在postProcesser处理时调用了 AutowiredAnnotationBeanPostProcesser#postProcessPropertyValues，最终调用了3.3.4.2.1.1.1) findAutowireCandidates，它调用了C所依赖的A的getBean方法。 10)A在调用getSingleton时发现singletonFactories存在CircleA，然后调用其getObject方法（调用了getEarlyReference)，之后将CircleA放入earlySingletonObjects，然后从singletonFactories中移除。 - 11)从getBean(CircleA)中返回，回到CircleC的findAutowireCandidates，然后带着CircleA实例回到了inject方法，将CircleA实例设置给了CircleC。 - 12)CircleC去检测循环依赖，没有循环依赖，返回bean。 - 13)在调用getSingleton时发现singletonFactories存在CircleA，然后调用其getObject方法（调用了getEarlyReference)，之后将CircleA放入earlySingletonObjects，然后从singletonFactories中移除。 - 13)从getBean(CircleC)中返回，回到CircleB的findAutowireCandidates，然后带着CircleC实例回到了inject方法，将CircleC实例设置给了CircleB。 - 14)CircleB去检测循环依赖，没有循环依赖，返回bean。 - 15)从getBean(CircleB)中返回，回到CircleA的findAutowireCandidates，然后带着CircleB实例回到了inject方法，将CircleB实例设置给了CircleA。 - 16)CircleA去检测循环依赖，没有循环依赖，返回bean。结束。 setter注入 public class A { private B b; @Autowired public void setB(B b) { this.b = b; } } public class B { private C c; @Autowired public C getC() { return c; } } public class C { private A a; @Autowired public void setA(A a) { this.a = a; } } 调用栈与注解注入一致 构造器注入（抛出BeanCurrentlyInCreationException异常) public class A { private B b; @Autowired public A(B b) { this.b = b; } } 步骤类似，只有两个地方不同。 1)调用doCreateBean时，会采用构造方法初始化的方式，此时会递归地初始化构造方法参数bean。因为是构造方法初始化，所以递归获取参数bean是在将自己放入singletonFactories之前。 2)正因为没有将自己放入singletonFactories，所以不会在getBean从singletonFactories返回已经创建过的bean。 存在代理时的循环依赖（抛出BeanCurrentlyInCreationException异常) @Component(\"CircleA\") public class A { @Autowired private B b; } @Component(\"CircleB\") public class B { @Autowired private A a; } @Component public class ABeanPostProcessor implements BeanPostProcessor { @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { if(beanName.equals(\"CircleA\")) { System.out.println(\"proxy-A\"); return new Object(); } return bean; } @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { return bean; } } Exception in thread \"main\" org.springframework.beans.factory.BeanCurrentlyInCreationException: Error creating bean with name 'CircleA': Bean with name 'CircleA' has been injected into other beans [CircleB] in its raw version as part of a circular reference, but has eventually been wrapped. This means that said other beans do not use the final version of the bean. This is often the result of over-eager type matching - consider using 'getBeanNamesOfType' with the 'allowEagerInit' flag turned off, for example. 此时就与earlySIngletonObjects有关系了。 1)getBean(A)时，将A加入了singletonFactories，注入属性时setter注入B，调用getBean(B) 2)调用getBean(B)时，将B加入了singletonFactories，注入属性时setter注入A，调用getBean(A) 3)因为A已经存在在singletonFactories，于是取出，调用getObject，然后将A加入到earlySIngletonObjects，返回A。 4)B注入属性A完毕后，B构造完毕，将B加入singletonObjects，从 earlySIngletonObjects和singletonFactories中移除B 5)A注入属性B完毕后，执行BeanPostProcessor，此时A变为了Object（CurrentA)。检测代理bean循环依赖，发现singletonObjects中存在Cached A，于是取出，将CachedA 与 CurrentA比较，发现不同，然后发现有B依赖着Cached A，数据发生不一致，抛出异常。 https://www.iflym.com/index.php/code/201208280003.html Spring AOP（AspectJ) AOP术语 1、切面（aspect/advisor) 类是对物体特征的抽象，切面就是对横切关注点的抽象。组合了Pointcut与Advice，在Spring中有时候也称为Advisor。 2、连接点（join point) 被拦截到的点，因为Spring只支持方法类型的连接点，所以在Spring中连接点指的就是被拦截到的方法，实际上连接点还可以是字段或者构造器。 3、切入点（pointcut) 描述的一组符合某个条件的join point，通常使用pointcut表达式来限定join point。 4、通知（advice) 所谓通知指的就是指拦截到连接点之后要执行的代码，通知分为前置、后置、异常、返回、环绕通知五类。 5、目标对象 代理的目标对象 6、织入（weave) 将Advice织入join point的这个过程 7、引介（introduction) 在不修改代码的前提下，引介可以在运行期为类动态地添加一些方法或字段 Advisor 通知Advice是Spring提供的一种切面(Aspect)。但其功能过于简单，只能将切面织入到目标类的所有目标方法中，无法完成将切面织入到指定目标方法中。 顾问Advisor是Spring提供的另一种切面。其可以完成更为复杂的切面织入功能。PointcutAdvisor是顾问的一种，可以指定具体的切入点。顾问对通知进行了包装，会根据不同的通知类型，在不同的时间点，将切面织入到不同的切入点。 Advisor组合了Pointcut与Advice。 除了引介Advisor外，几乎所有的advisor都是PointcutAdvisor。 public interface Advisor { Advice getAdvice(); /** * @return whether this advice is associated with a particular target instance */ boolean isPerInstance(); } public interface PointcutAdvisor extends Advisor { /** * Get the Pointcut that drives this advisor. */ Pointcut getPointcut(); } Advice public interface Advice { } 增强（advice)主要包括如下五种类型 前置增强(BeforeAdvice)：在目标方法执行前实施增强 后置增强(AfterAdvice)：在目标方法执行后（无论是否抛出遗产)实施增强 环绕增强(MethodInterceptor)：在目标方法执行前后实施增强 异常抛出增强(ThrowsAdvice)：在目标方法抛出异常后实施增强 返回增强（AfterReturningAdvice)：在目标方法正常返回后实施增强 引介增强(IntroductionIntercrptor)：在目标类中添加一些新的方法和属性 BeanPostProcessor public interface BeanPostProcessor { Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException; } JDK动态代理与CGLIB代理 JDK动态代理： 其代理对象必须是某个接口的实现，它是通过在运行时创建一个接口的实现类来完成对目标对象的代理 CGLIB代理：在运行时生成的代理对象是针对目标类扩展的子类。 CGLIB是高效的代码生产包，底层是依靠ASM操作字节码实现的，性能比JDK强。 相关标签 true表示使用CGLIB代理。 解析AOP标签 解析配置文件时，一旦遇到aspectj-autoproxy注解时就会使用解析器 AspectJAutoProxyBeanDefinitionParser进行解析。 解析结果是注册了一个bean：AnnotationAwareAspectJAutoProxyCreator。 与IOC的衔接 处理自定义标签 public BeanDefinition parseCustomElement(Element ele, BeanDefinition containingBd) { String namespaceUri = getNamespaceURI(ele); // AopNamespaceHandler NamespaceHandler handler = this.readerContext.getNamespaceHandlerResolver().resolve(namespaceUri); if (handler == null) {error(\"Unable to locate Spring NamespaceHandler for XML schema namespace [\" + namespaceUri + \"]\", ele); return null; } return handler.parse(ele, new ParserContext(this.readerContext, this, containingBd)); } NamespaceHandler#parse NamespaceHandler是一个接口，它有一个实现是NamespaceHandlerSupport，实现了它的parse方法，而AopNamespaceHandler直接继承了parse方法。 它继承自NamespaceHandlerSupport，实现了NamespaceHandler接口的parse方法，而AopNamespaceHandler直接继承该parse方法。 NamespaceHandlerSupport#parse public BeanDefinition parse(Element element, ParserContext parserContext) { return findParserForElement(element, parserContext).parse(element, parserContext); } private BeanDefinitionParser findParserForElement(Element element, ParserContext parserContext) { String localName = parserContext.getDelegate().getLocalName(element); BeanDefinitionParser parser = this.parsers.get(localName); if (parser == null) { parserContext.getReaderContext().fatal( \"Cannot locate BeanDefinitionParser for element [\" + localName + \"]\", element); } return parser; } 这里返回的parser即为AspectJAutoProxyBeanDefinitionParser。 AspectJAutoProxyBeanDefinitionParser#parse public BeanDefinition parse(Element element, ParserContext parserContext) { AopNamespaceUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(parserContext, element); extendBeanDefinition(element, parserContext); return null; } AopNamespaceUtils#registerAspectJAnnotationAutoProxyCreatorIfNecessary 注册这个creator public static void registerAspectJAnnotationAutoProxyCreatorIfNecessary( ParserContext parserContext, Element sourceElement) { // 注册或升级AutoProxyCreator定义beanName为internalAutoProxyCreator的BeanDefinition BeanDefinition beanDefinition = AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary( parserContext.getRegistry(), parserContext.extractSource(sourceElement)); // 对于proxy-target-class以及expose-proxy属性的增强 useClassProxyingIfNecessary(parserContext.getRegistry(), sourceElement); // 注册组件并通知，便于监听器做进一步处理 // 其中beanDefinition的className为AnnotationAwareAspectJAutoProxyCreator registerComponentIfNecessary(beanDefinition, parserContext); } 1) AopConfigUtils#registerAspectJAnnotationAutoProxyCreatorIfNecessary public static BeanDefinition registerAspectJAnnotationAutoProxyCreatorIfNecessary(BeanDefinitionRegistry registry, Object source) { return registerOrEscalateApcAsRequired(AnnotationAwareAspectJAutoProxyCreator.class, registry, source); } registerOrEscalateApcAsRequired private static BeanDefinition registerOrEscalateApcAsRequired(Class cls, BeanDefinitionRegistry registry, Object source) { Assert.notNull(registry, \"BeanDefinitionRegistry must not be null\"); // 如果已经存在自动代理创建器，且存在的在自动代理创建器与现在的不一致，那么需要根据优先级来判断到底需要使用哪个 if (registry.containsBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME)) {BeanDefinition apcDefinition = registry.getBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME); if (!cls.getName().equals(apcDefinition.getBeanClassName())) { int currentPriority = findPriorityForClass(apcDefinition.getBeanClassName()); int requiredPriority = findPriorityForClass(cls); if (currentPriority // 改变bean最重要的是改变bean所对应的className属性 apcDefinition.setBeanClassName(cls.getName()); } } // 如果已经存在自动代理创建器，且与将要创建的一致，那么无需再次创建return null; } RootBeanDefinition beanDefinition = new RootBeanDefinition(cls); beanDefinition.setSource(source); beanDefinition.getPropertyValues().add(\"order\", Ordered.HIGHEST_PRECEDENCE); beanDefinition.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); registry.registerBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME, beanDefinition); return beanDefinition; } public static final String AUTO_PROXY_CREATOR_BEAN_NAME = \"org.springframework.aop.config.internalAutoProxyCreator\"; 这里的registery.registerBeanDefinition即为 DefaultListableBeanFactory.registerBeanDefinition。 2) useClassProxyingIfNecessary 处理proxy-target-class属性和expose-proxy属性 private static void useClassProxyingIfNecessary(BeanDefinitionRegistry registry, Element sourceElement) { if (sourceElement != null) { boolean proxyTargetClass = Boolean.valueOf(sourceElement.getAttribute(PROXY_TARGET_CLASS_ATTRIBUTE)); if (proxyTargetClass) { AopConfigUtils.forceAutoProxyCreatorToUseClassProxying(registry); } boolean exposeProxy = Boolean.valueOf(sourceElement.getAttribute(EXPOSE_PROXY_ATTRIBUTE)); if (exposeProxy) { AopConfigUtils.forceAutoProxyCreatorToExposeProxy(registry); } } } public static void forceAutoProxyCreatorToUseClassProxying(BeanDefinitionRegistry registry) { if (registry.containsBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME)) { BeanDefinition definition = registry.getBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME); definition.getPropertyValues().add(\"proxyTargetClass\", Boolean.TRUE); } } public static void forceAutoProxyCreatorToExposeProxy(BeanDefinitionRegistry registry) { if (registry.containsBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME)) { BeanDefinition definition = registry.getBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME); definition.getPropertyValues().add(\"exposeProxy\", Boolean.TRUE); } } 创建AOP代理 上文是通过自定义配置完成了读AnnotationAwareAspectJAutoProxyCreator类的自动注册。 可见这个类实现了BeanPostProcessor接口，而实现该接口后，当Spring加载这个Bean时会在实例化前 调用其postProcessAfterInitialization方法。 与IOC的衔接 beanPostProcessor在两个地方被调用，一个是 另一个是： 前一个地方是针对于实现了InstantiationAwareBeanPostProcessor接口的BeanPostProcessor，前一个地方创建代理成功后会直接返回。其他的BeanPostProcessor会在第二个地方被调用。 public Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException { Object result = existingBean; for (BeanPostProcessor beanProcessor : getBeanPostProcessors()) { result = beanProcessor.postProcessAfterInitialization(result, beanName); if (result == null) { return result; } } return result; } 在解析AOP标签中注册的AnnotationAwareAspectJAutoProxyCreator实现了BeanPostProcessor接口，所以在这里会被调用其postProcessAfterInstantiation方法。 AbstractAutoProxyCreator#postProcessAfterInitialization public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { if (bean != null) { Object cacheKey = getCacheKey(bean.getClass(), beanName); if (!this.earlyProxyReferences.contains(cacheKey)) { return wrapIfNecessary(bean, beanName, cacheKey); } } return bean; } wrapIfNecessary 逻辑： 1)获取可以应用到该bean的所有advisor 2)创建代理 protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) { // 如果已经处理过 if (beanName != null && this.targetSourcedBeans.contains(beanName)) {return bean; } // 无需增强 if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) {return bean; } // 给定的bean类是否代表一个基础设施类，基础设施类不应代理，或者配置了指定bean不需要自动代理 if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) {this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; } // 获取适合应用到该bean的所有advisor // Create proxy if we have advice. Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); // 如果获取到了增强则需要增强创建代理 if (specificInterceptors != DO_NOT_PROXY) {this.advisedBeans.put(cacheKey, Boolean.TRUE); // 创建代理 Object proxy = createProxy( bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; } this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; } 创建代理主要包含了两个步骤： 1)获取增强方法或者增强器 2)根据获取的增强进行代理 1) getAdvicesAndAdvisorsForBean（获取可以应用到该bean的所有advisor) AbstractAdvisorAutoProxyCreator.getAdvicesAndAdvisorsForBean protected Object[] getAdvicesAndAdvisorsForBean(Class beanClass, String beanName, TargetSource targetSource) { List advisors = findEligibleAdvisors(beanClass, beanName); if (advisors.isEmpty()) { return DO_NOT_PROXY; } return advisors.toArray(); } findEligibleAdvisors（合格的) 对于指定bean的增强方法的获取包含两个步骤： 1)获取所有的增强 2)寻找所有的增强中适用于bean的增强并应用 protected List findEligibleAdvisors(Class beanClass, String beanName) { List candidateAdvisors = findCandidateAdvisors(); List eligibleAdvisors = findAdvisorsThatCanApply(candidateAdvisors, beanClass, beanName); extendAdvisors(eligibleAdvisors); if (!eligibleAdvisors.isEmpty()) {eligibleAdvisors = sortAdvisors(eligibleAdvisors); } return eligibleAdvisors; } 1.1) findCandidateAdvisors（获取所有的增强) AnnotationAwareAspectJAutoProxyCreator.findCandidateAdvisors protected List findCandidateAdvisors() { // Add all the Spring advisors found according to superclass rules. // 当使用注解方式配置AOP的方式的时候，并不是丢弃了对XML配置的支持 // 在这里调用父类方法加载配置文件中的AOP声明 List advisors = super.findCandidateAdvisors(); // Build Advisors for all AspectJ aspects in the bean factory. advisors.addAll(this.aspectJAdvisorsBuilder.buildAspectJAdvisors()); return advisors; } 1.1.1) AbstractAdvisorAutoProxyCreator#findCandidateAdvisors（获取配置文件中的增强) protected List findCandidateAdvisors() { return this.advisorRetrievalHelper.findAdvisorBeans(); } 1.1.1.1) BeanFactoryAdvisorRetrievalHelper#findAdvisorBeans public List findAdvisorBeans() { // Determine list of advisor bean names, if not cached already. String[] advisorNames = null; synchronized (this) { advisorNames = this.cachedAdvisorBeanNames; if (advisorNames == null) { // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let the auto-proxy creator apply to them! // 从BeanFactory中获取所有对应Advisor的类 advisorNames = BeanFactoryUtils.beanNamesForTypeIncludingAncestors( this.beanFactory, Advisor.class, true, false); this.cachedAdvisorBeanNames = advisorNames; } } if (advisorNames.length == 0) { return new LinkedList(); } List advisors = new LinkedList(); for (String name : advisorNames) { if (isEligibleBean(name)) { if (this.beanFactory.isCurrentlyInCreation(name)) { if (logger.isDebugEnabled()) { logger.debug(\"Skipping currently created advisor '\" + name + \"'\"); } } else { try { // getBean方法可以获取Advisor advisors.add(this.beanFactory.getBean(name, Advisor.class)); } catch (BeanCreationException ex) { Throwable rootCause = ex.getMostSpecificCause(); if (rootCause instanceof BeanCurrentlyInCreationException) { BeanCreationException bce = (BeanCreationException) rootCause; if (this.beanFactory.isCurrentlyInCreation(bce.getBeanName())) { if (logger.isDebugEnabled()) { logger.debug(\"Skipping advisor '\" + name + \"' with dependency on currently created bean: \" + ex.getMessage()); } // Ignore: indicates a reference back to the bean we're trying to advise. // We want to find advisors other than the currently created bean itself. continue; } } throw ex; } } } } return advisors; } advisorNames = BeanFactoryUtils.beanNamesForTypeIncludingAncestors( this.beanFactory, Advisor.class, true, false); 1.1.1.1.1) BeanFactoryUtils.beanNamesForTypeIncludingAncestors public static String[] beanNamesForTypeIncludingAncestors( ListableBeanFactory lbf, Class type, boolean includeNonSingletons, boolean allowEagerInit) { Assert.notNull(lbf, \"ListableBeanFactory must not be null\"); //获取Class为Advisor的所有bean的名字 String[] result = lbf.getBeanNamesForType(type, includeNonSingletons, allowEagerInit); if (lbf instanceof HierarchicalBeanFactory) {HierarchicalBeanFactory hbf = (HierarchicalBeanFactory) lbf; if (hbf.getParentBeanFactory() instanceof ListableBeanFactory) { String[] parentResult = beanNamesForTypeIncludingAncestors( (ListableBeanFactory) hbf.getParentBeanFactory(), type, includeNonSingletons, allowEagerInit); List resultList = new ArrayList(); resultList.addAll(Arrays.asList(result)); for (String beanName : parentResult) { if (!resultList.contains(beanName) && !hbf.containsLocalBean(beanName)) { resultList.add(beanName); } } result = StringUtils.toStringArray(resultList); } } return result; } - 1.1.2) BeanFactoryAspectJAdvisorsBuilder#buildAspectJAdvisors（获取标记@Aspect注解的类中的增强) 逻辑： 1)遍历所有beanName，所有在beanFactory中注册的bean都会被提取出来 2)遍历所有beanName，找出声明@Aspect注解的类，进行进一步的处理 3)对标记为AspectJ注解的类进行增强的提取 4)将提取结果加入缓存 public List buildAspectJAdvisors() { List aspectNames = this.aspectBeanNames; if (aspectNames == null) { synchronized (this) { aspectNames = this.aspectBeanNames; if (aspectNames == null) { List advisors = new LinkedList(); aspectNames = new LinkedList(); // 获取所有的beanName String[] beanNames = BeanFactoryUtils.beanNamesForTypeIncludingAncestors( this.beanFactory, Object.class, true, false); // 遍历所有的beanName找出对应的增强方法 for (String beanName : beanNames) { // 不合法的bean则略过 if (!isEligibleBean(beanName)) { continue; } // We must be careful not to instantiate beans eagerly as in this case they // would be cached by the Spring container but would not have been weaved. // 获取对应的bean的类型 Class beanType = this.beanFactory.getType(beanName); if (beanType == null) { continue; } // 如果存在@Aspect注解 if (this.advisorFactory.isAspect(beanType)) { aspectNames.add(beanName); AspectMetadata amd = new AspectMetadata(beanType, beanName); if (amd.getAjType().getPerClause().getKind() == PerClauseKind.SINGLETON) { MetadataAwareAspectInstanceFactory factory = new BeanFactoryAspectInstanceFactory(this.beanFactory, beanName); // 解析标记@Aspect中的增强方法 List classAdvisors = this.advisorFactory.getAdvisors(factory); if (this.beanFactory.isSingleton(beanName)) { this.advisorsCache.put(beanName, classAdvisors); } else { this.aspectFactoryCache.put(beanName, factory); } advisors.addAll(classAdvisors); } else { // Per target or per this. if (this.beanFactory.isSingleton(beanName)) { throw new IllegalArgumentException(\"Bean with name '\" + beanName + \"' is a singleton, but aspect instantiation model is not singleton\"); } MetadataAwareAspectInstanceFactory factory = new PrototypeAspectInstanceFactory(this.beanFactory, beanName); this.aspectFactoryCache.put(beanName, factory); advisors.addAll(this.advisorFactory.getAdvisors(factory)); } } } this.aspectBeanNames = aspectNames; return advisors; } } } if (aspectNames.isEmpty()) { return Collections.emptyList(); } // 记录在缓存中 List advisors = new LinkedList(); for (String aspectName : aspectNames) {List cachedAdvisors = this.advisorsCache.get(aspectName); if (cachedAdvisors != null) { advisors.addAll(cachedAdvisors); } else { MetadataAwareAspectInstanceFactory factory = this.aspectFactoryCache.get(aspectName); advisors.addAll(this.advisorFactory.getAdvisors(factory)); } } return advisors; } 获取 this.advisorFactory.getAdvisors(factory) 1.1.2.1) getAdvisors（增强器的获取) ReflectiveAspectJAdvisorFactory.getAdvisors 逻辑： 1)对增强器的获取 2)加入同步实例化增强器以保证增强使用前的实例化 3)对DeclareParents注解的获取 public List getAdvisors(MetadataAwareAspectInstanceFactory aspectInstanceFactory) { // 获取标记为@Aspect的类 Class aspectClass = aspectInstanceFactory.getAspectMetadata().getAspectClass(); String aspectName = aspectInstanceFactory.getAspectMetadata().getAspectName(); // 验证 validate(aspectClass); // We need to wrap the MetadataAwareAspectInstanceFactory with a decorator // so that it will only instantiate once. MetadataAwareAspectInstanceFactory lazySingletonAspectInstanceFactory = new LazySingletonAspectInstanceFactoryDecorator(aspectInstanceFactory); List advisors = new LinkedList(); for (Method method : getAdvisorMethods(aspectClass)) { // 获取普通的advisor Advisor advisor = getAdvisor(method, lazySingletonAspectInstanceFactory, advisors.size(), aspectName); if (advisor != null) { advisors.add(advisor); } } // If it's a per target aspect, emit the dummy instantiating aspect. if (!advisors.isEmpty() && lazySingletonAspectInstanceFactory.getAspectMetadata().isLazilyInstantiated()) { // 如果寻找的增强器不为空，而且又配置了增强延迟初始化，那么需要在advisors开头加入同步实例化增强器 Advisor instantiationAdvisor = new SyntheticInstantiationAdvisor(lazySingletonAspectInstanceFactory); advisors.add(0, instantiationAdvisor); } // 获取DeclareParents注解（引介增强IntroductionAdvisor) // Find introduction fields. for (Field field : aspectClass.getDeclaredFields()) { Advisor advisor = getDeclareParentsAdvisor(field); if (advisor != null) { advisors.add(advisor); } } return advisors; } 1.1.2.1.1) getAdvisor（普通增强器的获取) 逻辑： 1)切点信息的获取 2)根据切点信息生成增强 public Advisor getAdvisor(Method candidateAdviceMethod, MetadataAwareAspectInstanceFactory aspectInstanceFactory, int declarationOrderInAspect, String aspectName) { validate(aspectInstanceFactory.getAspectMetadata().getAspectClass()); //切点信息的获取 AspectJExpressionPointcut expressionPointcut = getPointcut( candidateAdviceMethod, aspectInstanceFactory.getAspectMetadata().getAspectClass()); if (expressionPointcut == null) { return null; } // 根据切点信息生成增强器 return new InstantiationModelAwarePointcutAdvisorImpl(expressionPointcut, candidateAdviceMethod, this, aspectInstanceFactory, declarationOrderInAspect, aspectName); } 1.1.2.1.1.1) getPointcut（方法上切点信息的获取) private AspectJExpressionPointcut getPointcut(Method candidateAdviceMethod, Class candidateAspectClass) { // 获取方法上的注解 AspectJAnnotation aspectJAnnotation = AbstractAspectJAdvisorFactory.findAspectJAnnotationOnMethod(candidateAdviceMethod); if (aspectJAnnotation == null) {return null; } // 使用AspectJExpressionPointcut实例封装获取的信息 AspectJExpressionPointcut ajexp = new AspectJExpressionPointcut(candidateAspectClass, new String[0], new Class[0]); // 提取得到的注解中的表达式，如@Pointcut(“execution( ....)”) ajexp.setExpression(aspectJAnnotation.getPointcutExpression()); ajexp.setBeanFactory(this.beanFactory); return ajexp; } 1.1.2.1.1.1.1) findAspectJAnnotationOnMethod protected static AspectJAnnotation findAspectJAnnotationOnMethod(Method method) { // 寻找特定的注解类 Class[] classesToLookFor = new Class[] { Before.class, Around.class, After.class, AfterReturning.class, AfterThrowing.class, Pointcut.class}; for (Class c : classesToLookFor) { AspectJAnnotation foundAnnotation = findAnnotation(method, (Class) c); if (foundAnnotation != null) { return foundAnnotation; } } return null; } 获取指定方法上的注解并使用AspectJAnnotation封装 private static AspectJAnnotation findAnnotation(Method method, Class toLookFor) { A result = AnnotationUtils.findAnnotation(method, toLookFor); if (result != null) { return new AspectJAnnotation(result); } else { return null; } } 1.1.2.1.1.2) InstantiationModelAwarePointcutAdvisorImpl（根据切点信息生成增强器) 所有的增强都由Advisor的实现类InstantiationModelAwarePointcutAdvisorImpl统一封装的。 在实例初始化的过程中还完成了对于增强器的初始化，根据注解中的信息初始化对应的增强器是在instantiateAdvice函数中实现的。 public InstantiationModelAwarePointcutAdvisorImpl(AspectJExpressionPointcut declaredPointcut, Method aspectJAdviceMethod, AspectJAdvisorFactory aspectJAdvisorFactory, MetadataAwareAspectInstanceFactory aspectInstanceFactory, int declarationOrder, String aspectName) { this.declaredPointcut = declaredPointcut; this.declaringClass = aspectJAdviceMethod.getDeclaringClass(); this.methodName = aspectJAdviceMethod.getName(); this.parameterTypes = aspectJAdviceMethod.getParameterTypes(); this.aspectJAdviceMethod = aspectJAdviceMethod; this.aspectJAdvisorFactory = aspectJAdvisorFactory; this.aspectInstanceFactory = aspectInstanceFactory; this.declarationOrder = declarationOrder; this.aspectName = aspectName; if (aspectInstanceFactory.getAspectMetadata().isLazilyInstantiated()) { // Static part of the pointcut is a lazy type. Pointcut preInstantiationPointcut = Pointcuts.union( aspectInstanceFactory.getAspectMetadata().getPerClausePointcut(), this.declaredPointcut); // Make it dynamic: must mutate from pre-instantiation to post-instantiation state. // If it's not a dynamic pointcut, it may be optimized out // by the Spring AOP infrastructure after the first evaluation. this.pointcut = new PerTargetInstantiationModelPointcut( this.declaredPointcut, preInstantiationPointcut, aspectInstanceFactory); this.lazy = true; } else { // A singleton aspect. this.pointcut = this.declaredPointcut; this.lazy = false; this.instantiatedAdvice = instantiateAdvice(this.declaredPointcut); } } private Advice instantiateAdvice(AspectJExpressionPointcut pcut) { return this.aspectJAdvisorFactory.getAdvice(this.aspectJAdviceMethod, pcut, this.aspectInstanceFactory, this.declarationOrder, this.aspectName); } public Advice getAdvice(Method candidateAdviceMethod, AspectJExpressionPointcut expressionPointcut, MetadataAwareAspectInstanceFactory aspectInstanceFactory, int declarationOrder, String aspectName) { Class candidateAspectClass = aspectInstanceFactory.getAspectMetadata().getAspectClass(); validate(candidateAspectClass); AspectJAnnotation aspectJAnnotation = AbstractAspectJAdvisorFactory.findAspectJAnnotationOnMethod(candidateAdviceMethod); if (aspectJAnnotation == null) { return null; } // If we get here, we know we have an AspectJ method. // Check that it's an AspectJ-annotated class if (!isAspect(candidateAspectClass)) { throw new AopConfigException(\"Advice must be declared inside an aspect type: \" + \"Offending method '\" + candidateAdviceMethod + \"' in class [\" + candidateAspectClass.getName() + \"]\"); } if (logger.isDebugEnabled()) { logger.debug(\"Found AspectJ method: \" + candidateAdviceMethod); } AbstractAspectJAdvice springAdvice; // 根据不同的注解类型封装不同的增强器 switch (aspectJAnnotation.getAnnotationType()) { case AtBefore: springAdvice = new AspectJMethodBeforeAdvice( candidateAdviceMethod, expressionPointcut, aspectInstanceFactory); break; case AtAfter: springAdvice = new AspectJAfterAdvice( candidateAdviceMethod, expressionPointcut, aspectInstanceFactory); break; case AtAfterReturning: springAdvice = new AspectJAfterReturningAdvice( candidateAdviceMethod, expressionPointcut, aspectInstanceFactory); AfterReturning afterReturningAnnotation = (AfterReturning) aspectJAnnotation.getAnnotation(); if (StringUtils.hasText(afterReturningAnnotation.returning())) { springAdvice.setReturningName(afterReturningAnnotation.returning()); } break; case AtAfterThrowing: springAdvice = new AspectJAfterThrowingAdvice( candidateAdviceMethod, expressionPointcut, aspectInstanceFactory); AfterThrowing afterThrowingAnnotation = (AfterThrowing) aspectJAnnotation.getAnnotation(); if (StringUtils.hasText(afterThrowingAnnotation.throwing())) { springAdvice.setThrowingName(afterThrowingAnnotation.throwing()); } break; case AtAround: springAdvice = new AspectJAroundAdvice( candidateAdviceMethod, expressionPointcut, aspectInstanceFactory); break; case AtPointcut: if (logger.isDebugEnabled()) { logger.debug(\"Processing pointcut '\" + candidateAdviceMethod.getName() + \"'\"); } return null; default: throw new UnsupportedOperationException( \"Unsupported advice type on method: \" + candidateAdviceMethod); } // Now to configure the advice... springAdvice.setAspectName(aspectName); springAdvice.setDeclarationOrder(declarationOrder); String[] argNames = this.parameterNameDiscoverer.getParameterNames(candidateAdviceMethod); if (argNames != null) { springAdvice.setArgumentNamesFromStringArray(argNames); } springAdvice.calculateArgumentBindings(); return springAdvice; } 以AspectJMethodBeforeAdvice 为例，这个类是被MethodBeforeAdviceInterceptor持有的。 MethodBeforeAdviceInterceptor会在createProxy中被织入到代理对象中。 public class MethodBeforeAdviceInterceptor implements MethodInterceptor, Serializable { private MethodBeforeAdvice advice; /** * Create a new MethodBeforeAdviceInterceptor for the given advice. * @param advice the MethodBeforeAdvice to wrap */ public MethodBeforeAdviceInterceptor(MethodBeforeAdvice advice) { Assert.notNull(advice, \"Advice must not be null\"); this.advice = advice; } @Override public Object invoke(MethodInvocation mi) throws Throwable { this.advice.before(mi.getMethod(), mi.getArguments(), mi.getThis() ); return mi.proceed(); } } AspectJMethodBeforeAdvice public class AspectJMethodBeforeAdvice extends AbstractAspectJAdvice implements MethodBeforeAdvice, Serializable { public AspectJMethodBeforeAdvice( Method aspectJBeforeAdviceMethod, AspectJExpressionPointcut pointcut, AspectInstanceFactory aif) { super(aspectJBeforeAdviceMethod, pointcut, aif); } @Override public void before(Method method, Object[] args, Object target) throws Throwable { invokeAdviceMethod(getJoinPointMatch(), null, null); } @Override public boolean isBeforeAdvice() { return true; } @Override public boolean isAfterAdvice() { return false; } } invokeAdviceMethod protected Object invokeAdviceMethod(JoinPointMatch jpMatch, Object returnValue, Throwable ex) throws Throwable { return invokeAdviceMethodWithGivenArgs(argBinding(getJoinPoint(), jpMatch, returnValue, ex)); } invokeAdviceMethodWithGivenArgs protected Object invokeAdviceMethodWithGivenArgs(Object[] args) throws Throwable { Object[] actualArgs = args; if (this.aspectJAdviceMethod.getParameterTypes().length == 0) {actualArgs = null; } try {ReflectionUtils.makeAccessible(this.aspectJAdviceMethod); // TODO AopUtils.invokeJoinpointUsingReflection // 激活增强方法 method.invoke()return this.aspectJAdviceMethod.invoke(this.aspectInstanceFactory.getAspectInstance(), actualArgs); } catch (IllegalArgumentException ex) {throw new AopInvocationException(\"Mismatch on arguments to advice method [\" + this.aspectJAdviceMethod + \"]; pointcut expression [\" + this.pointcut.getPointcutExpression() + \"]\", ex); } catch (InvocationTargetException ex) {throw ex.getTargetException(); } } 1.1.2.1.2) SyntheticInstantiationAdvisor（同步实例化增强器) Advisor instantiationAdvisor = new SyntheticInstantiationAdvisor(lazySingletonAspectInstanceFactory); advisors.add(0, instantiationAdvisor); protected static class SyntheticInstantiationAdvisor extends DefaultPointcutAdvisor { public SyntheticInstantiationAdvisor(final MetadataAwareAspectInstanceFactory aif) { super(aif.getAspectMetadata().getPerClausePointcut(), new MethodBeforeAdvice() { // 目标方法前调用，类似@Before @Override public void before(Method method, Object[] args, Object target) { // Simply instantiate the aspect // 简单初始化aspect aif.getAspectInstance(); } }); } } 1.1.2.1.3) getDeclareParentsAdvisor（获取DeclareParents) DeclareParents主要用于引介增强的注解形式的实现，而其实现方式和普通增强很类似，只不过只用DeclareParentsAdvisor对功能进行封装。 private Advisor getDeclareParentsAdvisor(Field introductionField) { DeclareParents declareParents = introductionField.getAnnotation(DeclareParents.class); if (declareParents == null) { // Not an introduction field return null; } if (DeclareParents.class == declareParents.defaultImpl()) { throw new IllegalStateException(\"'defaultImpl' attribute must be set on DeclareParents\"); } return new DeclareParentsAdvisor( introductionField.getType(), declareParents.value(), declareParents.defaultImpl()); } 1.2) findAdvisorsThatCanApply（获取匹配的增强并应用) protected List findAdvisorsThatCanApply( List candidateAdvisors, Class beanClass, String beanName) { ProxyCreationContext.setCurrentProxiedBeanName(beanName); try { // 过滤已经得到的advisor return AopUtils.findAdvisorsThatCanApply(candidateAdvisors, beanClass); } finally { ProxyCreationContext.setCurrentProxiedBeanName(null); } } AopUtils.findAdvisorsThatCanApply public static List findAdvisorsThatCanApply(List candidateAdvisors, Class clazz) { if (candidateAdvisors.isEmpty()) { return candidateAdvisors; } List eligibleAdvisors = new LinkedList(); for (Advisor candidate : candidateAdvisors) { // 首先处理引介增强if (candidate instanceof IntroductionAdvisor && canApply(candidate, clazz)) { eligibleAdvisors.add(candidate); } } boolean hasIntroductions = !eligibleAdvisors.isEmpty(); for (Advisor candidate : candidateAdvisors) {if (candidate instanceof IntroductionAdvisor) { // already processed continue; } // 对于普通bean的处理if (canApply(candidate, clazz, hasIntroductions)) { eligibleAdvisors.add(candidate); } } return eligibleAdvisors; } 1.2.1) canApply（真正的匹配) public static boolean canApply(Advisor advisor, Class targetClass, boolean hasIntroductions) { // 处理引入增强 if (advisor instanceof IntroductionAdvisor) {return ((IntroductionAdvisor) advisor).getClassFilter().matches(targetClass); } // 处理PointcutAdvisor，是指有切入点的Advisor else if (advisor instanceof PointcutAdvisor) {PointcutAdvisor pca = (PointcutAdvisor) advisor; return canApply(pca.getPointcut(), targetClass, hasIntroductions); } else { // 没有切入点的始终匹配// It doesn't have a pointcut so we assume it applies. return true; } } public static boolean canApply(Pointcut pc, Class targetClass, boolean hasIntroductions) { Assert.notNull(pc, \"Pointcut must not be null\"); if (!pc.getClassFilter().matches(targetClass)) { return false; } MethodMatcher methodMatcher = pc.getMethodMatcher(); if (methodMatcher == MethodMatcher.TRUE) { // No need to iterate the methods if we're matching any method anyway... return true; } IntroductionAwareMethodMatcher introductionAwareMethodMatcher = null; if (methodMatcher instanceof IntroductionAwareMethodMatcher) { introductionAwareMethodMatcher = (IntroductionAwareMethodMatcher) methodMatcher; } // 获取bean目标类和所有接口，放到集合中 Set> classes = new LinkedHashSet>(ClassUtils.getAllInterfacesForClassAsSet(targetClass)); classes.add(targetClass); // 遍历集合，获取每个类/接口的所有方法，并对方法进行逐个匹配 for (Class clazz : classes) { Method[] methods = ReflectionUtils.getAllDeclaredMethods(clazz); for (Method method : methods) { if ((introductionAwareMethodMatcher != null && introductionAwareMethodMatcher.matches(method, targetClass, hasIntroductions)) || methodMatcher.matches(method, targetClass)) { return true; } } } return false; } 2) createProxy（创建代理) AbstractAutoProxyCreator#createProxy 在获取了所有对应的bean的增强后，便可以进行代理的创建了。 逻辑： 1)获取当前类中的属性 2)添加代理接口 3)封装Advisor并加入到ProxyFactory中 4)设置要代理的类 5)子类可以在此函数中进行对ProxyFactory的进一步封装 6)进行获取代理操作 specificInterceptors就是增强们（advisors) targetSource 是new SingletonTargetSource(bean) protected Object createProxy( Class beanClass, String beanName, Object[] specificInterceptors, TargetSource targetSource) { if (this.beanFactory instanceof ConfigurableListableBeanFactory) { AutoProxyUtils.exposeTargetClass((ConfigurableListableBeanFactory) this.beanFactory, beanName, beanClass); } ProxyFactory proxyFactory = new ProxyFactory(); // 获取当前类中的相关属性 proxyFactory.copyFrom(this); if (!proxyFactory.isProxyTargetClass()) { if (shouldProxyTargetClass(beanClass, beanName)) { proxyFactory.setProxyTargetClass(true); } else { // 添加代理接口 evaluateProxyInterfaces(beanClass, proxyFactory); } } Advisor[] advisors = buildAdvisors(beanName, specificInterceptors); for (Advisor advisor : advisors) { // 加入增强proxyFactory.addAdvisor(advisor); } // 设置要代理的类 proxyFactory.setTargetSource(targetSource); // 定制代理 customizeProxyFactory(proxyFactory); // 用来控制proxyFactory被配置后，是否还允许修改增强，缺省值为false proxyFactory.setFrozen(this.freezeProxy); if (advisorsPreFiltered()) { proxyFactory.setPreFiltered(true); } return proxyFactory.getProxy(getProxyClassLoader()); } 2.1) buildAdvisors（封装拦截器为Advisor) protected Advisor[] buildAdvisors(String beanName, Object[] specificInterceptors) { // Handle prototypes correctly... Advisor[] commonInterceptors = resolveInterceptorNames(); List allInterceptors = new ArrayList(); if (specificInterceptors != null) { allInterceptors.addAll(Arrays.asList(specificInterceptors)); if (commonInterceptors.length > 0) { if (this.applyCommonInterceptorsFirst) { allInterceptors.addAll(0, Arrays.asList(commonInterceptors)); } else { allInterceptors.addAll(Arrays.asList(commonInterceptors)); } } } if (logger.isDebugEnabled()) { int nrOfCommonInterceptors = commonInterceptors.length; int nrOfSpecificInterceptors = (specificInterceptors != null ? specificInterceptors.length : 0); logger.debug(\"Creating implicit proxy for bean '\" + beanName + \"' with \" + nrOfCommonInterceptors + \" common interceptors and \" + nrOfSpecificInterceptors + \" specific interceptors\"); } Advisor[] advisors = new Advisor[allInterceptors.size()]; for (int i = 0; i // 拦截器进行封装转化为advisoradvisors[i] = this.advisorAdapterRegistry.wrap(allInterceptors.get(i)); } return advisors; } 2.1.1) DefaultAdvisorAdapterRegistery#wrap public Advisor wrap(Object adviceObject) throws UnknownAdviceTypeException { // 如果要封装的对象本身就是Advisor类型的，那么直接返回 if (adviceObject instanceof Advisor) {return (Advisor) adviceObject; } // 因为此封装方法只对Advisor和Advice有效，如果不是则不能封装 if (!(adviceObject instanceof Advice)) {throw new UnknownAdviceTypeException(adviceObject); } Advice advice = (Advice) adviceObject; if (advice instanceof MethodInterceptor) {// So well-known it doesn't even need an adapter. // 如果是MethodInterceptor类型则使用DefaultPointcutAdvisor封装return new DefaultPointcutAdvisor(advice); } // 如果存在Advisor的Adapter，那么也同样需要进行封装 for (AdvisorAdapter adapter : this.adapters) { // Check that it is supported. if (adapter.supportsAdvice(advice)) { return new DefaultPointcutAdvisor(advice); } } throw new UnknownAdviceTypeException(advice); } 2.2) getProxy public Object getProxy(ClassLoader classLoader) { return createAopProxy().getProxy(classLoader); } - 2.2.1) createAopProxy（创建代理) protected final synchronized AopProxy createAopProxy() { if (!this.active) { activate(); } return getAopProxyFactory().createAopProxy(this); } 2.2.1.1) DefaultAopProxyFactory#createAopProxy 逻辑： 如果目标对象实现了接口，默认情况下会使用JDK的动态代理 如果目标对象实现了接口，可以强制使用CGLIB（proxy-target-class=false) 如果目标对象没有实现接口，必须采用CGLIB public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException { if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) { Class targetClass = config.getTargetClass(); if (targetClass == null) { throw new AopConfigException(\"TargetSource cannot determine target class: \" + \"Either an interface or a target is required for proxy creation.\"); } if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) { return new JdkDynamicAopProxy(config); } return new ObjenesisCglibAopProxy(config); } else { return new JdkDynamicAopProxy(config); } } - 2.2.2) getProxy（获取代理) 1>JdkDynamicAopProxy#getProxy 该AopProxy实现了InvocationHandler接口，重写了invoke方法。 public Object getProxy(ClassLoader classLoader) { if (logger.isDebugEnabled()) { logger.debug(\"Creating JDK dynamic proxy: target source is \" + this.advised.getTargetSource()); } Class[] proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised, true); findDefinedEqualsAndHashCodeMethods(proxiedInterfaces); return Proxy.newProxyInstance(classLoader, proxiedInterfaces, this); } invoke方法： public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { MethodInvocation invocation; Object oldProxy = null; boolean setProxyContext = false; TargetSource targetSource = this.advised.targetSource; Class targetClass = null; Object target = null; try { // 处理equalsif (!this.equalsDefined && AopUtils.isEqualsMethod(method)) { // The target does not implement the equals(Object) method itself. return equals(args[0]); } // 处理hashcode else if (!this.hashCodeDefined && AopUtils.isHashCodeMethod(method)) { // The target does not implement the hashCode() method itself. return hashCode(); } else if (method.getDeclaringClass() == DecoratingProxy.class) { // There is only getDecoratedClass() declared -> dispatch to proxy config. return AopProxyUtils.ultimateTargetClass(this.advised); } else if (!this.advised.opaque && method.getDeclaringClass().isInterface() && method.getDeclaringClass().isAssignableFrom(Advised.class)) { // Service invocations on ProxyConfig with the proxy config... return AopUtils.invokeJoinpointUsingReflection(this.advised, method, args); } Object retVal; // 有时候目标对象内部的自我调用将无法实施切面中的增强，则需要通过此属性暴露代理 if (this.advised.exposeProxy) { // Make invocation available if necessary. oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; } // May be null. Get as late as possible to minimize the time we \"own\" the target, // in case it comes from a pool. target = targetSource.getTarget(); if (target != null) { targetClass = target.getClass(); } // 获取当前方法的拦截器链 // Get the interception chain for this method. List chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); // Check whether we have any advice. If we don't, we can fallback on direct // reflective invocation of the target, and avoid creating a MethodInvocation. if (chain.isEmpty()) { // We can skip creating a MethodInvocation: just invoke the target directly // Note that the final invoker must be an InvokerInterceptor so we know it does // nothing but a reflective operation on the target, and no hot swapping or fancy proxying. Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); // 如果没有发现任何拦截器，那么直接调用切点方法（method.invoke()) retVal = AopUtils.invokeJoinpointUsingReflection(target, method, argsToUse); } else { // 将拦截器封装在ReflectiveMethodInvocation // We need to create a method invocation... invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); // Proceed to the joinpoint through the interceptor chain. // proceed中实现了拦截器方法的逐一调用 retVal = invocation.proceed(); } // Massage return value if necessary. Class returnType = method.getReturnType(); // 返回结果if (retVal != null && retVal == target && returnType != Object.class && returnType.isInstance(proxy) && !RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) { // Special case: it returned \"this\" and the return type of the method // is type-compatible. Note that we can't help if the target sets // a reference to itself in another returned object. retVal = proxy; } else if (retVal == null && returnType != Void.TYPE && returnType.isPrimitive()) { throw new AopInvocationException( \"Null return value from advice does not match primitive return type for: \" + method); } return retVal; } finally {if (target != null && !targetSource.isStatic()) { // Must have come from TargetSource. targetSource.releaseTarget(target); } if (setProxyContext) { // Restore old proxy. AopContext.setCurrentProxy(oldProxy); } } } 2.2.2.1) ReflectiveMethodInvocation#proceed（执行拦截器链的方法) public Object proceed() throws Throwable { - // 执行完所有增强后，执行切点方法(method.invoke()) // We start with an index of -1 and increment early. if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) { return invokeJoinpoint(); } // 获取下一个要执行的拦截器 Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex); if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) { // 动态匹配// Evaluate dynamic method matcher here: static part will already have // been evaluated and found to match. InterceptorAndDynamicMethodMatcher dm = (InterceptorAndDynamicMethodMatcher) interceptorOrInterceptionAdvice; if (dm.methodMatcher.matches(this.method, this.targetClass, this.arguments)) { return dm.interceptor.invoke(this); } else { // 不匹配则不执行拦截器，递归调用自己，执行下一个拦截器 // Dynamic matching failed. // Skip this interceptor and invoke the next in the chain. return proceed(); } } else { // 若为普通拦截器则直接调用拦截器 // It's an interceptor, so we just invoke it: The pointcut will have // been evaluated statically before this object was constructed. return ((MethodInterceptor) interceptorOrInterceptionAdvice).invoke(this); } } 2.2.2.1.1) invokeJoinpoint（执行切点方法) protected Object invokeJoinpoint() throws Throwable { return AopUtils.invokeJoinpointUsingReflection(this.target, this.method, this.arguments); } public static Object invokeJoinpointUsingReflection(Object target, Method method, Object[] args) throws Throwable { // Use reflection to invoke the method. try { ReflectionUtils.makeAccessible(method); return method.invoke(target, args); } catch (InvocationTargetException ex) { // Invoked method threw a checked exception. // We must rethrow it. The client won't see the interceptor. throw ex.getTargetException(); } catch (IllegalArgumentException ex) { throw new AopInvocationException(\"AOP configuration seems to be invalid: tried calling method [\" + method + \"] on target [\" + target + \"]\", ex); } catch (IllegalAccessException ex) { throw new AopInvocationException(\"Could not access method [\" + method + \"]\", ex); } } - 2.2.2.1.2) invoke （执行拦截器方法) public interface MethodInterceptor extends Interceptor { Object invoke(MethodInvocation invocation) throws Throwable; } 2>CglibAopProxy#getProxy 虽然返回的Proxy是ObjenesisCglibAopProxy，但它继承了CglibAopProxy 的getProxy方法。 实现了Enhancer的创建及接口封装。 public Object getProxy(ClassLoader classLoader) { if (logger.isDebugEnabled()) { logger.debug(\"Creating CGLIB proxy: target source is \" + this.advised.getTargetSource()); } try { Class rootClass = this.advised.getTargetClass(); Assert.state(rootClass != null, \"Target class must be available for creating a CGLIB proxy\"); Class proxySuperClass = rootClass; if (ClassUtils.isCglibProxyClass(rootClass)) { proxySuperClass = rootClass.getSuperclass(); Class[] additionalInterfaces = rootClass.getInterfaces(); for (Class additionalInterface : additionalInterfaces) { this.advised.addInterface(additionalInterface); } } // 验证class // Validate the class, writing log messages as necessary. validateClassIfNecessary(proxySuperClass, classLoader); // 创建及配置Enhancer // Configure CGLIB Enhancer... Enhancer enhancer = createEnhancer(); if (classLoader != null) { enhancer.setClassLoader(classLoader); if (classLoader instanceof SmartClassLoader && ((SmartClassLoader) classLoader).isClassReloadable(proxySuperClass)) { enhancer.setUseCache(false); } } enhancer.setSuperclass(proxySuperClass); enhancer.setInterfaces(AopProxyUtils.completeProxiedInterfaces(this.advised)); enhancer.setNamingPolicy(SpringNamingPolicy.INSTANCE); enhancer.setStrategy(new ClassLoaderAwareUndeclaredThrowableStrategy(classLoader)); // 设置拦截器 Callback[] callbacks = getCallbacks(rootClass); Class[] types = new Class[callbacks.length]; for (int x = 0; x 2.2.2.1) getCallbacks private Callback[] getCallbacks(Class rootClass) throws Exception { // Parameters used for optimisation choices... boolean exposeProxy = this.advised.isExposeProxy(); boolean isFrozen = this.advised.isFrozen(); boolean isStatic = this.advised.getTargetSource().isStatic(); // Choose an \"aop\" interceptor (used for AOP calls). // 将拦截器封装在DynamicAdvisedInterceptor中 Callback aopInterceptor = new DynamicAdvisedInterceptor(this.advised); // Choose a \"straight to target\" interceptor. (used for calls that are // unadvised but can return this). May be required to expose the proxy. Callback targetInterceptor; if (exposeProxy) { targetInterceptor = isStatic ? new StaticUnadvisedExposedInterceptor(this.advised.getTargetSource().getTarget()) : new DynamicUnadvisedExposedInterceptor(this.advised.getTargetSource()); } else { targetInterceptor = isStatic ? new StaticUnadvisedInterceptor(this.advised.getTargetSource().getTarget()) : new DynamicUnadvisedInterceptor(this.advised.getTargetSource()); } // Choose a \"direct to target\" dispatcher (used for // unadvised calls to static targets that cannot return this). Callback targetDispatcher = isStatic ? new StaticDispatcher(this.advised.getTargetSource().getTarget()) : new SerializableNoOp(); Callback[] mainCallbacks = new Callback[] { // 将拦截器链加入到Callback中 aopInterceptor, // for normal advice targetInterceptor, // invoke target without considering advice, if optimized new SerializableNoOp(), // no override for methods mapped to this targetDispatcher, this.advisedDispatcher, new EqualsInterceptor(this.advised), new HashCodeInterceptor(this.advised) }; Callback[] callbacks; // If the target is a static one and the advice chain is frozen, // then we can make some optimisations by sending the AOP calls // direct to the target using the fixed chain for that method. if (isStatic && isFrozen) { Method[] methods = rootClass.getMethods(); Callback[] fixedCallbacks = new Callback[methods.length]; this.fixedInterceptorMap = new HashMap(methods.length); // TODO: small memory optimisation here (can skip creation for methods with no advice) for (int x = 0; x chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(methods[x], rootClass); fixedCallbacks[x] = new FixedChainStaticTargetInterceptor( chain, this.advised.getTargetSource().getTarget(), this.advised.getTargetClass()); this.fixedInterceptorMap.put(methods[x].toString(), x); } // Now copy both the callbacks from mainCallbacks // and fixedCallbacks into the callbacks array. callbacks = new Callback[mainCallbacks.length + fixedCallbacks.length]; System.arraycopy(mainCallbacks, 0, callbacks, 0, mainCallbacks.length); System.arraycopy(fixedCallbacks, 0, callbacks, mainCallbacks.length, fixedCallbacks.length); this.fixedInterceptorOffset = mainCallbacks.length; } else { callbacks = mainCallbacks; } return callbacks; } CGLIB对于方法的拦截是通过将自定义的拦截器（实现了MethodInterceptor接口)加入Callback中并在调用代理时直接激活拦截器的intercept方法实现的，那么在getCallback中实现了这样一个目的：DynamicAdvisedInterceptor继承自MethodInterceptor，加入到Callback中后，在再次调用代理时会直接调用DynamicAdvisedInterceptor中的intercept方法。 CGLIB方式实现的代理，其核心逻辑在DynamicAdvisedInterceptor中的intercept方法中（JDK动态代理的核心逻辑是在invoke方法中)。 private static class DynamicAdvisedInterceptor implements MethodInterceptor, Serializable { private final AdvisedSupport advised; public DynamicAdvisedInterceptor(AdvisedSupport advised) { this.advised = advised; } @Override public Object intercept(Object proxy, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { Object oldProxy = null; boolean setProxyContext = false; Class targetClass = null; Object target = null; try { if (this.advised.exposeProxy) { // Make invocation available if necessary. oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; } // May be null. Get as late as possible to minimize the time we // \"own\" the target, in case it comes from a pool... target = getTarget(); if (target != null) { targetClass = target.getClass(); } // 获取拦截器链 List chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); Object retVal; // Check whether we only have one InvokerInterceptor: that is, // no real advice, but just reflective invocation of the target. if (chain.isEmpty() && Modifier.isPublic(method.getModifiers())) { // We can skip creating a MethodInvocation: just invoke the target directly. // Note that the final invoker must be an InvokerInterceptor, so we know // it does nothing but a reflective operation on the target, and no hot // swapping or fancy proxying. Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); // 如果拦截器为空，则直接激活原方法 retVal = methodProxy.invoke(target, argsToUse); } else { // 进入拦截器链 // We need to create a method invocation... retVal = new CglibMethodInvocation(proxy, target, method, args, targetClass, chain, methodProxy).proceed(); } retVal = processReturnType(proxy, target, method, retVal); return retVal; } finally { if (target != null) { releaseTarget(target); } if (setProxyContext) { // Restore old proxy. AopContext.setCurrentProxy(oldProxy); } } } @Override public boolean equals(Object other) { return (this == other || (other instanceof DynamicAdvisedInterceptor && this.advised.equals(((DynamicAdvisedInterceptor) other).advised))); } /** * CGLIB uses this to drive proxy creation. */ @Override public int hashCode() { return this.advised.hashCode(); } protected Object getTarget() throws Exception { return this.advised.getTargetSource().getTarget(); } protected void releaseTarget(Object target) throws Exception { this.advised.getTargetSource().releaseTarget(target); } } - 2.2.2.2) createProxyClassAndInstance protected Object createProxyClassAndInstance(Enhancer enhancer, Callback[] callbacks) { Class proxyClass = enhancer.createClass(); Object proxyInstance = null; if (objenesis.isWorthTrying()) { try { proxyInstance = objenesis.newInstance(proxyClass, enhancer.getUseCache()); } catch (Throwable ex) { logger.debug(\"Unable to instantiate proxy using Objenesis, \" + \"falling back to regular proxy construction\", ex); } } if (proxyInstance == null) { // Regular instantiation via default constructor... try { proxyInstance = (this.constructorArgs != null ? proxyClass.getConstructor(this.constructorArgTypes).newInstance(this.constructorArgs) : proxyClass.newInstance()); } catch (Throwable ex) { throw new AopConfigException(\"Unable to instantiate proxy using Objenesis, \" + \"and regular proxy instantiation via default constructor fails as well\", ex); } } ((Factory) proxyInstance).setCallbacks(callbacks); return proxyInstance; } 实例 public class LoggingAspect { @Pointcut(\"@annotation(org.springframework.web.bind.annotation.RequestMapping)\") public void declareJoinPointExpression() { } @Before(\"declareJoinPointExpression()\") public void beforeMethod(JoinPoint joinPoint) {// 连接点 Object[] args = joinPoint.getArgs();// 取得方法参数 log.info(\"The method [ {} ] begins with Parameters: {}\", joinPoint.getSignature(), Arrays.toString(args)); } @AfterReturning(value = \"declareJoinPointExpression()\", returning = \"result\") public void afterMethodReturn(JoinPoint joinPoint, Object result) { log.info(\"The method [ {} ] ends with Result: {}\", joinPoint.getSignature(), result); } @AfterThrowing(value = \"declareJoinPointExpression()\", throwing = \"e\") public void doAfterThrowing(JoinPoint joinPoint, Exception e) { log.error(\"Error happened in method: [ {} ]\", joinPoint.getSignature()); log.error(\"Parameters: {}\", Arrays.toString(joinPoint.getArgs())); log.error(\"Exception StackTrace: {}\", e); } } @Controller public class HelloController { @RequestMapping(\"/hello\") public ModelAndView hello(ModelAndView modelAndView){ modelAndView.addObject(\"user\",new RegisterDTO(\"admin\")); modelAndView.setViewName(\"hello\"); return modelAndView; } } 来看一下这个HelloController是怎么被AOP代理的。 从AbstractAutoProxyCreator.postProcessAfterInitialization开始看起。 在findCandidateAdvisors中 super.findCandidateAdvisors()返回了三个Advisor 在buildAdvisors中，又添加了LoggingAspect对应的三个Advisor，类型是InstantiationModelAwarePointcutAdvisorImpl。 它getPointcut返回的是AspectJExpressionPointcut 然后下面会找到所有Controller，得到Controller的所有方法，查看是否匹配。 最后返回的interceptors有5个， 之后调用createProxy，最后调用了DefaultAopProxyFactory.createAopProxy方法，返回了CglibAopProxy。 它再调用getProxy方法，将5个advisor封装为Callback。 最后生成cglib代理实例。 Spring Transaction（声明式事务) 事务介绍（两类事务) 编程式事务：所谓编程式事务指的是通过编码方式实现事务，即类似于JDBC编程实现事务管理。管理使用TransactionTemplate或者直接使用底层的PlatformTransactionManager。对于编程式事务管理，Spring推荐使用TransactionTemplate。 声明式事务：管理建立在AOP之上的。其本质是对方法前后进行拦截，然后在目标方法开始之前创建或者加入一个事务，在执行完目标方法之后根据执行情况提交或者回滚事务。声明式事务最大的优点就是不需要通过编程的方式管理事务，这样就不需要在业务逻辑代码中掺杂事务管理的代码，只需在配置文件中做相关的事务规则声明(或通过基于@Transactional注解的方式)，便可以将事务规则应用到业务逻辑中。 显然声明式事务管理要优于编程式事务管理，这正是spring倡导的非侵入式的开发方式。 声明式事务管理使业务代码不受污染，一个普通的POJO对象，只要加上注解就可以获得完全的事务支持。和编程式事务相比，声明式事务唯一不足地方是，后者的最细粒度只能作用到方法级别，无法做到像编程式事务那样可以作用到代码块级别。但是即便有这样的需求，也存在很多变通的方法，比如，可以将需要进行事务管理的代码块独立为方法等等。 事务传播行为 PROPAGATION_REQUIRED 如果当前存在一个事务，则加入当前事务；如果不存在任何事务，则创建一个新的事务。总之，要至少保证在一个事务中运行。PROPAGATION_REQUIRED通常作为默认的事务传播行为。 PROPAGATION_SUPPORTS 如果当前存在一个事务，则加入当前事务；如果当前不存在事务，则直接执行。 对于一些查询方法来说，PROPAGATION_SUPPORTS通常是比较合适的传播行为选择。 如果当前方法直接执行，那么不需要事务的支持；如果当前方法被其他方法调用，而其他方法启动了一个事务的时候，使用PROPAGATION_SUPPORTS可以保证当前方法能够加入当前事务并洞察当前事务对数据资源所做的更新。 比如说，A.service()会首先更新数据库，然后调用B.service()进行查询，那么，B.service()如果是PROPAGATION_SUPPORTS的传播行为， 就可以读取A.service()之前所做的最新更新结果，而如果使用稍后所提到的PROPAGATION_NOT_SUPPORTED，则B.service()将无法读取最新的更新结果，因为A.service()的事务在这个时候还没有提交(除非隔离级别是read uncommitted)。 PROPAGATION_MANDATORY PROPAGATION_MANDATORY强制要求当前存在一个事务，如果不存在，则抛出异常。 如果某个方法需要事务支持，但自身又不管理事务提交或者回滚的时候，比较适合使用 PROPAGATION_MANDATORY。 PROPAGATION_REQUIRES_NEW 不管当前是否存在事务，都会创建新的事务。如果当前存在事务的话，会将当前的事务挂起(suspend)。 如果某个业务对象所做的事情不想影响到外层事务的话，PROPAGATION_REQUIRES_NEW应该是合适的选择，比如，假设当前的业务方法需要向数据库中更新某些日志信息， 但即使这些日志信息更新失败，我们也不想因为该业务方法的事务回滚而影响到外层事务的成功提交，因为这种情况下，当前业务方法的事务成功与否对外层事务来说是无关紧要的。 PROPAGATION_NOT_SUPPORTED 不支持当前事务，而是在没有事务的情况下执行。如果当前存在事务的话，当前事务原则上将被挂起(suspend)，但要依赖于对应的PlatformTransactionManager实现类是否支持事务的挂起(suspend)，更多情况请参照TransactionDefinition的javadoc文档。 PROPAGATION_NOT_SUPPORTED与PROPAGATION_SUPPORTS之间的区别，可以参照PROPAGATION_SUPPORTS部分的实例内容。 PROPAGATION_NEVER 永远不需要当前存在事务，如果存在当前事务，则抛出异常。 PROPAGATION_NESTED 如果存在当前事务，则在当前事务的一个嵌套事务中执行，否则与PROPAGATION_REQUIRED的行为类似，即创建新的事务，在新创建的事务中执行。 PROPAGATION_NESTED粗看起来好像与PROPAGATION_REQUIRES_NEW的行为类似，实际上二者是有差别的。 PROPAGATION_REQUIRES_NEW创建的新事务与外层事务属于同一个“档次”，即二者的地位是相同的，当新创建的事务运行的时候，外层事务将被暂时挂起(suspend)； 而PROPAGATION_NESTED创建的嵌套事务则不然，它是寄生于当前外层事务的，它的地位比当前外层事务的地位要小一号，当内部嵌套事务运行的时候，外层事务也是处于active状态。是已经存在事务的一个真正的子事务. 嵌套事务开始执行时, 它将取得一个 savepoint. 如果这个嵌套事务失败, 我们将回滚到此 savepoint. 嵌套事务是外部事务的一部分, 只有外部事务结束后它才会被提交，外部事务回滚，它也会被回滚。 解析事务标签 同AOP标签，需要一个对应的BeanDefinitionParser。 AnnotationDrivenBeanDefinitionParser#parse public BeanDefinition parse(Element element, ParserContext parserContext) { registerTransactionalEventListenerFactory(parserContext); String mode = element.getAttribute(\"mode\"); if (\"aspectj\".equals(mode)) { // mode=\"aspectj\" registerTransactionAspect(element, parserContext); } else { // mode=\"proxy\" AopAutoProxyConfigurer.configureAutoProxyCreator(element, parserContext); } return null; } AopAutoProxyConfigurer#configureAutoProxyCreator 逻辑： 注册了一个creator和三个bean。这三个bean支撑起了整个的事务功能。 其中的两个bean（TransactionInterceptor和AnnotationTransactionAttributeSource)被注入到了一个名为BeanFactoryTransactionAttributeSourceAdvisor这个bean中。 public static void configureAutoProxyCreator(Element element, ParserContext parserContext) { // 注册InfrastructureAdvisorAutoProxyCreator这个类 AopNamespaceUtils.registerAutoProxyCreatorIfNecessary(parserContext, element); String txAdvisorBeanName = TransactionManagementConfigUtils.TRANSACTION_ADVISOR_BEAN_NAME; if (!parserContext.getRegistry().containsBeanDefinition(txAdvisorBeanName)) { Object eleSource = parserContext.extractSource(element); // Create the TransactionAttributeSource definition. //创建TransactionAttributeSource的bean RootBeanDefinition sourceDef = new RootBeanDefinition( \"org.springframework.transaction.annotation.AnnotationTransactionAttributeSource\"); sourceDef.setSource(eleSource); sourceDef.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); // 注册bean，并使用Spring中的定义规则生成beanNameString sourceName = parserContext.getReaderContext().registerWithGeneratedName(sourceDef); // 创建TransactionInterceptor的bean RootBeanDefinition interceptorDef = new RootBeanDefinition(TransactionInterceptor.class); interceptorDef.setSource(eleSource); interceptorDef.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); registerTransactionManager(element, interceptorDef); interceptorDef.getPropertyValues().add(\"transactionAttributeSource\", new RuntimeBeanReference(sourceName)); // 注册bean，并使用Spring中的定义规则生成beanName String interceptorName = parserContext.getReaderContext().registerWithGeneratedName(interceptorDef); // Create the TransactionAttributeSourceAdvisor definition. // 创建TransactionAttributeSourceAdvisor的bean RootBeanDefinition advisorDef = new RootBeanDefinition(BeanFactoryTransactionAttributeSourceAdvisor.class);advisorDef.setSource(eleSource); advisorDef.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); // 将sourceName的bean注入advisorDef的transactionAttributeSourceadvisorDef.getPropertyValues().add(\"transactionAttributeSource\", new RuntimeBeanReference(sourceName)); // 将interceptorName的bean注入advisorDef的adviceBeanName advisorDef.getPropertyValues().add(\"adviceBeanName\", interceptorName); if (element.hasAttribute(\"order\")) { advisorDef.getPropertyValues().add(\"order\", element.getAttribute(\"order\")); } parserContext.getRegistry().registerBeanDefinition(txAdvisorBeanName, advisorDef); CompositeComponentDefinition compositeDef = new CompositeComponentDefinition(element.getTagName(), eleSource); compositeDef.addNestedComponent(new BeanComponentDefinition(sourceDef, sourceName)); compositeDef.addNestedComponent(new BeanComponentDefinition(interceptorDef, interceptorName)); compositeDef.addNestedComponent(new BeanComponentDefinition(advisorDef, txAdvisorBeanName)); parserContext.registerComponent(compositeDef); } } AopNamespaceUtils#registerAutoProxyCreatorIfNecessary public static void registerAutoProxyCreatorIfNecessary( ParserContext parserContext, Element sourceElement) { BeanDefinition beanDefinition = AopConfigUtils.registerAutoProxyCreatorIfNecessary( parserContext.getRegistry(), parserContext.extractSource(sourceElement)); useClassProxyingIfNecessary(parserContext.getRegistry(), sourceElement); registerComponentIfNecessary(beanDefinition, parserContext); } AopConfigUtils#registerAutoProxyCreatorIfNecessary public static BeanDefinition registerAutoProxyCreatorIfNecessary(BeanDefinitionRegistry registry, Object source) { return registerOrEscalateApcAsRequired(InfrastructureAdvisorAutoProxyCreator.class, registry, source); } 这里注册了InfrastructureAdvisorAutoProxyCreator（基础设施)这个类。 BeanFactoryTransactionAttributeSourceAdvisor（用于对事务方法进行增强) 在AOP的BeanFactoryAdvisorRetrievalHelper.findAdvisorBeans中获取了所有类型为Advisor的bean，包括BeanFactoryTransactionAttributeSourceAdvisor这个类，并随着其他的Advisor一起在后续的步骤中被织入代理。 public class BeanFactoryTransactionAttributeSourceAdvisor extends AbstractBeanFactoryPointcutAdvisor { private TransactionAttributeSource transactionAttributeSource; private final TransactionAttributeSourcePointcut pointcut = new TransactionAttributeSourcePointcut() { @Override protected TransactionAttributeSource getTransactionAttributeSource() { return transactionAttributeSource; } }; /** * Set the transaction attribute source which is used to find transaction * attributes. This should usually be identical to the source reference * set on the transaction interceptor itself. * @see TransactionInterceptor#setTransactionAttributeSource */ public void setTransactionAttributeSource(TransactionAttributeSource transactionAttributeSource) { this.transactionAttributeSource = transactionAttributeSource; } /** * Set the {@link ClassFilter} to use for this pointcut. * Default is {@link ClassFilter#TRUE}. */ public void setClassFilter(ClassFilter classFilter) { this.pointcut.setClassFilter(classFilter); } @Override public Pointcut getPointcut() { return this.pointcut; } } 与IOC的衔接 InfrastructureAdvisorAutoProxyCreator作为一个AbstractAutoProxyCreator，会在getBean时调用其postProcessAfterInstantiation方法，该方法会创建事务代理。 InfrastructureAdvisorAutoProxyCreator#postProcessAfterInstantiation public Object postProcessBeforeInstantiation(Class beanClass, String beanName) throws BeansException { Object cacheKey = getCacheKey(beanClass, beanName); if (beanName == null || !this.targetSourcedBeans.contains(beanName)) { if (this.advisedBeans.containsKey(cacheKey)) { return null; } if (isInfrastructureClass(beanClass) || shouldSkip(beanClass, beanName)) { this.advisedBeans.put(cacheKey, Boolean.FALSE); return null; } } // Create proxy here if we have a custom TargetSource. // Suppresses unnecessary default instantiation of the target bean: // The TargetSource will handle target instances in a custom fashion. if (beanName != null) { TargetSource targetSource = getCustomTargetSource(beanClass, beanName); if (targetSource != null) { this.targetSourcedBeans.add(beanName); Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(beanClass, beanName, targetSource); Object proxy = createProxy(beanClass, beanName, specificInterceptors, targetSource); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; } } return null; } getAdvicesAndAdvisorsForBean会寻找所有实现Advisor接口的类。 我们之前注册了BeanFactoryTransactionAttributeSourceAdvisor这个类，这个类实现了Advisor接口。BeanFactoryTransactionAttributeSourceAdvisor作为一个advisor，用于对事务方法进行增强。只要类或方法实现了@Transactional接口，该Advisor一定会被加到拦截器链中，对原方法进行事务增强。 返回的Advisor类型是BeanFactoryTransactionAttributeSourceAdvisor，而其beanName是TransactionInterceptor。 之后调用了findAdvisorsThatCanApply 方法，又调用canApply方法。 1) canApply（判断bean是否需要添加事务增强) canApply(BeanFactoryTransactionAttributeSourceAdvisor,targetClass) 关键在于是否从指定的类或类中的方法找到对应的事务属性 public static boolean canApply(Advisor advisor, Class targetClass, boolean hasIntroductions) { // 处理引入增强 if (advisor instanceof IntroductionAdvisor) {return ((IntroductionAdvisor) advisor).getClassFilter().matches(targetClass); } // 处理PointcutAdvisor，是指有切入点的Advisor else if (advisor instanceof PointcutAdvisor) {PointcutAdvisor pca = (PointcutAdvisor) advisor; return canApply(pca.getPointcut(), targetClass, hasIntroductions); } else { // 没有切入点的始终匹配 // It doesn't have a pointcut so we assume it applies. return true; } } pca.getPointcut()对于BeanFactoryTransactionAttributeSourceAdvisor而言，是TransactionAttributeSourcePointcut。 private final TransactionAttributeSourcePointcut pointcut = new TransactionAttributeSourcePointcut() { @Override protected TransactionAttributeSource getTransactionAttributeSource() { return transactionAttributeSource; } }; pc.getMethodMatcher()对于TransactionAttributeSourcePointcut而言，就是this。 public final MethodMatcher getMethodMatcher() { return this; } public static boolean canApply(Pointcut pc, Class targetClass, boolean hasIntroductions) { Assert.notNull(pc, \"Pointcut must not be null\"); if (!pc.getClassFilter().matches(targetClass)) { return false; } MethodMatcher methodMatcher = pc.getMethodMatcher(); if (methodMatcher == MethodMatcher.TRUE) { // No need to iterate the methods if we're matching any method anyway... return true; } IntroductionAwareMethodMatcher introductionAwareMethodMatcher = null; if (methodMatcher instanceof IntroductionAwareMethodMatcher) { introductionAwareMethodMatcher = (IntroductionAwareMethodMatcher) methodMatcher; } // 获取bean目标类和所有接口，放到集合中 Set> classes = new LinkedHashSet>(ClassUtils.getAllInterfacesForClassAsSet(targetClass)); classes.add(targetClass); // 遍历集合，获取每个类/接口的所有方法，并对方法进行逐个匹配 for (Class clazz : classes) {Method[] methods = ReflectionUtils.getAllDeclaredMethods(clazz); for (Method method : methods) { if ((introductionAwareMethodMatcher != null && introductionAwareMethodMatcher.matches(method, targetClass, hasIntroductions)) || // 实际的匹配方法 methodMatcher.matches(method, targetClass)) { return true; } } } return false; } methodMatcher.matches(method, targetClass)会使用 TransactionAttributeSourcePointcut类的matches方法。 1.1) matches（匹配方法) 这里的tas就是TransactionAttributeSource。 如果该bean的该方法中存在事务属性，那么该类/方法需要继续事务增强。 public boolean matches(Method method, Class targetClass) { if (TransactionalProxy.class.isAssignableFrom(targetClass)) { return false; } TransactionAttributeSource tas = getTransactionAttributeSource(); return (tas == null || tas.getTransactionAttribute(method, targetClass) != null); } - 1.1.1) AnnotationTransactionAttributeSource#getTransactionAttribute（获取事务属性，封装了@Transactional中的配置信息) 先尝试从缓存加载，如果对应信息没有被缓存的话，工作又委托给了computeTransactionAttribute方法。 public TransactionAttribute getTransactionAttribute(Method method, Class targetClass) { if (method.getDeclaringClass() == Object.class) { return null; } // First, see if we have a cached value. Object cacheKey = getCacheKey(method, targetClass); Object cached = this.attributeCache.get(cacheKey); if (cached != null) { // Value will either be canonical value indicating there is no transaction attribute, // or an actual transaction attribute. if (cached == NULL_TRANSACTION_ATTRIBUTE) { return null; } else { return (TransactionAttribute) cached; } } else { // We need to work it out. TransactionAttribute txAttr = computeTransactionAttribute(method, targetClass); // Put it in the cache. if (txAttr == null) { this.attributeCache.put(cacheKey, NULL_TRANSACTION_ATTRIBUTE); } else { String methodIdentification = ClassUtils.getQualifiedMethodName(method, targetClass); if (txAttr instanceof DefaultTransactionAttribute) { ((DefaultTransactionAttribute) txAttr).setDescriptor(methodIdentification); } if (logger.isDebugEnabled()) { logger.debug(\"Adding transactional method '\" + methodIdentification + \"' with attribute: \" + txAttr); } this.attributeCache.put(cacheKey, txAttr); } return txAttr; } } - 1.1.1.1) computeTransactionAttribute（提取事务注解) 逻辑：如果方法中存在事务属性，则使用方法上的属性，否则使用方法所在的类上的属性。如果方法所在类的属性上还是没有搜寻到对应的事务属性，那么再搜寻接口中的方法，再没有的话，最后尝试搜寻接口的类上面的声明。 protected TransactionAttribute computeTransactionAttribute(Method method, Class targetClass) { // Don't allow no-public methods as required. if (allowPublicMethodsOnly() && !Modifier.isPublic(method.getModifiers())) { return null; } // Ignore CGLIB subclasses - introspect the actual user class. Class userClass = ClassUtils.getUserClass(targetClass); // The method may be on an interface, but we need attributes from the target class. // If the target class is null, the method will be unchanged. // method 代表接口中的方法，specificMethod代表实现类中的方法 Method specificMethod = ClassUtils.getMostSpecificMethod(method, userClass); // If we are dealing with method with generic parameters, find the original method. specificMethod = BridgeMethodResolver.findBridgedMethod(specificMethod); // 查看方法中是否存在事务声明 // First try is the method in the target class. TransactionAttribute txAttr = findTransactionAttribute(specificMethod); if (txAttr != null) {return txAttr; } // 查看方法所在类中是否存在事务声明 // Second try is the transaction attribute on the target class. txAttr = findTransactionAttribute(specificMethod.getDeclaringClass()); if (txAttr != null && ClassUtils.isUserLevelMethod(method)) {return txAttr; } // 如果存在接口，则到接口中寻找 if (specificMethod != method) {// Fallback is to look at the original method. // 查看接口方法txAttr = findTransactionAttribute(method); if (txAttr != null) { return txAttr; } // Last fallback is the class of the original method. // 到接口中的类中去寻找 txAttr = findTransactionAttribute(method.getDeclaringClass()); if (txAttr != null && ClassUtils.isUserLevelMethod(method)) { return txAttr; } } return null; } protected TransactionAttribute findTransactionAttribute(Method method) { return determineTransactionAttribute(method); } protected TransactionAttribute determineTransactionAttribute(AnnotatedElement ae) { if (ae.getAnnotations().length > 0) { for (TransactionAnnotationParser annotationParser : this.annotationParsers) { TransactionAttribute attr = annotationParser.parseTransactionAnnotation(ae); if (attr != null) { return attr; } } } return null; } 1.1.1.1.1) TransactionAnnotationParser#parseTransactionAnnotation（解析注解) 以SpringTransactionAnnotationParser为例： public TransactionAttribute parseTransactionAnnotation(AnnotatedElement ae) { //寻找@Transactional注解，有则解析该注解 AnnotationAttributes attributes = AnnotatedElementUtils.getMergedAnnotationAttributes(ae, Transactional.class); if (attributes != null) { return parseTransactionAnnotation(attributes); } else { return null; } } 解析@Transactional中的各个属性，并封装到TransactionAttribute中返回。 protected TransactionAttribute parseTransactionAnnotation(AnnotationAttributes attributes) { RuleBasedTransactionAttribute rbta = new RuleBasedTransactionAttribute(); Propagation propagation = attributes.getEnum(\"propagation\"); rbta.setPropagationBehavior(propagation.value()); Isolation isolation = attributes.getEnum(\"isolation\"); rbta.setIsolationLevel(isolation.value()); rbta.setTimeout(attributes.getNumber(\"timeout\").intValue()); rbta.setReadOnly(attributes.getBoolean(\"readOnly\")); rbta.setQualifier(attributes.getString(\"value\")); ArrayList rollBackRules = new ArrayList(); Class[] rbf = attributes.getClassArray(\"rollbackFor\"); for (Class rbRule : rbf) {RollbackRuleAttribute rule = new RollbackRuleAttribute(rbRule); rollBackRules.add(rule); } String[] rbfc = attributes.getStringArray(\"rollbackForClassName\"); for (String rbRule : rbfc) {RollbackRuleAttribute rule = new RollbackRuleAttribute(rbRule); rollBackRules.add(rule); } Class[] nrbf = attributes.getClassArray(\"noRollbackFor\"); for (Class rbRule : nrbf) {NoRollbackRuleAttribute rule = new NoRollbackRuleAttribute(rbRule); rollBackRules.add(rule); } String[] nrbfc = attributes.getStringArray(\"noRollbackForClassName\"); for (String rbRule : nrbfc) {NoRollbackRuleAttribute rule = new NoRollbackRuleAttribute(rbRule); rollBackRules.add(rule); } rbta.getRollbackRules().addAll(rollBackRules); return rbta; } 创建事务代理 重要类/接口介绍 PlatformTransactionManager public interface PlatformTransactionManager { TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException; void commit(TransactionStatus status) throws TransactionException; void rollback(TransactionStatus status) throws TransactionException; } getTransaction()：返回一个已经激活的事务或创建一个新的事务（根据给定的TransactionDefinition类型参数定义的事务属性)，返回的是TransactionStatus对象代表了当前事务的状态，其中该方法抛出TransactionException（未检查异常)表示事务由于某种原因失败。 commit()：用于提交TransactionStatus参数代表的事务 rollback()：用于回滚TransactionStatus参数代表的事务 TransactionSynchronizationManager（持有一系列事务相关的ThreadLocal对象) public abstract class TransactionSynchronizationManager { private static final ThreadLocal> resources = new NamedThreadLocal>(\"Transactional resources\"); private static final ThreadLocal> synchronizations = new NamedThreadLocal>(\"Transaction synchronizations\"); private static final ThreadLocal currentTransactionName = new NamedThreadLocal(\"Current transaction name\"); private static final ThreadLocal currentTransactionReadOnly = new NamedThreadLocal(\"Current transaction read-only status\"); private static final ThreadLocal currentTransactionIsolationLevel = new NamedThreadLocal(\"Current transaction isolation level\"); private static final ThreadLocal actualTransactionActive = new NamedThreadLocal(\"Actual transaction active\"); } getResource（获取当前线程绑定的连接) public static Object getResource(Object key) { Object actualKey = TransactionSynchronizationUtils.unwrapResourceIfNecessary(key); Object value = doGetResource(actualKey); if (value != null && logger.isTraceEnabled()) { logger.trace(\"Retrieved value [\" + value + \"] for key [\" + actualKey + \"] bound to thread [\" + Thread.currentThread().getName() + \"]\"); } return value; } private static Object doGetResource(Object actualKey) { Map map = resources.get(); if (map == null) { return null; } Object value = map.get(actualKey); // Transparently remove ResourceHolder that was marked as void... if (value instanceof ResourceHolder && ((ResourceHolder) value).isVoid()) { map.remove(actualKey); // Remove entire ThreadLocal if empty... if (map.isEmpty()) { resources.remove(); } value = null; } return value; } bindResouce（将新连接绑定到当前线程) public static void bindResource(Object key, Object value) throws IllegalStateException { Object actualKey = TransactionSynchronizationUtils.unwrapResourceIfNecessary(key); Assert.notNull(value, \"Value must not be null\"); Map map = resources.get(); // set ThreadLocal Map if none found if (map == null) { map = new HashMap(); resources.set(map); } Object oldValue = map.put(actualKey, value); // Transparently suppress a ResourceHolder that was marked as void... if (oldValue instanceof ResourceHolder && ((ResourceHolder) oldValue).isVoid()) { oldValue = null; } if (oldValue != null) { throw new IllegalStateException(\"Already value [\" + oldValue + \"] for key [\" + actualKey + \"] bound to thread [\" + Thread.currentThread().getName() + \"]\"); } if (logger.isTraceEnabled()) { logger.trace(\"Bound value [\" + value + \"] for key [\" + actualKey + \"] to thread [\" + Thread.currentThread().getName() + \"]\"); } } unbindResource（释放当前线程绑定的连接) public static Object unbindResource(Object key) throws IllegalStateException { Object actualKey = TransactionSynchronizationUtils.unwrapResourceIfNecessary(key); Object value = doUnbindResource(actualKey); if (value == null) { throw new IllegalStateException( \"No value for key [\" + actualKey + \"] bound to thread [\" + Thread.currentThread().getName() + \"]\"); } return value; } setActualTransactionActive（设置当前线程是否存在事务) public static void setActualTransactionActive(boolean active) { actualTransactionActive.set(active ? Boolean.TRUE : null); } setCurrentTransactionIsolationLevel（设置当前线程对应事务的隔离级别) public static void setCurrentTransactionIsolationLevel(Integer isolationLevel) { currentTransactionIsolationLevel.set(isolationLevel); } isSynchronizationActive（当前线程对应的事务synchronization不为空) public static boolean isSynchronizationActive() { return (synchronizations.get() != null); } initSynchronization（初始化当前线程的synchronization) public static void initSynchronization() throws IllegalStateException { if (isSynchronizationActive()) { throw new IllegalStateException(\"Cannot activate transaction synchronization - already active\"); } logger.trace(\"Initializing transaction synchronization\"); synchronizations.set(new LinkedHashSet()); } clearSynchronization（清空当前线程的synchronization) public static void clearSynchronization() throws IllegalStateException { if (!isSynchronizationActive()) { throw new IllegalStateException(\"Cannot deactivate transaction synchronization - not active\"); } logger.trace(\"Clearing transaction synchronization\"); synchronizations.remove(); } TransactionSynchronization（自定义触发器) TransactionStatus（事务状态) public interface TransactionStatus extends SavepointManager, Flushable { boolean isNewTransaction(); boolean hasSavepoint(); void setRollbackOnly(); boolean isRollbackOnly(); @Override void flush(); boolean isCompleted(); } DefaultTransactionStatus是其实现类 isNewTransaction（是否是新事务) public boolean isNewTransaction() { return (hasTransaction() && this.newTransaction); } hasTransaction（是否有事务) public boolean hasTransaction() { return (this.transaction != null); } isReadOnly（是否是只读事务) public boolean isReadOnly() { return this.readOnly; } isGlobalRollbackOnly（是否是rollback-only) public boolean isGlobalRollbackOnly() { return ((this.transaction instanceof SmartTransactionObject) && ((SmartTransactionObject) this.transaction).isRollbackOnly()); } 与AOP的衔接 ReflectiveMethodInvocation#proceed（执行拦截器链的方法) 其中调用了MethodInterceptor.invoke()拦截器方法。 对于标记了@Transactional的方法而言，会被代理，增强事务功能。 这些方法的Advisor增强中包括了TransactionInterceptor （BeanFactoryTransactionAttributeSourceAdvisor对应的bean)。 TransactionInterceptor支撑着整个事务功能的架构，它继承了MethodInterceptor。 public Object proceed() throws Throwable { - // 执行完所有增强后，执行切点方法(method.invoke()) // We start with an index of -1 and increment early. if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) { return invokeJoinpoint(); } // 获取下一个要执行的拦截器 Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex); if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) { // 动态匹配// Evaluate dynamic method matcher here: static part will already have // been evaluated and found to match. InterceptorAndDynamicMethodMatcher dm = (InterceptorAndDynamicMethodMatcher) interceptorOrInterceptionAdvice; if (dm.methodMatcher.matches(this.method, this.targetClass, this.arguments)) { return dm.interceptor.invoke(this); } else { // 不匹配则不执行拦截器，递归调用自己，执行下一个拦截器 // Dynamic matching failed. // Skip this interceptor and invoke the next in the chain. return proceed(); } } else { // 若为普通拦截器则直接调用拦截器 // It's an interceptor, so we just invoke it: The pointcut will have // been evaluated statically before this object was constructed. return ((MethodInterceptor) interceptorOrInterceptionAdvice).invoke(this); } } TransactionInterceptor#invoke（事务增强) public Object invoke(final MethodInvocation invocation) throws Throwable { // Work out the target class: may be {@code null}. // The TransactionAttributeSource should be passed the target class // as well as the method, which may be from an interface. Class targetClass = (invocation.getThis() != null ? AopUtils.getTargetClass(invocation.getThis()) : null); // Adapt to TransactionAspectSupport's invokeWithinTransaction... return invokeWithinTransaction(invocation.getMethod(), targetClass, new InvocationCallback() { @Override public Object proceedWithInvocation() throws Throwable { return invocation.proceed(); } }); } TransactionAspectSupport#invokeWithinTransaction 逻辑： 1)获取事务属性 TransactionAttribute 2)加载配置中的TransactionManager 3)不同的事务处理方式使用不同的逻辑，就声明式事务而言，会获取方法信息并创建事务信息TransactionInfo（此时已经创建了事务) 事务信息（TransactionInfo)与事务属性（TransactionAttribute)并不相同。 前者包含了后者，且包含了其他事务信息，比如PlatformTransactionManager以及TransactionStatus相关信息。 4)try:执行原始方法 5)catch:异常，回滚事务，再次抛出异常，7)及以后的不会执行 6)finally:清除事务信息 7)提交事务 8)返回原始方法的返回值 protected Object invokeWithinTransaction(Method method, Class targetClass, final InvocationCallback invocation) throws Throwable { // If the transaction attribute is null, the method is non-transactional. // 获取对应的事务属性 final TransactionAttribute txAttr = getTransactionAttributeSource().getTransactionAttribute(method, targetClass); // 获取BeanFactory中的transactionManager final PlatformTransactionManager tm = determineTransactionManager(txAttr); // 获取方法唯一标识（类、方法) final String joinpointIdentification = methodIdentification(method, targetClass, txAttr); // 声明式事务处理 if (txAttr == null || !(tm instanceof CallbackPreferringPlatformTransactionManager)) {// Standard transaction demarcation with getTransaction and commit/rollback calls. // 创建TransactionInfo（创建事务)TransactionInfo txInfo = createTransactionIfNecessary(tm, txAttr, joinpointIdentification); Object retVal = null; try { // This is an around advice: Invoke the next interceptor in the chain. // This will normally result in a target object being invoked. // 执行原始方法 retVal = invocation.proceedWithInvocation(); } catch (Throwable ex) { // target invocation exception // 异常回滚 completeTransactionAfterThrowing(txInfo, ex); // 回滚后又将异常抛了出来 throw ex; } finally { // 清除消息 cleanupTransactionInfo(txInfo); } // 提交事务 commitTransactionAfterReturning(txInfo); return retVal; } else { // 编程式事务，略过 // It's a CallbackPreferringPlatformTransactionManager: pass a TransactionCallback in. try { Object result = ((CallbackPreferringPlatformTransactionManager) tm).execute(txAttr, new TransactionCallback() { @Override public Object doInTransaction(TransactionStatus status) { TransactionInfo txInfo = prepareTransactionInfo(tm, txAttr, joinpointIdentification, status); try { return invocation.proceedWithInvocation(); } catch (Throwable ex) { if (txAttr.rollbackOn(ex)) { // A RuntimeException: will lead to a rollback. if (ex instanceof RuntimeException) { throw (RuntimeException) ex; } else { throw new ThrowableHolderException(ex); } } else { // A normal return value: will lead to a commit. return new ThrowableHolder(ex); } } finally { cleanupTransactionInfo(txInfo); } } }); // Check result: It might indicate a Throwable to rethrow. if (result instanceof ThrowableHolder) { throw ((ThrowableHolder) result).getThrowable(); } else { return result; } } catch (ThrowableHolderException ex) { throw ex.getCause(); } } } - 1)createTransactionIfNecessary（创建事务) 逻辑： 1)使用DelegatingTransactionAttribute封装传入的TransactionAttribute。 TransactionAttribute在这里的实际类型是RuleBasedTransactionAttribute，是由获取事务属性时生成，主要用于数据承载，使用DelegatingTransactionAttribute承载可以提供更多的功能。 2)获取事务 3)构建事务信息 protected TransactionInfo createTransactionIfNecessary( PlatformTransactionManager tm, TransactionAttribute txAttr, final String joinpointIdentification) { // If no name specified, apply method identification as transaction name. // 如果没有指定名称，则使用方法唯一标识，并使用DelegatingTransactionAttribute封装TransactionAttribute if (txAttr != null && txAttr.getName() == null) { txAttr = new DelegatingTransactionAttribute(txAttr) { @Override public String getName() { return joinpointIdentification; } }; } TransactionStatus status = null; if (txAttr != null) { if (tm != null) { // 创建事务，返回TransactionStatus status = tm.getTransaction(txAttr); } else { if (logger.isDebugEnabled()) { logger.debug(\"Skipping transactional joinpoint [\" + joinpointIdentification + \"] because no transaction manager has been configured\"); } } } // 根据指定的属性与status准备一个TransactionInfo return prepareTransactionInfo(tm, txAttr, joinpointIdentification, status); } 1.1)getTransaction（开启事务，返回TransactionStatus) 逻辑： 1)获取事务，创建对应的事务实例 2)如果当前线程存在事务，那么根据传播行为进行相应处理 3)事务超时的验证 4)事务传播行为的验证 5)构建DefaultTransactionStatus，创建当前事务的状态 6)完善transaction，包括设置ConnectionHolder、隔离级别、timeout，如果是新连接，则绑定到当前线程 7)将事务信息记录在当前线程中。 public final TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException { Object transaction = doGetTransaction(); // Cache debug flag to avoid repeated checks. boolean debugEnabled = logger.isDebugEnabled(); if (definition == null) { // Use defaults if no transaction definition given. definition = new DefaultTransactionDefinition(); } // 判断当前线程是否存在事务，依据（DataSourceTransactionManager)是当前线程记录的连接connectionHolder不为空，且connectionHolder中的transactionActive属性为true if (isExistingTransaction(transaction)) { // Existing transaction found -> check propagation behavior to find out how to behave. // 当前线程已存在事务，根据传播行为进行相应的处理，直接返回return handleExistingTransaction(definition, transaction, debugEnabled); } // 当前线程不存在事务 // 事务超时的验证 // Check definition settings for new transaction. if (definition.getTimeout() throw new InvalidTimeoutException(\"Invalid transaction timeout\", definition.getTimeout()); } // No existing transaction found -> check propagation behavior to find out how to proceed. // 事务传播行为的验证 // 当前线程不存在事务，但是传播行为却被声明为PROPAGATION_MANDATORY（支持当前事务，如果当前没有事务，就抛出异常)，则抛出异常 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_MANDATORY) {throw new IllegalTransactionStateException( \"No existing transaction found for transaction marked with propagation 'mandatory'\"); } else if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRED || definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW || definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) { // 当前线程没有事务，且传播行为是以上传播行为，那么空挂起 // 考虑到如果有注册的Synchronization的话，需要暂时将这些与将要开启的新事务无关的Synchronization先放一边。 // 剩下的其他情况，则返回不包含任何transaction object的TransactionStatus并返回 // 这种情况下虽然是空的事务，但有可能需要处理在事务过程中相关的Synchronization。SuspendedResourcesHolder suspendedResources = suspend(null); if (debugEnabled) { logger.debug(\"Creating new transaction with name [\" + definition.getName() + \"]: \" + definition); } try { boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); // 创建当前事务的状态 DefaultTransactionStatus status = newTransactionStatus( definition, transaction, true, newSynchronization, debugEnabled, suspendedResources); // 完善事务，包括设置ConnectionHolder、隔离级别、timeout // 另外如果是新连接，绑定到当前线程 doBegin(transaction, definition); // 将事务信息记录在当前线程中 prepareSynchronization(status, definition); return status; } catch (RuntimeException ex) { resume(null, suspendedResources); throw ex; } catch (Error err) { resume(null, suspendedResources); throw err; } } else { // Create \"empty\" transaction: no actual transaction, but potentially synchronization. if (definition.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT && logger.isWarnEnabled()) { logger.warn(\"Custom isolation level specified but no actual transaction initiated; \" + \"isolation level will effectively be ignored: \" + definition); } boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); return prepareTransactionStatus(definition, null, true, newSynchronization, debugEnabled, null); } } 1.1.1) doGetTransaction（创建事务实例) DataSourceTransactionManager.doGetTransaction protected Object doGetTransaction() { DataSourceTransactionObject txObject = new DataSourceTransactionObject(); txObject.setSavepointAllowed(isNestedTransactionAllowed()); // 如果当前线程已经记录数据库连接，则使用原有连接 ConnectionHolder conHolder = (ConnectionHolder) TransactionSynchronizationManager.getResource(this.dataSource); txObject.setConnectionHolder(conHolder, false); return txObject; } 1.1.1.1) DataSourceTransactionObject.setConnectionHolder public void setConnectionHolder(ConnectionHolder connectionHolder, boolean newConnectionHolder) { super.setConnectionHolder(connectionHolder); this.newConnectionHolder = newConnectionHolder; } - 1.1.2) handleExistingTransaction （处理已存在的事务) 值得注意的有两点： 1)REQUIRES_NEW表示当前方法必须在它自己的事务里运行，一个新的事务将被启动。而如果有一个事务正在运行的话，则在这个方法运行期间被挂起（suspend)。 2)NESTED表示如果当前正在有一个事务在运行中，则该方法应该运行在一个嵌套的事务中，被嵌套的事务可以独立于封装事务进行提交或者回滚。如果封装事务不存在，行为就像REQUIRES_NEW。 Spring主要有两种处理NESTED的方式： 首选设置保存点的方式作为异常处理的回滚 JTA无法使用保存点，那么处理方式和REQUIRES_NEW相同，而一旦出现异常，则由Spring的事务异常处理机制去完成后续操作。 private TransactionStatus handleExistingTransaction( TransactionDefinition definition, Object transaction, boolean debugEnabled) throws TransactionException { if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NEVER) { throw new IllegalTransactionStateException( \"Existing transaction found for transaction marked with propagation 'never'\"); } if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NOT_SUPPORTED) { if (debugEnabled) { logger.debug(\"Suspending current transaction\"); } Object suspendedResources = suspend(transaction); boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); return prepareTransactionStatus( definition, null, false, newSynchronization, debugEnabled, suspendedResources); } if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW) { if (debugEnabled) { logger.debug(\"Suspending current transaction, creating new transaction with name [\" + definition.getName() + \"]\"); } SuspendedResourcesHolder suspendedResources = suspend(transaction); try { boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); DefaultTransactionStatus status = newTransactionStatus( definition, transaction, true, newSynchronization, debugEnabled, suspendedResources); doBegin(transaction, definition); prepareSynchronization(status, definition); return status; } catch (RuntimeException beginEx) { resumeAfterBeginException(transaction, suspendedResources, beginEx); throw beginEx; } catch (Error beginErr) { resumeAfterBeginException(transaction, suspendedResources, beginErr); throw beginErr; } } if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) { if (!isNestedTransactionAllowed()) { throw new NestedTransactionNotSupportedException( \"Transaction manager does not allow nested transactions by default - \" + \"specify 'nestedTransactionAllowed' property with value 'true'\"); } if (debugEnabled) { logger.debug(\"Creating nested transaction with name [\" + definition.getName() + \"]\"); } if (useSavepointForNestedTransaction()) { // Create savepoint within existing Spring-managed transaction, // through the SavepointManager API implemented by TransactionStatus. // Usually uses JDBC 3.0 savepoints. Never activates Spring synchronization. DefaultTransactionStatus status = prepareTransactionStatus(definition, transaction, false, false, debugEnabled, null); status.createAndHoldSavepoint(); return status; } else { // Nested transaction through nested begin and commit/rollback calls. // Usually only for JTA: Spring synchronization might get activated here // in case of a pre-existing JTA transaction. boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); DefaultTransactionStatus status = newTransactionStatus( definition, transaction, true, newSynchronization, debugEnabled, null); doBegin(transaction, definition); prepareSynchronization(status, definition); return status; } } // 处理SUPPORTS和REQUIRED // Assumably PROPAGATION_SUPPORTS or PROPAGATION_REQUIRED. if (debugEnabled) { logger.debug(\"Participating in existing transaction\"); } if (isValidateExistingTransaction()) { if (definition.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT) { Integer currentIsolationLevel = TransactionSynchronizationManager.getCurrentTransactionIsolationLevel(); if (currentIsolationLevel == null || currentIsolationLevel != definition.getIsolationLevel()) { Constants isoConstants = DefaultTransactionDefinition.constants; throw new IllegalTransactionStateException(\"Participating transaction with definition [\" + definition + \"] specifies isolation level which is incompatible with existing transaction: \" + (currentIsolationLevel != null ? isoConstants.toCode(currentIsolationLevel, DefaultTransactionDefinition.PREFIX_ISOLATION) : \"(unknown)\")); } } if (!definition.isReadOnly()) { if (TransactionSynchronizationManager.isCurrentTransactionReadOnly()) { throw new IllegalTransactionStateException(\"Participating transaction with definition [\" + definition + \"] is not marked as read-only but existing transaction is\"); } } } boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); return prepareTransactionStatus(definition, transaction, false, newSynchronization, debugEnabled, null); } - 1.1.3) suspend（null->doNothing) 如果当前线程不存在事务，并且事务传播行为是Required、Required_New、Nested，那么执行suspend(null)。 考虑到如果有注册的synchronization的话，需要暂时将这些与将要开启的新事务无关的synchronization先放一边。 正常的suspend是记录原有事务的状态，以便后续操作对事务的恢复 （TransactionSynchronizationManager.unBindResource) protected final SuspendedResourcesHolder suspend(Object transaction) throws TransactionException { if (TransactionSynchronizationManager.isSynchronizationActive()) {List suspendedSynchronizations = doSuspendSynchronization(); try { Object suspendedResources = null; if (transaction != null) { suspendedResources = doSuspend(transaction); } String name = TransactionSynchronizationManager.getCurrentTransactionName(); TransactionSynchronizationManager.setCurrentTransactionName(null); boolean readOnly = TransactionSynchronizationManager.isCurrentTransactionReadOnly(); TransactionSynchronizationManager.setCurrentTransactionReadOnly(false); Integer isolationLevel = TransactionSynchronizationManager.getCurrentTransactionIsolationLevel(); TransactionSynchronizationManager.setCurrentTransactionIsolationLevel(null); boolean wasActive = TransactionSynchronizationManager.isActualTransactionActive(); TransactionSynchronizationManager.setActualTransactionActive(false); return new SuspendedResourcesHolder( suspendedResources, suspendedSynchronizations, name, readOnly, isolationLevel, wasActive); } catch (RuntimeException ex) { // doSuspend failed - original transaction is still active... doResumeSynchronization(suspendedSynchronizations); throw ex; } catch (Error err) { // doSuspend failed - original transaction is still active... doResumeSynchronization(suspendedSynchronizations); throw err; } } else if (transaction != null) {// Transaction active but no synchronization active. Object suspendedResources = doSuspend(transaction); return new SuspendedResourcesHolder(suspendedResources); } else {// Neither transaction nor synchronization active. return null; } } 1.1.3.1) doSuspendSynchronization private List doSuspendSynchronization() { List suspendedSynchronizations = TransactionSynchronizationManager.getSynchronizations(); for (TransactionSynchronization synchronization : suspendedSynchronizations) { synchronization.suspend(); } // 清空synchronization TransactionSynchronizationManager.clearSynchronization(); return suspendedSynchronizations; } 1.1.4) newTransactionStatus （创建当前事务的状态) boolean newSynchronization = (getTransactionSynchronization() != SYNCHRONIZATION_NEVER); DefaultTransactionStatus status = newTransactionStatus(definition, transaction, true, newSynchronization, debugEnabled, null); 新事务则传入的newTransaction为true，否则为false definition.isReadOnly()返回的是@Transactional中的属性，默认为false。 protected DefaultTransactionStatus newTransactionStatus( TransactionDefinition definition, Object transaction, boolean newTransaction, boolean newSynchronization, boolean debug, Object suspendedResources) { boolean actualNewSynchronization = newSynchronization && !TransactionSynchronizationManager.isSynchronizationActive(); return new DefaultTransactionStatus( transaction, newTransaction, actualNewSynchronization, definition.isReadOnly(), debug, suspendedResources); } 理解newTransaction newTransaction标识该切面方法是否新建了事务，后续切面方法执行完毕时，通过该字段判断是否 需要提交事务或者回滚事务。 1.1.5) doBegin（完善事务实例) 逻辑： 1)尝试获取连接。如果当前线程中的connectionHolder已经存在，则没有必要再次获取；对于事务同步设置为true的需要重新获取连接 2)设置隔离级别以及只读标识 3)更改默认的提交设置，将提交操作委托给Spring来处理 4)设置标志位，标识当前连接已经被事务激活 5)设置超时时间 6)如果是新连接，则将connectionHolder绑定到当前线程 protected void doBegin(Object transaction, TransactionDefinition definition) { DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; Connection con = null; try { if (txObject.getConnectionHolder() == null || txObject.getConnectionHolder().isSynchronizedWithTransaction()) { Connection newCon = this.dataSource.getConnection(); if (logger.isDebugEnabled()) { logger.debug(\"Acquired Connection [\" + newCon + \"] for JDBC transaction\"); } // 设置ConnectionHolder txObject.setConnectionHolder(new ConnectionHolder(newCon), true); } txObject.getConnectionHolder().setSynchronizedWithTransaction(true); con = txObject.getConnectionHolder().getConnection(); // 设置隔离级别 Integer previousIsolationLevel = DataSourceUtils.prepareConnectionForTransaction(con, definition); txObject.setPreviousIsolationLevel(previousIsolationLevel); // Switch to manual commit if necessary. This is very expensive in some JDBC drivers, // so we don't want to do it unnecessarily (for example if we've explicitly // configured the connection pool to set it already). // 更改自动提交设置，由Spring控制提交 if (con.getAutoCommit()) { txObject.setMustRestoreAutoCommit(true); if (logger.isDebugEnabled()) { logger.debug(\"Switching JDBC Connection [\" + con + \"] to manual commit\"); } con.setAutoCommit(false); } prepareTransactionalConnection(con, definition); // 设置判断当前线程是否存在事务的依据，即transactionActive txObject.getConnectionHolder().setTransactionActive(true); int timeout = determineTimeout(definition); if (timeout != TransactionDefinition.TIMEOUT_DEFAULT) { //设置timeout属性 txObject.getConnectionHolder().setTimeoutInSeconds(timeout); } // Bind the connection holder to the thread. // 如果是新的连接，则将当前获取到的连接绑定到当前线程 if (txObject.isNewConnectionHolder()) { TransactionSynchronizationManager.bindResource(getDataSource(), txObject.getConnectionHolder()); } } catch (Throwable ex) { if (txObject.isNewConnectionHolder()) { DataSourceUtils.releaseConnection(con, this.dataSource); txObject.setConnectionHolder(null, false); } throw new CannotCreateTransactionException(\"Could not open JDBC Connection for transaction\", ex); } } 1.1.6) prepareSynchronization（记录事务信息至当前线程) protected void prepareSynchronization(DefaultTransactionStatus status, TransactionDefinition definition) { if (status.isNewSynchronization()) { TransactionSynchronizationManager.setActualTransactionActive(status.hasTransaction()); TransactionSynchronizationManager.setCurrentTransactionIsolationLevel( definition.getIsolationLevel() != TransactionDefinition.ISOLATION_DEFAULT ? definition.getIsolationLevel() : null); TransactionSynchronizationManager.setCurrentTransactionReadOnly(definition.isReadOnly()); TransactionSynchronizationManager.setCurrentTransactionName(definition.getName()); TransactionSynchronizationManager.initSynchronization(); } } 1.2) prepareTransactionInfo（构建事务信息) 当已经建立事务连接并完成了事务信息的提取后，需要将所有的事务信息统一记录在TransactionInfo实例中，这个实例包含了目标方法开始前的所有状态信息。一旦事务执行失败，Spring会通过TransactionInfo实例来进行回滚等后续工作。 protected TransactionInfo prepareTransactionInfo(PlatformTransactionManager tm, TransactionAttribute txAttr, String joinpointIdentification, TransactionStatus status) { TransactionInfo txInfo = new TransactionInfo(tm, txAttr, joinpointIdentification); if (txAttr != null) { // We need a transaction for this method... if (logger.isTraceEnabled()) { logger.trace(\"Getting transaction for [\" + txInfo.getJoinpointIdentification() + \"]\"); } // The transaction manager will flag an error if an incompatible tx already exists. // 记录事务状态 txInfo.newTransactionStatus(status); } else { // The TransactionInfo.hasTransaction() method will return false. We created it only // to preserve the integrity of the ThreadLocal stack maintained in this class. if (logger.isTraceEnabled()) logger.trace(\"Don't need to create transaction for [\" + joinpointIdentification + \"]: This method isn't transactional.\"); } // We always bind the TransactionInfo to the thread, even if we didn't create // a new transaction here. This guarantees that the TransactionInfo stack // will be managed correctly even if no transaction was created by this aspect. txInfo.bindToThread(); return txInfo; } 2)completeTransactionAfterThrowing （回滚事务) protected void completeTransactionAfterThrowing(TransactionInfo txInfo, Throwable ex) { // 当抛出异常时首先判断当前是否存在事务，这是基础依据 if (txInfo != null && txInfo.hasTransaction()) {if (logger.isTraceEnabled()) { logger.trace(\"Completing transaction for [\" + txInfo.getJoinpointIdentification() + \"] after exception: \" + ex); } // 判断是否回滚 默认的依据是 抛出的异常是否是RuntimeException或者是Error的类型if (txInfo.transactionAttribute.rollbackOn(ex)) { try { // 根据TransactionStatus信息进行回滚处理 txInfo.getTransactionManager().rollback(txInfo.getTransactionStatus()); } catch (TransactionSystemException ex2) { logger.error(\"Application exception overridden by rollback exception\", ex); ex2.initApplicationException(ex); throw ex2; } catch (RuntimeException ex2) { logger.error(\"Application exception overridden by rollback exception\", ex); throw ex2; } catch (Error err) { logger.error(\"Application exception overridden by rollback error\", ex); throw err; } } else { // 如果不满足回滚条件，即使抛出异常也会提交 // We don't roll back on this exception. // Will still roll back if TransactionStatus.isRollbackOnly() is true. try { txInfo.getTransactionManager().commit(txInfo.getTransactionStatus()); } catch (TransactionSystemException ex2) { logger.error(\"Application exception overridden by commit exception\", ex); ex2.initApplicationException(ex); throw ex2; } catch (RuntimeException ex2) { logger.error(\"Application exception overridden by commit exception\", ex); throw ex2; } catch (Error err) { logger.error(\"Application exception overridden by commit error\", ex); throw err; } } } } 2.1) TransactionAttribute.rollbackOn（判断是否需要回滚) DefaultTransactionAttribute的实现是 public boolean rollbackOn(Throwable ex) { return (ex instanceof RuntimeException || ex instanceof Error); } RuleBasedTransactionAttribute的实现是 public boolean rollbackOn(Throwable ex) { if (logger.isTraceEnabled()) { logger.trace(\"Applying rules to determine whether transaction should rollback on \" + ex); } RollbackRuleAttribute winner = null; int deepest = Integer.MAX_VALUE; if (this.rollbackRules != null) { for (RollbackRuleAttribute rule : this.rollbackRules) { int depth = rule.getDepth(ex); if (depth >= 0 && depth - 2.2) rollback（回滚) public final void rollback(TransactionStatus status) throws TransactionException { if (status.isCompleted()) { throw new IllegalTransactionStateException( \"Transaction is already completed - do not call commit or rollback more than once per transaction\"); } DefaultTransactionStatus defStatus = (DefaultTransactionStatus) status; processRollback(defStatus); } processRollback 1)首先是自定义触发器的调用，包括在回滚前、完成回滚后的调用，当然完成回滚包括正常回滚与回滚过程中出现异常，自定义的触发器会根据这些信息做进一步处理，而对于触发器的注册，常见是在回调过程过程中提供TransactionSynchronizationManager类中的静态方法直接注册。 public static void registerSynchronization(TransactionSynchronization synchronization) - 2)除了触发监听函数外，就是真正的回滚逻辑处理了。有保存点则回滚到保存点，是新事务则回滚整个事务；存在事务又不是新事务，则做回滚标记。 - 3)回滚后进行信息清除 private void processRollback(DefaultTransactionStatus status) { try { try { // 激活所有TransactionSynchronization中对应的beforeCompletion方法 triggerBeforeCompletion(status); if (status.hasSavepoint()) { if (status.isDebug()) { logger.debug(\"Rolling back transaction to savepoint\"); } // 如果有保存点，也就是当前事务为单独的线程，则会退到保存点 status.rollbackToHeldSavepoint(); } else if (status.isNewTransaction()) { if (status.isDebug()) { logger.debug(\"Initiating transaction rollback\"); } // 如果当前事务为独立的新事务，则直接回滚 doRollback(status); } else if (status.hasTransaction()) { if (status.isLocalRollbackOnly() || isGlobalRollbackOnParticipationFailure()) { if (status.isDebug()) { logger.debug(\"Participating transaction failed - marking existing transaction as rollback-only\"); } // 如果当前事务不是独立的事务，那么只能标记状态，等到事务链执行完毕后统一回滚 doSetRollbackOnly(status); } else { if (status.isDebug()) { logger.debug(\"Participating transaction failed - letting transaction originator decide on rollback\"); } } } else { logger.debug(\"Should roll back transaction but cannot - no transaction available\"); } } catch (RuntimeException ex) { triggerAfterCompletion(status, TransactionSynchronization.STATUS_UNKNOWN); throw ex; } catch (Error err) { triggerAfterCompletion(status, TransactionSynchronization.STATUS_UNKNOWN); throw err; } // 激活所有TransactionSynchronization中对应的方法 triggerAfterCompletion(status, TransactionSynchronization.STATUS_ROLLED_BACK); } finally { // 清空记录的资源并将挂起的资源恢复 cleanupAfterCompletion(status); } } 2.2.1) triggerBeforeCompletion（调用触发器) protected final void triggerBeforeCompletion(DefaultTransactionStatus status) { if (status.isNewSynchronization()) { if (status.isDebug()) { logger.trace(\"Triggering beforeCompletion synchronization\"); } TransactionSynchronizationUtils.triggerBeforeCompletion(); } } TransactionSynchronizationUtils.triggerBeforeCompletion() public static void triggerBeforeCompletion() { for (TransactionSynchronization synchronization : TransactionSynchronizationManager.getSynchronizations()) { try { synchronization.beforeCompletion(); } catch (Throwable tsex) { logger.error(\"TransactionSynchronization.beforeCompletion threw exception\", tsex); } } } - 2.2.2) rollbackToHeldSavepoint（回滚至保存点) public void rollbackToHeldSavepoint() throws TransactionException { if (!hasSavepoint()) { throw new TransactionUsageException( \"Cannot roll back to savepoint - no savepoint associated with current transaction\"); } getSavepointManager().rollbackToSavepoint(getSavepoint()); getSavepointManager().releaseSavepoint(getSavepoint()); setSavepoint(null); } JdbcTransactionObjectSupport.rollbackToSavepoint public void rollbackToSavepoint(Object savepoint) throws TransactionException { ConnectionHolder conHolder = getConnectionHolderForSavepoint(); try { conHolder.getConnection().rollback((Savepoint) savepoint); } catch (Throwable ex) { throw new TransactionSystemException(\"Could not roll back to JDBC savepoint\", ex); } } - 2.2.3) doRollback（回滚整个事务) protected void doRollback(DefaultTransactionStatus status) { DataSourceTransactionObject txObject = (DataSourceTransactionObject) status.getTransaction(); Connection con = txObject.getConnectionHolder().getConnection(); if (status.isDebug()) { logger.debug(\"Rolling back JDBC transaction on Connection [\" + con + \"]\"); } try { con.rollback(); } catch (SQLException ex) { throw new TransactionSystemException(\"Could not roll back JDBC transaction\", ex); } } 2.2.4) doSetRollbackOnly（设置回滚标记) protected void doSetRollbackOnly(DefaultTransactionStatus status) { DataSourceTransactionObject txObject = (DataSourceTransactionObject) status.getTransaction(); if (status.isDebug()) {logger.debug(\"Setting JDBC transaction [\" + txObject.getConnectionHolder().getConnection() + \"] rollback-only\"); } txObject.setRollbackOnly(); } public void setRollbackOnly() { this.rollbackOnly = true; } - 2.2.5) triggerAfterCompletion（调用触发器) private void triggerAfterCompletion(DefaultTransactionStatus status, int completionStatus) { if (status.isNewSynchronization()) { List synchronizations = TransactionSynchronizationManager.getSynchronizations(); TransactionSynchronizationManager.clearSynchronization(); if (!status.hasTransaction() || status.isNewTransaction()) { if (status.isDebug()) { logger.trace(\"Triggering afterCompletion synchronization\"); } // No transaction or new transaction for the current scope -> // invoke the afterCompletion callbacks immediately invokeAfterCompletion(synchronizations, completionStatus); } else if (!synchronizations.isEmpty()) { // Existing transaction that we participate in, controlled outside // of the scope of this Spring transaction manager -> try to register // an afterCompletion callback with the existing (JTA) transaction. registerAfterCompletionWithExistingTransaction(status.getTransaction(), synchronizations); } } } - 2.2.6) cleanupAfterCompletion（回滚后清除信息) 逻辑： 1)设置状态是对事务信息做完成标识以避免重复调用 2)如果当前事务是新的同步状态，需要将绑定到当前线程的事务信息清除 3)如果是新事务需要做些清除资源的工作 private void cleanupAfterCompletion(DefaultTransactionStatus status) { // 设置完成状态 status.setCompleted(); if (status.isNewSynchronization()) {TransactionSynchronizationManager.clear(); } if (status.isNewTransaction()) { // 清除资源doCleanupAfterCompletion(status.getTransaction()); } if (status.getSuspendedResources() != null) {if (status.isDebug()) { logger.debug(\"Resuming suspended transaction after completion of inner transaction\"); } // 结束之前事务的挂起状态resume(status.getTransaction(), (SuspendedResourcesHolder) status.getSuspendedResources()); } } 2.2.6.1) doCleanupAfterCompletion（新事务则释放资源) protected void doCleanupAfterCompletion(Object transaction) { DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; // 将连接从当前线程中解除绑定 // Remove the connection holder from the thread, if exposed. if (txObject.isNewConnectionHolder()) { TransactionSynchronizationManager.unbindResource(this.dataSource); } // Reset connection. Connection con = txObject.getConnectionHolder().getConnection(); try { if (txObject.isMustRestoreAutoCommit()) { con.setAutoCommit(true); } // 重置数据库连接 DataSourceUtils.resetConnectionAfterTransaction(con, txObject.getPreviousIsolationLevel()); } catch (Throwable ex) { logger.debug(\"Could not reset JDBC Connection after transaction\", ex); } if (txObject.isNewConnectionHolder()) { if (logger.isDebugEnabled()) { logger.debug(\"Releasing JDBC Connection [\" + con + \"] after transaction\"); } // 如果当前事务是独立的新创建的事务，则在事务完成时释放数据库连接 DataSourceUtils.releaseConnection(con, this.dataSource); } txObject.getConnectionHolder().clear(); } 2.2.6.2) resume（将挂起事务恢复) 如果在事务执行前有事务挂起，那么当前事务执行结束后需要将挂起事务恢复 protected final void resume(Object transaction, SuspendedResourcesHolder resourcesHolder) throws TransactionException { if (resourcesHolder != null) { Object suspendedResources = resourcesHolder.suspendedResources; if (suspendedResources != null) { doResume(transaction, suspendedResources); } List suspendedSynchronizations = resourcesHolder.suspendedSynchronizations; if (suspendedSynchronizations != null) { TransactionSynchronizationManager.setActualTransactionActive(resourcesHolder.wasActive); TransactionSynchronizationManager.setCurrentTransactionIsolationLevel(resourcesHolder.isolationLevel); TransactionSynchronizationManager.setCurrentTransactionReadOnly(resourcesHolder.readOnly); TransactionSynchronizationManager.setCurrentTransactionName(resourcesHolder.name); doResumeSynchronization(suspendedSynchronizations); } } } 3)commitTransactionAfterReturning（提交事务) protected void commitTransactionAfterReturning(TransactionInfo txInfo) { if (txInfo != null && txInfo.hasTransaction()) { if (logger.isTraceEnabled()) { logger.trace(\"Completing transaction for [\" + txInfo.getJoinpointIdentification() + \"]\"); } txInfo.getTransactionManager().commit(txInfo.getTransactionStatus()); } } commit 某个事务是另一个事务的嵌入事务，但是这些事务又不在Spring的管理范围之内，或者无法设置保存点，那么Spring会通过设置回滚标识的方式来禁止提交。首先当某个嵌入事务发生回滚的时候会设置回滚标识，而等到外部事务提交时，一旦判断出当前事务流被设置了回滚标识，则由外部事务来统一进行整体事务的回滚。 public final void commit(TransactionStatus status) throws TransactionException { if (status.isCompleted()) { throw new IllegalTransactionStateException( \"Transaction is already completed - do not call commit or rollback more than once per transaction\"); } DefaultTransactionStatus defStatus = (DefaultTransactionStatus) status; // 如果在事务链中已经被标记回滚，那么不会尝试提交事务，直接回滚 if (defStatus.isLocalRollbackOnly()) { if (defStatus.isDebug()) { logger.debug(\"Transactional code has requested rollback\"); } processRollback(defStatus); return; } if (!shouldCommitOnGlobalRollbackOnly() && defStatus.isGlobalRollbackOnly()) { if (defStatus.isDebug()) { logger.debug(\"Global transaction is marked as rollback-only but transactional code requested commit\"); } processRollback(defStatus); // Throw UnexpectedRollbackException only at outermost transaction boundary // or if explicitly asked to. if (status.isNewTransaction() || isFailEarlyOnGlobalRollbackOnly()) { throw new UnexpectedRollbackException( \"Transaction rolled back because it has been marked as rollback-only\"); } return; } // 处理事务提交 processCommit(defStatus); } processCommit 在提交过程中也不是直接提交的，而是考虑了诸多方面。 符合提交的条件如下： 当事务状态中有保存点信息的话便不会提交事务； 当事务不是新事务的时候也不会提交事务 原因是： 对于内嵌事务，在Spring中会将其在开始之前设置保存点，一旦内嵌事务出现异常便根据保存点信息进行回滚，但是如果没有出现异常，内嵌事务并不会单独提交，而是根据事务流由最外层事务负责提交，所以如果当前存在保存点信息便不是最外层事务，不做提交操作。 private void processCommit(DefaultTransactionStatus status) throws TransactionException { try { boolean beforeCompletionInvoked = false; try { // 预留 prepareForCommit(status); // 添加的TransactionSynchronization中对应方法的调用 triggerBeforeCommit(status); // 添加的TransactionSynchronization中对应方法的调用 triggerBeforeCompletion(status); beforeCompletionInvoked = true; boolean globalRollbackOnly = false; if (status.isNewTransaction() || isFailEarlyOnGlobalRollbackOnly()) { globalRollbackOnly = status.isGlobalRollbackOnly(); } if (status.hasSavepoint()) { if (status.isDebug()) { logger.debug(\"Releasing transaction savepoint\"); } // 如果存在保存点，则清除保存点信息 status.releaseHeldSavepoint(); } else if (status.isNewTransaction()) { if (status.isDebug()) { logger.debug(\"Initiating transaction commit\"); } // 如果是新事务，则提交 doCommit(status); } // Throw UnexpectedRollbackException if we have a global rollback-only // marker but still didn't get a corresponding exception from commit. if (globalRollbackOnly) { throw new UnexpectedRollbackException( \"Transaction silently rolled back because it has been marked as rollback-only\"); } } catch (UnexpectedRollbackException ex) { // can only be caused by doCommit triggerAfterCompletion(status, TransactionSynchronization.STATUS_ROLLED_BACK); throw ex; } catch (TransactionException ex) { // can only be caused by doCommit if (isRollbackOnCommitFailure()) { doRollbackOnCommitException(status, ex); } else { triggerAfterCompletion(status, TransactionSynchronization.STATUS_UNKNOWN); } throw ex; } catch (RuntimeException ex) { if (!beforeCompletionInvoked) { // 添加的TransactionSynchronization中对应方法的调用 triggerBeforeCompletion(status); } // 提交过程中出现异常则回滚 doRollbackOnCommitException(status, ex); throw ex; } catch (Error err) { if (!beforeCompletionInvoked) { triggerBeforeCompletion(status); } doRollbackOnCommitException(status, err); throw err; } // Trigger afterCommit callbacks, with an exception thrown there // propagated to callers but the transaction still considered as committed. try { // 添加的TransactionSynchronization中对应方法的调用 triggerAfterCommit(status); } finally { triggerAfterCompletion(status, TransactionSynchronization.STATUS_COMMITTED); } } finally { cleanupAfterCompletion(status); } } 实例 public interface LoginService { void login(RegisterDTO dto); } @Service public class LoginServiceImpl implements LoginService{ @Override @Transactional(rollbackFor = RuntimeException.class,propagation = Propagation.NESTED) public void login(RegisterDTO dto) { System.out.println(\"login...\"); throw new RuntimeException(\"exception in loginservice\"); } } public interface UserService { void addUser(RegisterDTO dto); } @Service public class UserServiceImpl implements UserService{ @Transactional(rollbackFor = RuntimeException.class,propagation = Propagation.NESTED) @Override public void addUser(RegisterDTO dto) { System.out.println(\"addUser...\"); } } public interface RegisterService { void register(RegisterDTO dto); } @Service public class RegisterServiceImpl implements RegisterService{ @Autowired private LoginService loginService; @Autowired private UserService userService; @Transactional(rollbackFor = RuntimeException.class,propagation = Propagation.NESTED) @Override public void register(RegisterDTO dto) { System.out.println(\"registering...\"); loginService.login(dto); System.out.println(\"invoke other methods...\"); userService.addUser(dto); } } @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(\"classpath*:spring/spring-*.xml\") @WebAppConfiguration public class TransactionTest { @Autowired private RegisterService registerService; @Test public void test(){ registerService.register(new RegisterDTO()); } } REQUIRED（内层事务newTransaction为false，内层事务回滚时仅设置回滚标记，外层事务进行外层回滚) 如果几个不同的service都是共享同一个事务（也就是service对象嵌套传播机制为Propagation.REQUIRED)，那么它们会一起提交，一起回滚。 同一个事务，如果一个service已经提交了，在另外service中rollback自然对第一个service提交的代码回滚不了的。所以spring处理嵌套事务，就是在TransactionInterceptor方法中，根据一系列开关（事务传播行为)，来处理事务是同一个还是重新获取，如果是同一个事务,不同service的commit与rollback的时机。 这里有一个外层切面register，使用了事务，里面调用了两个service，它们也是要求使用事务的。 因为传播行为是REQUIRED，所以共用同一个事务。 当调用register时，在getTransaction时会将TransactionStatus中的newTransaction设置为true，并且将连接绑定到当前线程，设置当前线程存在事务。 当后续调用login和addUser时，它们的TransactionStatus中的newTransaction设置为false。 如果register方法中抛出运行时异常，那么直接rollback整个事务，因为它是一个新事务。 如果login方法中抛出运行时异常，只能将rollbackOnly设置为true。因为login中没有catch该异常（回滚后又抛出该异常)，所以异常被register捕获（跳过了addUser)，所以register又要执行rollback方法，整个事务进行回滚。 REQUIRES_NEW（内外层事务平级，内层事务newTransaction为true，suspend外层事务，抛出异常后内层事务进行内层回滚，resume外层事务，外层事务捕获到内层抛出的异常后进行外层回滚) register创建一个事务，newTransaction为true。 调用login方法时检测到已存在事务，则将已存在事务suspend，并且创建的新事务，newTransaction为true。 当login方法抛出异常时，因为newTransaction为true，则回滚该事务。异常被抛出到外层register，register捕获该异常，并回滚。 NESTED（内外层事务嵌套，内层事务newTransaction为false，并创建还原点，抛出异常后rollback至还原点，外层事务捕获到内层抛出的异常后进行外层回滚) 如果PlatformTransactionManager支持还原点，便如上执行；如果不支持，那么行为与REQUIRES_NEW相同。 外层事务register的newTransaction为true，进入内层事务login。 内层事务login的newTransaction为false，并在获取Transaction时创建了一个还原点。（JTA不支持还原点，此时行为与REQUIRES_NEW相同)。抛出异常后在rollback时直接rollback至之前创建的还原点，并删除了该还原点。 外层事务捕获到该异常，进入rollback，因为是新事务，执行外层事务的回滚。 Spring MVC 配置文件示例 web.xml contextConfigLocation classpath:spring/spring-.xml org.springframework.web.context.ContextLoaderListener springDispatcherServlet org.springframework.web.servlet.DispatcherServlet contextConfigLocation classpath:spring/spring-web.xml springDispatcherServlet / 在这里配置了contextConfigLocation，指定了配置文件的位置； 并配置了DispatcherServlet，Spring使用该类拦截Web请求并进行转发； 还配置了ContextLoaderListener，用于初始化Spring。 在Spring配置文件中，需要配置viewResolver。 运行流程 DispatcherServlet： SpringMVC总的拦截器 HandlerMapping：请求和处理器之间的映射，用于获取HandlerExecutionChain HandlerExecutionChain：持有一组Interceptor和实际请求处理器HandlerAdapter，负责执行Interceptor的各个方法和处理方法。 HandlerAdapter：实际的请求处理器，处理后返回ModelAndView HandlerExceptionResolver：异常处理器，当拦截器的postHandle方法调用后检查异常。 ViewResolver：视图解析器,解析视图名，得到View，由逻辑视图变为物理视图。 View ：有render方法，渲染视图 渲染完毕后调用转发 初始化ApplicationContext ContextLoaderListener（入口) ContextLoaderListener的作用就是启动Web容器时，自动装配ApplicationContext的配置信息。 因为它实现了ServletContextListener这个接口，在web.xml配置这个监听器，启动容器时，就会默认执行它实现的方法，使用ServletContextListener接口，开发者能够在为客户端请求提供服务之前向ServletContext中添加任意的对象。 在ServletContextListener中的核心逻辑是初始化WebApplicationContext实例并存放在ServletContext中。 public void contextInitialized(ServletContextEvent event) { initWebApplicationContext(event.getServletContext()); } ContextLoader#initWebApplicationContext public WebApplicationContext initWebApplicationContext(ServletContext servletContext) { if (servletContext.getAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE) != null) { throw new IllegalStateException( \"Cannot initialize context because there is already a root application context present - \" + \"check whether you have multiple ContextLoader* definitions in your web.xml!\"); } Log logger = LogFactory.getLog(ContextLoader.class); servletContext.log(\"Initializing Spring root WebApplicationContext\"); if (logger.isInfoEnabled()) { logger.info(\"Root WebApplicationContext: initialization started\"); } long startTime = System.currentTimeMillis(); try { // Store context in local instance variable, to guarantee that // it is available on ServletContext shutdown. if (this.context == null) { // 创建WebApplicationContext this.context = createWebApplicationContext(servletContext); } if (this.context instanceof ConfigurableWebApplicationContext) { ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) this.context; if (!cwac.isActive()) { // The context has not yet been refreshed -> provide services such as // setting the parent context, setting the application context id, etc if (cwac.getParent() == null) { // The context instance was injected without an explicit parent -> // determine parent for root web application context, if any. ApplicationContext parent = loadParentContext(servletContext); cwac.setParent(parent); } configureAndRefreshWebApplicationContext(cwac, servletContext); } } // 记录在ServletContext中 servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, this.context); ClassLoader ccl = Thread.currentThread().getContextClassLoader(); if (ccl == ContextLoader.class.getClassLoader()) { currentContext = this.context; } else if (ccl != null) { currentContextPerThread.put(ccl, this.context); } if (logger.isDebugEnabled()) { logger.debug(\"Published root WebApplicationContext as ServletContext attribute with name [\" + WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE + \"]\"); } if (logger.isInfoEnabled()) { long elapsedTime = System.currentTimeMillis() - startTime; logger.info(\"Root WebApplicationContext: initialization completed in \" + elapsedTime + \" ms\"); } return this.context; } catch (RuntimeException ex) { logger.error(\"Context initialization failed\", ex); servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, ex); throw ex; } catch (Error err) { logger.error(\"Context initialization failed\", err); servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, err); throw err; } } ContextLoader#createWebApplicationContext（创建WebApplicationContext实例) protected WebApplicationContext createWebApplicationContext(ServletContext sc) { Class contextClass = determineContextClass(sc); if (!ConfigurableWebApplicationContext.class.isAssignableFrom(contextClass)) { throw new ApplicationContextException(\"Custom context class [\" + contextClass.getName() + \"] is not of type [\" + ConfigurableWebApplicationContext.class.getName() + \"]\"); } return (ConfigurableWebApplicationContext) BeanUtils.instantiateClass(contextClass); } determineContextClass protected Class determineContextClass(ServletContext servletContext) { String contextClassName = servletContext.getInitParameter(CONTEXT_CLASS_PARAM); // 如果在web.xml中配置了contextClass,则直接加载这个类 if (contextClassName != null) {try { return ClassUtils.forName(contextClassName, ClassUtils.getDefaultClassLoader()); } catch (ClassNotFoundException ex) { throw new ApplicationContextException( \"Failed to load custom context class [\" + contextClassName + \"]\", ex); } } else { // 否则使用默认值 contextClassName = defaultStrategies.getProperty(WebApplicationContext.class.getName()); try { return ClassUtils.forName(contextClassName, ContextLoader.class.getClassLoader()); } catch (ClassNotFoundException ex) { throw new ApplicationContextException( \"Failed to load default context class [\" + contextClassName + \"]\", ex); } } } 看defaultStrategies是怎么初始化的 static { // Load default strategy implementations from properties file. // This is currently strictly internal and not meant to be customized // by application developers. try {ClassPathResource resource = new ClassPathResource(DEFAULT_STRATEGIES_PATH, ContextLoader.class); defaultStrategies = PropertiesLoaderUtils.loadProperties(resource); } catch (IOException ex) {throw new IllegalStateException(\"Could not load 'ContextLoader.properties': \" + ex.getMessage()); } } private static final String DEFAULT_STRATEGIES_PATH = \"ContextLoader.properties\"; 在ContextLoader目录下有一个配置文件ContextLoader.properties 内容为： org.springframework.web.context.WebApplicationContext=org.springframework.web.context.support.XmlWebApplicationContext 在初始化过程中，程序首先会读取ContextLoader类的同目录下的配置文件ContextLoader.properties，并根据其中的配置提取将要实现WebApplicationContext接口的实现类，并根据这个实现类通过反射的方式进行实例的创建。 初始化DispatcherServlet 第一次访问网站时，会初始化访问到的servlet。 初始化阶段会调用servlet的init方法，在DispatcherServlet中是由其父类HttpServletBean实现的。 逻辑： 1)封装及验证初始化参数 解析init-param并封装到PropertyValues中 2)将DispatcherServlet转化为BeanWrapper实例 3)注册相对于Resource的属性编辑器 4)PropertyValues属性注入 5)servletBean的初始化（initServletBean) public final void init() throws ServletException { if (logger.isDebugEnabled()) { logger.debug(\"Initializing servlet '\" + getServletName() + \"'\"); } // Set bean properties from init parameters. try { // 解析init-param并封装到PropertyValues中PropertyValues pvs = new ServletConfigPropertyValues(getServletConfig(), this.requiredProperties); // 把DispatcherServlet转化为一个BeanWrapper，从而能够以Spring的方式来对init-param的值进行注入BeanWrapper bw = PropertyAccessorFactory.forBeanPropertyAccess(this); ResourceLoader resourceLoader = new ServletContextResourceLoader(getServletContext()); // 设置自定义属性编辑器，如果遇到Resource类型的属性将会使用ResourceEditor进行解析bw.registerCustomEditor(Resource.class, new ResourceEditor(resourceLoader, getEnvironment())); // 空实现，留给子类覆盖initBeanWrapper(bw); // 属性注入 bw.setPropertyValues(pvs, true); } catch (BeansException ex) { if (logger.isErrorEnabled()) { logger.error(\"Failed to set bean properties on servlet '\" + getServletName() + \"'\", ex); } throw ex; } // Let subclasses do whatever initialization they like. // 留给子类扩展 initServletBean(); if (logger.isDebugEnabled()) { logger.debug(\"Servlet '\" + getServletName() + \"' configured successfully\"); } } 1)ServletConfigPropertyValues（封装init-param) private static class ServletConfigPropertyValues extends MutablePropertyValues { /** * Create new ServletConfigPropertyValues. * @param config ServletConfig we'll use to take PropertyValues from * @param requiredProperties set of property names we need, where * we can't accept default values * @throws ServletException if any required properties are missing */ public ServletConfigPropertyValues(ServletConfig config, Set requiredProperties) throws ServletException { Set missingProps = (requiredProperties != null && !requiredProperties.isEmpty() ? new HashSet(requiredProperties) : null); Enumeration paramNames = config.getInitParameterNames(); while (paramNames.hasMoreElements()) { String property = paramNames.nextElement(); Object value = config.getInitParameter(property); addPropertyValue(new PropertyValue(property, value)); if (missingProps != null) { missingProps.remove(property); } } // Fail if we are still missing properties. if (!CollectionUtils.isEmpty(missingProps)) { throw new ServletException( \"Initialization from ServletConfig for servlet '\" + config.getServletName() + \"' failed; the following required properties were missing: \" + StringUtils.collectionToDelimitedString(missingProps, \", \")); } } } - 2)FrameworkServlet#initServletBean（对WebApplicationContext实例补充初始化) protected final void initServletBean() throws ServletException { getServletContext().log(\"Initializing Spring FrameworkServlet '\" + getServletName() + \"'\"); if (this.logger.isInfoEnabled()) { this.logger.info(\"FrameworkServlet '\" + getServletName() + \"': initialization started\"); } long startTime = System.currentTimeMillis(); try { this.webApplicationContext = initWebApplicationContext(); // 留给子类覆盖 initFrameworkServlet(); } catch (ServletException ex) { this.logger.error(\"Context initialization failed\", ex); throw ex; } catch (RuntimeException ex) { this.logger.error(\"Context initialization failed\", ex); throw ex; } if (this.logger.isInfoEnabled()) { long elapsedTime = System.currentTimeMillis() - startTime; this.logger.info(\"FrameworkServlet '\" + getServletName() + \"': initialization completed in \" + elapsedTime + \" ms\"); } } 2.1)FrameworkServlet#initWebApplicationContext 创建或刷新WebApplicationContext实例，并对servlet功能所使用的变量进行初始化 逻辑： 1)寻找或创建对应的WebApplicationContext实例 通过构造函数的注入进行初始化 通过contextAttribute进行初始化 通过在web.xml中配置的servlet参数contextAttribute来查找ServletContext中对应的属性，默认为WebApplicationContext.class.getName() + “.ROOT”，也就是在ContextLoaderListener加载时会创建WebApplicationContext实例，并将实例以 WebApplicationContext.class.getName() + “.ROOT”为key放入ServletContext中。 重新创建WebApplicationContext实例 2)对已经创建的WebApplicationContext实例进行配置和刷新 3)刷新Spring在Web功能实现中必须使用的全局变量 protected WebApplicationContext initWebApplicationContext() { WebApplicationContext rootContext = WebApplicationContextUtils.getWebApplicationContext(getServletContext()); WebApplicationContext wac = null; if (this.webApplicationContext != null) { // applicationContext实例在构造函数中被注入// A context instance was injected at construction time -> use it wac = this.webApplicationContext; if (wac instanceof ConfigurableWebApplicationContext) { ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) wac; if (!cwac.isActive()) { // The context has not yet been refreshed -> provide services such as // setting the parent context, setting the application context id, etc if (cwac.getParent() == null) { // The context instance was injected without an explicit parent -> set // the root application context (if any; may be null) as the parent cwac.setParent(rootContext); } // 刷新上下文环境 configureAndRefreshWebApplicationContext(cwac); } } } if (wac == null) {// No context instance was injected at construction time -> see if one // has been registered in the servlet context. If one exists, it is assumed // that the parent context (if any) has already been set and that the // user has performed any initialization such as setting the context id // 根据contextAttribute属性加载WebApplicationContext wac = findWebApplicationContext(); } if (wac == null) { // No context instance is defined for this servlet -> create a local one // 重新创建WebApplicationContext wac = createWebApplicationContext(rootContext); } if (!this.refreshEventReceived) { // Either the context is not a ConfigurableApplicationContext with refresh // support or the context injected at construction time had already been // refreshed -> trigger initial onRefresh manually here. // 刷新Spring在web功能实现中必须使用的全局变量 onRefresh(wac); } if (this.publishContext) { // Publish the context as a servlet context attribute. String attrName = getServletContextAttributeName(); getServletContext().setAttribute(attrName, wac); if (this.logger.isDebugEnabled()) { this.logger.debug(\"Published WebApplicationContext of servlet '\" + getServletName() + \"' as ServletContext attribute with name [\" + attrName + \"]\"); } } return wac; } 2.1.1) （与IOC衔接)configureAndRefreshWebApplicationContext protected void configureAndRefreshWebApplicationContext(ConfigurableWebApplicationContext wac) { if (ObjectUtils.identityToString(wac).equals(wac.getId())) { // The application context id is still set to its original default value // -> assign a more useful id based on available information if (this.contextId != null) { wac.setId(this.contextId); } else { // Generate default id... wac.setId(ConfigurableWebApplicationContext.APPLICATION_CONTEXT_ID_PREFIX + ObjectUtils.getDisplayString(getServletContext().getContextPath()) + '/' + getServletName()); } } wac.setServletContext(getServletContext()); wac.setServletConfig(getServletConfig()); wac.setNamespace(getNamespace()); wac.addApplicationListener(new SourceFilteringListener(wac, new ContextRefreshListener())); // The wac environment's #initPropertySources will be called in any case when the context // is refreshed; do it eagerly here to ensure servlet property sources are in place for // use in any post-processing or initialization that occurs below prior to #refresh ConfigurableEnvironment env = wac.getEnvironment(); if (env instanceof ConfigurableWebEnvironment) { ((ConfigurableWebEnvironment) env).initPropertySources(getServletContext(), getServletConfig()); } postProcessWebApplicationContext(wac); applyInitializers(wac); // 加载配置文件以及整合parent到wac（就是ApplicationContext中的refresh方法) wac.refresh(); } 2.1.2) findWebApplicationContext protected WebApplicationContext findWebApplicationContext() { String attrName = getContextAttribute(); if (attrName == null) { return null; } WebApplicationContext wac = WebApplicationContextUtils.getWebApplicationContext(getServletContext(), attrName); if (wac == null) { throw new IllegalStateException(\"No WebApplicationContext found: initializer not registered?\"); } return wac; } 2.1.3) FrameworkServlet#createWebApplicationContext protected WebApplicationContext createWebApplicationContext(ApplicationContext parent) { // 获取servlet的初始化参数contextClass，如果没有配置默认为XMLWebApplicationContext.class Class contextClass = getContextClass(); if (this.logger.isDebugEnabled()) {this.logger.debug(\"Servlet with name '\" + getServletName() + \"' will try to create custom WebApplicationContext context of class '\" + contextClass.getName() + \"'\" + \", using parent context [\" + parent + \"]\"); } if (!ConfigurableWebApplicationContext.class.isAssignableFrom(contextClass)) {throw new ApplicationContextException( \"Fatal initialization error in servlet with name '\" + getServletName() + \"': custom WebApplicationContext class [\" + contextClass.getName() + \"] is not of type ConfigurableWebApplicationContext\"); } // 通过反射实例化contextClass ConfigurableWebApplicationContext wac = (ConfigurableWebApplicationContext) BeanUtils.instantiateClass(contextClass); wac.setEnvironment(getEnvironment()); // parent为在ContextLoaderListener中创建的实例 wac.setParent(parent); // 获取contextConfigLocation属性，配置在servlet初始化参数中 wac.setConfigLocation(getContextConfigLocation()); // 初始化Spring环境包括加载配置文件等（即为2.1.1) configureAndRefreshWebApplicationContext(wac); return wac; } 2.1.4) DispatcherServlet#onRefresh protected void onRefresh(ApplicationContext context) { initStrategies(context); } HandlerMapping：请求和处理器之间的映射，用于获取HandlerExecutionChain HandlerAdapter：实际的请求处理器，处理后返回ModelAndView HandlerExceptionResolver：异常处理器，当拦截器的postHandle方法调用后检查异常。 ViewResolver：视图解析器,解析视图名，得到View，由逻辑视图变为物理视图。 protected void initStrategies(ApplicationContext context) { // 初始化文件上传模块 initMultipartResolver(context); // 初始化国际化模块 initLocaleResolver(context); // 初始化主题模块 initThemeResolver(context); // 初始化HandlerMappings initHandlerMappings(context); // 初始化HandlerAdapters initHandlerAdapters(context); // 初始化HandlerExceptionResolvers initHandlerExceptionResolvers(context); initRequestToViewNameTranslator(context); // 初始化ViewResolvers initViewResolvers(context); initFlashMapManager(context); } 2.1.4.1) initHandlerMappings 当客户端发出Request时DispatcherServlet会将Request提交给HandlerMapping，然后HandlerMapping根据WebApplicationContext的配置，回传给DispatcherServlet相应的Controller。 在基于SpringMVC的web应用程序中，我们可以为DispatcherServlet提供多个HandlerMapping供其使用。DispatcherServlet在选用HandlerMapping的过程中，将根据我们所指定的一系列HandlerMapping的优先级进行排序，然后优先使用优先级在前的HandlerMapping。如果当前的HandlerMapping能够返回可用的Handler，则使用当前的Handler进行Web请求的处理，而不再继续询问其他的HandlerMapping。否则，将继续按照各个HandlerMapping的优先级询问，直到获取一个可用的Handler为止。 默认情况下，SpringMVC将加载当前系统中所有实现了HandlerMapping接口的bean。如果只期望SpringMVC加载指定的handlerMapping时，可以修改web.xml中的DispatcherServlet的初始参数，将detectAllHandlerMappings的值设置为false。 此时，SpingMVC将查找名为handlerMapping的bean，并作为当前系统中唯一的handlerMapping。如果没有定义handlerMapping的话，则SpringMVC将按照DispatcherServlet所在目录下的DispatcherServlet.properties中所定义的内容来加载默认的handlerMapping。 private void initHandlerMappings(ApplicationContext context) { this.handlerMappings = null; if (this.detectAllHandlerMappings) { // Find all HandlerMappings in the ApplicationContext, including ancestor contexts. Map matchingBeans = BeanFactoryUtils.beansOfTypeIncludingAncestors(context, HandlerMapping.class, true, false); if (!matchingBeans.isEmpty()) { this.handlerMappings = new ArrayList(matchingBeans.values()); // We keep HandlerMappings in sorted order. AnnotationAwareOrderComparator.sort(this.handlerMappings); } } else { try { HandlerMapping hm = context.getBean(HANDLER_MAPPING_BEAN_NAME, HandlerMapping.class); this.handlerMappings = Collections.singletonList(hm); } catch (NoSuchBeanDefinitionException ex) { // Ignore, we'll add a default HandlerMapping later. } } // Ensure we have at least one HandlerMapping, by registering // a default HandlerMapping if no other mappings are found. if (this.handlerMappings == null) { this.handlerMappings = getDefaultStrategies(context, HandlerMapping.class); if (logger.isDebugEnabled()) { logger.debug(\"No HandlerMappings found in servlet '\" + getServletName() + \"': using default\"); } } } 默认的handlerMappings有 org.springframework.web.servlet.HandlerMapping=org.springframework.web.servlet.handler.BeanNameUrlHandlerMapping, org.springframework.web.servlet.mvc.annotation.DefaultAnnotationHandlerMapping getBean时会调用afterPropertiesSet public void afterPropertiesSet() { initHandlerMethods(); } 2.1.4.1.1) AbstractHandlerMethodMapping#initHandlerMethods（怎么把Controller里的各个方法封装为HandlerMethod的) protected void initHandlerMethods() { if (logger.isDebugEnabled()) { logger.debug(\"Looking for request mappings in application context: \" + getApplicationContext()); } String[] beanNames = (this.detectHandlerMethodsInAncestorContexts ? BeanFactoryUtils.beanNamesForTypeIncludingAncestors(getApplicationContext(), Object.class) : getApplicationContext().getBeanNamesForType(Object.class)); for (String beanName : beanNames) { if (!beanName.startsWith(SCOPED_TARGET_NAME_PREFIX)) { Class beanType = null; try { beanType = getApplicationContext().getType(beanName); } catch (Throwable ex) { // An unresolvable bean type, probably from a lazy bean - let's ignore it. if (logger.isDebugEnabled()) { logger.debug(\"Could not resolve target class for bean with name '\" + beanName + \"'\", ex); } } if (beanType != null && isHandler(beanType)) { detectHandlerMethods(beanName); } } } handlerMethodsInitialized(getHandlerMethods()); } 重点！凡是有Controller或者RequestMapping注解的Class类都被视为HandlerMethod，之后会遍历该类的Method检查是否符合要求。 protected boolean isHandler(Class beanType) { return (AnnotatedElementUtils.hasAnnotation(beanType, Controller.class) || AnnotatedElementUtils.hasAnnotation(beanType, RequestMapping.class)); } 2.1.4.1.1.1) detectHandlerMethods protected void detectHandlerMethods(final Object handler) { Class handlerType = (handler instanceof String ? getApplicationContext().getType((String) handler) : handler.getClass()); final Class userType = ClassUtils.getUserClass(handlerType); Map methods = MethodIntrospector.selectMethods(userType, new MethodIntrospector.MetadataLookup() { @Override public T inspect(Method method) { try { return getMappingForMethod(method, userType); } catch (Throwable ex) { throw new IllegalStateException(\"Invalid mapping on handler class [\" + userType.getName() + \"]: \" + method, ex); } } }); if (logger.isDebugEnabled()) { logger.debug(methods.size() + \" request handler methods found on \" + userType + \": \" + methods); } for (Map.Entry entry : methods.entrySet()) { Method invocableMethod = AopUtils.selectInvocableMethod(entry.getKey(), userType); T mapping = entry.getValue(); registerHandlerMethod(handler, invocableMethod, mapping); } } - 2.1.4.1.1.1.1) MethodIntrospector#selectMethods public static Map selectMethods(Class targetType, final MetadataLookup metadataLookup) { final Map methodMap = new LinkedHashMap(); Set> handlerTypes = new LinkedHashSet>(); Class specificHandlerType = null; if (!Proxy.isProxyClass(targetType)) { handlerTypes.add(targetType); specificHandlerType = targetType; } handlerTypes.addAll(Arrays.asList(targetType.getInterfaces())); for (Class currentHandlerType : handlerTypes) { final Class targetClass = (specificHandlerType != null ? specificHandlerType : currentHandlerType); ReflectionUtils.doWithMethods(currentHandlerType, new ReflectionUtils.MethodCallback() { @Override public void doWith(Method method) { Method specificMethod = ClassUtils.getMostSpecificMethod(method, targetClass); T result = metadataLookup.inspect(specificMethod); if (result != null) { Method bridgedMethod = BridgeMethodResolver.findBridgedMethod(specificMethod); if (bridgedMethod == specificMethod || metadataLookup.inspect(bridgedMethod) == null) { methodMap.put(specificMethod, result); } } } }, ReflectionUtils.USER_DECLARED_METHODS); } return methodMap; } - 2.1.4.1.1.1.1.1) doWithMethods public static void doWithMethods(Class clazz, MethodCallback mc, MethodFilter mf) { // Keep backing up the inheritance hierarchy. Method[] methods = getDeclaredMethods(clazz); for (Method method : methods) { if (mf != null && !mf.matches(method)) { continue; } try { mc.doWith(method); } catch (IllegalAccessException ex) { throw new IllegalStateException(\"Not allowed to access method '\" + method.getName() + \"': \" + ex); } } if (clazz.getSuperclass() != null) { doWithMethods(clazz.getSuperclass(), mc, mf); } else if (clazz.isInterface()) { for (Class superIfc : clazz.getInterfaces()) { doWithMethods(superIfc, mc, mf); } } } - 2.1.4.1.1.1.1.1.1) metadataLookup#inspect（根据Method找到RequestMapping注解信息) public T inspect(Method method) { try { return getMappingForMethod(method, userType); } catch (Throwable ex) { throw new IllegalStateException(\"Invalid mapping on handler class [\" + userType.getName() + \"]: \" + method, ex); } } protected RequestMappingInfo getMappingForMethod(Method method, Class handlerType) { RequestMappingInfo info = createRequestMappingInfo(method); if (info != null) {RequestMappingInfo typeInfo = createRequestMappingInfo(handlerType); if (typeInfo != null) { info = typeInfo.combine(info); } } return info; } private RequestMappingInfo createRequestMappingInfo(AnnotatedElement element) { RequestMapping requestMapping = AnnotatedElementUtils.findMergedAnnotation(element, RequestMapping.class); RequestCondition condition = (element instanceof Class ? getCustomTypeCondition((Class) element) : getCustomMethodCondition((Method) element)); return (requestMapping != null ? createRequestMappingInfo(requestMapping, condition) : null); } protected RequestMappingInfo createRequestMappingInfo( RequestMapping requestMapping, RequestCondition customCondition) { return RequestMappingInfo .paths(resolveEmbeddedValuesInPatterns(requestMapping.path())) .methods(requestMapping.method()) .params(requestMapping.params()) .headers(requestMapping.headers()) .consumes(requestMapping.consumes()) .produces(requestMapping.produces()) .mappingName(requestMapping.name()) .customCondition(customCondition) .options(this.config) .build(); } - 2.1.4.1.1.1.2) registerHandlerMethod protected void registerHandlerMethod(Object handler, Method method, T mapping) { this.mappingRegistry.register(mapping, handler, method); } 2.1.4.1.1.1.2.1) MappingRegistry#register public void register(T mapping, Object handler, Method method) { this.readWriteLock.writeLock().lock(); try { HandlerMethod handlerMethod = createHandlerMethod(handler, method); assertUniqueMethodMapping(handlerMethod, mapping); if (logger.isInfoEnabled()) { logger.info(\"Mapped \\\"\" + mapping + \"\\\" onto \" + handlerMethod); } this.mappingLookup.put(mapping, handlerMethod); List directUrls = getDirectUrls(mapping); for (String url : directUrls) { this.urlLookup.add(url, mapping); } String name = null; if (getNamingStrategy() != null) { name = getNamingStrategy().getName(handlerMethod, mapping); addMappingName(name, handlerMethod); } CorsConfiguration corsConfig = initCorsConfiguration(handler, method, mapping); if (corsConfig != null) { this.corsLookup.put(handlerMethod, corsConfig); } this.registry.put(mapping, new MappingRegistration(mapping, handlerMethod, directUrls, name)); } finally { this.readWriteLock.writeLock().unlock(); } } - 2.1.4.1.1.1.2.1.1) AbstractHandlerMethodMapping#createHandlerMethod protected HandlerMethod createHandlerMethod(Object handler, Method method) { HandlerMethod handlerMethod; if (handler instanceof String) {String beanName = (String) handler; handlerMethod = new HandlerMethod(beanName, getApplicationContext().getAutowireCapableBeanFactory(), method); } else {handlerMethod = new HandlerMethod(handler, method); } return handlerMethod; } public HandlerMethod(String beanName, BeanFactory beanFactory, Method method) { Assert.hasText(beanName, \"Bean name is required\"); Assert.notNull(beanFactory, \"BeanFactory is required\"); Assert.notNull(method, \"Method is required\"); this.bean = beanName; this.beanFactory = beanFactory; this.beanType = ClassUtils.getUserClass(beanFactory.getType(beanName)); this.method = method; this.bridgedMethod = BridgeMethodResolver.findBridgedMethod(method); // 初始化方法参数 this.parameters = initMethodParameters(); this.resolvedFromHandlerMethod = null; } private MethodParameter[] initMethodParameters() { int count = this.bridgedMethod.getParameterTypes().length; MethodParameter[] result = new MethodParameter[count]; for (int i = 0; i 2.1.4.2) initHandlerAdapters detectAllHandlerAdapters这个变量和detectAllHandlerMappings作用差不多。 private void initHandlerAdapters(ApplicationContext context) { this.handlerAdapters = null; if (this.detectAllHandlerAdapters) { // Find all HandlerAdapters in the ApplicationContext, including ancestor contexts. Map matchingBeans = BeanFactoryUtils.beansOfTypeIncludingAncestors(context, HandlerAdapter.class, true, false); if (!matchingBeans.isEmpty()) { this.handlerAdapters = new ArrayList(matchingBeans.values()); // We keep HandlerAdapters in sorted order. AnnotationAwareOrderComparator.sort(this.handlerAdapters); } } else { try { HandlerAdapter ha = context.getBean(HANDLER_ADAPTER_BEAN_NAME, HandlerAdapter.class); this.handlerAdapters = Collections.singletonList(ha); } catch (NoSuchBeanDefinitionException ex) { // Ignore, we'll add a default HandlerAdapter later. } } // Ensure we have at least some HandlerAdapters, by registering // default HandlerAdapters if no other adapters are found. if (this.handlerAdapters == null) { this.handlerAdapters = getDefaultStrategies(context, HandlerAdapter.class); if (logger.isDebugEnabled()) { logger.debug(\"No HandlerAdapters found in servlet '\" + getServletName() + \"': using default\"); } } } getDefaultStrategies(context, HandlerAdapter.class) defaultStrategies是从配置文件DispatcherServlet.properties中加载得到的。 默认有以下HandlerAdapter： org.springframework.web.servlet.HandlerAdapter=org.springframework.web.servlet.mvc.HttpRequestHandlerAdapter, org.springframework.web.servlet.mvc.SimpleControllerHandlerAdapter, org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter protected List getDefaultStrategies(ApplicationContext context, Class strategyInterface) { String key = strategyInterface.getName(); String value = defaultStrategies.getProperty(key); if (value != null) { String[] classNames = StringUtils.commaDelimitedListToStringArray(value); List strategies = new ArrayList(classNames.length); for (String className : classNames) { try { Class clazz = ClassUtils.forName(className, DispatcherServlet.class.getClassLoader()); Object strategy = createDefaultStrategy(context, clazz); strategies.add((T) strategy); } catch (ClassNotFoundException ex) { throw new BeanInitializationException( \"Could not find DispatcherServlet's default strategy class [\" + className + \"] for interface [\" + key + \"]\", ex); } catch (LinkageError err) { throw new BeanInitializationException( \"Error loading DispatcherServlet's default strategy class [\" + className + \"] for interface [\" + key + \"]: problem with class file or dependent class\", err); } } return strategies; } else { return new LinkedList(); } } HttpRequestHandlerAdapter（HTTP请求处理器适配器) 仅仅支持对HTTP请求处理器的适配，它简单地将HTTP请求和响应对象传递给HTTP请求处理器的实现，并不需要返回值，主要应用在基于HTTP的远程调用的实现上。 SimpleControllerHandlerAdapter（简单控制器处理器适配器) 这个实现类将HTTP请求适配到一个Controller的实现进行处理。这里控制器的实现是一个简单的Controller接口的实现。SimpleControllerHandlerAdapter 被设计成一个框架类的实现，不需要被改写，客户化的业务逻辑通常是在Controller接口的实现类中实现的。 AnnotationMethodHandlerAdapter（注解方法处理器适配器) 这个类的实现是基于注解的实现，它需要结合注解方法映射和注解方法处理器协同工作。它通过解析声明在注解控制器的请求映射信息来解析相应的处理器方法来处理当前的HTTP请求。 在处理的过程中，它通过反射来发现探测处理器方法的参数，调用处理器方法，并且映射返回值到模型和控制器对象，最后返回模型和控制器对象给DispatcherServlet。 RequestMappingHandlerAdapter（请求映射处理器适配器) 初始化该Adapter时初始化一些解析器： public void afterPropertiesSet() { // Do this first, it may add ResponseBody advice beans initControllerAdviceCache(); if (this.argumentResolvers == null) { List resolvers = getDefaultArgumentResolvers(); this.argumentResolvers = new HandlerMethodArgumentResolverComposite().addResolvers(resolvers); } if (this.initBinderArgumentResolvers == null) { List resolvers = getDefaultInitBinderArgumentResolvers(); this.initBinderArgumentResolvers = new HandlerMethodArgumentResolverComposite().addResolvers(resolvers); } if (this.returnValueHandlers == null) { List handlers = getDefaultReturnValueHandlers(); this.returnValueHandlers = new HandlerMethodReturnValueHandlerComposite().addHandlers(handlers); } } private List getDefaultArgumentResolvers() { List resolvers = new ArrayList(); // Annotation-based argument resolution resolvers.add(new RequestParamMethodArgumentResolver(getBeanFactory(), false)); resolvers.add(new RequestParamMapMethodArgumentResolver()); resolvers.add(new PathVariableMethodArgumentResolver()); resolvers.add(new PathVariableMapMethodArgumentResolver()); resolvers.add(new MatrixVariableMethodArgumentResolver()); resolvers.add(new MatrixVariableMapMethodArgumentResolver()); resolvers.add(new ServletModelAttributeMethodProcessor(false)); resolvers.add(new RequestResponseBodyMethodProcessor(getMessageConverters(), this.requestResponseBodyAdvice)); resolvers.add(new RequestPartMethodArgumentResolver(getMessageConverters(), this.requestResponseBodyAdvice)); resolvers.add(new RequestHeaderMethodArgumentResolver(getBeanFactory())); resolvers.add(new RequestHeaderMapMethodArgumentResolver()); resolvers.add(new ServletCookieValueMethodArgumentResolver(getBeanFactory())); resolvers.add(new ExpressionValueMethodArgumentResolver(getBeanFactory())); resolvers.add(new SessionAttributeMethodArgumentResolver()); resolvers.add(new RequestAttributeMethodArgumentResolver()); // Type-based argument resolution resolvers.add(new ServletRequestMethodArgumentResolver()); resolvers.add(new ServletResponseMethodArgumentResolver()); resolvers.add(new HttpEntityMethodProcessor(getMessageConverters(), this.requestResponseBodyAdvice)); resolvers.add(new RedirectAttributesMethodArgumentResolver()); resolvers.add(new ModelMethodProcessor()); resolvers.add(new MapMethodProcessor()); resolvers.add(new ErrorsMethodArgumentResolver()); resolvers.add(new SessionStatusMethodArgumentResolver()); resolvers.add(new UriComponentsBuilderMethodArgumentResolver()); // Custom arguments if (getCustomArgumentResolvers() != null) { resolvers.addAll(getCustomArgumentResolvers()); } // Catch-all resolvers.add(new RequestParamMethodArgumentResolver(getBeanFactory(), true)); resolvers.add(new ServletModelAttributeMethodProcessor(true)); return resolvers; } Spring中所使用的Handler并没有任何特殊的联系，为了统一处理，Spring提供了不同情况下的适配器。 2.1.4.3) initHandlerExceptionResolvers HanlderExceptionResolver是可以被用户定制的，只要实现该接口，实现resolveException方法并定义为一个bean即可。 resolveException方法返回一个ModelAndView对象，在方法内部对异常的类型进行判断，然后尝试生成对应的ModelAndView对象，如果该方法返回null，则Spring会继续寻找其他实现了HanlderExceptionResolver接口的bean。 private void initHandlerExceptionResolvers(ApplicationContext context) { this.handlerExceptionResolvers = null; if (this.detectAllHandlerExceptionResolvers) { // Find all HandlerExceptionResolvers in the ApplicationContext, including ancestor contexts. Map matchingBeans = BeanFactoryUtils .beansOfTypeIncludingAncestors(context, HandlerExceptionResolver.class, true, false); if (!matchingBeans.isEmpty()) { this.handlerExceptionResolvers = new ArrayList(matchingBeans.values()); // We keep HandlerExceptionResolvers in sorted order. AnnotationAwareOrderComparator.sort(this.handlerExceptionResolvers); } } else { try { HandlerExceptionResolver her = context.getBean(HANDLER_EXCEPTION_RESOLVER_BEAN_NAME, HandlerExceptionResolver.class); this.handlerExceptionResolvers = Collections.singletonList(her); } catch (NoSuchBeanDefinitionException ex) { // Ignore, no HandlerExceptionResolver is fine too. } } // Ensure we have at least some HandlerExceptionResolvers, by registering // default HandlerExceptionResolvers if no other resolvers are found. if (this.handlerExceptionResolvers == null) { this.handlerExceptionResolvers = getDefaultStrategies(context, HandlerExceptionResolver.class); if (logger.isDebugEnabled()) { logger.debug(\"No HandlerExceptionResolvers found in servlet '\" + getServletName() + \"': using default\"); } } } - 2.1.4.4) initRequestToViewNameTranslator 当Controller方法没有返回一个View或者逻辑视图名称，并且在该方法中没有直接往response输出流里面写数据的时候，Spring就会按照约定好的方式提供一个逻辑视图名称（最简答的情况就是加prefix和suffix)。 这个逻辑视图名称是通过Spring定义的RequestToViewNameTranslator接口的getViewName方法来实现的，用户也可以自定义自己的RequestToViewNameTranslator。 Spring为我们提供了一个默认的实现DefaultRequestToViewNameTranslator。 public static final String REQUEST_TO_VIEW_NAME_TRANSLATOR_BEAN_NAME = \"viewNameTranslator\"; private void initRequestToViewNameTranslator(ApplicationContext context) { try { this.viewNameTranslator = context.getBean(REQUEST_TO_VIEW_NAME_TRANSLATOR_BEAN_NAME, RequestToViewNameTranslator.class); if (logger.isDebugEnabled()) { logger.debug(\"Using RequestToViewNameTranslator [\" + this.viewNameTranslator + \"]\"); } } catch (NoSuchBeanDefinitionException ex) { // We need to use the default. this.viewNameTranslator = getDefaultStrategy(context, RequestToViewNameTranslator.class); if (logger.isDebugEnabled()) { logger.debug(\"Unable to locate RequestToViewNameTranslator with name '\" + REQUEST_TO_VIEW_NAME_TRANSLATOR_BEAN_NAME + \"': using default [\" + this.viewNameTranslator + \"]\"); } } } - 2.1.4.5) initViewResolvers 当Controller将请求处理结果放到ModelAndView中以后，DispatcherServlet会根据ModelAndView选择合适的视图进行渲染。ViewResolver接口定义了resolveViewName方法，根据viewName创建合适类型的View实现。 可以在Spring的配置文件中定义viewResolver。 private void initViewResolvers(ApplicationContext context) { this.viewResolvers = null; if (this.detectAllViewResolvers) { // Find all ViewResolvers in the ApplicationContext, including ancestor contexts. Map matchingBeans = BeanFactoryUtils.beansOfTypeIncludingAncestors(context, ViewResolver.class, true, false); if (!matchingBeans.isEmpty()) { this.viewResolvers = new ArrayList(matchingBeans.values()); // We keep ViewResolvers in sorted order. AnnotationAwareOrderComparator.sort(this.viewResolvers); } } else { try { ViewResolver vr = context.getBean(VIEW_RESOLVER_BEAN_NAME, ViewResolver.class); this.viewResolvers = Collections.singletonList(vr); } catch (NoSuchBeanDefinitionException ex) { // Ignore, we'll add a default ViewResolver later. } } // Ensure we have at least one ViewResolver, by registering // a default ViewResolver if no other resolvers are found. if (this.viewResolvers == null) { this.viewResolvers = getDefaultStrategies(context, ViewResolver.class); if (logger.isDebugEnabled()) { logger.debug(\"No ViewResolvers found in servlet '\" + getServletName() + \"': using default\"); } } } - 2.1.4.6) initFlashMapManager flash attributes提供了一个请求存储属性，可供其他请求使用。在使用重定向的时候非常必要。 Spring MVC 有两个主要的抽象来支持 flash attributes。 FlashMap 用于保持 flash attributes 而 FlashMapManager用于存储，检索，管理FlashMap 实例。 Flash attribute 支持默认开启，并不需要显式启用，它永远不会导致HTTP Session的创建。 每一个请求都有一个 “input”FlashMap 具有从上一个请求（如果有的话)传过来的属性和一个 “output” FlashMap 具有将要在后续请求中保存的属性。 这两个 FlashMap 实例都可以通过静态方法RequestContextUtils从Spring MVC的任何位置访问。 private void initFlashMapManager(ApplicationContext context) { try { this.flashMapManager = context.getBean(FLASH_MAP_MANAGER_BEAN_NAME, FlashMapManager.class); if (logger.isDebugEnabled()) { logger.debug(\"Using FlashMapManager [\" + this.flashMapManager + \"]\"); } } catch (NoSuchBeanDefinitionException ex) { // We need to use the default. this.flashMapManager = getDefaultStrategy(context, FlashMapManager.class); if (logger.isDebugEnabled()) { logger.debug(\"Unable to locate FlashMapManager with name '\" + FLASH_MAP_MANAGER_BEAN_NAME + \"': using default [\" + this.flashMapManager + \"]\"); } } } 处理请求 FrameworkServlet#service（入口) DispatcherServlet（FrameworkServlet的子类)无论是doGet、doPost等方法都会调用processRequest方法处理请求。 FrameworkServlet#processRequest 逻辑： 1)为了保证当前线程的LocaleContext和RequestAttributes可以在当前请求处理完毕后还能恢复，提取当前线程的两个属性 2)根据当前request创建对应的LocaleContext和RequestAttributes，并绑定到当前线程 3)委托给doService方法进一步处理 4)请求处理结束后恢复线程到原始状态 5)请求处理结束后无论成功与否都会发布事件通知 protected final void processRequest(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { long startTime = System.currentTimeMillis(); Throwable failureCause = null; LocaleContext previousLocaleContext = LocaleContextHolder.getLocaleContext(); LocaleContext localeContext = buildLocaleContext(request); RequestAttributes previousAttributes = RequestContextHolder.getRequestAttributes(); ServletRequestAttributes requestAttributes = buildRequestAttributes(request, response, previousAttributes); WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); asyncManager.registerCallableInterceptor(FrameworkServlet.class.getName(), new RequestBindingInterceptor()); // 将localeContext和requestAttributes绑定到当前线程 initContextHolders(request, localeContext, requestAttributes); try { doService(request, response); } catch (ServletException ex) { failureCause = ex; throw ex; } catch (IOException ex) { failureCause = ex; throw ex; } catch (Throwable ex) { failureCause = ex; throw new NestedServletException(\"Request processing failed\", ex); } finally { // 恢复线程到原始状态 resetContextHolders(request, previousLocaleContext, previousAttributes); if (requestAttributes != null) { requestAttributes.requestCompleted(); } if (logger.isDebugEnabled()) { if (failureCause != null) { this.logger.debug(\"Could not complete request\", failureCause); } else { if (asyncManager.isConcurrentHandlingStarted()) { logger.debug(\"Leaving response open for concurrent processing\"); } else { this.logger.debug(\"Successfully completed request\"); } } } // 发布事件通知 publishRequestHandledEvent(request, response, startTime, failureCause); } } DispatcherServlet#doService 将已经初始化的功能辅助工具变量设置在request属性中。 主要业务逻辑是在doDispatch方法中处理。 protected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception { if (logger.isDebugEnabled()) { String resumed = WebAsyncUtils.getAsyncManager(request).hasConcurrentResult() ? \" resumed\" : \"\"; logger.debug(\"DispatcherServlet with name '\" + getServletName() + \"'\" + resumed + \" processing \" + request.getMethod() + \" request for [\" + getRequestUri(request) + \"]\"); } // Keep a snapshot of the request attributes in case of an include, // to be able to restore the original attributes after the include. Map attributesSnapshot = null; if (WebUtils.isIncludeRequest(request)) { attributesSnapshot = new HashMap(); Enumeration attrNames = request.getAttributeNames(); while (attrNames.hasMoreElements()) { String attrName = (String) attrNames.nextElement(); if (this.cleanupAfterInclude || attrName.startsWith(\"org.springframework.web.servlet\")) { attributesSnapshot.put(attrName, request.getAttribute(attrName)); } } } // Make framework objects available to handlers and view objects. request.setAttribute(WEB_APPLICATION_CONTEXT_ATTRIBUTE, getWebApplicationContext()); request.setAttribute(LOCALE_RESOLVER_ATTRIBUTE, this.localeResolver); request.setAttribute(THEME_RESOLVER_ATTRIBUTE, this.themeResolver); request.setAttribute(THEME_SOURCE_ATTRIBUTE, getThemeSource()); FlashMap inputFlashMap = this.flashMapManager.retrieveAndUpdate(request, response); if (inputFlashMap != null) { request.setAttribute(INPUT_FLASH_MAP_ATTRIBUTE, Collections.unmodifiableMap(inputFlashMap)); } request.setAttribute(OUTPUT_FLASH_MAP_ATTRIBUTE, new FlashMap()); request.setAttribute(FLASH_MAP_MANAGER_ATTRIBUTE, this.flashMapManager); try { doDispatch(request, response); } finally { if (!WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) { // Restore the original attribute snapshot, in case of an include. if (attributesSnapshot != null) { restoreAttributesAfterInclude(request, attributesSnapshot); } } } } doDispatch（主体) protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception { HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try { ModelAndView mv = null; Exception dispatchException = null; try { // 如果request是MultiPartContent类型的，则将其转为MultiPartHttpServletRequest processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // 根据request信息寻找对应的Handler mappedHandler = getHandler(processedRequest); if (mappedHandler == null || mappedHandler.getHandler() == null) { // 没有找到对应的handler，则通过response反馈错误信息 noHandlerFound(processedRequest, response); return; } // 根据当前的handler寻找对应的HandlerAdapter HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // 如果当前handler支持Last-Modified请求头，则对其进行处理 // Process last-modified header, if supported by the handler. String method = request.getMethod(); boolean isGet = \"GET\".equals(method); if (isGet || \"HEAD\".equals(method)) { long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (logger.isDebugEnabled()) { logger.debug(\"Last-Modified value for [\" + getRequestUri(request) + \"] is: \" + lastModified); } if (new ServletWebRequest(request, response).checkNotModified(lastModified) && isGet) { return; } } // 调用拦截器的preHandle方法 if (!mappedHandler.applyPreHandle(processedRequest, response)) { return; } // 调用handler并返回视图 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) { return; } // 转换视图名称（加prefix和suffix) applyDefaultViewName(processedRequest, mv); // 调用拦截器的postHandle方法 mappedHandler.applyPostHandle(processedRequest, response, mv); } catch (Exception ex) { dispatchException = ex; } catch (Throwable err) { // As of 4.3, we're processing Errors thrown from handler methods as well, // making them available for @ExceptionHandler methods and other scenarios. dispatchException = new NestedServletException(\"Handler dispatch failed\", err); } // 处理handle的结果 processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); } catch (Exception ex) {triggerAfterCompletion(processedRequest, response, mappedHandler, ex); } catch (Throwable err) {triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(\"Handler processing failed\", err)); } finally {if (asyncManager.isConcurrentHandlingStarted()) { // Instead of postHandle and afterCompletion if (mappedHandler != null) { mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); } } else { // Clean up any resources used by a multipart request. if (multipartRequestParsed) { cleanupMultipart(processedRequest); } } } } 1) checkMultiPart（处理文件上传请求) protected HttpServletRequest checkMultipart(HttpServletRequest request) throws MultipartException { if (this.multipartResolver != null && this.multipartResolver.isMultipart(request)) {if (WebUtils.getNativeRequest(request, MultipartHttpServletRequest.class) != null) { logger.debug(\"Request is already a MultipartHttpServletRequest - if not in a forward, \" + \"this typically results from an additional MultipartFilter in web.xml\"); } else if (hasMultipartException(request) ) { logger.debug(\"Multipart resolution failed for current request before - \" + \"skipping re-resolution for undisturbed error rendering\"); } else { try { return this.multipartResolver.resolveMultipart(request); } catch (MultipartException ex) { if (request.getAttribute(WebUtils.ERROR_EXCEPTION_ATTRIBUTE) != null) { logger.debug(\"Multipart resolution failed for error dispatch\", ex); // Keep processing error dispatch with regular request handle below } else { throw ex; } } } } // If not returned before: return original request. return request; } MultipartResolver.resolveMultipart MultipartHttpServletRequest resolveMultipart(HttpServletRequest request) throws MultipartException; 2) getHandler（获取HandlerExecutionChain) protected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception { for (HandlerMapping hm : this.handlerMappings) {if (logger.isTraceEnabled()) { logger.trace( \"Testing handler map [\" + hm + \"] in DispatcherServlet with name '\" + getServletName() + \"'\"); } HandlerExecutionChain handler = hm.getHandler(request); if (handler != null) { return handler; } } return null; } HandlerMapping.getHandler HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception; 2.1) AbstractHandlerMapping#getHandler public final HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception { // 根据request获取对应的handler Object handler = getHandlerInternal(request); if (handler == null) { // 如果没有对应的request的handler则使用默认的handlerhandler = getDefaultHandler(); } // 没有默认的handler则无法继续处理 if (handler == null) {return null; } // 当查找的Controller为String，就意味着返回的是配置的bean名称，需要根据bean名称查找对应的bean // Bean name or resolved handler? if (handler instanceof String) { String handlerName = (String) handler; handler = getApplicationContext().getBean(handlerName); } HandlerExecutionChain executionChain = getHandlerExecutionChain(handler, request); if (CorsUtils.isCorsRequest(request)) { CorsConfiguration globalConfig = this.corsConfigSource.getCorsConfiguration(request); CorsConfiguration handlerConfig = getCorsConfiguration(handler, request); CorsConfiguration config = (globalConfig != null ? globalConfig.combine(handlerConfig) : handlerConfig); executionChain = getCorsHandlerExecutionChain(request, executionChain, config); } return executionChain; } 2.1.1-a) AbstractUrlHandlerMapping#getHandlerInternal 以AbstractUrlHandlerMapping为例 protected Object getHandlerInternal(HttpServletRequest request) throws Exception { // 截取用于匹配url的有效路径 String lookupPath = getUrlPathHelper().getLookupPathForRequest(request); // 根据路径寻找handler Object handler = lookupHandler(lookupPath, request); if (handler == null) {// We need to care for the default handler directly, since we need to // expose the PATH_WITHIN_HANDLER_MAPPING_ATTRIBUTE for it as well. Object rawHandler = null; if (\"/\".equals(lookupPath)) { // 如果请求路径是/，那么使用RootHandler进行处理 rawHandler = getRootHandler(); } if (rawHandler == null) { // 如果无法找到handler，则使用默认handler rawHandler = getDefaultHandler(); } if (rawHandler != null) { // Bean name or resolved handler? if (rawHandler instanceof String) { // 根据beanName寻找对应的beanName String handlerName = (String) rawHandler; rawHandler = getApplicationContext().getBean(handlerName); } validateHandler(rawHandler, request); // 模板方法 handler = buildPathExposingHandler(rawHandler, lookupPath, lookupPath, null);} } if (handler != null && logger.isDebugEnabled()) {logger.debug(\"Mapping [\" + lookupPath + \"] to \" + handler); } else if (handler == null && logger.isTraceEnabled()) {logger.trace(\"No handler mapping found for [\" + lookupPath + \"]\"); } return handler; } 2.1.1.1) lookupHandler protected Object lookupHandler(String urlPath, HttpServletRequest request) throws Exception { // Direct match? // 直接匹配 Object handler = this.handlerMap.get(urlPath); if (handler != null) { // Bean name or resolved handler? if (handler instanceof String) { String handlerName = (String) handler; handler = getApplicationContext().getBean(handlerName); } validateHandler(handler, request); return buildPathExposingHandler(handler, urlPath, urlPath, null); } // 通配符匹配 // Pattern match? List matchingPatterns = new ArrayList(); for (String registeredPattern : this.handlerMap.keySet()) { if (getPathMatcher().match(registeredPattern, urlPath)) { matchingPatterns.add(registeredPattern); } else if (useTrailingSlashMatch()) { if (!registeredPattern.endsWith(\"/\") && getPathMatcher().match(registeredPattern + \"/\", urlPath)) { matchingPatterns.add(registeredPattern +\"/\"); } } } String bestMatch = null; Comparator patternComparator = getPathMatcher().getPatternComparator(urlPath); if (!matchingPatterns.isEmpty()) { Collections.sort(matchingPatterns, patternComparator); if (logger.isDebugEnabled()) { logger.debug(\"Matching patterns for request [\" + urlPath + \"] are \" + matchingPatterns); } bestMatch = matchingPatterns.get(0); } if (bestMatch != null) { handler = this.handlerMap.get(bestMatch); if (handler == null) { if (bestMatch.endsWith(\"/\")) { handler = this.handlerMap.get(bestMatch.substring(0, bestMatch.length() - 1)); } if (handler == null) { throw new IllegalStateException( \"Could not find handler for best pattern match [\" + bestMatch + \"]\"); } } // Bean name or resolved handler? if (handler instanceof String) { String handlerName = (String) handler; handler = getApplicationContext().getBean(handlerName); } validateHandler(handler, request); String pathWithinMapping = getPathMatcher().extractPathWithinPattern(bestMatch, urlPath); // There might be multiple 'best patterns', let's make sure we have the correct URI template variables // for all of them Map uriTemplateVariables = new LinkedHashMap(); for (String matchingPattern : matchingPatterns) { if (patternComparator.compare(bestMatch, matchingPattern) == 0) { Map vars = getPathMatcher().extractUriTemplateVariables(matchingPattern, urlPath); Map decodedVars = getUrlPathHelper().decodePathVariables(request, vars); uriTemplateVariables.putAll(decodedVars); } } if (logger.isDebugEnabled()) { logger.debug(\"URI Template variables for request [\" + urlPath + \"] are \" + uriTemplateVariables); } return buildPathExposingHandler(handler, bestMatch, pathWithinMapping, uriTemplateVariables); } // No handler found... return null; } 2.1.1.1.1) buildPathExposingHandler（将handler封装为HandlerExecutionChain，并添加两个拦截器) protected Object buildPathExposingHandler(Object rawHandler, String bestMatchingPattern, String pathWithinMapping, Map uriTemplateVariables) { HandlerExecutionChain chain = new HandlerExecutionChain(rawHandler); chain.addInterceptor(new PathExposingHandlerInterceptor(bestMatchingPattern, pathWithinMapping)); if (!CollectionUtils.isEmpty(uriTemplateVariables)) { chain.addInterceptor(new UriTemplateVariablesHandlerInterceptor(uriTemplateVariables)); } return chain; } 2.1.1-b) AbstractHandlerMethodMapping#getHandlerInternal（加入拦截器) protected HandlerMethod getHandlerInternal(HttpServletRequest request) throws Exception { String lookupPath = getUrlPathHelper().getLookupPathForRequest(request); if (logger.isDebugEnabled()) {logger.debug(\"Looking up handler method for path \" + lookupPath); } this.mappingRegistry.acquireReadLock(); try {HandlerMethod handlerMethod = lookupHandlerMethod(lookupPath, request); if (logger.isDebugEnabled()) { if (handlerMethod != null) { logger.debug(\"Returning handler method [\" + handlerMethod + \"]\"); } else { logger.debug(\"Did not find handler method for [\" + lookupPath + \"]\"); } } return (handlerMethod != null ? handlerMethod.createWithResolvedBean() : null); } finally {this.mappingRegistry.releaseReadLock(); } } 2.1.1.1) lookupHandlerMethod protected HandlerMethod lookupHandlerMethod(String lookupPath, HttpServletRequest request) throws Exception { List matches = new ArrayList(); // 直接匹配 List directPathMatches = this.mappingRegistry.getMappingsByUrl(lookupPath); if (directPathMatches != null) {addMatchingMappings(directPathMatches, matches, request); } if (matches.isEmpty()) {// No choice but to go through all mappings... // 加入所有的映射 addMatchingMappings(this.mappingRegistry.getMappings().keySet(), matches, request); } if (!matches.isEmpty()) { Comparator comparator = new MatchComparator(getMappingComparator(request)); // 按匹配程度排序 Collections.sort(matches, comparator); if (logger.isTraceEnabled()) { logger.trace(\"Found \" + matches.size() + \" matching mapping(s) for [\" + lookupPath + \"] : \" + matches); } // 得到最符合的匹配结果 Match bestMatch = matches.get(0); if (matches.size() > 1) { if (CorsUtils.isPreFlightRequest(request)) { return PREFLIGHT_AMBIGUOUS_MATCH; } Match secondBestMatch = matches.get(1); if (comparator.compare(bestMatch, secondBestMatch) == 0) { Method m1 = bestMatch.handlerMethod.getMethod(); Method m2 = secondBestMatch.handlerMethod.getMethod(); throw new IllegalStateException(\"Ambiguous handler methods mapped for HTTP path '\" + request.getRequestURL() + \"': {\" + m1 + \", \" + m2 + \"}\"); } } handleMatch(bestMatch.mapping, lookupPath, request); return bestMatch.handlerMethod; } else { return handleNoMatch(this.mappingRegistry.getMappings().keySet(), lookupPath, request); } } 2.1.2) getHandlerExecutionChain（加入拦截器) 将handler实例和所有匹配的拦截器封装到HandlerExecutionChain中。 protected HandlerExecutionChain getHandlerExecutionChain(Object handler, HttpServletRequest request) { HandlerExecutionChain chain = (handler instanceof HandlerExecutionChain ? (HandlerExecutionChain) handler : new HandlerExecutionChain(handler)); String lookupPath = this.urlPathHelper.getLookupPathForRequest(request); for (HandlerInterceptor interceptor : this.adaptedInterceptors) { if (interceptor instanceof MappedInterceptor) { MappedInterceptor mappedInterceptor = (MappedInterceptor) interceptor; if (mappedInterceptor.matches(lookupPath, this.pathMatcher)) { chain.addInterceptor(mappedInterceptor.getInterceptor()); } } else { chain.addInterceptor(interceptor); } } return chain; } 3) noHandlerFound（没有找到HandlerExecutionChain) protected void noHandlerFound(HttpServletRequest request, HttpServletResponse response) throws Exception { if (pageNotFoundLogger.isWarnEnabled()) {pageNotFoundLogger.warn(\"No mapping found for HTTP request with URI [\" + getRequestUri(request) + \"] in DispatcherServlet with name '\" + getServletName() + \"'\"); } // 默认为false if (this.throwExceptionIfNoHandlerFound) { throw new NoHandlerFoundException(request.getMethod(), getRequestUri(request), new ServletServerHttpRequest(request).getHeaders()); } else { response.sendError(HttpServletResponse.SC_NOT_FOUND); } } 4) getHandlerAdapter（根据HandlerExecutionChain获取HandlerAdapter) protected HandlerAdapter getHandlerAdapter(Object handler) throws ServletException { for (HandlerAdapter ha : this.handlerAdapters) { if (logger.isTraceEnabled()) { logger.trace(\"Testing handler adapter [\" + ha + \"]\"); } if (ha.supports(handler)) { return ha; } } throw new ServletException(\"No adapter for handler [\" + handler + \"]: The DispatcherServlet configuration needs to include a HandlerAdapter that supports this handler\"); } 4.1-a) SimpleControllerHandlerAdapter#supports public boolean supports(Object handler) { return (handler instanceof Controller); } 4.1-b) AbstractHandlerMethodAdapter#supports public final boolean supports(Object handler) { return (handler instanceof HandlerMethod && supportsInternal((HandlerMethod) handler)); } - 5) HandlerExecutionChain#applyPreHandle（拦截器preHandle) boolean applyPreHandle(HttpServletRequest request, HttpServletResponse response) throws Exception { HandlerInterceptor[] interceptors = getInterceptors(); if (!ObjectUtils.isEmpty(interceptors)) { for (int i = 0; i } return true; } 6) HandlerAdapter#handle（处理请求) ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception; 6.1-a) SimpleControllerHandlerAdapter.handle public ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { return ((Controller) handler).handleRequest(request, response); } - 6.1.1) AbstractController#handleRequest public ModelAndView handleRequest(HttpServletRequest request, HttpServletResponse response) throws Exception { if (HttpMethod.OPTIONS.matches(request.getMethod())) { response.setHeader(\"Allow\", getAllowHeader()); return null; } // Delegate to WebContentGenerator for checking and preparing. checkRequest(request); prepareResponse(response); // 如果需要session内的同步执行 // Execute handleRequestInternal in synchronized block if required. if (this.synchronizeOnSession) { HttpSession session = request.getSession(false); if (session != null) { Object mutex = WebUtils.getSessionMutex(session); synchronized (mutex) { // 调用用户的逻辑 return handleRequestInternal(request, response); } } } // 调用用户的逻辑 return handleRequestInternal(request, response); } 6.1-b) AbstractHandlerMethodAdapter#handle public final ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { return handleInternal(request, response, (HandlerMethod) handler); } - 6.1.1) RequestMappingHandlerAdapter#handleInternal protected ModelAndView handleInternal(HttpServletRequest request, HttpServletResponse response, HandlerMethod handlerMethod) throws Exception { ModelAndView mav; checkRequest(request); // Execute invokeHandlerMethod in synchronized block if required. if (this.synchronizeOnSession) { HttpSession session = request.getSession(false); if (session != null) { Object mutex = WebUtils.getSessionMutex(session); synchronized (mutex) { mav = invokeHandlerMethod(request, response, handlerMethod); } } else { // No HttpSession available -> no mutex necessary mav = invokeHandlerMethod(request, response, handlerMethod); } } else { // No synchronization on session demanded at all... mav = invokeHandlerMethod(request, response, handlerMethod); } if (!response.containsHeader(HEADER_CACHE_CONTROL)) { if (getSessionAttributesHandler(handlerMethod).hasSessionAttributes()) { applyCacheSeconds(response, this.cacheSecondsForSessionAttributeHandlers); } else { prepareResponse(response); } } return mav; } 6.1.1.1) invokeHandlerMethod protected ModelAndView invokeHandlerMethod(HttpServletRequest request, HttpServletResponse response, HandlerMethod handlerMethod) throws Exception { ServletWebRequest webRequest = new ServletWebRequest(request, response); try { WebDataBinderFactory binderFactory = getDataBinderFactory(handlerMethod); ModelFactory modelFactory = getModelFactory(handlerMethod, binderFactory); ServletInvocableHandlerMethod invocableMethod = createInvocableHandlerMethod(handlerMethod); invocableMethod.setHandlerMethodArgumentResolvers(this.argumentResolvers); invocableMethod.setHandlerMethodReturnValueHandlers(this.returnValueHandlers); invocableMethod.setDataBinderFactory(binderFactory); invocableMethod.setParameterNameDiscoverer(this.parameterNameDiscoverer); ModelAndViewContainer mavContainer = new ModelAndViewContainer(); mavContainer.addAllAttributes(RequestContextUtils.getInputFlashMap(request)); modelFactory.initModel(webRequest, mavContainer, invocableMethod); mavContainer.setIgnoreDefaultModelOnRedirect(this.ignoreDefaultModelOnRedirect); AsyncWebRequest asyncWebRequest = WebAsyncUtils.createAsyncWebRequest(request, response); asyncWebRequest.setTimeout(this.asyncRequestTimeout); WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); asyncManager.setTaskExecutor(this.taskExecutor); asyncManager.setAsyncWebRequest(asyncWebRequest); asyncManager.registerCallableInterceptors(this.callableInterceptors); asyncManager.registerDeferredResultInterceptors(this.deferredResultInterceptors); if (asyncManager.hasConcurrentResult()) { Object result = asyncManager.getConcurrentResult(); mavContainer = (ModelAndViewContainer) asyncManager.getConcurrentResultContext()[0]; asyncManager.clearConcurrentResult(); if (logger.isDebugEnabled()) { logger.debug(\"Found concurrent result value [\" + result + \"]\"); } invocableMethod = invocableMethod.wrapConcurrentResult(result); } invocableMethod.invokeAndHandle(webRequest, mavContainer); if (asyncManager.isConcurrentHandlingStarted()) { return null; } return getModelAndView(mavContainer, modelFactory, webRequest); } finally { webRequest.requestCompleted(); } } protected ServletInvocableHandlerMethod createInvocableHandlerMethod(HandlerMethod handlerMethod) { return new ServletInvocableHandlerMethod(handlerMethod); } private ModelAndView getModelAndView(ModelAndViewContainer mavContainer, ModelFactory modelFactory, NativeWebRequest webRequest) throws Exception { modelFactory.updateModel(webRequest, mavContainer); if (mavContainer.isRequestHandled()) { return null; } // 将返回值ModelMap转为ModelAndView ModelMap model = mavContainer.getModel(); ModelAndView mav = new ModelAndView(mavContainer.getViewName(), model, mavContainer.getStatus()); if (!mavContainer.isViewReference()) { mav.setView((View) mavContainer.getView()); } if (model instanceof RedirectAttributes) { Map flashAttributes = ((RedirectAttributes) model).getFlashAttributes(); HttpServletRequest request = webRequest.getNativeRequest(HttpServletRequest.class); RequestContextUtils.getOutputFlashMap(request).putAll(flashAttributes); } return mav; } 6.1.1.1.1) ServletInvocableHandlerMethod#invokeAndHandle public void invokeAndHandle(ServletWebRequest webRequest, ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception { Object returnValue = invokeForRequest(webRequest, mavContainer, providedArgs); setResponseStatus(webRequest); if (returnValue == null) { if (isRequestNotModified(webRequest) || hasResponseStatus() || mavContainer.isRequestHandled()) { mavContainer.setRequestHandled(true); return; } } else if (StringUtils.hasText(this.responseReason)) { mavContainer.setRequestHandled(true); return; } mavContainer.setRequestHandled(false); try { this.returnValueHandlers.handleReturnValue( returnValue, getReturnValueType(returnValue), mavContainer, webRequest); } catch (Exception ex) { if (logger.isTraceEnabled()) { logger.trace(getReturnValueHandlingErrorMessage(\"Error handling return value\", returnValue), ex); } throw ex; } } 6.1.1.1.1.1) InvocableHandlerMethod#invokeForRequest public Object invokeForRequest(NativeWebRequest request, ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception { Object[] args = getMethodArgumentValues(request, mavContainer, providedArgs); if (logger.isTraceEnabled()) { logger.trace(\"Invoking '\" + ClassUtils.getQualifiedMethodName(getMethod(), getBeanType()) + \"' with arguments \" + Arrays.toString(args)); } Object returnValue = doInvoke(args); if (logger.isTraceEnabled()) { logger.trace(\"Method [\" + ClassUtils.getQualifiedMethodName(getMethod(), getBeanType()) + \"] returned [\" + returnValue + \"]\"); } return returnValue; } - 6.1.1.1.1.1.1) （处理参数)getMethodArgumentValues（方法参数注入，返回值即为方法参数) private Object[] getMethodArgumentValues(NativeWebRequest request, ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception { // 遍历请求参数，然后委托给HandlerMethodArgumentResolver#resolveArgument进行参数解析 MethodParameter[] parameters = getMethodParameters(); Object[] args = new Object[parameters.length]; for (int i = 0; i - 6.1.1.1.1.1.1.1) HandlerMethodArgumentResolverComposite#resolveArgument（将形参解析为实参) public Object resolveArgument(MethodParameter parameter, ModelAndViewContainer mavContainer, NativeWebRequest webRequest, WebDataBinderFactory binderFactory) throws Exception { HandlerMethodArgumentResolver resolver = getArgumentResolver(parameter); if (resolver == null) { throw new IllegalArgumentException(\"Unknown parameter type [\" + parameter.getParameterType().getName() + \"]\"); } return resolver.resolveArgument(parameter, mavContainer, webRequest, binderFactory); } - 6.1.1.1.1.1.1.1.1) getArgumentResolver private HandlerMethodArgumentResolver getArgumentResolver(MethodParameter parameter) { HandlerMethodArgumentResolver result = this.argumentResolverCache.get(parameter); if (result == null) { for (HandlerMethodArgumentResolver methodArgumentResolver : this.argumentResolvers) { if (logger.isTraceEnabled()) { logger.trace(\"Testing if argument resolver [\" + methodArgumentResolver + \"] supports [\" + parameter.getGenericParameterType() + \"]\"); } if (methodArgumentResolver.supportsParameter(parameter)) { result = methodArgumentResolver; this.argumentResolverCache.put(parameter, result); break; } } } return result; } - 6.1.1.1.1.1.1.1.2) （例子)RequestParamMethodArgumentResolver public boolean supportsParameter(MethodParameter parameter) { if (parameter.hasParameterAnnotation(RequestParam.class)) { if (Map.class.isAssignableFrom(parameter.nestedIfOptional().getNestedParameterType())) { String paramName = parameter.getParameterAnnotation(RequestParam.class).name(); return StringUtils.hasText(paramName); } else { return true; } } else { if (parameter.hasParameterAnnotation(RequestPart.class)) { return false; } parameter = parameter.nestedIfOptional(); if (MultipartResolutionDelegate.isMultipartArgument(parameter)) { return true; } else if (this.useDefaultResolution) { return BeanUtils.isSimpleProperty(parameter.getNestedParameterType()); } else { return false; } } } AbstractNamedValueMethodArgumentResolver#resolveArgument public final Object resolveArgument(MethodParameter parameter, ModelAndViewContainer mavContainer, NativeWebRequest webRequest, WebDataBinderFactory binderFactory) throws Exception { NamedValueInfo namedValueInfo = getNamedValueInfo(parameter); MethodParameter nestedParameter = parameter.nestedIfOptional(); Object resolvedName = resolveStringValue(namedValueInfo.name); if (resolvedName == null) { throw new IllegalArgumentException( \"Specified name must not resolve to null: [\" + namedValueInfo.name + \"]\"); } Object arg = resolveName(resolvedName.toString(), nestedParameter, webRequest); if (arg == null) { if (namedValueInfo.defaultValue != null) { arg = resolveStringValue(namedValueInfo.defaultValue); } else if (namedValueInfo.required && !nestedParameter.isOptional()) { handleMissingValue(namedValueInfo.name, nestedParameter, webRequest); } arg = handleNullValue(namedValueInfo.name, arg, nestedParameter.getNestedParameterType()); } else if (\"\".equals(arg) && namedValueInfo.defaultValue != null) { arg = resolveStringValue(namedValueInfo.defaultValue); } if (binderFactory != null) { WebDataBinder binder = binderFactory.createBinder(webRequest, null, namedValueInfo.name); try { // WebDataBinder进行数据转换 arg = binder.convertIfNecessary(arg, parameter.getParameterType(), parameter); } catch (ConversionNotSupportedException ex) { throw new MethodArgumentConversionNotSupportedException(arg, ex.getRequiredType(), namedValueInfo.name, parameter, ex.getCause()); } catch (TypeMismatchException ex) { throw new MethodArgumentTypeMismatchException(arg, ex.getRequiredType(), namedValueInfo.name, parameter, ex.getCause()); } } handleResolvedValue(arg, namedValueInfo.name, parameter, mavContainer, webRequest); return arg; } 6.1.1.1.1.1.1.1.2.1) resolveName protected Object resolveName(String name, MethodParameter parameter, NativeWebRequest request) throws Exception { HttpServletRequest servletRequest = request.getNativeRequest(HttpServletRequest.class); MultipartHttpServletRequest multipartRequest = WebUtils.getNativeRequest(servletRequest, MultipartHttpServletRequest.class); Object mpArg = MultipartResolutionDelegate.resolveMultipartArgument(name, parameter, servletRequest); if (mpArg != MultipartResolutionDelegate.UNRESOLVABLE) { return mpArg; } Object arg = null; if (multipartRequest != null) { List files = multipartRequest.getFiles(name); if (!files.isEmpty()) { arg = (files.size() == 1 ? files.get(0) : files); } } if (arg == null) { String[] paramValues = request.getParameterValues(name); if (paramValues != null) { arg = (paramValues.length == 1 ? paramValues[0] : paramValues); } } return arg; } 6.1.1.1.1.1.1.1.2.2) WebDataBinder#convertIfNecessary public T convertIfNecessary(Object value, Class requiredType, MethodParameter methodParam) throws TypeMismatchException { return getTypeConverter().convertIfNecessary(value, requiredType, methodParam); } - 6.1.1.1.1.1.2)（处理调用) InvocableHandlerMethod#doInvoke protected Object doInvoke(Object... args) throws Exception { ReflectionUtils.makeAccessible(getBridgedMethod()); try { // Controller中的方法，method.invoke return getBridgedMethod().invoke(getBean(), args); } catch (IllegalArgumentException ex) { assertTargetBean(getBridgedMethod(), getBean(), args); String text = (ex.getMessage() != null ? ex.getMessage() : \"Illegal argument\"); throw new IllegalStateException(getInvocationErrorMessage(text, args), ex); } catch (InvocationTargetException ex) { // Unwrap for HandlerExceptionResolvers ... Throwable targetException = ex.getTargetException(); if (targetException instanceof RuntimeException) { throw (RuntimeException) targetException; } else if (targetException instanceof Error) { throw (Error) targetException; } else if (targetException instanceof Exception) { throw (Exception) targetException; } else { String text = getInvocationErrorMessage(\"Failed to invoke handler method\", args); throw new IllegalStateException(text, targetException); } } } 6.1.1.1.1.2) （处理返回值)HandlerMethodReturnValueHandler#handleReturnValue 以HandlerMethodReturnValueHandlerComposite为例： public void handleReturnValue(Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws Exception { HandlerMethodReturnValueHandler handler = selectHandler(returnValue, returnType); if (handler == null) { throw new IllegalArgumentException(\"Unknown return value type: \" + returnType.getParameterType().getName()); } handler.handleReturnValue(returnValue, returnType, mavContainer, webRequest); } - 6.1.1.1.1.2.1) selectHandler private HandlerMethodReturnValueHandler selectHandler(Object value, MethodParameter returnType) { boolean isAsyncValue = isAsyncReturnValue(value, returnType); for (HandlerMethodReturnValueHandler handler : this.returnValueHandlers) { if (isAsyncValue && !(handler instanceof AsyncHandlerMethodReturnValueHandler)) { continue; } if (handler.supportsReturnType(returnType)) { return handler; } } return null; } 如果返回值是ModelAndView，那么handler是ModelAndViewMethodReturnValueHandler。 如果返回值是普通的对象(@ResponseBody)，那么handler是 RequestResponseBodyMethodProcessor。 6.1.1.1.1.2.2) handleReturnValue 以RequestResponseBodyMethodProcessor为例： public void handleReturnValue(Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws IOException, HttpMediaTypeNotAcceptableException, HttpMessageNotWritableException { mavContainer.setRequestHandled(true); ServletServerHttpRequest inputMessage = createInputMessage(webRequest); ServletServerHttpResponse outputMessage = createOutputMessage(webRequest); // Try even with null return value. ResponseBodyAdvice could get involved. writeWithMessageConverters(returnValue, returnType, inputMessage, outputMessage); } 以ModelAndViewMethodReturnValueHandler为例： public void handleReturnValue(Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws Exception { if (returnValue == null) { mavContainer.setRequestHandled(true); return; } ModelAndView mav = (ModelAndView) returnValue; if (mav.isReference()) { String viewName = mav.getViewName(); mavContainer.setViewName(viewName); if (viewName != null && isRedirectViewName(viewName)) { mavContainer.setRedirectModelScenario(true); } } else { View view = mav.getView(); mavContainer.setView(view); if (view instanceof SmartView) { if (((SmartView) view).isRedirectView()) { mavContainer.setRedirectModelScenario(true); } } } mavContainer.setStatus(mav.getStatus()); mavContainer.addAllAttributes(mav.getModel()); } 7) applyDefaultViewName（转换视图名称) private void applyDefaultViewName(HttpServletRequest request, ModelAndView mv) throws Exception { if (mv != null && !mv.hasView()) { mv.setViewName(getDefaultViewName(request)); } } protected String getDefaultViewName(HttpServletRequest request) throws Exception { return this.viewNameTranslator.getViewName(request); } DefaultRequestToViewNameTransaltor.getDefaultViewName public String getViewName(HttpServletRequest request) { String lookupPath = this.urlPathHelper.getLookupPathForRequest(request); return (this.prefix + transformPath(lookupPath) + this.suffix); } - 8) HandlerExecutionChain#applyPostHandle（拦截器postHandle) void applyPostHandle(HttpServletRequest request, HttpServletResponse response, ModelAndView mv) throws Exception { HandlerInterceptor[] interceptors = getInterceptors(); if (!ObjectUtils.isEmpty(interceptors)) { for (int i = interceptors.length - 1; i >= 0; i--) { HandlerInterceptor interceptor = interceptors[i]; interceptor.postHandle(request, response, this.handler, mv); } } } 9) processDispatchResult（处理ModelAndView请求结果) 如果返回的是纯数据（@ResponseBody)，mv就是null，该方法基本上是空方法。 private void processDispatchResult(HttpServletRequest request, HttpServletResponse response, HandlerExecutionChain mappedHandler, ModelAndView mv, Exception exception) throws Exception { boolean errorView = false; if (exception != null) { if (exception instanceof ModelAndViewDefiningException) { logger.debug(\"ModelAndViewDefiningException encountered\", exception); mv = ((ModelAndViewDefiningException) exception).getModelAndView(); } else { Object handler = (mappedHandler != null ? mappedHandler.getHandler() : null); // 处理异常 mv = processHandlerException(request, response, handler, exception); errorView = (mv != null); } } // Did the handler return a view to render? // 如果handler处理结果中返回了view，那么需要对页面进行渲染 if (mv != null && !mv.wasCleared()) { render(mv, request, response); if (errorView) { WebUtils.clearErrorRequestAttributes(request); } } else { if (logger.isDebugEnabled()) { logger.debug(\"Null ModelAndView returned to DispatcherServlet with name '\" + getServletName() + \"': assuming HandlerAdapter completed request handling\"); } } if (WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) { // Concurrent handling started during a forward return; } if (mappedHandler != null) { mappedHandler.triggerAfterCompletion(request, response, null); } } 9.1) processHandlerException（处理异常) protected ModelAndView processHandlerException(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { // Check registered HandlerExceptionResolvers... ModelAndView exMv = null; for (HandlerExceptionResolver handlerExceptionResolver : this.handlerExceptionResolvers) { // 使用handlerExceptionResolver来处理异常 exMv = handlerExceptionResolver.resolveException(request, response, handler, ex); if (exMv != null) { break; } } if (exMv != null) { if (exMv.isEmpty()) { request.setAttribute(EXCEPTION_ATTRIBUTE, ex); return null; } // We might still need view name translation for a plain error model... if (!exMv.hasView()) { exMv.setViewName(getDefaultViewName(request)); } if (logger.isDebugEnabled()) { logger.debug(\"Handler execution resulted in exception - forwarding to resolved error view: \" + exMv, ex); } WebUtils.exposeErrorRequestAttributes(request, ex, getServletName()); return exMv; } throw ex; } 9.2) render（渲染视图) protected void render(ModelAndView mv, HttpServletRequest request, HttpServletResponse response) throws Exception { // Determine locale for request and apply it to the response. Locale locale = this.localeResolver.resolveLocale(request); response.setLocale(locale); View view; if (mv.isReference()) { // We need to resolve the view name. view = resolveViewName(mv.getViewName(), mv.getModelInternal(), locale, request); if (view == null) { throw new ServletException(\"Could not resolve view with name '\" + mv.getViewName() + \"' in servlet with name '\" + getServletName() + \"'\"); } } else { // No need to lookup: the ModelAndView object contains the actual View object. view = mv.getView(); if (view == null) { throw new ServletException(\"ModelAndView [\" + mv + \"] neither contains a view name nor a \" + \"View object in servlet with name '\" + getServletName() + \"'\"); } } // Delegate to the View object for rendering. if (logger.isDebugEnabled()) { logger.debug(\"Rendering view [\" + view + \"] in DispatcherServlet with name '\" + getServletName() + \"'\"); } try { if (mv.getStatus() != null) { response.setStatus(mv.getStatus().value()); } view.render(mv.getModelInternal(), request, response); } catch (Exception ex) { if (logger.isDebugEnabled()) { logger.debug(\"Error rendering view [\" + view + \"] in DispatcherServlet with name '\" + getServletName() + \"'\", ex); } throw ex; } } 9.2.1) resolveViewName（创建视图) protected View resolveViewName(String viewName, Map model, Locale locale, HttpServletRequest request) throws Exception { for (ViewResolver viewResolver : this.viewResolvers) { // 使用viewResolver解析视图名称View view = viewResolver.resolveViewName(viewName, locale); if (view != null) { return view; } } return null; } 9.2.1.1) AbstractCachingViewResolver#resolveViewName public View resolveViewName(String viewName, Locale locale) throws Exception { if (!isCache()) { // 不存在缓存的情况下直接创建视图return createView(viewName, locale); } else { // 直接从缓存中获取 Object cacheKey = getCacheKey(viewName, locale); View view = this.viewAccessCache.get(cacheKey); if (view == null) { synchronized (this.viewCreationCache) { view = this.viewCreationCache.get(cacheKey); if (view == null) { // Ask the subclass to create the View object. view = createView(viewName, locale); if (view == null && this.cacheUnresolved) { view = UNRESOLVED_VIEW; } if (view != null) { this.viewAccessCache.put(cacheKey, view); this.viewCreationCache.put(cacheKey, view); if (logger.isTraceEnabled()) { logger.trace(\"Cached view [\" + cacheKey + \"]\"); } } } } } return (view != UNRESOLVED_VIEW ? view : null); } } 9.2.1.1.1) UrlBasedViewResolver#createView protected View createView(String viewName, Locale locale) throws Exception { // If this resolver is not supposed to handle the given view, // return null to pass on to the next resolver in the chain. // 如果当前解析器不支持当前解析器，如viewName为空等情况 if (!canHandle(viewName, locale)) {return null; } // 处理redirect:xx的情况 // Check for special \"redirect:\" prefix. if (viewName.startsWith(REDIRECT_URL_PREFIX)) {String redirectUrl = viewName.substring(REDIRECT_URL_PREFIX.length()); RedirectView view = new RedirectView(redirectUrl, isRedirectContextRelative(), isRedirectHttp10Compatible()); view.setHosts(getRedirectHosts()); return applyLifecycleMethods(viewName, view); } // 处理forward:xx的情况 // Check for special \"forward:\" prefix. if (viewName.startsWith(FORWARD_URL_PREFIX)) { String forwardUrl = viewName.substring(FORWARD_URL_PREFIX.length()); return new InternalResourceView(forwardUrl); } // Else fall back to superclass implementation: calling loadView. return super.createView(viewName, locale); } AbstractCachingViewResolver.createView protected View createView(String viewName, Locale locale) throws Exception { return loadView(viewName, locale); } UrlBasedViewResolver.loadView protected View loadView(String viewName, Locale locale) throws Exception { AbstractUrlBasedView view = buildView(viewName); View result = applyLifecycleMethods(viewName, view); return (view.checkResource(locale) ? result : null); } UrlBasedViewResolver.buildView protected AbstractUrlBasedView buildView(String viewName) throws Exception { AbstractUrlBasedView view = (AbstractUrlBasedView) BeanUtils.instantiateClass(getViewClass()); // 添加前缀和后缀 view.setUrl(getPrefix() + viewName + getSuffix()); String contentType = getContentType(); if (contentType != null) { // 设置ContentType view.setContentType(contentType); } view.setRequestContextAttribute(getRequestContextAttribute()); view.setAttributesMap(getAttributesMap()); Boolean exposePathVariables = getExposePathVariables(); if (exposePathVariables != null) { view.setExposePathVariables(exposePathVariables); } Boolean exposeContextBeansAsAttributes = getExposeContextBeansAsAttributes(); if (exposeContextBeansAsAttributes != null) { view.setExposeContextBeansAsAttributes(exposeContextBeansAsAttributes); } String[] exposedContextBeanNames = getExposedContextBeanNames(); if (exposedContextBeanNames != null) { view.setExposedContextBeanNames(exposedContextBeanNames); } return view; } 9.2.2) AbstractView.render（页面跳转) public void render(Map model, HttpServletRequest request, HttpServletResponse response) throws Exception { if (logger.isTraceEnabled()) { logger.trace(\"Rendering view with name '\" + this.beanName + \"' with model \" + model + \" and static attributes \" + this.staticAttributes); } // 处理Model Map mergedModel = createMergedOutputModel(model, request, response); prepareResponse(request, response); // 处理页面跳转 renderMergedOutputModel(mergedModel, getRequestToExpose(request), response); } 9.2.2.1) AbstractView#createMergedOutputModel（处理Model) protected Map createMergedOutputModel(Map model, HttpServletRequest request, HttpServletResponse response) { @SuppressWarnings(\"unchecked\") Map pathVars = (this.exposePathVariables ? (Map) request.getAttribute(View.PATH_VARIABLES) : null); // Consolidate static and dynamic model attributes. int size = this.staticAttributes.size(); size += (model != null ? model.size() : 0); size += (pathVars != null ? pathVars.size() : 0); Map mergedModel = new LinkedHashMap(size); mergedModel.putAll(this.staticAttributes); if (pathVars != null) { mergedModel.putAll(pathVars); } if (model != null) { mergedModel.putAll(model); } // Expose RequestContext? if (this.requestContextAttribute != null) { mergedModel.put(this.requestContextAttribute, createRequestContext(request, response, mergedModel)); } return mergedModel; } 9.2.2.2) renderMergedOutputModel#renderMergedOutputModel（处理页面跳转) protected void renderMergedOutputModel( Map model, HttpServletRequest request, HttpServletResponse response) throws Exception { // 将model中的数据以属性方式设置到request中 // Expose the model object as request attributes. exposeModelAsRequestAttributes(model, request); // Expose helpers as request attributes, if any. exposeHelpers(request); // Determine the path for the request dispatcher. String dispatcherPath = prepareForRendering(request, response); // Obtain a RequestDispatcher for the target resource (typically a JSP). RequestDispatcher rd = getRequestDispatcher(request, dispatcherPath); if (rd == null) { throw new ServletException(\"Could not get RequestDispatcher for [\" + getUrl() + \"]: Check that the corresponding file exists within your web application archive!\"); } // If already included or response already committed, perform include, else forward. if (useInclude(request, response)) { response.setContentType(getContentType()); if (logger.isDebugEnabled()) { logger.debug(\"Including resource [\" + getUrl() + \"] in InternalResourceView '\" + getBeanName() + \"'\"); } rd.include(request, response); } else { // Note: The forwarded resource is supposed to determine the content type itself. if (logger.isDebugEnabled()) { logger.debug(\"Forwarding to resource [\" + getUrl() + \"] in InternalResourceView '\" + getBeanName() + \"'\"); } rd.forward(request, response); } } 实例-MVC @Controller public class IndexController { @RequestMapping(\"/hello\") public ModelAndView index(ModelAndView modelAndView){ modelAndView.addObject(\"user\",new RegisterDTO(\"admin\")); modelAndView.setViewName(\"hello\"); return modelAndView; } } hello.jsp Hello World! ${requestScope.user.username} 执行路径： getHandler中handlerMappings为 hm是0号，它调用getHandler。 HandlerExecutionChain handler = hm.getHandler(request); 该方法中的getHandlerInternal方法这里用到的类是AbstractHandlerMethodMapping。 最终返回的是该handler，类型是HandlerMethod。 getHandlerExecutionChain执行完后多了三个interceptor。 getHandlerAdapter中的handlerAdapters是 遍历时先用到的是RequestMappingHandlerAdapter，调用的supports是AbstractHandlerMethodAdapter。匹配后，返回的adapter是RequestMappingHandlerAdapter类型的 然后调用该adapter的handle方法。 handle方法最终会调用Controller的对应方法，然后获取ModelAndView类型返回值的ModelAndViewMethodReturnValueHandler，对modelAndView进行处理。 然后调用applyDefaultViewName方法. 在调用processDispatchResult之前，modelAndView是这样的。 然后去调用render方法。 render ->InternalResourceViewResolver.resolveViewName ->UriBasedViewResolver.createView ->UriBasedViewResolver.loadView ->InternalResourceViewResolver.buildView 此时得到的view如下： 之后又调用了view的render方法，最终调用了requestDispatcher.forward方法结束整个过程。 实例-REST @RequestMapping(\"/users/{name}\") @ResponseBody public RegisterDTO findUserByName(@PathVariable(\"name\") String name){ return new RegisterDTO(name); } 前面都一样，但是处理请求结果时使用的handler不一样，它使用的是RequestResponseBodyMethodProcessor，将返回值写入输出流。 "},"zother5-Java-Interview/二十一、Tomcat源码解析.html":{"url":"zother5-Java-Interview/二十一、Tomcat源码解析.html","title":"二十一、Tomcat源码解析","keywords":"","body":"Tomcat Tomcat 参数调优 默认值： connectionTimeout=\"20000\" redirectPort=\"8443\" /> 修改配置： connectionTimeout=\"20000\" redirectPort=\"8443\" executor=\"TomcatThreadPool\" enableLookups=\"false\" acceptCount=\"100\" maxPostSize=\"10485760\" compression=\"on\" disableUploadTimeout=\"true\" compressionMinSize=\"2048\" noCompressionUserAgents=\"gozilla, traviata\" acceptorThreadCount=\"2\" compressableMimeType=\"text/html,text/xml,text/plain,text/css,text/javascript,application/javascript\" URIEncoding=\"utf-8\"/> Connector Tomcat有一个acceptor线程来accept socket连接，然后有工作线程来进行业务处理。对于client端的一个请求进来，流程是这样的：tcp的三次握手建立连接，建立连接的过程中，OS维护了半连接队列(syn队列)以及完全连接队列(accept队列)，在第三次握手之后，server收到了client的ack，则进入establish的状态，然后该连接由syn队列移动到accept队列。 Tomcat的acceptor线程则负责从accept队列中取出该connection，接受该connection，然后交给工作线程去处理(读取请求参数、处理逻辑、返回响应等等；如果该连接不是keep alived的话，则关闭该连接，然后该工作线程释放回线程池，如果是keep alived的话，则等待下一个数据包的到来直到keepAliveTimeout，然后关闭该连接释放回线程池)， 然后自己接着去accept队列取connection(当当前socket连接超过maxConnections的时候，acceptor线程自己会阻塞等待，等连接降下去之后，才去处理accept队列的下一个连接)。acceptCount指的就是这个accept队列的大小。 protocol（IO方式) Tomcat 8 设置 nio2 更好：org.apache.coyote.http11.Http11Nio2Protocol（如果这个用不了，就用下面那个) Tomcat 6、7 设置 nio 更好：org.apache.coyote.http11.Http11NioProtocol apr：调用httpd核心链接库来读取或文件传输，从而提高tomat对静态文件的处理性能。Tomcat APR模式也是Tomcat在高并发下的首选运行模式URIEncoding（URL编码) URIEncoding=”UTF-8” 使得Tomcat可以解析含有中文名的文件的url Executor（启用Worker线程池) maxThreads=\"150\" minSpareThreads=\"100\" prestartminSpareThreads=\"true\" maxQueueSize=\"100\"/> minSpareThreads（初始化时创建的线程数，类似于corePoolSize) 最小备用线程数，Tomcat启动时的初始化的线程数。maxThreads（最大并发数，类似于maxPoolSize) maxThreads Tomcat使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数，即最大并发数。 默认设置 200，一般建议在 500 ~ 800，根据硬件设施和业务来判断。 虽然client的socket连接上了，但是可能都在Tomcat的task queue里头，等待worker线程处理返回响应。maxQueueSize（Task队列大小) 指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理，默认设置 100。connectionTimeout（超时时间) connectionTimeout为网络连接超时时间毫秒数。enableLookups（是否允许DNS查询) enableLookups=\"false\" 为了消除DNS查询对性能的影响我们可以关闭DNS查询，方式是修改 server.xml文件中的enableLookups参数值。maxConnections（接收的最大连接数) 这个值表示最多可以有多少个socket连接到Tomcat上。NIO模式下默认是10000. 当连接数达到最大值后，系统会继续接收连接但不会超过acceptCount的值。acceptCount（accept队列大小) 当accept队列满了之后，即使client继续向server发送ACK的包，也会不被响应，此时，server通过/proc/sys/net/ipv4/tcp_abort_on_overflow来决定如何返回，0表示直接丢丢弃该ACK，1表示发送RST通知client；相应的，client则会分别返回read timeout 或者 connection reset by peer。 acceptCount在源码里对应的是backlog参数。backlog参数提示内核监听队列的最大长度。监听队列的长度如果超过backlog，服务器将不受理新的客户连接，客户端也将收到ECONNREFUSED错误信息。Linux自内核版本2.2之后，它只表示处于完全连接状态的socket的上限，处于半连接状态的socket的上限则由/proc/sys/net/ipv4/tcp_max_syn_backlog内核参数定义。 client端的socket等待队列： 当第一次握手，建立半连接状态：client 通过 connect 向 server 发出 SYN 包时，client 会维护一个 socket 队列，如果 socket 等待队列满了，而 client 也会由此返回 connection time out，只要是 client 没有收到 第二次握手SYN+ACK，3s 之后，client 会再次发送，如果依然没有收到，9s 之后会继续发送。 server端的半连接队列(syn队列)： 此时server 会维护一个 SYN 队列，半连接 syn 队列的长度为 max(64, /proc/sys/net/ipv4/tcp_max_syn_backlog) ，在机器的tcp_max_syn_backlog值在/proc/sys/net/ipv4/tcp_max_syn_backlog下配置，当 server 收到 client 的 SYN 包后，会进行第二次握手发送SYN＋ACK 的包加以确认，client 的 TCP 协议栈会唤醒 socket 等待队列，发出 connect 调用。 server端的完全连接队列(accpet队列)： 当第三次握手时，当server接收到ACK 报之后， 会进入一个新的叫 accept 的队列，该队列的长度为 min(backlog, somaxconn)，默认情况下，somaxconn 的值为 128，表示最多有 129 的 ESTAB 的连接等待 accept()，而 backlog 的值则应该是由 int listen(int sockfd, int backlog) 中的第二个参数指定，listen 里面的 backlog 可以有我们的应用程序去定义的。 acceptorThreadCount（用于接收请求的线程数) 用于接收连接的线程的数量，默认值是1。一般这个指需要改动的时候是因为该服务器是一个多核CPU，如果是多核 CPU 一般配置为 2。HTTP压缩 compression=\"on\" compressionMinSize=\"2048\"compressableMimeType=\"text/html,text/xml,text/javascript,text/css,text/plain\" HTTP 压缩可以大大提高浏览网站的速度，它的原理是，在客户端请求网页后，从服务器端将网页文件压缩，再下载到客户端，由客户端的浏览器负责解压缩并浏览。相对于普通的浏览过程HTML,CSS,Javascript , Text ，它可以节省40%左右的流量。更为重要的是，它可以对动态生成的，包括CGI、PHP , JSP , ASP , Servlet,SHTML等输出的网页也能进行压缩，压缩效率惊人。 1)compression=\"on\" 打开压缩功能 2)compressionMinSize=\"2048\" 启用压缩的输出内容大小，这里面默认为2KB 3)noCompressionUserAgents=\"gozilla, traviata\" 对于以下的浏览器，不启用压缩 4)compressableMimeType=\"text/html,text/xml\"　压缩类型 组件与框架 - Bootstrap：作为 Tomcat 对外界的启动类,在 $CATALINA_BASE/bin 目录下，它通过反射创建 Catalina 的实例并对其进行初始化及启动。 - Catalina：解析 $CATALINA_BASE/conf/server.xml 文件并创建 StandardServer、StandardService、StandardEngine、StandardHost 等 - Server：代表整个Catalina Servlet容器，可以包含一个或多个Service - Service：包含一个或多个 Connector，和一个 Engine，Connector 和 Engine 都是在解析 conf/server.xml 文件时创建的，Engine 在 Tomcat 的标准实现是 StandardEngine - Connector：实现某一协议的连接器，用来处理客户端发送来的协议，如默认的实现协议有HTTP、HTTPS、AJP。 主要作用有： - 根据不同的协议解析客户端的请求 - 将解析完的请求转发给Connector关联的Engine容器处理 1.- Mapper 维护了 URL 到容器的映射关系。当请求到来时会根据 Mapper 中的映射信息决定将请求映射到哪一个 Host、Context、Wrapper。 2.- Http11NioProtocol 用于处理 HTTP/1.1 的请求 3.- NioEndpoint 是连接的端点，在请求处理流程中该类是核心类，会重点介绍。 4.- CoyoteAdapter 用于将请求从 Connctor 交给 Container 处理，使 Connctor 和 Container 解耦。 - MapperListener 实现了 LifecycleListener 和 ContainerListener 接口用于监听容器事件和生命周期事件。该监听器实例监听所有的容器，包括 StandardEngine、StandardHost、StandardContext、StandardWrapper，当容器有变动时，注册容器到 Mapper。 - Engine：代表的是Servlet引擎，接收来自不同Connector请求，处理后将结果返回给Connector。Engine是一个逻辑容器，包含一个或多个Host。默认实现是StandardEngine，主要有以下模块： - Cluster：实现Tomcat管理 - Realm：实现用户权限管理模块 - Pipeline和Valve（阀门)：处理Pipeline上的各个Valve，是一种责任链模式。只是简单的将Connector传过来的变量传给Host容器 - Host：虚拟主机，即域名或网络名，用于部署该虚拟主机上的应用程序。通常包含多个 Context (Context 在 Tomcat 中代表应用程序)。Context 在 Tomcat 中的标准实现是 StandardContext。 - Context：部署的具体Web应用，每个请求都在是相应的上下文里处理，如一个war包。默认实现是StandardContext，通常包含多个 Wrapper主要有以下模块： - Realm：实现用户权限管理模块 - Pipeline和Valve：处理Pipeline上的各个Valve，是一种责任链模式 - Manager: 它主要是应用的session管理模块 - Resources: 它是每个web app对应的部署结构的封装 - Loader：它是对每个web app的自有的classloader的封装 - Mapper：它封装了请求资源URI与每个相对应的处理wrapper容器的映射关系 - Wrapper：对应定义的Servlet，一一对应。默认实现是StandardWrapper，主要有以下模块： - Pipeline和Valve：处理Pipeline上的各个Valve，是一种责任链模式 - Servlet和Servlet Stack：保存Wrapper包装的Servlet - StandardPipeline 组件代表一个流水线，与 Valve（阀)结合，用于处理请求。 StandardPipeline 中含有多个 Valve， 当需要处理请求时，会逐一调用 Valve 的 invoke 方法对 Request 和 Response 进行处理。特别的，其中有一个特殊的 Valve 叫 basicValve,每一个标准容器都有一个指定的 BasicValve，他们做的是最核心的工作。 - StandardEngine 的是 StandardEngineValve，他用来将 Request 映射到指定的 Host; - StandardHost 的是 StandardHostValve, 他用来将 Request 映射到指定的 Context; - StandardContext 的是 StandardContextValve，它用来将 Request 映射到指定的 Wrapper； - StandardWrapper 的是 StandardWrapperValve，他用来加载 Rquest 所指定的 Servlet,并调用 Servlet 的 Service 方法。 由上可知，Catalina中有两个主要的模块：连接器（Connector)和容器（Container)、 以 Tomcat 为例，它的主线流程大致可以分为 3 个：启动、部署、请求处理。入口点就是 Bootstrap 类和 接受请求的 Acceptor 类！ 生命周期 在Tomcat启动时，会读取server.xml文件创建Server, Service, Connector, Engine, Host, Context, Wrapper等组件。Lifestyle Tomcat中的所有组件都继承了Lifecycle接口，Lifecycle接口定义了一整套生命周期管理的函数，从组件的新建、初始化完成、启动、停止、失败到销毁，都遵守同样的规则，Lifecycle组件的状态转换图如下。 正常的调用顺序是init()->start()->destroy()，父组件的init()和start()会触发子组件的init()和start()，所以Tomcat中只需调用Server组件的init()和start()即可。 每个实现组件都继承自LifecycleBase，LifecycleBase实现了Lifecycle接口，当容器状态发生变化时，都会调用fireLifecycleEvent方法，生成LifecycleEvent，并且交由此容器的事件监听器处理。 启动 tomcat/bin/startup.sh脚本是启动了org.apache.catalina.startup.Bootstra类的main方法，并传入start参数。 主要步骤如下： 1.- 新建Bootstrap对象daemon，并调用其init()方法 2.- 初始化Tomcat的类加载器（init) 3.- 用反射实例化org.apache.catalina.startup.Catalina对象catalinaDaemon（init) 4.- 调用daemon的load方法，实质上调用了catalinaDaemon的load方法（load) 5.- 加载和解析server.xml配置文件（load) 6.- 调用daemon的start方法，实质上调用了catalinaDaemon的start方法 （start) 7.- 启动Server组件，Server的启动会带动其他组件的启动，如Service, Container, Connector（start) 8.- 调用catalinaDaemon的await方法循环等待接收Tomcat的shutdown命令 BootStrap#main public static void main(String args[]) { if (daemon == null) { // Don't set daemon until init() has completed Bootstrap bootstrap = new Bootstrap(); try { bootstrap.init(); } catch (Throwable t) { handleThrowable(t); t.printStackTrace(); return; } daemon = bootstrap; } else { // When running as a service the call to stop will be on a new // thread so make sure the correct class loader is used to prevent // a range of class not found exceptions. Thread.currentThread().setContextClassLoader(daemon.catalinaLoader); } try { String command = \"start\"; if (args.length > 0) { command = args[args.length - 1]; } if (command.equals(\"startd\")) { args[args.length - 1] = \"start\"; daemon.load(args); daemon.start(); } else if (command.equals(\"stopd\")) { args[args.length - 1] = \"stop\"; daemon.stop(); } else if (command.equals(\"start\")) { - // 设置 Catalina 的 await 属性为 true。在 Start 阶段尾部，若该属性为 true，Tomcat 会在 main 线程中监听 SHUTDOWN 命令，默认端口是 8005.当收到该命令后执行 Catalina 的 stop() 方法关闭 Tomcat 服务器。 daemon.setAwait(true); daemon.load(args); daemon.start(); } else if (command.equals(\"stop\")) { daemon.stopServer(args); } else if (command.equals(\"configtest\")) { daemon.load(args); if (null==daemon.getServer()) { System.exit(1); } System.exit(0); } else { log.warn(\"Bootstrap: command \\\"\" + command + \"\\\" does not exist.\"); } } catch (Throwable t) { // Unwrap the Exception for clearer error reporting if (t instanceof InvocationTargetException && t.getCause() != null) { t = t.getCause(); } handleThrowable(t); t.printStackTrace(); System.exit(1); } } - 1) BootStrap#init 必须使用反射去实例化Catalina对象，此时可以使用Tomcat自己的Classloader。否则会使用Java的Classloader去加载Catalina对象。 public void init() throws Exception { // 初始化commonLoader、catalinaLoader和sharedLoader; initClassLoaders(); // 将catalinaLoader设置为Tomcat主线程的线程上下文类加载器； Thread.currentThread().setContextClassLoader(catalinaLoader); SecurityClassLoad.securityClassLoad(catalinaLoader); // Load our startup class and call its process() method if (log.isDebugEnabled()) log.debug(\"Loading startup class\"); Class startupClass = catalinaLoader.loadClass(\"org.apache.catalina.startup.Catalina\"); Object startupInstance = startupClass.getConstructor().newInstance(); // Set the shared extensions class loader if (log.isDebugEnabled()) log.debug(\"Setting startup class properties\"); String methodName = \"setParentClassLoader\"; Class paramTypes[] = new Class[1]; paramTypes[0] = Class.forName(\"java.lang.ClassLoader\"); Object paramValues[] = new Object[1]; paramValues[0] = sharedLoader; Method method = startupInstance.getClass().getMethod(methodName, paramTypes); method.invoke(startupInstance, paramValues); catalinaDaemon = startupInstance; } - 1.1) BootStrap#initClassLoaders() commonLoader、catalinaLoader和sharedLoader是在Tomcat容器初始化的的过程刚刚开始（即调用Bootstrap的init方法时)创建的。catalinaLoader会被设置为Tomcat主线程的线程上下文类加载器，并且使用catalinaLoader加载Tomcat容器自身的class。 private void initClassLoaders() { try { commonLoader = createClassLoader(\"common\", null); if( commonLoader == null ) { // no config file, default to this loader - we might be in a 'single' env. commonLoader=this.getClass().getClassLoader(); } catalinaLoader = createClassLoader(\"server\", commonLoader); sharedLoader = createClassLoader(\"shared\", commonLoader); } catch (Throwable t) { handleThrowable(t); log.error(\"Class loader creation threw exception\", t); System.exit(1); } } 1.1.1) createClassLoader createClassLoader的处理步骤如下： 定位资源路径与资源类型； 使用ClassLoaderFactory创建类加载器org.apache.catalina.loader.StandardClassLoader，这个StandardClassLoader仅仅继承了URLClassLoader而没有其他更多改动。 Tomcat默认只会指定commonLoader，catalinaLoader和sharedLoader实际也是commonLoader。（在catalina.properties配置文件中，我们可以看到common属性默认值为{catalina.base}/lib/.jar,{catalina.home}/lib/.jar，如下配置所示，属性catalina.home默认为Tomcat的根目录。) common.loader=${catalina.home}/lib,${catalina.home}/lib/*.jar private ClassLoader createClassLoader(String name, ClassLoader parent) throws Exception { String value = CatalinaProperties.getProperty(name + \".loader\"); if ((value == null) || (value.equals(\"\"))) return parent; value = replace(value); List repositories = new ArrayList<>(); String[] repositoryPaths = getPaths(value); for (String repository : repositoryPaths) { // Check for a JAR URL repository try { @SuppressWarnings(\"unused\") URL url = new URL(repository); repositories.add( new Repository(repository, RepositoryType.URL)); continue; } catch (MalformedURLException e) { // Ignore } // Local repository if (repository.endsWith(\"*.jar\")) { repository = repository.substring (0, repository.length() - \"*.jar\".length()); repositories.add( new Repository(repository, RepositoryType.GLOB)); } else if (repository.endsWith(\".jar\")) { repositories.add( new Repository(repository, RepositoryType.JAR)); } else { repositories.add( new Repository(repository, RepositoryType.DIR)); } } return ClassLoaderFactory.createClassLoader(repositories, parent); } - 1.1.1.1) ClassLoaderFactory#createClassLoader public static ClassLoader createClassLoader(List repositories, final ClassLoader parent) throws Exception { if (log.isDebugEnabled()) log.debug(\"Creating new class loader\"); // Construct the \"class path\" for this class loader Set set = new LinkedHashSet<>(); if (repositories != null) { for (Repository repository : repositories) { if (repository.getType() == RepositoryType.URL) { URL url = buildClassLoaderUrl(repository.getLocation()); if (log.isDebugEnabled()) log.debug(\" Including URL \" + url); set.add(url); } else if (repository.getType() == RepositoryType.DIR) { File directory = new File(repository.getLocation()); directory = directory.getCanonicalFile(); if (!validateFile(directory, RepositoryType.DIR)) { continue; } URL url = buildClassLoaderUrl(directory); if (log.isDebugEnabled()) log.debug(\" Including directory \" + url); set.add(url); } else if (repository.getType() == RepositoryType.JAR) { File file=new File(repository.getLocation()); file = file.getCanonicalFile(); if (!validateFile(file, RepositoryType.JAR)) { continue; } URL url = buildClassLoaderUrl(file); if (log.isDebugEnabled()) log.debug(\" Including jar file \" + url); set.add(url); } else if (repository.getType() == RepositoryType.GLOB) { File directory=new File(repository.getLocation()); directory = directory.getCanonicalFile(); if (!validateFile(directory, RepositoryType.GLOB)) { continue; } if (log.isDebugEnabled()) log.debug(\" Including directory glob \" + directory.getAbsolutePath()); String filenames[] = directory.list(); if (filenames == null) { continue; } for (int j = 0; j () { @Override public URLClassLoader run() { if (parent == null) return new URLClassLoader(array); else return new URLClassLoader(array, parent); } }); } - 2) BootStrap#load private void load(String[] arguments) throws Exception { // Call the load() method String methodName = \"load\"; Object param[]; Class paramTypes[]; if (arguments==null || arguments.length==0) { paramTypes = null; param = null; } else { paramTypes = new Class[1]; paramTypes[0] = arguments.getClass(); param = new Object[1]; param[0] = arguments; } Method method = catalinaDaemon.getClass().getMethod(methodName, paramTypes); if (log.isDebugEnabled()) log.debug(\"Calling startup class \" + method); method.invoke(catalinaDaemon, param); } - 2.1) Catalina#load public void load() { if (loaded) { return; } loaded = true; long t1 = System.nanoTime(); initDirs(); // Before digester - it may be needed initNaming(); // Create and execute our Digester Digester digester = createStartDigester(); InputSource inputSource = null; InputStream inputStream = null; File file = null; try { try { file = configFile(); inputStream = new FileInputStream(file); inputSource = new InputSource(file.toURI().toURL().toString()); } catch (Exception e) { if (log.isDebugEnabled()) { log.debug(sm.getString(\"catalina.configFail\", file), e); } } if (inputStream == null) { try { inputStream = getClass().getClassLoader() .getResourceAsStream(getConfigFile()); inputSource = new InputSource (getClass().getClassLoader() .getResource(getConfigFile()).toString()); } catch (Exception e) { if (log.isDebugEnabled()) { log.debug(sm.getString(\"catalina.configFail\", getConfigFile()), e); } } } // This should be included in catalina.jar // Alternative: don't bother with xml, just create it manually. if (inputStream == null) { try { inputStream = getClass().getClassLoader() .getResourceAsStream(\"server-embed.xml\"); inputSource = new InputSource (getClass().getClassLoader() .getResource(\"server-embed.xml\").toString()); } catch (Exception e) { if (log.isDebugEnabled()) { log.debug(sm.getString(\"catalina.configFail\", \"server-embed.xml\"), e); } } } if (inputStream == null || inputSource == null) { if (file == null) { log.warn(sm.getString(\"catalina.configFail\", getConfigFile() + \"] or [server-embed.xml]\")); } else { log.warn(sm.getString(\"catalina.configFail\", file.getAbsolutePath())); if (file.exists() && !file.canRead()) { log.warn(\"Permissions incorrect, read permission is not allowed on the file.\"); } } return; } try { inputSource.setByteStream(inputStream); digester.push(this); digester.parse(inputSource); } catch (SAXParseException spe) { log.warn(\"Catalina.start using \" + getConfigFile() + \": \" + spe.getMessage()); return; } catch (Exception e) { log.warn(\"Catalina.start using \" + getConfigFile() + \": \" , e); return; } } finally { if (inputStream != null) { try { inputStream.close(); } catch (IOException e) { // Ignore } } } getServer().setCatalina(this); getServer().setCatalinaHome(Bootstrap.getCatalinaHomeFile()); getServer().setCatalinaBase(Bootstrap.getCatalinaBaseFile()); // Stream redirection initStreams(); // Start the new server try { getServer().init(); } catch (LifecycleException e) { if (Boolean.getBoolean(\"org.apache.catalina.startup.EXIT_ON_INIT_FAILURE\")) { throw new java.lang.Error(e); } else { log.error(\"Catalina.start\", e); } } long t2 = System.nanoTime(); if(log.isInfoEnabled()) { log.info(\"Initialization processed in \" + ((t2 - t1) / 1000000) + \" ms\"); } } - 2.1.1) Digester#parse（配置文件解析，创建子容器) public Object parse(InputSource input) throws IOException, SAXException { configure(); getXMLReader().parse(input); return root; } org.apache.commons.digester 该包提供了基于规则的，可任意处理XML文档的类 org.apache.commons.digester.Digester是Digester类库的主类，该类可用于解析XML文档。 解析过程分为两步: 定义好模式(定义要匹配的标签) 将模式与规则(定义匹配到标签后的行为的对象)相关联 解析过程中会调用startElement方法，会按照既定的一些规则，在读取的同时去创建对象。 比如： digester.addRule(\"Server/Service/Connector\", new ConnectorCreateRule()); 2.1.1.1) ConnectorCreateRule#begin public void begin(String namespace, String name, Attributes attributes) throws Exception { Service svc = (Service)digester.peek(); Executor ex = null; if ( attributes.getValue(\"executor\")!=null ) { ex = svc.getExecutor(attributes.getValue(\"executor\")); } Connector con = new Connector(attributes.getValue(\"protocol\")); if (ex != null) { setExecutor(con, ex); } String sslImplementationName = attributes.getValue(\"sslImplementationName\"); if (sslImplementationName != null) { setSSLImplementationName(con, sslImplementationName); } digester.push(con); } - 2.1.1.1.1) Connector#constructor（从Connector开始的初始化) public Connector(String protocol) { boolean aprConnector = AprLifecycleListener.isAprAvailable() && AprLifecycleListener.getUseAprConnector(); if (\"HTTP/1.1\".equals(protocol) || protocol == null) { if (aprConnector) { protocolHandlerClassName = \"org.apache.coyote.http11.Http11AprProtocol\"; } else { protocolHandlerClassName = \"org.apache.coyote.http11.Http11NioProtocol\"; } } else if (\"AJP/1.3\".equals(protocol)) { if (aprConnector) { protocolHandlerClassName = \"org.apache.coyote.ajp.AjpAprProtocol\"; } else { protocolHandlerClassName = \"org.apache.coyote.ajp.AjpNioProtocol\"; } } else { protocolHandlerClassName = protocol; } // Instantiate protocol handler ProtocolHandler p = null; try { // 反射创建Http11NioProtocol Class clazz = Class.forName(protocolHandlerClassName); p = (ProtocolHandler) clazz.getConstructor().newInstance(); } catch (Exception e) { log.error(sm.getString( \"coyoteConnector.protocolHandlerInstantiationFailed\"), e); } finally { this.protocolHandler = p; } // Default for Connector depends on this system property setThrowOnFailure(Boolean.getBoolean(\"org.apache.catalina.startup.EXIT_ON_INIT_FAILURE\")); } 2.1.1.1.1.1) Http11NioProtocol#constructor public Http11NioProtocol() { super(new NioEndpoint()); } public AbstractHttp11JsseProtocol(AbstractJsseEndpoint endpoint) { super(endpoint); } public AbstractHttp11Protocol(AbstractEndpoint endpoint) { super(endpoint); setConnectionTimeout(Constants.DEFAULT_CONNECTION_TIMEOUT); ConnectionHandler cHandler = new ConnectionHandler<>(this); setHandler(cHandler); getEndpoint().setHandler(cHandler); } public AbstractProtocol(AbstractEndpoint endpoint) { this.endpoint = endpoint; setConnectionLinger(Constants.DEFAULT_CONNECTION_LINGER); setTcpNoDelay(Constants.DEFAULT_TCP_NO_DELAY); } - 2.1.1.1.1.1.1) ConnectionHandler#constructor public ConnectionHandler(AbstractProtocol proto) { this.proto = proto; } - 2.1.2) StandardServer#init 模板方法模式，调用的是自己重写的initInternal。 protected void initInternal() throws LifecycleException { super.initInternal(); // Register global String cache // Note although the cache is global, if there are multiple Servers // present in the JVM (may happen when embedding) then the same cache // will be registered under multiple names onameStringCache = register(new StringCache(), \"type=StringCache\"); // Register the MBeanFactory MBeanFactory factory = new MBeanFactory(); factory.setContainer(this); onameMBeanFactory = register(factory, \"type=MBeanFactory\"); // Register the naming resources globalNamingResources.init(); // Populate the extension validator with JARs from common and shared // class loaders if (getCatalina() != null) { ClassLoader cl = getCatalina().getParentClassLoader(); // Walk the class loader hierarchy. Stop at the system class loader. // This will add the shared (if present) and common class loaders while (cl != null && cl != ClassLoader.getSystemClassLoader()) { if (cl instanceof URLClassLoader) { URL[] urls = ((URLClassLoader) cl).getURLs(); for (URL url : urls) { if (url.getProtocol().equals(\"file\")) { try { File f = new File (url.toURI()); if (f.isFile() && f.getName().endsWith(\".jar\")) { ExtensionValidator.addSystemResource(f); } } catch (URISyntaxException e) { // Ignore } catch (IOException e) { // Ignore } } } } cl = cl.getParent(); } } // Initialize our defined Services for (int i = 0; i services[i].init(); } } 初始化StandardService 2.1.2.1) StandardService#init protected void initInternal() throws LifecycleException { super.initInternal(); if (engine != null) { engine.init(); } // Initialize any Executors for (Executor executor : findExecutors()) { if (executor instanceof JmxEnabled) { ((JmxEnabled) executor).setDomain(getDomain()); } executor.init(); } // Initialize mapper listener mapperListener.init(); // Initialize our defined Connectors synchronized (connectorsLock) { for (Connector connector : connectors) { connector.init(); } } } 2.1.2.1.1) Connector#init protected void initInternal() throws LifecycleException { super.initInternal(); if (protocolHandler == null) { throw new LifecycleException( sm.getString(\"coyoteConnector.protocolHandlerInstantiationFailed\")); } // Initialize adapter adapter = new CoyoteAdapter(this); // protocolHandler 即 Http11NioProtocol protocolHandler.setAdapter(adapter); // Make sure parseBodyMethodsSet has a default if (null == parseBodyMethodsSet) { setParseBodyMethods(getParseBodyMethods()); } if (protocolHandler.isAprRequired() && !AprLifecycleListener.isAprAvailable()) { throw new LifecycleException(sm.getString(\"coyoteConnector.protocolHandlerNoApr\", getProtocolHandlerClassName())); } if (AprLifecycleListener.isAprAvailable() && AprLifecycleListener.getUseOpenSSL() && protocolHandler instanceof AbstractHttp11JsseProtocol) { AbstractHttp11JsseProtocol jsseProtocolHandler = (AbstractHttp11JsseProtocol) protocolHandler; if (jsseProtocolHandler.isSSLEnabled() && jsseProtocolHandler.getSslImplementationName() == null) { // OpenSSL is compatible with the JSSE configuration, so use it if APR is available jsseProtocolHandler.setSslImplementationName(OpenSSLImplementation.class.getName()); } } try { protocolHandler.init(); } catch (Exception e) { throw new LifecycleException( sm.getString(\"coyoteConnector.protocolHandlerInitializationFailed\"), e); } } 2.1.2.1.1.1) AbstractHttp11Protocol#init public void init() throws Exception { // Upgrade protocols have to be configured first since the endpoint // init (triggered via super.init() below) uses this list to configure // the list of ALPN protocols to advertise for (UpgradeProtocol upgradeProtocol : upgradeProtocols) { configureUpgradeProtocol(upgradeProtocol); } super.init(); } AbstractProtocol#init public void init() throws Exception { if (getLog().isInfoEnabled()) { getLog().info(sm.getString(\"abstractProtocolHandler.init\", getName())); } if (oname == null) { // Component not pre-registered so register it oname = createObjectName(); if (oname != null) { Registry.getRegistry(null, null).registerComponent(this, oname, null); } } if (this.domain != null) { rgOname = new ObjectName(domain + \":type=GlobalRequestProcessor,name=\" + getName()); Registry.getRegistry(null, null).registerComponent( getHandler().getGlobal(), rgOname, null); } String endpointName = getName(); endpoint.setName(endpointName.substring(1, endpointName.length()-1)); endpoint.setDomain(domain); endpoint.init(); } - 2.1.2.1.1.1.1) AbstractEndPoint#init public final void init() throws Exception { if (bindOnInit) { bind(); bindState = BindState.BOUND_ON_INIT; } if (this.domain != null) { // Register endpoint (as ThreadPool - historical name) oname = new ObjectName(domain + \":type=ThreadPool,name=\\\"\" + getName() + \"\\\"\"); Registry.getRegistry(null, null).registerComponent(this, oname, null); for (SSLHostConfig sslHostConfig : findSslHostConfigs()) { registerJmx(sslHostConfig); } } } - 2.1.2.1.1.1.1.1) NioEndpoint#init public void bind() throws Exception { initServerSocket(); // Initialize thread count defaults for acceptor, poller if (acceptorThreadCount == 0) { // FIXME: Doesn't seem to work that well with multiple accept threads acceptorThreadCount = 1; } if (pollerThreadCount - 2.1.2.1.1.1.1.1.1) NioEndpoint#initServerSocket（创建阻塞的ServerSocket) protected void initServerSocket() throws Exception { serverSock = ServerSocketChannel.open(); socketProperties.setProperties(serverSock.socket()); InetSocketAddress addr = (getAddress()!=null?new InetSocketAddress(getAddress(),getPort()):new InetSocketAddress(getPort())); serverSock.socket().bind(addr,getAcceptCount()); serverSock.configureBlocking(true); //mimic APR behavior } 打开一个 ServerSocket，默认绑定到 8080 端口，默认的连接等待队列长度是 100， 当超过 100 个时会拒绝服务。我们可以通过配置 conf/server.xml 中 Connector 的 acceptCount 属性对其进行定制。 2.1.2.1.1.1.1.1.2) NioSelectorPool#open（辅助selector) protected static final boolean SHARED = Boolean.parseBoolean(System.getProperty(\"org.apache.tomcat.util.net.NioSelectorShared\", \"true\")); public void open() throws IOException { enabled = true; getSharedSelector(); if (SHARED) { blockingSelector = new NioBlockingSelector(); blockingSelector.open(getSharedSelector()); } } - 2.1.2.1.1.1.1.1.2.1) NioSelectorPool#getSharedSelector（开启selector) protected Selector getSharedSelector() throws IOException { if (SHARED && SHARED_SELECTOR == null) { synchronized ( NioSelectorPool.class ) { if ( SHARED_SELECTOR == null ) { SHARED_SELECTOR = Selector.open(); log.info(\"Using a shared selector for servlet write/read\"); } } } return SHARED_SELECTOR; } 2.1.2.1.1.1.1.1.2.2) NioBlockingSelector#open（启动blockPoller线程) public void open(Selector selector) { sharedSelector = selector; poller = new BlockPoller(); poller.selector = sharedSelector; poller.setDaemon(true); poller.setName(\"NioBlockingSelector.BlockPoller-\"+(++threadCounter)); poller.start(); } - 3) BootStrap#start public void start() throws Exception { if( catalinaDaemon==null ) init(); Method method = catalinaDaemon.getClass().getMethod(\"start\", (Class [] )null); method.invoke(catalinaDaemon, (Object [])null); } - 3.1) Catalina#start public void start() { if (getServer() == null) { load(); } if (getServer() == null) { log.fatal(\"Cannot start server. Server instance is not configured.\"); return; } long t1 = System.nanoTime(); // Start the new server try { getServer().start(); } catch (LifecycleException e) { log.fatal(sm.getString(\"catalina.serverStartFail\"), e); try { getServer().destroy(); } catch (LifecycleException e1) { log.debug(\"destroy() failed for failed Server \", e1); } return; } long t2 = System.nanoTime(); if(log.isInfoEnabled()) { log.info(\"Server startup in \" + ((t2 - t1) / 1000000) + \" ms\"); } // Register shutdown hook if (useShutdownHook) { if (shutdownHook == null) { shutdownHook = new CatalinaShutdownHook(); } Runtime.getRuntime().addShutdownHook(shutdownHook); // If JULI is being used, disable JULI's shutdown hook since // shutdown hooks run in parallel and log messages may be lost // if JULI's hook completes before the CatalinaShutdownHook() LogManager logManager = LogManager.getLogManager(); if (logManager instanceof ClassLoaderLogManager) { ((ClassLoaderLogManager) logManager).setUseShutdownHook( false); } } if (await) { await(); stop(); } } - 3.1.1) StandardServer#start start同样也是模板方法模式。 protected void startInternal() throws LifecycleException { fireLifecycleEvent(CONFIGURE_START_EVENT, null); setState(LifecycleState.STARTING); globalNamingResources.start(); // Start our defined Services synchronized (servicesLock) { for (int i = 0; i } } 3.1.1.1) StandardService#start protected void startInternal() throws LifecycleException { if(log.isInfoEnabled()) log.info(sm.getString(\"standardService.start.name\", this.name)); setState(LifecycleState.STARTING); // Start our defined Container first if (engine != null) { synchronized (engine) { engine.start(); } } synchronized (executors) { for (Executor executor: executors) { executor.start(); } } mapperListener.start(); // Start our defined Connectors second synchronized (connectorsLock) { for (Connector connector: connectors) { // If it has already failed, don't try and start it if (connector.getState() != LifecycleState.FAILED) { connector.start(); } } } } 3.1.1.1.1) StandardEngine#start protected synchronized void startInternal() throws LifecycleException { // Log our server identification information if(log.isInfoEnabled()) log.info( \"Starting Servlet Engine: \" + ServerInfo.getServerInfo()); // Standard container startup super.startInternal(); } - 3.1.1.1.1.1) ContainerBase#startInternal protected synchronized void startInternal() throws LifecycleException { // Start our subordinate components, if any logger = null; getLogger(); Cluster cluster = getClusterInternal(); if (cluster instanceof Lifecycle) { ((Lifecycle) cluster).start(); } Realm realm = getRealmInternal(); if (realm instanceof Lifecycle) { ((Lifecycle) realm).start(); } // Start our child containers, if any Container children[] = findChildren(); List> results = new ArrayList<>(); for (int i = 0; i results.add(startStopExecutor.submit(new StartChild(children[i]))); } boolean fail = false; for (Future result : results) { try { result.get(); } catch (Exception e) { log.error(sm.getString(\"containerBase.threadedStartFailed\"), e); fail = true; } } if (fail) { throw new LifecycleException( sm.getString(\"containerBase.threadedStartFailed\")); } // Start the Valves in our pipeline (including the basic), if any if (pipeline instanceof Lifecycle) ((Lifecycle) pipeline).start(); setState(LifecycleState.STARTING); // Start our thread threadStart(); } - 3.1.1.1.1.1.1) StandardHost#start protected synchronized void startInternal() throws LifecycleException { // Set error report valve String errorValve = getErrorReportValveClass(); if ((errorValve != null) && (!errorValve.equals(\"\"))) { try { boolean found = false; Valve[] valves = getPipeline().getValves(); for (Valve valve : valves) { if (errorValve.equals(valve.getClass().getName())) { found = true; break; } } if(!found) { Valve valve = (Valve) Class.forName(errorValve).getConstructor().newInstance(); getPipeline().addValve(valve); } } catch (Throwable t) { ExceptionUtils.handleThrowable(t); log.error(sm.getString( \"standardHost.invalidErrorReportValveClass\", errorValve), t); } } super.startInternal(); } 3.1.1.1.1.1.1.1) StandardContext#start（会初始化loadOnStartup的servlet) protected synchronized void startInternal() throws LifecycleException { if(log.isDebugEnabled()) log.debug(\"Starting \" + getBaseName()); // Send j2ee.state.starting notification if (this.getObjectName() != null) { Notification notification = new Notification(\"j2ee.state.starting\", this.getObjectName(), sequenceNumber.getAndIncrement()); broadcaster.sendNotification(notification); } setConfigured(false); boolean ok = true; // Currently this is effectively a NO-OP but needs to be called to // ensure the NamingResources follows the correct lifecycle if (namingResources != null) { namingResources.start(); } // Post work directory postWorkDirectory(); // Add missing components as necessary if (getResources() == null) { // (1) Required by Loader if (log.isDebugEnabled()) log.debug(\"Configuring default Resources\"); try { setResources(new StandardRoot(this)); } catch (IllegalArgumentException e) { log.error(sm.getString(\"standardContext.resourcesInit\"), e); ok = false; } } if (ok) { resourcesStart(); } // 初始化WebappLoader if (getLoader() == null) { WebappLoader webappLoader = new WebappLoader(getParentClassLoader()); webappLoader.setDelegate(getDelegate()); setLoader(webappLoader); } // An explicit cookie processor hasn't been specified; use the default if (cookieProcessor == null) { cookieProcessor = new Rfc6265CookieProcessor(); } // Initialize character set mapper getCharsetMapper(); // Validate required extensions boolean dependencyCheck = true; try { dependencyCheck = ExtensionValidator.validateApplication (getResources(), this); } catch (IOException ioe) { log.error(sm.getString(\"standardContext.extensionValidationError\"), ioe); dependencyCheck = false; } if (!dependencyCheck) { // do not make application available if dependency check fails ok = false; } // Reading the \"catalina.useNaming\" environment variable String useNamingProperty = System.getProperty(\"catalina.useNaming\"); if ((useNamingProperty != null) && (useNamingProperty.equals(\"false\"))) { useNaming = false; } if (ok && isUseNaming()) { if (getNamingContextListener() == null) { NamingContextListener ncl = new NamingContextListener(); ncl.setName(getNamingContextName()); ncl.setExceptionOnFailedWrite(getJndiExceptionOnFailedWrite()); addLifecycleListener(ncl); setNamingContextListener(ncl); } } // Standard container startup if (log.isDebugEnabled()) log.debug(\"Processing standard container startup\"); // Binding thread ClassLoader oldCCL = bindThread(); try { if (ok) { // Start our subordinate components, if any Loader loader = getLoader(); if (loader instanceof Lifecycle) { // 启动WebappClassLoader ((Lifecycle) loader).start(); } // since the loader just started, the webapp classloader is now // created. setClassLoaderProperty(\"clearReferencesRmiTargets\", getClearReferencesRmiTargets()); setClassLoaderProperty(\"clearReferencesStopThreads\", getClearReferencesStopThreads()); setClassLoaderProperty(\"clearReferencesStopTimerThreads\", getClearReferencesStopTimerThreads()); setClassLoaderProperty(\"clearReferencesHttpClientKeepAliveThread\", getClearReferencesHttpClientKeepAliveThread()); // By calling unbindThread and bindThread in a row, we setup the // current Thread CCL to be the webapp classloader unbindThread(oldCCL); oldCCL = bindThread(); // Initialize logger again. Other components might have used it // too early, so it should be reset. logger = null; getLogger(); Realm realm = getRealmInternal(); if(null != realm) { if (realm instanceof Lifecycle) { ((Lifecycle) realm).start(); } // Place the CredentialHandler into the ServletContext so // applications can have access to it. Wrap it in a \"safe\" // handler so application's can't modify it. CredentialHandler safeHandler = new CredentialHandler() { @Override public boolean matches(String inputCredentials, String storedCredentials) { return getRealmInternal().getCredentialHandler().matches(inputCredentials, storedCredentials); } @Override public String mutate(String inputCredentials) { return getRealmInternal().getCredentialHandler().mutate(inputCredentials); } }; context.setAttribute(Globals.CREDENTIAL_HANDLER, safeHandler); } // Notify our interested LifecycleListeners fireLifecycleEvent(Lifecycle.CONFIGURE_START_EVENT, null); // Start our child containers, if not already started for (Container child : findChildren()) { if (!child.getState().isAvailable()) { child.start(); } } // Start the Valves in our pipeline (including the basic), // if any if (pipeline instanceof Lifecycle) { ((Lifecycle) pipeline).start(); } // Acquire clustered manager Manager contextManager = null; Manager manager = getManager(); if (manager == null) { if (log.isDebugEnabled()) { log.debug(sm.getString(\"standardContext.cluster.noManager\", Boolean.valueOf((getCluster() != null)), Boolean.valueOf(distributable))); } if ( (getCluster() != null) && distributable) { try { contextManager = getCluster().createManager(getName()); } catch (Exception ex) { log.error(\"standardContext.clusterFail\", ex); ok = false; } } else { contextManager = new StandardManager(); } } // Configure default manager if none was specified if (contextManager != null) { if (log.isDebugEnabled()) { log.debug(sm.getString(\"standardContext.manager\", contextManager.getClass().getName())); } setManager(contextManager); } if (manager!=null && (getCluster() != null) && distributable) { //let the cluster know that there is a context that is distributable //and that it has its own manager getCluster().registerManager(manager); } } if (!getConfigured()) { log.error(sm.getString(\"standardContext.configurationFail\")); ok = false; } // We put the resources into the servlet context if (ok) getServletContext().setAttribute (Globals.RESOURCES_ATTR, getResources()); if (ok ) { if (getInstanceManager() == null) { javax.naming.Context context = null; if (isUseNaming() && getNamingContextListener() != null) { context = getNamingContextListener().getEnvContext(); } Map> injectionMap = buildInjectionMap( getIgnoreAnnotations() ? new NamingResourcesImpl(): getNamingResources()); setInstanceManager(new DefaultInstanceManager(context, injectionMap, this, this.getClass().getClassLoader())); } getServletContext().setAttribute( InstanceManager.class.getName(), getInstanceManager()); InstanceManagerBindings.bind(getLoader().getClassLoader(), getInstanceManager()); } // Create context attributes that will be required if (ok) { getServletContext().setAttribute( JarScanner.class.getName(), getJarScanner()); } // Set up the context init params mergeParameters(); // Call ServletContainerInitializers for (Map.Entry>> entry : initializers.entrySet()) { try { entry.getKey().onStartup(entry.getValue(), getServletContext()); } catch (ServletException e) { log.error(sm.getString(\"standardContext.sciFail\"), e); ok = false; break; } } // Configure and call application event listeners if (ok) { if (!listenerStart()) { log.error(sm.getString(\"standardContext.listenerFail\")); ok = false; } } // Check constraints for uncovered HTTP methods // Needs to be after SCIs and listeners as they may programmatically // change constraints if (ok) { checkConstraintsForUncoveredMethods(findConstraints()); } try { // Start manager Manager manager = getManager(); if (manager instanceof Lifecycle) { ((Lifecycle) manager).start(); } } catch(Exception e) { log.error(sm.getString(\"standardContext.managerFail\"), e); ok = false; } // Configure and call application filters if (ok) { if (!filterStart()) { log.error(sm.getString(\"standardContext.filterFail\")); ok = false; } } // Load and initialize all \"load on startup\" servlets if (ok) { if (!loadOnStartup(findChildren())){ log.error(sm.getString(\"standardContext.servletFail\")); ok = false; } } // Start ContainerBackgroundProcessor thread super.threadStart(); } finally { // Unbinding thread unbindThread(oldCCL); } // Set available status depending upon startup success if (ok) { if (log.isDebugEnabled()) log.debug(\"Starting completed\"); } else { log.error(sm.getString(\"standardContext.startFailed\", getName())); } startTime=System.currentTimeMillis(); // Send j2ee.state.running notification if (ok && (this.getObjectName() != null)) { Notification notification = new Notification(\"j2ee.state.running\", this.getObjectName(), sequenceNumber.getAndIncrement()); broadcaster.sendNotification(notification); } // The WebResources implementation caches references to JAR files. On // some platforms these references may lock the JAR files. Since web // application start is likely to have read from lots of JARs, trigger // a clean-up now. getResources().gc(); // Reinitializing if something went wrong if (!ok) { setState(LifecycleState.FAILED); } else { setState(LifecycleState.STARTING); } } - 3.1.1.1.1.1.1.1.1) StandardWrapper#start protected synchronized void startInternal() throws LifecycleException { // Send j2ee.state.starting notification if (this.getObjectName() != null) { Notification notification = new Notification(\"j2ee.state.starting\", this.getObjectName(), sequenceNumber++); broadcaster.sendNotification(notification); } // Start up this component super.startInternal(); setAvailable(0L); // Send j2ee.state.running notification if (this.getObjectName() != null) { Notification notification = new Notification(\"j2ee.state.running\", this.getObjectName(), sequenceNumber++); broadcaster.sendNotification(notification); } } - 3.1.1.1.1.1.1.1.2) WebappLoader#constructor if (getLoader() == null) { WebappLoader webappLoader = new WebappLoader(getParentClassLoader()); webappLoader.setDelegate(getDelegate()); setLoader(webappLoader); } /** * Construct a new WebappLoader with the specified class loader * to be defined as the parent of the ClassLoader we ultimately create. * * @param parent The parent class loader */ public WebappLoader(ClassLoader parent) { super(); this.parentClassLoader = parent; } @Override public void setLoader(Loader loader) { Lock writeLock = loaderLock.writeLock(); writeLock.lock(); Loader oldLoader = null; try { // Change components if necessary oldLoader = this.loader; if (oldLoader == loader) return; this.loader = loader; // Stop the old component if necessary if (getState().isAvailable() && (oldLoader != null) && (oldLoader instanceof Lifecycle)) { try { ((Lifecycle) oldLoader).stop(); } catch (LifecycleException e) { log.error(\"StandardContext.setLoader: stop: \", e); } } // Start the new component if necessary if (loader != null) loader.setContext(this); if (getState().isAvailable() && (loader != null) && (loader instanceof Lifecycle)) { try { ((Lifecycle) loader).start(); } catch (LifecycleException e) { log.error(\"StandardContext.setLoader: start: \", e); } } } finally { writeLock.unlock(); } // Report this property change to interested listeners support.firePropertyChange(\"loader\", oldLoader, loader); } - 3.1.1.1.1.1.1.1.3) WebappLoader#start protected void startInternal() throws LifecycleException { if (log.isDebugEnabled()) log.debug(sm.getString(\"webappLoader.starting\")); if (context.getResources() == null) { log.info(\"No resources for \" + context); setState(LifecycleState.STARTING); return; } // Construct a class loader based on our current repositories list try { classLoader = createClassLoader(); classLoader.setResources(context.getResources()); classLoader.setDelegate(this.delegate); // Configure our repositories setClassPath(); setPermissions(); classLoader.start(); String contextName = context.getName(); if (!contextName.startsWith(\"/\")) { contextName = \"/\" + contextName; } ObjectName cloname = new ObjectName(context.getDomain() + \":type=\" + classLoader.getClass().getSimpleName() + \",host=\" + context.getParent().getName() + \",context=\" + contextName); Registry.getRegistry(null, null) .registerComponent(classLoader, cloname, null); } catch (Throwable t) { t = ExceptionUtils.unwrapInvocationTargetException(t); ExceptionUtils.handleThrowable(t); log.error( \"LifecycleException \", t ); throw new LifecycleException(\"start: \", t); } setState(LifecycleState.STARTING); } 3.1.1.1.1.1.1.1.3.1) WebappLoader#createClassLoader private WebappClassLoaderBase createClassLoader() throws Exception { Class clazz = Class.forName(loaderClass); WebappClassLoaderBase classLoader = null; // parentClassLoader实际就是sharedLoader，即org.apache.catalina.loader.StandardClassLoader if (parentClassLoader == null) { parentClassLoader = context.getParentClassLoader(); } Class[] argTypes = { ClassLoader.class }; Object[] args = { parentClassLoader }; Constructor constr = clazz.getConstructor(argTypes); classLoader = (WebappClassLoaderBase) constr.newInstance(args); return classLoader; } - 3.1.1.1.1.1.2) StandardPipeline#start protected synchronized void startInternal() throws LifecycleException { // Start the Valves in our pipeline (including the basic), if any Valve current = first; if (current == null) { current = basic; } while (current != null) { if (current instanceof Lifecycle) ((Lifecycle) current).start(); current = current.getNext(); } setState(LifecycleState.STARTING); } 3.1.1.1.1.1.2) ContainerBase#threadStart（启动后台线程，检查session过期) /** * Start the background thread that will periodically check for * session timeouts. */ protected void threadStart() { if (thread != null) return; if (backgroundProcessorDelay - 3.1.1.1.2) Connector#start protected void startInternal() throws LifecycleException { // Validate settings before starting if (getPort() throw new LifecycleException(sm.getString( \"coyoteConnector.invalidPort\", Integer.valueOf(getPort()))); } setState(LifecycleState.STARTING); try { protocolHandler.start(); } catch (Exception e) { throw new LifecycleException( sm.getString(\"coyoteConnector.protocolHandlerStartFailed\"), e); } } 3.1.1.1.2.1) AbstractProtocol#start public void start() throws Exception { if (getLog().isInfoEnabled()) { getLog().info(sm.getString(\"abstractProtocolHandler.start\", getName())); } endpoint.start(); // Start async timeout thread asyncTimeout = new AsyncTimeout(); Thread timeoutThread = new Thread(asyncTimeout, getNameInternal() + \"-AsyncTimeout\"); int priority = endpoint.getThreadPriority(); if (priority Thread.MAX_PRIORITY) { priority = Thread.NORM_PRIORITY; } timeoutThread.setPriority(priority); timeoutThread.setDaemon(true); timeoutThread.start(); } - 3.1.1.1.2.1.1) NioEndpoint#start public void startInternal() throws Exception { if (!running) { running = true; paused = false; processorCache = new SynchronizedStack<>(SynchronizedStack.DEFAULT_SIZE, socketProperties.getProcessorCache()); eventCache = new SynchronizedStack<>(SynchronizedStack.DEFAULT_SIZE, socketProperties.getEventCache()); nioChannels = new SynchronizedStack<>(SynchronizedStack.DEFAULT_SIZE, socketProperties.getBufferPool()); // Create worker collection if ( getExecutor() == null ) { createExecutor(); } initializeConnectionLatch(); // Start poller threads pollers = new Poller[getPollerThreadCount()]; for (int i=0; i- 3.1.1.1.2.1.1.1) NioEndpoint#createExecutor（创建Worker线程池) 用于创建 Worker 线程池。默认会启动 10 个 Worker 线程，Tomcat 处理请求过程中，Woker 最多不超过 200 个。我们可以通过配置 conf/server.xml 中 Connector 的 minSpareThreads 和 maxThreads 对这两个属性进行定制。 public void createExecutor() { internalExecutor = true; TaskQueue taskqueue = new TaskQueue(); TaskThreadFactory tf = new TaskThreadFactory(getName() + \"-exec-\", daemon, getThreadPriority()); executor = new ThreadPoolExecutor(getMinSpareThreads(), getMaxThreads(), 60, TimeUnit.SECONDS,taskqueue, tf); taskqueue.setParent( (ThreadPoolExecutor) executor); } - 3.1.1.1.2.1.1.1.1) ThreadPoolExecutor#constructor（启动Worker) public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue workQueue, ThreadFactory threadFactory) { super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, new RejectHandler()); prestartAllCoreThreads(); } public int prestartAllCoreThreads() { int n = 0; while (addWorker(null, true)) ++n; return n; } private boolean addWorker(Runnable firstTask, boolean core) { retry: for (;;) { int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs >= SHUTDOWN && ! (rs == SHUTDOWN && firstTask == null && ! workQueue.isEmpty())) return false; for (;;) { int wc = workerCountOf(c); if (wc >= CAPACITY || wc >= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop } } boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try { w = new Worker(firstTask); final Thread t = w.thread; if (t != null) { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); if (rs largestPoolSize) largestPoolSize = s; workerAdded = true; } } finally { mainLock.unlock(); } if (workerAdded) { t.start(); workerStarted = true; } } } finally { if (! workerStarted) addWorkerFailed(w); } return workerStarted; } 每个Worker线程启动是一个后台线程完成的。 3.1.1.1.1.1.1.2) Poller(Runnable)#run（启动Poller) 以守护线程的方式运行。 用于检测已就绪的 Socket。 默认最多不超过 2 个， Math.min(2,Runtime.getRuntime().availableProcessors());。我们可以通过配置 pollerThreadCount 来定制。 3.1.1.1.1.1.1.3) NioEndpoint#startAcceptorThreads（启动Acceptors) Acceptors以后台线程方式运行 用于接受新连接。默认是 1 个。我们可以通过配置 acceptorThreadCount 对其进行定制。 protected final void startAcceptorThreads() { int count = getAcceptorThreadCount(); acceptors = new ArrayList<>(count); for (int i = 0; i Acceptor acceptor = new Acceptor<>(this); String threadName = getName() + \"-Acceptor-\" + i; acceptor.setThreadName(threadName); acceptors.add(acceptor); Thread t = new Thread(acceptor, threadName); t.setPriority(getAcceptorThreadPriority()); t.setDaemon(getDaemon()); t.start(); } } 3.1.2) StandardServer#await public void await() { // Negative values - don't wait on port - tomcat is embedded or we just don't like ports if( port == -2 ) { // undocumented yet - for embedding apps that are around, alive. return; } if( port==-1 ) { try { awaitThread = Thread.currentThread(); while(!stopAwait) { try { Thread.sleep( 10000 ); } catch( InterruptedException ex ) { // continue and check the flag } } } finally { awaitThread = null; } return; } // Set up a server socket to wait on try { awaitSocket = new ServerSocket(port, 1, InetAddress.getByName(address)); } catch (IOException e) { log.error(\"StandardServer.await: create[\" + address + \":\" + port + \"]: \", e); return; } try { awaitThread = Thread.currentThread(); // Loop waiting for a connection and a valid command while (!stopAwait) { ServerSocket serverSocket = awaitSocket; if (serverSocket == null) { break; } // Wait for the next connection Socket socket = null; StringBuilder command = new StringBuilder(); try { InputStream stream; long acceptStartTime = System.currentTimeMillis(); try { socket = serverSocket.accept(); socket.setSoTimeout(10 * 1000); // Ten seconds stream = socket.getInputStream(); } catch (SocketTimeoutException ste) { // This should never happen but bug 56684 suggests that // it does. log.warn(sm.getString(\"standardServer.accept.timeout\", Long.valueOf(System.currentTimeMillis() - acceptStartTime)), ste); continue; } catch (AccessControlException ace) { log.warn(\"StandardServer.accept security exception: \" + ace.getMessage(), ace); continue; } catch (IOException e) { if (stopAwait) { // Wait was aborted with socket.close() break; } log.error(\"StandardServer.await: accept: \", e); break; } // Read a set of characters from the socket int expected = 1024; // Cut off to avoid DoS attack while (expected 0) { int ch = -1; try { ch = stream.read(); } catch (IOException e) { log.warn(\"StandardServer.await: read: \", e); ch = -1; } // Control character or EOF (-1) terminates loop if (ch 停止 catalinaDaemon调用await等待停止命令，我们一般是通过执行tomcat/bin/shutdown.sh来关闭Tomcat，等价于执行org.apache.catalina.startup.Bootstra类的main方法，并传入stop参数。 逻辑： 1.- 新建Bootstrap对象daemon，并调用其init()方法 2.- 初始化Tomcat的类加载器 3.- 用反射实例化org.apache.catalina.startup.Catalina对象catalinaDaemon 4.- 调用daemon的stopServer方法，实质上调用了catalinaDaemon的stopServer方法 5.- 解析server.xml文件，构造出Server容器 6.- 获取Server的socket监听端口和地址，创建Socket对象连接启动Tomcat时创建的ServerSocket，最后向ServerSocket发送SHUTDOWN命令 7.- 运行中的Server调用stop方法停止 BootStrap#stopServer public void stopServer(String[] arguments) throws Exception { Object param[]; Class paramTypes[]; if (arguments==null || arguments.length==0) { paramTypes = null; param = null; } else { paramTypes = new Class[1]; paramTypes[0] = arguments.getClass(); param = new Object[1]; param[0] = arguments; } Method method = catalinaDaemon.getClass().getMethod(\"stopServer\", paramTypes); method.invoke(catalinaDaemon, param); } - 1) Catalina#stopServer public void stopServer() { stopServer(null); } public void stopServer(String[] arguments) { if (arguments != null) { arguments(arguments); } Server s = getServer(); if (s == null) { // Create and execute our Digester Digester digester = createStopDigester(); File file = configFile(); try (FileInputStream fis = new FileInputStream(file)) { InputSource is = new InputSource(file.toURI().toURL().toString()); is.setByteStream(fis); digester.push(this); digester.parse(is); } catch (Exception e) { log.error(\"Catalina.stop: \", e); System.exit(1); } } else { // Server object already present. Must be running as a service try { s.stop(); } catch (LifecycleException e) { log.error(\"Catalina.stop: \", e); } return; } // Stop the existing server s = getServer(); if (s.getPort()>0) { try (Socket socket = new Socket(s.getAddress(), s.getPort()); OutputStream stream = socket.getOutputStream()) { String shutdown = s.getShutdown(); for (int i = 0; i 请求处理 Connector 在Tomcat9中，Connector支持的协议是HTTP和AJP，协议处理类分别对应org.apache.coyote.http11.Http11NioProtocol和 org.apache.coyote.http11.Http11AprProtocol（已经取消BIO模式)。 Connector主要包含三个模块：Http11NioProtocol, Mapper, CoyoteAdapter，http请求在Connector中的流程如下： 1.- Acceptor为监听线程，调用serverSocketAccept()阻塞，本质上调用ServerSocketChannel.accept() 2.- Acceptor将接收到的Socket添加到Poller池中的一个Poller 3.- Poller通过worker线程把socket包装成SocketProcessor 4.- SocketProcessor调用getHandler()获取对应的ConnectionHandler 5.- ConnectionHandler把socket交由Http11Processor处理，解析http的Header和Body 6.- Http11Processor调用service()把包装好的request和response传给CoyoteAdapter 7.- CoyoteAdapter会通过Mapper把请求对应的session、servlet等关联好，准备传给Container Container 有4个Container，采用了责任链的设计模式。 Pipeline就像是每个容器的逻辑总线，在Pipeline上按照配置的顺序，加载各个Valve。通过Pipeline完成各个Valve之间的调用，各个Valve实现具体的应用逻辑。 每个请求在pipeline上流动，经过每个Container（对应着一个或多个Valve阀门)，各个Container按顺序处理请求，最终在Wrapper结束。 Connector中的CoyoteAdapter会调用invoke()把request和response传给Container，Container中依次调用各个Valve，每个Valve的作用如下： 1.- StandardEngineValve：StandardEngine中的唯一阀门，主要用于从request中选择其host映射的Host容器StandardHost 2.- AccessLogValve：StandardHost中的第一个阀门，主要用于管道执行结束之后记录日志信息 3.- ErrorReportValve：StandardHost中紧跟AccessLogValve的阀门，主要用于管道执行结束后，从request对象中获取异常信息，并封装到response中以便将问题展现给访问者 4.- StandardHostValve： StandardHost中最后的阀门，主要用于从request中选择其context映射的Context容器StandardContext以及访问request中的Session以更新会话的最后访问时间 5.- StandardContextValve：StandardContext中的唯一阀门，主要作用是禁止任何对WEB-INF或META-INF目录下资源的重定向访问，对应用程序热部署功能的实现，从request中获得StandardWrapper 6.- StandardWrapperValve：StandardWrapper中的唯一阀门，主要作用包括调用StandardWrapper的loadServlet方法生成Servlet实例和调用ApplicationFilterFactory生成Filter链 最终将Response返回给Connector完成一次http的请求。 NioEndPoint职责 在Tomcat中Endpoint主要用来接收网络请求，处理则由ConnectionHandler来执行。 包含了三个组件： Acceptor：后台线程，负责监听请求，将接收到的Socket请求放到Poller队列中 Poller：后台线程，当Socket就绪时，将Poller队列中的Socket交给Worker线程池处理 SocketProcessor（Worker)：处理socket，本质上委托ConnectionHandler处理 Connector 启动以后会启动一组线程用于不同阶段的请求处理过程。 Acceptor 线程组。用于接受新连接，并将新连接封装一下，选择一个 Poller 将新连接添加到 Poller 的事件队列中。 Poller 线程组。用于监听 Socket 事件，当 Socket 可读或可写等等时，将 Socket 封装一下添加到 worker 线程池的任务队列中。 worker 线程组。用于对请求进行处理，包括分析请求报文并创建 Request 对象，调用容器的 pipeline 进行处理。 Acceptor、Poller、worker 所在的 ThreadPoolExecutor 都维护在 NioEndpoint 中。 这种模式类似于Reactor的主从多线程方式。 1) Acceptor#run（BIO,阻塞接收Socket连接,mainReactor) 1.- Acceptor 在启动后会阻塞在 ServerSocketChannel.accept(); 方法处，当有新连接到达时，该方法返回一个 SocketChannel。 配置完 Socket 以后将 Socket 封装到 NioChannel 中，并注册到 Poller,值的一提的是，我们一开始就启动了多个 Poller 线程，注册的时候，连接是公平的分配到每个 Poller 的。NioEndpoint 维护了一个 Poller 数组，当一个连接分配给 pollers[index] 时，下一个连接就会分配给 pollers[(index+1)%pollers.length]. 3.- addEvent() 方法会将 Socket 添加到该 Poller 的 PollerEvent 队列中。到此 Acceptor 的任务就完成了。 持有Endpoint private final AbstractEndpoint endpoint; 在启动后会阻塞在 ServerSocketChannel.accept(); 方法处，当有新连接到达时，该方法返回一个 SocketChannel。 public void run() { int errorDelay = 0; // Loop until we receive a shutdown command while (endpoint.isRunning()) { // Loop if endpoint is paused while (endpoint.isPaused() && endpoint.isRunning()) { state = AcceptorState.PAUSED; try { Thread.sleep(50); } catch (InterruptedException e) { // Ignore } } if (!endpoint.isRunning()) { break; } state = AcceptorState.RUNNING; try { //if we have reached max connections, wait endpoint.countUpOrAwaitConnection(); // Endpoint might have been paused while waiting for latch // If that is the case, don't accept new connections if (endpoint.isPaused()) { continue; } U socket = null; try { // Accept the next incoming connection from the server // socket socket = endpoint.serverSocketAccept(); } catch (Exception ioe) { // We didn't get a socket endpoint.countDownConnection(); if (endpoint.isRunning()) { // Introduce delay if necessary errorDelay = handleExceptionWithDelay(errorDelay); // re-throw throw ioe; } else { break; } } // Successful accept, reset the error delay errorDelay = 0; // Configure the socket if (endpoint.isRunning() && !endpoint.isPaused()) { // setSocketOptions() will hand the socket off to // an appropriate processor if successful if (!endpoint.setSocketOptions(socket)) { endpoint.closeSocket(socket); } } else { endpoint.destroySocket(socket); } } catch (Throwable t) { ExceptionUtils.handleThrowable(t); String msg = sm.getString(\"endpoint.accept.fail\"); // APR specific. // Could push this down but not sure it is worth the trouble. if (t instanceof Error) { Error e = (Error) t; if (e.getError() == 233) { // Not an error on HP-UX so log as a warning // so it can be filtered out on that platform // See bug 50273 log.warn(msg, t); } else { log.error(msg, t); } } else { log.error(msg, t); } } } state = AcceptorState.ENDED; } - 1.1) NioEndpoint#setSocketOptions（处理Socket) - 配置完 Socket 以后将 Socket 封装到 NioChannel 中，并注册到 Poller,值的一提的是，我们一开始就启动了多个 Poller 线程，注册的时候，连接是公平的分配到每个 Poller 的。NioEndpoint 维护了一个 Poller 数组，当一个连接分配给 pollers[index] 时，下一个连接就会分配给 pollers[(index+1)%pollers.length]. protected boolean setSocketOptions(SocketChannel socket) { // Process the connection try { //disable blocking, APR style, we are gonna be polling it socket.configureBlocking(false); Socket sock = socket.socket(); socketProperties.setProperties(sock); NioChannel channel = nioChannels.pop(); if (channel == null) { SocketBufferHandler bufhandler = new SocketBufferHandler( socketProperties.getAppReadBufSize(), socketProperties.getAppWriteBufSize(), socketProperties.getDirectBuffer()); if (isSSLEnabled()) { channel = new SecureNioChannel(socket, bufhandler, selectorPool, this); } else { channel = new NioChannel(socket, bufhandler); } } else { channel.setIOChannel(socket); channel.reset(); } getPoller0().register(channel); } catch (Throwable t) { ExceptionUtils.handleThrowable(t); try { log.error(\"\",t); } catch (Throwable tt) { ExceptionUtils.handleThrowable(tt); } // Tell to close the socket return false; } return true; } public Poller getPoller0() { int idx = Math.abs(pollerRotater.incrementAndGet()) % pollers.length; return pollers[idx]; } - 1.1.1) Poller#register（将Socket放入Poller队列) addEvent() 方法会将 Socket 添加到该 Poller 的 PollerEvent 队列中。到此 Acceptor 的任务就完成了。 public void register(final NioChannel socket) { socket.setPoller(this); NioSocketWrapper ka = new NioSocketWrapper(socket, NioEndpoint.this); socket.setSocketWrapper(ka); ka.setPoller(this); ka.setReadTimeout(getConnectionTimeout()); ka.setWriteTimeout(getConnectionTimeout()); ka.setKeepAliveLeft(NioEndpoint.this.getMaxKeepAliveRequests()); ka.setSecure(isSSLEnabled()); PollerEvent r = eventCache.pop(); ka.interestOps(SelectionKey.OP_READ);//this is what OP_REGISTER turns into. if ( r==null) r = new PollerEvent(socket,ka,OP_REGISTER); else r.reset(socket,ka,OP_REGISTER); addEvent(r); } - 1.1.1.1) NioSocketWrapper#constructor（持有NioEndpoint的SelectorPool) public NioSocketWrapper(NioChannel channel, NioEndpoint endpoint) { super(channel, endpoint); pool = endpoint.getSelectorPool(); socketBufferHandler = channel.getBufHandler(); } public SocketWrapperBase(E socket, AbstractEndpoint endpoint) { this.socket = socket; this.endpoint = endpoint; ReentrantReadWriteLock lock = new ReentrantReadWriteLock(); this.blockingStatusReadLock = lock.readLock(); this.blockingStatusWriteLock = lock.writeLock(); } - 1.1.1.2) Poller#addEvent private void addEvent(PollerEvent event) { events.offer(event); if ( wakeupCounter.incrementAndGet() == 0 ) selector.wakeup(); } private final SynchronizedQueue events = new SynchronizedQueue<>(); 2) Poller#run（NIO,把队列中的就绪的Socket封装为SocketProcessor交给Worker线程池,subReactor) 1.- selector.select(1000)。当 Poller 启动后因为 selector 中并没有已注册的 Channel，所以当执行到该方法时只能阻塞。所有的 Poller 共用一个 Selector，其实现类是 sun.nio.ch.EPollSelectorImpl 2.- events() 方法会将通过 addEvent() 方法添加到事件队列中的 Socket 注册到 EPollSelectorImpl，当 Socket 可读时，Poller 才对其进行处理 3.- createSocketProcessor() 方法将 Socket 封装到 SocketProcessor 中，SocketProcessor 实现了 Runnable 接口。worker 线程通过调用其 run() 方法来对 Socket 进行处理。 4.- execute(SocketProcessor) 方法将 SocketProcessor 提交到线程池，放入线程池的 workQueue 中。workQueue 是 BlockingQueue 的实例。到此 Poller 的任务就完成了。 - 调用selector的select()函数，监听就绪事件 - 根据向selector中注册的key遍历channel中已经就绪的keys，并处理key - 处理key对应的channel，调用NioEndPoint的processSocket() - 从SocketProcessor池中取出空闲的SocketProcessor，关联socketWrapper，提交运行SocketProcessor public Poller() throws IOException { this.selector = Selector.open(); } 它的selector是初始化时开启的，每个Poller对应着自己的Selector，监听该Poller对应的SocketChannel的Read事件。当Poller队列中加入新的Socket时，会将Socket注册在selector上，这样selector就可以监测socket就绪事件了。 public void run() { // Loop until destroy() is called while (true) { boolean hasEvents = false; try { if (!close) { hasEvents = events(); if (wakeupCounter.getAndSet(-1) > 0) { //if we are here, means we have other stuff to do //do a non blocking select keyCount = selector.selectNow(); } else { keyCount = selector.select(selectorTimeout); } wakeupCounter.set(0); } if (close) { events(); timeout(0, false); try { selector.close(); } catch (IOException ioe) { log.error(sm.getString(\"endpoint.nio.selectorCloseFail\"), ioe); } break; } } catch (Throwable x) { ExceptionUtils.handleThrowable(x); log.error(\"\",x); continue; } //either we timed out or we woke up, process events first if ( keyCount == 0 ) hasEvents = (hasEvents | events()); Iterator iterator = keyCount > 0 ? selector.selectedKeys().iterator() : null; // Walk through the collection of ready keys and dispatch // any active event. while (iterator != null && iterator.hasNext()) { SelectionKey sk = iterator.next(); NioSocketWrapper attachment = (NioSocketWrapper)sk.attachment(); // Attachment may be null if another thread has called // cancelledKey() if (attachment == null) { iterator.remove(); } else { // 有Socket出现读事件 iterator.remove(); processKey(sk, attachment); } }//while //process timeouts timeout(keyCount,hasEvents); }//while getStopLatch().countDown(); } - 2.1) Poller#events（将队列中的Socket注册到Selector) events() 方法会将通过 addEvent() 方法添加到事件队列中的 Socket 注册到 EPollSelectorImpl，当 Socket 可读时，Poller 才对其进行处理。 public boolean events() { boolean result = false; PollerEvent pe = null; for (int i = 0, size = events.size(); i - 2.1.1) PollerEvent#run（注册到Selector) public void run() { if (interestOps == OP_REGISTER) { try { socket.getIOChannel().register( socket.getPoller().getSelector(), SelectionKey.OP_READ, socketWrapper); } catch (Exception x) { log.error(sm.getString(\"endpoint.nio.registerFail\"), x); } } else { final SelectionKey key = socket.getIOChannel().keyFor(socket.getPoller().getSelector()); try { if (key == null) { // The key was cancelled (e.g. due to socket closure) // and removed from the selector while it was being // processed. Count down the connections at this point // since it won't have been counted down when the socket // closed. socket.socketWrapper.getEndpoint().countDownConnection(); } else { final NioSocketWrapper socketWrapper = (NioSocketWrapper) key.attachment(); if (socketWrapper != null) { //we are registering the key to start with, reset the fairness counter. int ops = key.interestOps() | interestOps; socketWrapper.interestOps(ops); key.interestOps(ops); } else { socket.getPoller().cancelledKey(key); } } } catch (CancelledKeyException ckx) { try { socket.getPoller().cancelledKey(key); } catch (Exception ignore) {} } } } - 2.2) Poller#processKey（将就绪的Socket交给线程池) protected void processKey(SelectionKey sk, NioSocketWrapper attachment) { try { if ( close ) { cancelledKey(sk); } else if ( sk.isValid() && attachment != null ) { if (sk.isReadable() || sk.isWritable() ) { if ( attachment.getSendfileData() != null ) { processSendfile(sk,attachment, false); } else { unreg(sk, attachment, sk.readyOps()); boolean closeSocket = false; // Read goes before write if (sk.isReadable()) { if (!processSocket(attachment, SocketEvent.OPEN_READ, true)) { closeSocket = true; } } if (!closeSocket && sk.isWritable()) { if (!processSocket(attachment, SocketEvent.OPEN_WRITE, true)) { closeSocket = true; } } if (closeSocket) { cancelledKey(sk); } } } } else { //invalid key cancelledKey(sk); } } catch ( CancelledKeyException ckx ) { cancelledKey(sk); } catch (Throwable t) { ExceptionUtils.handleThrowable(t); log.error(\"\",t); } } 2.2.1) AbstractEndpoint#processSocket createSocketProcessor() 方法将 Socket 封装到 SocketProcessor 中，SocketProcessor 实现了 Runnable 接口。worker 线程通过调用其 run() 方法来对 Socket 进行处理。 public boolean processSocket(SocketWrapperBase socketWrapper, SocketEvent event, boolean dispatch) { try { if (socketWrapper == null) { return false; } SocketProcessorBase sc = processorCache.pop(); if (sc == null) { sc = createSocketProcessor(socketWrapper, event); } else { sc.reset(socketWrapper, event); } Executor executor = getExecutor(); if (dispatch && executor != null) { executor.execute(sc); } else { sc.run(); } } catch (RejectedExecutionException ree) { getLog().warn(sm.getString(\"endpoint.executor.fail\", socketWrapper) , ree); return false; } catch (Throwable t) { ExceptionUtils.handleThrowable(t); // This means we got an OOM or similar creating a thread, or that // the pool and its queue are full getLog().error(sm.getString(\"endpoint.process.fail\"), t); return false; } return true; } - 2.2.1.1) NioEndpoint#createSocketProcessor protected SocketProcessorBase createSocketProcessor( SocketWrapperBase socketWrapper, SocketEvent event) { return new SocketProcessor(socketWrapper, event); } public SocketProcessor(SocketWrapperBase socketWrapper, SocketEvent event) { super(socketWrapper, event); } public SocketProcessorBase(SocketWrapperBase socketWrapper, SocketEvent event) { reset(socketWrapper, event); } public void reset(SocketWrapperBase socketWrapper, SocketEvent event) { Objects.requireNonNull(event); this.socketWrapper = socketWrapper; this.event = event; } 3) Worker#run（将SocketProcessor封装为Request,IO Handler) 1.- worker 线程被创建以后就执行 ThreadPoolExecutor 的 runWorker() 方法，试图从 workQueue 中取待处理任务，但是一开始 workQueue 是空的，所以 worker 线程会阻塞在 workQueue.take() 方法。 2.- 当新任务添加到 workQueue后，workQueue.take() 方法会返回一个 Runnable，通常是 SocketProcessor,然后 worker 线程调用 SocketProcessor 的 run() 方法对 Socket 进行处理。 3.- createProcessor() 会创建一个 Http11Processor, 它用来解析 Socket，将 Socket 中的内容封装到 Request 中。注意这个 Request 是临时使用的一个类，它的全类名是 org.apache.coyote.Request， 4.- postParseRequest() 方法封装一下 Request，并处理一下映射关系(从 URL 映射到相应的 Host、Context、Wrapper)。 5.- CoyoteAdapter 将 Rquest 提交给 Container 处理之前，并将 org.apache.coyote.Request 封装到 org.apache.catalina.connector.Request，传递给 Container 处理的 Request 是 org.apache.catalina.connector.Request。 6.- connector.getService().getMapper().map()，用来在 Mapper 中查询 URL 的映射关系。映射关系会保留到 org.apache.catalina.connector.Request 中，Container 处理阶段 request.getHost() 是使用的就是这个阶段查询到的映射主机，以此类推 request.getContext()、request.getWrapper() 都是。 7.- connector.getService().getContainer().getPipeline().getFirst().invoke() 会将请求传递到 Container 处理，当然了 Container 处理也是在 Worker 线程中执行的，但是这是一个相对独立的模块，所以单独分出来一节。 /** Delegates main run loop to outer runWorker */ public void run() { runWorker(this); } final void runWorker(Worker w) { Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try { while (task != null || (task = getTask()) != null) { w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() && runStateAtLeast(ctl.get(), STOP))) && !wt.isInterrupted()) wt.interrupt(); try { beforeExecute(wt, task); Throwable thrown = null; try { task.run(); } catch (RuntimeException x) { thrown = x; throw x; } catch (Error x) { thrown = x; throw x; } catch (Throwable x) { thrown = x; throw new Error(x); } finally { afterExecute(task, thrown); } } finally { task = null; w.completedTasks++; w.unlock(); } } completedAbruptly = false; } finally { processWorkerExit(w, completedAbruptly); } } task是SocketProcessor类型 3.1) SocketProcessor#run public final void run() { synchronized (socketWrapper) { // It is possible that processing may be triggered for read and // write at the same time. The sync above makes sure that processing // does not occur in parallel. The test below ensures that if the // first event to be processed results in the socket being closed, // the subsequent events are not processed. if (socketWrapper.isClosed()) { return; } doRun(); } } protected void doRun() { NioChannel socket = socketWrapper.getSocket(); SelectionKey key = socket.getIOChannel().keyFor(socket.getPoller().getSelector()); try { int handshake = -1; try { if (key != null) { if (socket.isHandshakeComplete()) { // No TLS handshaking required. Let the handler // process this socket / event combination. handshake = 0; } else if (event == SocketEvent.STOP || event == SocketEvent.DISCONNECT || event == SocketEvent.ERROR) { // Unable to complete the TLS handshake. Treat it as // if the handshake failed. handshake = -1; } else { handshake = socket.handshake(key.isReadable(), key.isWritable()); // The handshake process reads/writes from/to the // socket. status may therefore be OPEN_WRITE once // the handshake completes. However, the handshake // happens when the socket is opened so the status // must always be OPEN_READ after it completes. It // is OK to always set this as it is only used if // the handshake completes. event = SocketEvent.OPEN_READ; } } } catch (IOException x) { handshake = -1; if (log.isDebugEnabled()) log.debug(\"Error during SSL handshake\",x); } catch (CancelledKeyException ckx) { handshake = -1; } if (handshake == 0) { SocketState state = SocketState.OPEN; // Process the request from this socket if (event == null) { state = getHandler().process(socketWrapper, SocketEvent.OPEN_READ); } else { state = getHandler().process(socketWrapper, event); } if (state == SocketState.CLOSED) { close(socket, key); } } else if (handshake == -1 ) { close(socket, key); } else if (handshake == SelectionKey.OP_READ){ socketWrapper.registerReadInterest(); } else if (handshake == SelectionKey.OP_WRITE){ socketWrapper.registerWriteInterest(); } } catch (CancelledKeyException cx) { socket.getPoller().cancelledKey(key); } catch (VirtualMachineError vme) { ExceptionUtils.handleThrowable(vme); } catch (Throwable t) { log.error(\"\", t); socket.getPoller().cancelledKey(key); } finally { socketWrapper = null; event = null; //return to cache // keep-alive的实现 if (running && !paused) { processorCache.push(this); } } } 3.1.1) AbstractProtocol#process 先试图从connections中获取当前Socket对应的Processor，如果没有找到的话从recycledProcessors中获取，也就是已经处理过连接但是没有被销毁的Processor，这样做的好处是避免频繁地创建和销毁对象。processor还是为空的话，那就使用createProcessor创建。 public SocketState process(SocketWrapperBase wrapper, SocketEvent status) { if (getLog().isDebugEnabled()) { getLog().debug(sm.getString(\"abstractConnectionHandler.process\", wrapper.getSocket(), status)); } if (wrapper == null) { // Nothing to do. Socket has been closed. return SocketState.CLOSED; } S socket = wrapper.getSocket(); Processor processor = connections.get(socket); if (getLog().isDebugEnabled()) { getLog().debug(sm.getString(\"abstractConnectionHandler.connectionsGet\", processor, socket)); } if (processor != null) { // Make sure an async timeout doesn't fire getProtocol().removeWaitingProcessor(processor); } else if (status == SocketEvent.DISCONNECT || status == SocketEvent.ERROR) { // Nothing to do. Endpoint requested a close and there is no // longer a processor associated with this socket. return SocketState.CLOSED; } ContainerThreadMarker.set(); try { if (processor == null) { String negotiatedProtocol = wrapper.getNegotiatedProtocol(); if (negotiatedProtocol != null) { UpgradeProtocol upgradeProtocol = getProtocol().getNegotiatedProtocol(negotiatedProtocol); if (upgradeProtocol != null) { processor = upgradeProtocol.getProcessor( wrapper, getProtocol().getAdapter()); } else if (negotiatedProtocol.equals(\"http/1.1\")) { // Explicitly negotiated the default protocol. // Obtain a processor below. } else { // TODO: // OpenSSL 1.0.2's ALPN callback doesn't support // failing the handshake with an error if no // protocol can be negotiated. Therefore, we need to // fail the connection here. Once this is fixed, // replace the code below with the commented out // block. if (getLog().isDebugEnabled()) { getLog().debug(sm.getString( \"abstractConnectionHandler.negotiatedProcessor.fail\", negotiatedProtocol)); } return SocketState.CLOSED; /* * To replace the code above once OpenSSL 1.1.0 is * used. // Failed to create processor. This is a bug. throw new IllegalStateException(sm.getString( \"abstractConnectionHandler.negotiatedProcessor.fail\", negotiatedProtocol)); */ } } } if (processor == null) { processor = recycledProcessors.pop(); if (getLog().isDebugEnabled()) { getLog().debug(sm.getString(\"abstractConnectionHandler.processorPop\", processor)); } } if (processor == null) { processor = getProtocol().createProcessor(); register(processor); } processor.setSslSupport( wrapper.getSslSupport(getProtocol().getClientCertProvider())); // Associate the processor with the connection connections.put(socket, processor); SocketState state = SocketState.CLOSED; do { state = processor.process(wrapper, status); if (state == SocketState.UPGRADING) { // Get the HTTP upgrade handler UpgradeToken upgradeToken = processor.getUpgradeToken(); // Retrieve leftover input ByteBuffer leftOverInput = processor.getLeftoverInput(); if (upgradeToken == null) { // Assume direct HTTP/2 connection UpgradeProtocol upgradeProtocol = getProtocol().getUpgradeProtocol(\"h2c\"); if (upgradeProtocol != null) { processor = upgradeProtocol.getProcessor( wrapper, getProtocol().getAdapter()); wrapper.unRead(leftOverInput); // Associate with the processor with the connection connections.put(socket, processor); } else { if (getLog().isDebugEnabled()) { getLog().debug(sm.getString( \"abstractConnectionHandler.negotiatedProcessor.fail\", \"h2c\")); } return SocketState.CLOSED; } } else { HttpUpgradeHandler httpUpgradeHandler = upgradeToken.getHttpUpgradeHandler(); // Release the Http11 processor to be re-used release(processor); // Create the upgrade processor processor = getProtocol().createUpgradeProcessor(wrapper, upgradeToken); if (getLog().isDebugEnabled()) { getLog().debug(sm.getString(\"abstractConnectionHandler.upgradeCreate\", processor, wrapper)); } wrapper.unRead(leftOverInput); // Mark the connection as upgraded wrapper.setUpgraded(true); // Associate with the processor with the connection connections.put(socket, processor); // Initialise the upgrade handler (which may trigger // some IO using the new protocol which is why the lines // above are necessary) // This cast should be safe. If it fails the error // handling for the surrounding try/catch will deal with // it. if (upgradeToken.getInstanceManager() == null) { httpUpgradeHandler.init((WebConnection) processor); } else { ClassLoader oldCL = upgradeToken.getContextBind().bind(false, null); try { httpUpgradeHandler.init((WebConnection) processor); } finally { upgradeToken.getContextBind().unbind(false, oldCL); } } } } } while ( state == SocketState.UPGRADING); if (state == SocketState.LONG) { // In the middle of processing a request/response. Keep the // socket associated with the processor. Exact requirements // depend on type of long poll longPoll(wrapper, processor); if (processor.isAsync()) { getProtocol().addWaitingProcessor(processor); } } else if (state == SocketState.OPEN) { // In keep-alive but between requests. OK to recycle // processor. Continue to poll for the next request. connections.remove(socket); release(processor); wrapper.registerReadInterest(); } else if (state == SocketState.SENDFILE) { // Sendfile in progress. If it fails, the socket will be // closed. If it works, the socket either be added to the // poller (or equivalent) to await more data or processed // if there are any pipe-lined requests remaining. } else if (state == SocketState.UPGRADED) { // Don't add sockets back to the poller if this was a // non-blocking write otherwise the poller may trigger // multiple read events which may lead to thread starvation // in the connector. The write() method will add this socket // to the poller if necessary. if (status != SocketEvent.OPEN_WRITE) { longPoll(wrapper, processor); } } else if (state == SocketState.SUSPENDED) { // Don't add sockets back to the poller. // The resumeProcessing() method will add this socket // to the poller. } else { // Connection closed. OK to recycle the processor. Upgrade // processors are not recycled. connections.remove(socket); if (processor.isUpgrade()) { UpgradeToken upgradeToken = processor.getUpgradeToken(); HttpUpgradeHandler httpUpgradeHandler = upgradeToken.getHttpUpgradeHandler(); InstanceManager instanceManager = upgradeToken.getInstanceManager(); if (instanceManager == null) { httpUpgradeHandler.destroy(); } else { ClassLoader oldCL = upgradeToken.getContextBind().bind(false, null); try { httpUpgradeHandler.destroy(); } finally { try { instanceManager.destroyInstance(httpUpgradeHandler); } catch (Throwable e) { ExceptionUtils.handleThrowable(e); getLog().error(sm.getString(\"abstractConnectionHandler.error\"), e); } upgradeToken.getContextBind().unbind(false, oldCL); } } } else { release(processor); } } return state; } catch(java.net.SocketException e) { // SocketExceptions are normal getLog().debug(sm.getString( \"abstractConnectionHandler.socketexception.debug\"), e); } catch (java.io.IOException e) { // IOExceptions are normal getLog().debug(sm.getString( \"abstractConnectionHandler.ioexception.debug\"), e); } catch (ProtocolException e) { // Protocol exceptions normally mean the client sent invalid or // incomplete data. getLog().debug(sm.getString( \"abstractConnectionHandler.protocolexception.debug\"), e); } // Future developers: if you discover any other // rare-but-nonfatal exceptions, catch them here, and log as // above. catch (Throwable e) { ExceptionUtils.handleThrowable(e); // any other exception or error is odd. Here we log it // with \"ERROR\" level, so it will show up even on // less-than-verbose logs. getLog().error(sm.getString(\"abstractConnectionHandler.error\"), e); } finally { ContainerThreadMarker.clear(); } // Make sure socket/processor is removed from the list of current // connections connections.remove(socket); release(processor); return SocketState.CLOSED; } - 3.1.1.1) AbstractHttp11Protocol#createProcessor createProcessor() 会创建一个 Http11Processor, 它用来解析 Socket，将 Socket 中的内容封装到 Request 中。注意这个 Request 是临时使用的一个类，它的全类名是 org.apache.coyote.Request。 protected Processor createProcessor() { Http11Processor processor = new Http11Processor(this, adapter); return processor; } 3.1.1.1.1) Http11Processor#constructor（创建req和resp缓冲区) public Http11Processor(AbstractHttp11Protocol protocol, Adapter adapter) { super(adapter); this.protocol = protocol; userDataHelper = new UserDataHelper(log); inputBuffer = new Http11InputBuffer(request, protocol.getMaxHttpHeaderSize(), protocol.getRejectIllegalHeaderName()); request.setInputBuffer(inputBuffer); outputBuffer = new Http11OutputBuffer(response, protocol.getMaxHttpHeaderSize()); response.setOutputBuffer(outputBuffer); // Create and add the identity filters. inputBuffer.addFilter(new IdentityInputFilter(protocol.getMaxSwallowSize())); outputBuffer.addFilter(new IdentityOutputFilter()); // Create and add the chunked filters. inputBuffer.addFilter(new ChunkedInputFilter(protocol.getMaxTrailerSize(), protocol.getAllowedTrailerHeadersInternal(), protocol.getMaxExtensionSize(), protocol.getMaxSwallowSize())); outputBuffer.addFilter(new ChunkedOutputFilter()); // Create and add the void filters. inputBuffer.addFilter(new VoidInputFilter()); outputBuffer.addFilter(new VoidOutputFilter()); // Create and add buffered input filter inputBuffer.addFilter(new BufferedInputFilter()); // Create and add the chunked filters. //inputBuffer.addFilter(new GzipInputFilter()); outputBuffer.addFilter(new GzipOutputFilter()); pluggableFilterIndex = inputBuffer.getFilters().length; } public AbstractProcessor(Adapter adapter) { this(adapter, new Request(), new Response()); } protected AbstractProcessor(Adapter adapter, Request coyoteRequest, Response coyoteResponse) { this.adapter = adapter; asyncStateMachine = new AsyncStateMachine(this); request = coyoteRequest; response = coyoteResponse; response.setHook(this); request.setResponse(response); request.setHook(this); } 3.1.1.1.1.1) Http11InputBuffer#constructor（存放解析后的Request信息) public Http11InputBuffer(Request request, int headerBufferSize, boolean rejectIllegalHeaderName) { this.request = request; headers = request.getMimeHeaders(); this.headerBufferSize = headerBufferSize; this.rejectIllegalHeaderName = rejectIllegalHeaderName; filterLibrary = new InputFilter[0]; activeFilters = new InputFilter[0]; lastActiveFilter = -1; parsingHeader = true; parsingRequestLine = true; parsingRequestLinePhase = 0; parsingRequestLineEol = false; parsingRequestLineStart = 0; parsingRequestLineQPos = -1; headerParsePos = HeaderParsePosition.HEADER_START; swallowInput = true; inputStreamInputBuffer = new SocketInputBuffer(); } - 3.1.1.2) ConnectionHandler#register（注册Http11Processor) protected void register(Processor processor) { if (getProtocol().getDomain() != null) { synchronized (this) { try { long count = registerCount.incrementAndGet(); RequestInfo rp = processor.getRequest().getRequestProcessor(); rp.setGlobalProcessor(global); ObjectName rpName = new ObjectName( getProtocol().getDomain() + \":type=RequestProcessor,worker=\" + getProtocol().getName() + \",name=\" + getProtocol().getProtocolName() + \"Request\" + count); if (getLog().isDebugEnabled()) { getLog().debug(\"Register \" + rpName); } Registry.getRegistry(null, null).registerComponent(rp, rpName, null); rp.setRpName(rpName); } catch (Exception e) { getLog().warn(\"Error registering request\"); } } } } 3.1.1.3) AbstractProcessorLight#process（Http11Processor进行处理) public SocketState process(SocketWrapperBase socketWrapper, SocketEvent status) throws IOException { SocketState state = SocketState.CLOSED; Iterator dispatches = null; do { if (dispatches != null) { DispatchType nextDispatch = dispatches.next(); state = dispatch(nextDispatch.getSocketStatus()); } else if (status == SocketEvent.DISCONNECT) { // Do nothing here, just wait for it to get recycled } else if (isAsync() || isUpgrade() || state == SocketState.ASYNC_END) { state = dispatch(status); if (state == SocketState.OPEN) { // There may be pipe-lined data to read. If the data isn't // processed now, execution will exit this loop and call // release() which will recycle the processor (and input // buffer) deleting any pipe-lined data. To avoid this, // process it now. state = service(socketWrapper); } } else if (status == SocketEvent.OPEN_WRITE) { // Extra write event likely after async, ignore state = SocketState.LONG; } else if (status == SocketEvent.OPEN_READ){ state = service(socketWrapper); } else { // Default to closing the socket if the SocketEvent passed in // is not consistent with the current state of the Processor state = SocketState.CLOSED; } if (state != SocketState.CLOSED && isAsync()) { state = asyncPostProcess(); } if (getLog().isDebugEnabled()) { getLog().debug(\"Socket: [\" + socketWrapper + \"], Status in: [\" + status + \"], State out: [\" + state + \"]\"); } if (dispatches == null || !dispatches.hasNext()) { // Only returns non-null iterator if there are // dispatches to process. dispatches = getIteratorAndClearDispatches(); } } while (state == SocketState.ASYNC_END || dispatches != null && state != SocketState.CLOSED); return state; } - 3.1.1.3.1) （service骨架)Http11Processor#service（包含servlet后续处理，keep-alive的实现) 1，org.apache.coyote.Request 是tomcat内部使用用于存放关于request消息的数据结构 2，org.apache.tomcat.util.buf.MessageBytes 用于存放消息，在org.apache.coyote.Request中大量用于存放解析后的byte字符 3，org.apache.tomcat.util.buf.ByteChunk 真正用于存放数据的数据结构，存放的是byte[],org.apache.tomcat.util.buf.MessageBytes使用它。 Request存放着解析后的Request信息，其数据来自于InputBuffer。 http消息通过inputBuffer解析后放到Request中，Request把它放到相应的MessageBytes，最后MessageBytes把它存到ByteChunk里。 public SocketState service(SocketWrapperBase socketWrapper) throws IOException { RequestInfo rp = request.getRequestProcessor(); rp.setStage(org.apache.coyote.Constants.STAGE_PARSE); // Setting up the I/O setSocketWrapper(socketWrapper); inputBuffer.init(socketWrapper); outputBuffer.init(socketWrapper); // Flags keepAlive = true; openSocket = false; readComplete = true; boolean keptAlive = false; SendfileState sendfileState = SendfileState.DONE; while (!getErrorState().isError() && keepAlive && !isAsync() && upgradeToken == null && sendfileState == SendfileState.DONE && !protocol.isPaused()) { // Parsing the request header try { if (!inputBuffer.parseRequestLine(keptAlive, protocol.getConnectionTimeout(), protocol.getKeepAliveTimeout())) { if (inputBuffer.getParsingRequestLinePhase() == -1) { return SocketState.UPGRADING; } else if (handleIncompleteRequestLineRead()) { break; } } if (protocol.isPaused()) { // 503 - Service unavailable response.setStatus(503); setErrorState(ErrorState.CLOSE_CLEAN, null); } else { keptAlive = true; // Set this every time in case limit has been changed via JMX request.getMimeHeaders().setLimit(protocol.getMaxHeaderCount()); if (!inputBuffer.parseHeaders()) { // We've read part of the request, don't recycle it // instead associate it with the socket openSocket = true; readComplete = false; break; } if (!protocol.getDisableUploadTimeout()) { socketWrapper.setReadTimeout(protocol.getConnectionUploadTimeout()); } } } catch (IOException e) { if (log.isDebugEnabled()) { log.debug(sm.getString(\"http11processor.header.parse\"), e); } setErrorState(ErrorState.CLOSE_CONNECTION_NOW, e); break; } catch (Throwable t) { ExceptionUtils.handleThrowable(t); UserDataHelper.Mode logMode = userDataHelper.getNextMode(); if (logMode != null) { String message = sm.getString(\"http11processor.header.parse\"); switch (logMode) { case INFO_THEN_DEBUG: message += sm.getString(\"http11processor.fallToDebug\"); //$FALL-THROUGH$ case INFO: log.info(message, t); break; case DEBUG: log.debug(message, t); } } // 400 - Bad Request response.setStatus(400); setErrorState(ErrorState.CLOSE_CLEAN, t); getAdapter().log(request, response, 0); } // Has an upgrade been requested? Enumeration connectionValues = request.getMimeHeaders().values(\"Connection\"); boolean foundUpgrade = false; while (connectionValues.hasMoreElements() && !foundUpgrade) { foundUpgrade = connectionValues.nextElement().toLowerCase( Locale.ENGLISH).contains(\"upgrade\"); } if (foundUpgrade) { // Check the protocol String requestedProtocol = request.getHeader(\"Upgrade\"); UpgradeProtocol upgradeProtocol = protocol.getUpgradeProtocol(requestedProtocol); if (upgradeProtocol != null) { if (upgradeProtocol.accept(request)) { // TODO Figure out how to handle request bodies at this // point. response.setStatus(HttpServletResponse.SC_SWITCHING_PROTOCOLS); response.setHeader(\"Connection\", \"Upgrade\"); response.setHeader(\"Upgrade\", requestedProtocol); action(ActionCode.CLOSE, null); getAdapter().log(request, response, 0); InternalHttpUpgradeHandler upgradeHandler = upgradeProtocol.getInternalUpgradeHandler( socketWrapper, getAdapter(), cloneRequest(request)); UpgradeToken upgradeToken = new UpgradeToken(upgradeHandler, null, null); action(ActionCode.UPGRADE, upgradeToken); return SocketState.UPGRADING; } } } if (!getErrorState().isError()) { // Setting up filters, and parse some request headers rp.setStage(org.apache.coyote.Constants.STAGE_PREPARE); try { prepareRequest(); } catch (Throwable t) { ExceptionUtils.handleThrowable(t); if (log.isDebugEnabled()) { log.debug(sm.getString(\"http11processor.request.prepare\"), t); } // 500 - Internal Server Error response.setStatus(500); setErrorState(ErrorState.CLOSE_CLEAN, t); getAdapter().log(request, response, 0); } } int maxKeepAliveRequests = protocol.getMaxKeepAliveRequests(); if (maxKeepAliveRequests == 1) { keepAlive = false; } else if (maxKeepAliveRequests > 0 && socketWrapper.decrementKeepAlive() 0) { socketWrapper.setReadTimeout(connectionTimeout); } else { socketWrapper.setReadTimeout(0); } } rp.setStage(org.apache.coyote.Constants.STAGE_KEEPALIVE); sendfileState = processSendfile(socketWrapper); } rp.setStage(org.apache.coyote.Constants.STAGE_ENDED); if (getErrorState().isError() || protocol.isPaused()) { return SocketState.CLOSED; } else if (isAsync()) { return SocketState.LONG; } else if (isUpgrade()) { return SocketState.UPGRADING; } else { if (sendfileState == SendfileState.PENDING) { return SocketState.SENDFILE; } else { if (openSocket) { if (readComplete) { return SocketState.OPEN; } else { return SocketState.LONG; } } else { return SocketState.CLOSED; } } } } - 3.1.1.3.1.1) Http11inputBuffer#init（初始化InputBuffer) void init(SocketWrapperBase socketWrapper) { wrapper = socketWrapper; wrapper.setAppReadBufHandler(this); int bufLength = headerBufferSize + wrapper.getSocketBufferHandler().getReadBuffer().capacity(); if (byteBuffer == null || byteBuffer.capacity() byteBuffer = ByteBuffer.allocate(bufLength); byteBuffer.position(0).limit(0); } } 3.1.1.3.1.2) Http11inputBuffer#parseRequestLine（解析请求行) 将SocketBufferHandler中的readBuffer的部分数据填充到byteBuffer中，读取byteBuffer，解析，将结果存入Request boolean parseRequestLine(boolean keptAlive) throws IOException { // check state if (!parsingRequestLine) { return true; } // // Skipping blank lines // if (parsingRequestLinePhase // Read new bytes if needed if (byteBuffer.position() >= byteBuffer.limit()) { if (keptAlive) { // Haven't read any request data yet so use the keep-alive // timeout. wrapper.setReadTimeout(wrapper.getEndpoint().getKeepAliveTimeout()); } if (!fill(false)) { // A read is pending, so no longer in initial state parsingRequestLinePhase = 1; return false; } // At least one byte of the request has been received. // Switch to the socket timeout. wrapper.setReadTimeout(wrapper.getEndpoint().getSoTimeout()); } if (!keptAlive && byteBuffer.position() == 0 && byteBuffer.limit() >= CLIENT_PREFACE_START.length - 1) { boolean prefaceMatch = true; for (int i = 0; i } while ((chr == Constants.CR) || (chr == Constants.LF)); byteBuffer.position(byteBuffer.position() - 1); parsingRequestLineStart = byteBuffer.position(); parsingRequestLinePhase = 2; if (log.isDebugEnabled()) { log.debug(\"Received [\" + new String(byteBuffer.array(), byteBuffer.position(), byteBuffer.remaining(), StandardCharsets.ISO_8859_1) + \"]\"); } } if (parsingRequestLinePhase == 2) { // // Reading the method name // Method name is a token // boolean space = false; while (!space) { // Read new bytes if needed if (byteBuffer.position() >= byteBuffer.limit()) { if (!fill(false)) // request line parsing return false; } // Spec says method name is a token followed by a single SP but // also be tolerant of multiple SP and/or HT. int pos = byteBuffer.position(); byte chr = byteBuffer.get(); if (chr == Constants.SP || chr == Constants.HT) { space = true; request.method().setBytes(byteBuffer.array(), parsingRequestLineStart, pos - parsingRequestLineStart); } else if (!HttpParser.isToken(chr)) { byteBuffer.position(byteBuffer.position() - 1); throw new IllegalArgumentException(sm.getString(\"iib.invalidmethod\")); } } parsingRequestLinePhase = 3; } if (parsingRequestLinePhase == 3) { // Spec says single SP but also be tolerant of multiple SP and/or HT boolean space = true; while (space) { // Read new bytes if needed if (byteBuffer.position() >= byteBuffer.limit()) { if (!fill(false)) // request line parsing return false; } byte chr = byteBuffer.get(); if (!(chr == Constants.SP || chr == Constants.HT)) { space = false; byteBuffer.position(byteBuffer.position() - 1); } } parsingRequestLineStart = byteBuffer.position(); parsingRequestLinePhase = 4; } if (parsingRequestLinePhase == 4) { // Mark the current buffer position int end = 0; // // Reading the URI // boolean space = false; while (!space) { // Read new bytes if needed if (byteBuffer.position() >= byteBuffer.limit()) { if (!fill(false)) // request line parsing return false; } int pos = byteBuffer.position(); byte chr = byteBuffer.get(); if (chr == Constants.SP || chr == Constants.HT) { space = true; end = pos; } else if (chr == Constants.CR || chr == Constants.LF) { // HTTP/0.9 style request parsingRequestLineEol = true; space = true; end = pos; } else if (chr == Constants.QUESTION && parsingRequestLineQPos == -1) { parsingRequestLineQPos = pos; } else if (HttpParser.isNotRequestTarget(chr)) { throw new IllegalArgumentException(sm.getString(\"iib.invalidRequestTarget\")); } } if (parsingRequestLineQPos >= 0) { request.queryString().setBytes(byteBuffer.array(), parsingRequestLineQPos + 1, end - parsingRequestLineQPos - 1); request.requestURI().setBytes(byteBuffer.array(), parsingRequestLineStart, parsingRequestLineQPos - parsingRequestLineStart); } else { request.requestURI().setBytes(byteBuffer.array(), parsingRequestLineStart, end - parsingRequestLineStart); } parsingRequestLinePhase = 5; } if (parsingRequestLinePhase == 5) { // Spec says single SP but also be tolerant of multiple and/or HT boolean space = true; while (space) { // Read new bytes if needed if (byteBuffer.position() >= byteBuffer.limit()) { if (!fill(false)) // request line parsing return false; } byte chr = byteBuffer.get(); if (!(chr == Constants.SP || chr == Constants.HT)) { space = false; byteBuffer.position(byteBuffer.position() - 1); } } parsingRequestLineStart = byteBuffer.position(); parsingRequestLinePhase = 6; // Mark the current buffer position end = 0; } if (parsingRequestLinePhase == 6) { // // Reading the protocol // Protocol is always \"HTTP/\" DIGIT \".\" DIGIT // while (!parsingRequestLineEol) { // Read new bytes if needed if (byteBuffer.position() >= byteBuffer.limit()) { if (!fill(false)) // request line parsing return false; } int pos = byteBuffer.position(); byte chr = byteBuffer.get(); if (chr == Constants.CR) { end = pos; } else if (chr == Constants.LF) { if (end == 0) { end = pos; } parsingRequestLineEol = true; } else if (!HttpParser.isHttpProtocol(chr)) { throw new IllegalArgumentException(sm.getString(\"iib.invalidHttpProtocol\")); } } if ((end - parsingRequestLineStart) > 0) { request.protocol().setBytes(byteBuffer.array(), parsingRequestLineStart, end - parsingRequestLineStart); } else { request.protocol().setString(\"\"); } parsingRequestLine = false; parsingRequestLinePhase = 0; parsingRequestLineEol = false; parsingRequestLineStart = 0; return true; } throw new IllegalStateException( \"Invalid request line parse phase:\" + parsingRequestLinePhase); } 3.1.1.3.1.2.1) Http11InputBuffer#fill（) /** * Attempts to read some data into the input buffer. * * @return true if more data was added to the input buffer * otherwise false */ private boolean fill(boolean block) throws IOException { if (parsingHeader) { if (byteBuffer.limit() >= headerBufferSize) { throw new IllegalArgumentException(sm.getString(\"iib.requestheadertoolarge.error\")); } } else { byteBuffer.limit(end).position(end); } byteBuffer.mark(); if (byteBuffer.position() 0) { return true; } else if (nRead == -1) { throw new EOFException(sm.getString(\"iib.eof.error\")); } else { return false; } } - 3.1.1.3.1.2.1.1) NioEndpoint#read public int read(boolean block, ByteBuffer to) throws IOException { int nRead = populateReadBuffer(to); if (nRead > 0) { return nRead; /* * Since more bytes may have arrived since the buffer was last * filled, it is an option at this point to perform a * non-blocking read. However correctly handling the case if * that read returns end of stream adds complexity. Therefore, * at the moment, the preference is for simplicity. */ } // The socket read buffer capacity is socket.appReadBufSize int limit = socketBufferHandler.getReadBuffer().capacity(); // 如果to的剩余可用比read buffer还要大。那么直接从socketchannel读到to if (to.remaining() >= limit) { to.limit(to.position() + limit); nRead = fillReadBuffer(block, to); updateLastRead(); } else { // Fill the read buffer as best we can. nRead = fillReadBuffer(block); updateLastRead(); // Fill as much of the remaining byte array as possible with the // data that was just read if (nRead > 0) { nRead = populateReadBuffer(to); } } return nRead; } 3.1.1.3.1.2.1.1.1) SocketWrapperBase#populateReadBuffer（将SocketBufferHandler中的ByteBuffer拷贝到Http11InputBuffer中的ByteBuffer) protected int populateReadBuffer(ByteBuffer to) { // Is there enough data in the read buffer to satisfy this request? // Copy what data there is in the read buffer to the byte array socketBufferHandler.configureReadBufferForRead(); int nRead = transfer(socketBufferHandler.getReadBuffer(), to); if (log.isDebugEnabled()) { log.debug(\"Socket: [\" + this + \"], Read from buffer: [\" + nRead + \"]\"); } return nRead; } protected static int transfer(ByteBuffer from, ByteBuffer to) { int max = Math.min(from.remaining(), to.remaining()); if (max > 0) { int fromLimit = from.limit(); from.limit(from.position() + max); to.put(from); from.limit(fromLimit); } return max; } 3.1.1.3.1.2.1.1.2) NioEndpoint#fillReadBuffer（从channel或者selectorPool中读到ByteBuffer中) private int fillReadBuffer(boolean block, ByteBuffer to) throws IOException { int nRead; NioChannel channel = getSocket(); if (block) { Selector selector = null; try { selector = pool.get(); } catch (IOException x) { // Ignore } try { NioEndpoint.NioSocketWrapper att = (NioEndpoint.NioSocketWrapper) channel .getAttachment(); if (att == null) { throw new IOException(\"Key must be cancelled.\"); } nRead = pool.read(to, channel, selector, att.getReadTimeout()); } finally { if (selector != null) { pool.put(selector); } } } else { nRead = channel.read(to); if (nRead == -1) { throw new EOFException(); } } return nRead; } - 3.1.1.3.1.3) Http11inputBuffer#parseHeaders（解析请求头) 读取byteBuffer，解析，将结果存入Request 3.1.1.3.1.4) prepareRequest（封装InputFilter) private void prepareRequest() { http11 = true; http09 = false; contentDelimitation = false; if (protocol.isSSLEnabled()) { request.scheme().setString(\"https\"); } MessageBytes protocolMB = request.protocol(); if (protocolMB.equals(Constants.HTTP_11)) { http11 = true; protocolMB.setString(Constants.HTTP_11); } else if (protocolMB.equals(Constants.HTTP_10)) { http11 = false; keepAlive = false; protocolMB.setString(Constants.HTTP_10); } else if (protocolMB.equals(\"\")) { // HTTP/0.9 http09 = true; http11 = false; keepAlive = false; } else { // Unsupported protocol http11 = false; // Send 505; Unsupported HTTP version response.setStatus(505); setErrorState(ErrorState.CLOSE_CLEAN, null); if (log.isDebugEnabled()) { log.debug(sm.getString(\"http11processor.request.prepare\")+ \" Unsupported HTTP version \\\"\"+protocolMB+\"\\\"\"); } } MimeHeaders headers = request.getMimeHeaders(); // Check connection header MessageBytes connectionValueMB = headers.getValue(Constants.CONNECTION); if (connectionValueMB != null) { ByteChunk connectionValueBC = connectionValueMB.getByteChunk(); if (findBytes(connectionValueBC, Constants.CLOSE_BYTES) != -1) { keepAlive = false; } else if (findBytes(connectionValueBC, Constants.KEEPALIVE_BYTES) != -1) { keepAlive = true; } } if (http11) { MessageBytes expectMB = headers.getValue(\"expect\"); if (expectMB != null) { if (expectMB.indexOfIgnoreCase(\"100-continue\", 0) != -1) { inputBuffer.setSwallowInput(false); request.setExpectation(true); } else { response.setStatus(HttpServletResponse.SC_EXPECTATION_FAILED); setErrorState(ErrorState.CLOSE_CLEAN, null); } } } // Check user-agent header Pattern restrictedUserAgents = protocol.getRestrictedUserAgentsPattern(); if (restrictedUserAgents != null && (http11 || keepAlive)) { MessageBytes userAgentValueMB = headers.getValue(\"user-agent\"); // Check in the restricted list, and adjust the http11 // and keepAlive flags accordingly if(userAgentValueMB != null) { String userAgentValue = userAgentValueMB.toString(); if (restrictedUserAgents.matcher(userAgentValue).matches()) { http11 = false; keepAlive = false; } } } // Check host header MessageBytes hostValueMB = null; try { hostValueMB = headers.getUniqueValue(\"host\"); } catch (IllegalArgumentException iae) { // Multiple Host headers are not permitted // 400 - Bad request response.setStatus(400); setErrorState(ErrorState.CLOSE_CLEAN, null); if (log.isDebugEnabled()) { log.debug(sm.getString(\"http11processor.request.multipleHosts\")); } } if (http11 && hostValueMB == null) { // 400 - Bad request response.setStatus(400); setErrorState(ErrorState.CLOSE_CLEAN, null); if (log.isDebugEnabled()) { log.debug(sm.getString(\"http11processor.request.prepare\")+ \" host header missing\"); } } // Check for a full URI (including protocol://host:port/) ByteChunk uriBC = request.requestURI().getByteChunk(); if (uriBC.startsWithIgnoreCase(\"http\", 0)) { int pos = uriBC.indexOf(\"://\", 0, 3, 4); int uriBCStart = uriBC.getStart(); int slashPos = -1; if (pos != -1) { pos += 3; byte[] uriB = uriBC.getBytes(); slashPos = uriBC.indexOf('/', pos); int atPos = uriBC.indexOf('@', pos); if (slashPos == -1) { slashPos = uriBC.getLength(); // Set URI as \"/\" request.requestURI().setBytes (uriB, uriBCStart + pos - 2, 1); } else { request.requestURI().setBytes (uriB, uriBCStart + slashPos, uriBC.getLength() - slashPos); } // Skip any user info if (atPos != -1) { pos = atPos + 1; } if (http11) { // Missing host header is illegal but handled above if (hostValueMB != null) { // Any host in the request line must be consistent with // the Host header if (!hostValueMB.getByteChunk().equals( uriB, uriBCStart + pos, slashPos - pos)) { if (protocol.getAllowHostHeaderMismatch()) { // The requirements of RFC 2616 are being // applied. If the host header and the request // line do not agree, the request line takes // precedence hostValueMB = headers.setValue(\"host\"); hostValueMB.setBytes(uriB, uriBCStart + pos, slashPos - pos); } else { // The requirements of RFC 7230 are being // applied. If the host header and the request // line do not agree, trigger a 400 response. response.setStatus(400); setErrorState(ErrorState.CLOSE_CLEAN, null); if (log.isDebugEnabled()) { log.debug(sm.getString(\"http11processor.request.inconsistentHosts\")); } } } } } else { // Not HTTP/1.1 - no Host header so generate one since // Tomcat internals assume it is set hostValueMB = headers.setValue(\"host\"); hostValueMB.setBytes(uriB, uriBCStart + pos, slashPos - pos); } } } // Input filter setup InputFilter[] inputFilters = inputBuffer.getFilters(); // Parse transfer-encoding header if (http11) { MessageBytes transferEncodingValueMB = headers.getValue(\"transfer-encoding\"); if (transferEncodingValueMB != null) { String transferEncodingValue = transferEncodingValueMB.toString(); // Parse the comma separated list. \"identity\" codings are ignored int startPos = 0; int commaPos = transferEncodingValue.indexOf(','); String encodingName = null; while (commaPos != -1) { encodingName = transferEncodingValue.substring(startPos, commaPos); addInputFilter(inputFilters, encodingName); startPos = commaPos + 1; commaPos = transferEncodingValue.indexOf(',', startPos); } encodingName = transferEncodingValue.substring(startPos); addInputFilter(inputFilters, encodingName); } } // Parse content-length header long contentLength = request.getContentLengthLong(); if (contentLength >= 0) { if (contentDelimitation) { // contentDelimitation being true at this point indicates that // chunked encoding is being used but chunked encoding should // not be used with a content length. RFC 2616, section 4.4, // bullet 3 states Content-Length must be ignored in this case - // so remove it. headers.removeHeader(\"content-length\"); request.setContentLength(-1); } else { inputBuffer.addActiveFilter (inputFilters[Constants.IDENTITY_FILTER]); contentDelimitation = true; } } parseHost(hostValueMB); if (!contentDelimitation) { // If there's no content length // (broken HTTP/1.0 or HTTP/1.1), assume // the client is not broken and didn't send a body inputBuffer.addActiveFilter (inputFilters[Constants.VOID_FILTER]); contentDelimitation = true; } if (getErrorState().isError()) { getAdapter().log(request, response, 0); } } - 3.1.1.3.1.4)（service骨架) CoyoteAdapter#service（将coyote的req和resp转为catalina的req和resp) public void service(org.apache.coyote.Request req, org.apache.coyote.Response res) throws Exception { Request request = (Request) req.getNote(ADAPTER_NOTES); Response response = (Response) res.getNote(ADAPTER_NOTES); if (request == null) { // Create objects request = connector.createRequest(); request.setCoyoteRequest(req); response = connector.createResponse(); response.setCoyoteResponse(res); // Link objects request.setResponse(response); response.setRequest(request); // Set as notes req.setNote(ADAPTER_NOTES, request); res.setNote(ADAPTER_NOTES, response); // Set query string encoding req.getParameters().setQueryStringCharset(connector.getURICharset()); } if (connector.getXpoweredBy()) { response.addHeader(\"X-Powered-By\", POWERED_BY); } boolean async = false; boolean postParseSuccess = false; req.getRequestProcessor().setWorkerThreadName(THREAD_NAME.get()); try { // Parse and set Catalina and configuration specific // request parameters postParseSuccess = postParseRequest(req, request, res, response); if (postParseSuccess) { //check valves if we support async request.setAsyncSupported( connector.getService().getContainer().getPipeline().isAsyncSupported()); // Calling the container // 加入到pipeline中进行调用 connector.getService().getContainer().getPipeline().getFirst().invoke( request, response); } if (request.isAsync()) { async = true; ReadListener readListener = req.getReadListener(); if (readListener != null && request.isFinished()) { // Possible the all data may have been read during service() // method so this needs to be checked here ClassLoader oldCL = null; try { oldCL = request.getContext().bind(false, null); if (req.sendAllDataReadEvent()) { req.getReadListener().onAllDataRead(); } } finally { request.getContext().unbind(false, oldCL); } } Throwable throwable = (Throwable) request.getAttribute(RequestDispatcher.ERROR_EXCEPTION); // If an async request was started, is not going to end once // this container thread finishes and an error occurred, trigger // the async error process if (!request.isAsyncCompleting() && throwable != null) { request.getAsyncContextInternal().setErrorState(throwable, true); } } else { request.finishRequest(); response.finishResponse(); } } catch (IOException e) { // Ignore } finally { AtomicBoolean error = new AtomicBoolean(false); res.action(ActionCode.IS_ERROR, error); if (request.isAsyncCompleting() && error.get()) { // Connection will be forcibly closed which will prevent // completion happening at the usual point. Need to trigger // call to onComplete() here. res.action(ActionCode.ASYNC_POST_PROCESS, null); async = false; } // Access log if (!async && postParseSuccess) { // Log only if processing was invoked. // If postParseRequest() failed, it has already logged it. Context context = request.getContext(); // If the context is null, it is likely that the endpoint was // shutdown, this connection closed and the request recycled in // a different thread. That thread will have updated the access // log so it is OK not to update the access log here in that // case. if (context != null) { context.logAccess(request, response, System.currentTimeMillis() - req.getStartTime(), false); } } req.getRequestProcessor().setWorkerThreadName(null); // Recycle the wrapper request and response if (!async) { request.recycle(); response.recycle(); } } } - 3.1.1.3.1.4.1)（Mapper#map) CoyoteAdapter#postParseRequest（req和resp的转换) postParseRequest() 方法封装一下 Request，并处理一下映射关系(从 URL 映射到相应的 Host、Context、Wrapper)。 CoyoteAdapter 将 Rquest 提交给 Container 处理之前，并将 org.apache.coyote.Request 封装到 org.apache.catalina.connector.Request，传递给 Container 处理的 Request 是 org.apache.catalina.connector.Request。 connector.getService().getMapper().map()，用来在 Mapper 中查询 URL 的映射关系。映射关系会保留到 org.apache.catalina.connector.Request 中，Container 处理阶段 request.getHost() 是使用的就是这个阶段查询到的映射主机，以此类推 request.getContext()、request.getWrapper() 都是。 protected boolean postParseRequest(org.apache.coyote.Request req, Request request, org.apache.coyote.Response res, Response response) throws IOException, ServletException { // If the processor has set the scheme (AJP does this, HTTP does this if // SSL is enabled) use this to set the secure flag as well. If the // processor hasn't set it, use the settings from the connector if (req.scheme().isNull()) { // Use connector scheme and secure configuration, (defaults to // \"http\" and false respectively) req.scheme().setString(connector.getScheme()); request.setSecure(connector.getSecure()); } else { // Use processor specified scheme to determine secure state request.setSecure(req.scheme().equals(\"https\")); } // At this point the Host header has been processed. // Override if the proxyPort/proxyHost are set String proxyName = connector.getProxyName(); int proxyPort = connector.getProxyPort(); if (proxyPort != 0) { req.setServerPort(proxyPort); } else if (req.getServerPort() == -1) { // Not explicitly set. Use default ports based on the scheme if (req.scheme().equals(\"https\")) { req.setServerPort(443); } else { req.setServerPort(80); } } if (proxyName != null) { req.serverName().setString(proxyName); } MessageBytes undecodedURI = req.requestURI(); // Check for ping OPTIONS * request if (undecodedURI.equals(\"*\")) { if (req.method().equalsIgnoreCase(\"OPTIONS\")) { StringBuilder allow = new StringBuilder(); allow.append(\"GET, HEAD, POST, PUT, DELETE, OPTIONS\"); // Trace if allowed if (connector.getAllowTrace()) { allow.append(\", TRACE\"); } // Always allow options res.setHeader(\"Allow\", allow.toString()); // Access log entry as processing won't reach AccessLogValve connector.getService().getContainer().logAccess(request, response, 0, true); return false; } else { response.sendError(400, \"Invalid URI\"); } } MessageBytes decodedURI = req.decodedURI(); if (undecodedURI.getType() == MessageBytes.T_BYTES) { // Copy the raw URI to the decodedURI decodedURI.duplicate(undecodedURI); // Parse the path parameters. This will: // - strip out the path parameters // - convert the decodedURI to bytes parsePathParameters(req, request); // URI decoding // %xx decoding of the URL try { req.getURLDecoder().convert(decodedURI, false); } catch (IOException ioe) { response.sendError(400, \"Invalid URI: \" + ioe.getMessage()); } // Normalization if (!normalize(req.decodedURI())) { response.sendError(400, \"Invalid URI\"); } // Character decoding convertURI(decodedURI, request); // Check that the URI is still normalized if (!checkNormalize(req.decodedURI())) { response.sendError(400, \"Invalid URI\"); } } else { /* The URI is chars or String, and has been sent using an in-memory * protocol handler. The following assumptions are made: * - req.requestURI() has been set to the 'original' non-decoded, * non-normalized URI * - req.decodedURI() has been set to the decoded, normalized form * of req.requestURI() */ decodedURI.toChars(); // Remove all path parameters; any needed path parameter should be set // using the request object rather than passing it in the URL CharChunk uriCC = decodedURI.getCharChunk(); int semicolon = uriCC.indexOf(';'); if (semicolon > 0) { decodedURI.setChars(uriCC.getBuffer(), uriCC.getStart(), semicolon); } } // Request mapping. MessageBytes serverName; if (connector.getUseIPVHosts()) { serverName = req.localName(); if (serverName.isNull()) { // well, they did ask for it res.action(ActionCode.REQ_LOCAL_NAME_ATTRIBUTE, null); } } else { serverName = req.serverName(); } // Version for the second mapping loop and // Context that we expect to get for that version String version = null; Context versionContext = null; boolean mapRequired = true; if (response.isError()) { // An error this early means the URI is invalid. Ensure invalid data // is not passed to the mapper. Note we still want the mapper to // find the correct host. decodedURI.recycle(); } while (mapRequired) { - // 使用Mapper将当前request映射到Host、Context、Wrapper // This will map the the latest version by default connector.getService().getMapper().map(serverName, decodedURI, version, request.getMappingData()); // If there is no context at this point, either this is a 404 // because no ROOT context has been deployed or the URI was invalid // so no context could be mapped. if (request.getContext() == null) { // Don't overwrite an existing error if (!response.isError()) { response.sendError(404, \"Not found\"); } // Allow processing to continue. // If present, the error reporting valve will provide a response // body. return true; } // Now we have the context, we can parse the session ID from the URL // (if any). Need to do this before we redirect in case we need to // include the session id in the redirect String sessionID; if (request.getServletContext().getEffectiveSessionTrackingModes() .contains(SessionTrackingMode.URL)) { // Get the session ID if there was one sessionID = request.getPathParameter( SessionConfig.getSessionUriParamName( request.getContext())); if (sessionID != null) { request.setRequestedSessionId(sessionID); request.setRequestedSessionURL(true); } } // Look for session ID in cookies and SSL session parseSessionCookiesId(request); parseSessionSslId(request); sessionID = request.getRequestedSessionId(); mapRequired = false; if (version != null && request.getContext() == versionContext) { // We got the version that we asked for. That is it. } else { version = null; versionContext = null; Context[] contexts = request.getMappingData().contexts; // Single contextVersion means no need to remap // No session ID means no possibility of remap if (contexts != null && sessionID != null) { // Find the context associated with the session for (int i = contexts.length; i > 0; i--) { Context ctxt = contexts[i - 1]; if (ctxt.getManager().findSession(sessionID) != null) { // We found a context. Is it the one that has // already been mapped? if (!ctxt.equals(request.getMappingData().context)) { // Set version so second time through mapping // the correct context is found version = ctxt.getWebappVersion(); versionContext = ctxt; // Reset mapping request.getMappingData().recycle(); mapRequired = true; // Recycle cookies and session info in case the // correct context is configured with different // settings request.recycleSessionInfo(); request.recycleCookieInfo(true); } break; } } } } if (!mapRequired && request.getContext().getPaused()) { // Found a matching context but it is paused. Mapping data will // be wrong since some Wrappers may not be registered at this // point. try { Thread.sleep(1000); } catch (InterruptedException e) { // Should never happen } // Reset mapping request.getMappingData().recycle(); mapRequired = true; } } // Possible redirect MessageBytes redirectPathMB = request.getMappingData().redirectPath; if (!redirectPathMB.isNull()) { String redirectPath = URLEncoder.DEFAULT.encode( redirectPathMB.toString(), StandardCharsets.UTF_8); String query = request.getQueryString(); if (request.isRequestedSessionIdFromURL()) { // This is not optimal, but as this is not very common, it // shouldn't matter redirectPath = redirectPath + \";\" + SessionConfig.getSessionUriParamName( request.getContext()) + \"=\" + request.getRequestedSessionId(); } if (query != null) { // This is not optimal, but as this is not very common, it // shouldn't matter redirectPath = redirectPath + \"?\" + query; } response.sendRedirect(redirectPath); request.getContext().logAccess(request, response, 0, true); return false; } // Filter trace method if (!connector.getAllowTrace() && req.method().equalsIgnoreCase(\"TRACE\")) { Wrapper wrapper = request.getWrapper(); String header = null; if (wrapper != null) { String[] methods = wrapper.getServletMethods(); if (methods != null) { for (int i=0; i} - 3.1.1.3.1.4.2) （->4))Valve#invoke public void invoke(Request request, Response response) throws IOException, ServletException; connector.getService().getContainer().getPipeline().getFirst().invoke() 会将请求传递到 Container 处理，当然了 Container 处理也是在 Worker 线程中执行的，但是这是一个相对独立的模块，所以单独分出来一节。 第一个Container#Valve是StandardEngineValve。 按照这样的顺序：engine->host->context->wrapper。 3.1.1.3.1.4.3) Request#finishRequest（非异步Servlet被调用) public void finishRequest() throws IOException { if (response.getStatus() == HttpServletResponse.SC_REQUEST_ENTITY_TOO_LARGE) { checkSwallowInput(); } } - 3.1.1.3.1.4.4) Response#finishResponse（非异步Servlet被调用) public void finishResponse() throws IOException { // Writing leftover bytes outputBuffer.close(); } - 3.1.1.3.1.4.5) Request#recycle（非异步Servlet被调用，释放资源，待被复用) /** * Release all object references, and initialize instance variables, in * preparation for reuse of this object. */ public void recycle() { internalDispatcherType = null; requestDispatcherPath = null; authType = null; inputBuffer.recycle(); usingInputStream = false; usingReader = false; userPrincipal = null; subject = null; parametersParsed = false; if (parts != null) { for (Part part: parts) { try { part.delete(); } catch (IOException ignored) { // ApplicationPart.delete() never throws an IOEx } } parts = null; } partsParseException = null; locales.clear(); localesParsed = false; secure = false; remoteAddr = null; remoteHost = null; remotePort = -1; localPort = -1; localAddr = null; localName = null; attributes.clear(); sslAttributesParsed = false; notes.clear(); recycleSessionInfo(); recycleCookieInfo(false); if (Globals.IS_SECURITY_ENABLED || Connector.RECYCLE_FACADES) { parameterMap = new ParameterMap<>(); } else { parameterMap.setLocked(false); parameterMap.clear(); } mappingData.recycle(); applicationMapping.recycle(); applicationRequest = null; if (Globals.IS_SECURITY_ENABLED || Connector.RECYCLE_FACADES) { if (facade != null) { facade.clear(); facade = null; } if (inputStream != null) { inputStream.clear(); inputStream = null; } if (reader != null) { reader.clear(); reader = null; } } asyncSupported = null; if (asyncContext!=null) { asyncContext.recycle(); } asyncContext = null; } - 3.1.1.3.1.4.6) Response#recycle（非异步Servlet被调用，释放资源，待被复用) /** * Release all object references, and initialize instance variables, in * preparation for reuse of this object. */ public void recycle() { cookies.clear(); outputBuffer.recycle(); usingOutputStream = false; usingWriter = false; appCommitted = false; included = false; isCharacterEncodingSet = false; applicationResponse = null; if (Globals.IS_SECURITY_ENABLED || Connector.RECYCLE_FACADES) { if (facade != null) { facade.clear(); facade = null; } if (outputStream != null) { outputStream.clear(); outputStream = null; } if (writer != null) { writer.clear(); writer = null; } } else if (writer != null) { writer.recycle(); } } - 3.1.1.3.1.5) endRequest（非异步Servlet被调用) /* * No more input will be passed to the application. Remaining input will be * swallowed or the connection dropped depending on the error and * expectation status. */ private void endRequest() { if (getErrorState().isError()) { // If we know we are closing the connection, don't drain // input. This way uploading a 100GB file doesn't tie up the // thread if the servlet has rejected it. inputBuffer.setSwallowInput(false); } else { // Need to check this again here in case the response was // committed before the error that requires the connection // to be closed occurred. checkExpectationAndResponseStatus(); } // Finish the handling of the request if (getErrorState().isIoAllowed()) { try { inputBuffer.endRequest(); } catch (IOException e) { setErrorState(ErrorState.CLOSE_CONNECTION_NOW, e); } catch (Throwable t) { ExceptionUtils.handleThrowable(t); // 500 - Internal Server Error // Can't add a 500 to the access log since that has already been // written in the Adapter.service method. response.setStatus(500); setErrorState(ErrorState.CLOSE_NOW, t); log.error(sm.getString(\"http11processor.request.finish\"), t); } } if (getErrorState().isIoAllowed()) { try { action(ActionCode.COMMIT, null); outputBuffer.end(); } catch (IOException e) { setErrorState(ErrorState.CLOSE_CONNECTION_NOW, e); } catch (Throwable t) { ExceptionUtils.handleThrowable(t); setErrorState(ErrorState.CLOSE_NOW, t); log.error(sm.getString(\"http11processor.response.finish\"), t); } } } - 3.1.1.3.1.5.1) Http11InputBuffer#endRequest - void endRequest() throws IOException { if (swallowInput && (lastActiveFilter != -1)) { int extraBytes = (int) activeFilters[lastActiveFilter].end(); byteBuffer.position(byteBuffer.position() - extraBytes); } } - 3.1.1.3.1.5.2) AbstractProcessor#action(COMMIT) case COMMIT: { if (!response.isCommitted()) { try { // Validate and write response headers prepareResponse(); } catch (IOException e) { setErrorState(ErrorState.CLOSE_CONNECTION_NOW, e); } } break; } Http11Processor#prepareResponse protected final void prepareResponse() throws IOException { boolean entityBody = true; contentDelimitation = false; OutputFilter[] outputFilters = outputBuffer.getFilters(); if (http09 == true) { // HTTP/0.9 outputBuffer.addActiveFilter(outputFilters[Constants.IDENTITY_FILTER]); outputBuffer.commit(); return; } int statusCode = response.getStatus(); if (statusCode statusCode == 304) { // No entity body outputBuffer.addActiveFilter (outputFilters[Constants.VOID_FILTER]); entityBody = false; contentDelimitation = true; if (statusCode == 205) { // RFC 7231 requires the server to explicitly signal an empty // response in this case response.setContentLength(0); } else { response.setContentLength(-1); } } MessageBytes methodMB = request.method(); if (methodMB.equals(\"HEAD\")) { // No entity body outputBuffer.addActiveFilter (outputFilters[Constants.VOID_FILTER]); contentDelimitation = true; } // Sendfile support if (protocol.getUseSendfile()) { prepareSendfile(outputFilters); } // Check for compression boolean useCompression = false; if (entityBody && sendfileData == null) { useCompression = protocol.useCompression(request, response); } MimeHeaders headers = response.getMimeHeaders(); // A SC_NO_CONTENT response may include entity headers if (entityBody || statusCode == HttpServletResponse.SC_NO_CONTENT) { String contentType = response.getContentType(); if (contentType != null) { headers.setValue(\"Content-Type\").setString(contentType); } String contentLanguage = response.getContentLanguage(); if (contentLanguage != null) { headers.setValue(\"Content-Language\") .setString(contentLanguage); } } long contentLength = response.getContentLengthLong(); boolean connectionClosePresent = false; if (http11 && response.getTrailerFields() != null) { // If trailer fields are set, always use chunking outputBuffer.addActiveFilter(outputFilters[Constants.CHUNKED_FILTER]); contentDelimitation = true; headers.addValue(Constants.TRANSFERENCODING).setString(Constants.CHUNKED); } else if (contentLength != -1) { headers.setValue(\"Content-Length\").setLong(contentLength); outputBuffer.addActiveFilter(outputFilters[Constants.IDENTITY_FILTER]); contentDelimitation = true; } else { // If the response code supports an entity body and we're on // HTTP 1.1 then we chunk unless we have a Connection: close header connectionClosePresent = isConnectionClose(headers); if (http11 && entityBody && !connectionClosePresent) { outputBuffer.addActiveFilter(outputFilters[Constants.CHUNKED_FILTER]); contentDelimitation = true; headers.addValue(Constants.TRANSFERENCODING).setString(Constants.CHUNKED); } else { outputBuffer.addActiveFilter(outputFilters[Constants.IDENTITY_FILTER]); } } if (useCompression) { outputBuffer.addActiveFilter(outputFilters[Constants.GZIP_FILTER]); } // Add date header unless application has already set one (e.g. in a // Caching Filter) if (headers.getValue(\"Date\") == null) { headers.addValue(\"Date\").setString( FastHttpDateFormat.getCurrentDate()); } // FIXME: Add transfer encoding header if ((entityBody) && (!contentDelimitation)) { // Mark as close the connection after the request, and add the // connection: close header keepAlive = false; } // This may disabled keep-alive to check before working out the // Connection header. checkExpectationAndResponseStatus(); // If we know that the request is bad this early, add the // Connection: close header. if (keepAlive && statusDropsConnection(statusCode)) { keepAlive = false; } if (!keepAlive) { // Avoid adding the close header twice if (!connectionClosePresent) { headers.addValue(Constants.CONNECTION).setString( Constants.CLOSE); } } else if (!http11 && !getErrorState().isError()) { headers.addValue(Constants.CONNECTION).setString(Constants.KEEPALIVE); } // Add server header String server = protocol.getServer(); if (server == null) { if (protocol.getServerRemoveAppProvidedValues()) { headers.removeHeader(\"server\"); } } else { // server always overrides anything the app might set headers.setValue(\"Server\").setString(server); } // Build the response header try { outputBuffer.sendStatus(); int size = headers.size(); for (int i = 0; i outputBuffer.sendHeader(headers.getName(i), headers.getValue(i)); } outputBuffer.endHeaders(); } catch (Throwable t) { ExceptionUtils.handleThrowable(t); // If something goes wrong, reset the header buffer so the error // response can be written instead. outputBuffer.resetHeaderBuffer(); throw t; } outputBuffer.commit(); } Http11OutputBuffer#commit protected void commit() throws IOException { response.setCommitted(true); if (headerBuffer.position() > 0) { // Sending the response header buffer headerBuffer.flip(); try { socketWrapper.write(isBlocking(), headerBuffer); } finally { headerBuffer.position(0).limit(headerBuffer.capacity()); } } } - 3.1,1,3,1,5,3) Http11OutputBuffer#end public void end() throws IOException { if (responseFinished) { return; } if (lastActiveFilter == -1) { outputStreamOutputBuffer.end(); } else { activeFilters[lastActiveFilter].end(); } responseFinished = true; } 3.1.1.3.2) asyncPostProcess（异步Servlet) public SocketState asyncPostProcess() { return asyncStateMachine.asyncPostProcess(); } - 3.1.1.3.2.1) AsyncStateMachine#asyncPostProcess synchronized SocketState asyncPostProcess() { if (state == AsyncState.COMPLETE_PENDING) { doComplete(); return SocketState.ASYNC_END; } else if (state == AsyncState.DISPATCH_PENDING) { doDispatch(); return SocketState.ASYNC_END; } else if (state == AsyncState.STARTING || state == AsyncState.READ_WRITE_OP) { state = AsyncState.STARTED; return SocketState.LONG; } else if (state == AsyncState.MUST_COMPLETE || state == AsyncState.COMPLETING) { asyncCtxt.fireOnComplete(); state = AsyncState.DISPATCHED; return SocketState.ASYNC_END; } else if (state == AsyncState.MUST_DISPATCH) { state = AsyncState.DISPATCHING; return SocketState.ASYNC_END; } else if (state == AsyncState.DISPATCHING) { state = AsyncState.DISPATCHED; return SocketState.ASYNC_END; } else if (state == AsyncState.STARTED) { // This can occur if an async listener does a dispatch to an async // servlet during onTimeout return SocketState.LONG; } else { throw new IllegalStateException( sm.getString(\"asyncStateMachine.invalidAsyncState\", \"asyncPostProcess()\", state)); } } 4) Container#Valve#invoke（在Worker线程池中执行) 1.- 需要注意的是，基本上每一个容器的 StandardPipeline 上都会有多个已注册的 Valve，我们只关注每个容器的 Basic Valve。其他 Valve 都是在 Basic Valve 前执行。 2.- request.getHost().getPipeline().getFirst().invoke() 先获取对应的 StandardHost，并执行其 pipeline。 3.- request.getContext().getPipeline().getFirst().invoke() 先获取对应的 StandardContext,并执行其 pipeline。 4.- request.getWrapper().getPipeline().getFirst().invoke() 先获取对应的 StandardWrapper，并执行其 pipeline。 5.- 最值得说的就是 StandardWrapper 的 Basic Valve，StandardWrapperValve 6.- allocate() 用来加载并初始化 Servlet，值的一提的是 Servlet 并不都是单例的，当 Servlet 实现了 SingleThreadModel 接口后，StandardWrapper 会维护一组 Servlet 实例，这是享元模式。当然了 SingleThreadModel 在 Servlet 2.4 以后就弃用了。 7.- createFilterChain() 方法会从 StandardContext 中获取到所有的过滤器，然后将匹配 Request URL 的所有过滤器挑选出来添加到 filterChain 中。 8.- doFilter() 执行过滤链,当所有的过滤器都执行完毕后调用 Servlet 的 service() 方法。 第一个Container#Valve是StandardEngineValve。 按照这样的顺序：engine->host->context->wrapper。 这四个容器都继承自ContainerBase。ContainerBase public abstract class ContainerBase extends LifecycleMBeanBase implements Container { /** * The Pipeline object with which this Container is associated. */ protected final Pipeline pipeline = new StandardPipeline(this); } 持有一个StandardPipeline对象。 Pipeline（一个pipeline只能与一个Container关联，多对一) StandardPipeline 组件代表一个流水线，与 Valve（阀)结合，用于处理请求。 StandardPipeline 中含有多个 Valve， 当需要处理请求时，会逐一调用 Valve 的 invoke 方法对 Request 和 Response 进行处理。特别的，其中有一个特殊的 Valve 叫 basicValve,每一个标准容器都有一个指定的 BasicValve，他们做的是最核心的工作。 public class StandardPipeline extends LifecycleBase implements Pipeline { private static final Log log = LogFactory.getLog(StandardPipeline.class); // ----------------------------------------------------------- Constructors /** * Construct a new StandardPipeline instance with no associated Container. */ public StandardPipeline() { this(null); } /** * Construct a new StandardPipeline instance that is associated with the * specified Container. * * @param container The container we should be associated with */ public StandardPipeline(Container container) { super(); setContainer(container); } // ----------------------------------------------------- Instance Variables /** * The basic Valve (if any) associated with this Pipeline. */ protected Valve basic = null; /** * The Container with which this Pipeline is associated. */ protected Container container = null; /** * The first valve associated with this Pipeline. */ protected Valve first = null; }Valve（一个pipeline对应着多个Valve，一对多，链表结构) Valve是一个接口，其基本实现的BaseValve类。 public abstract class ValveBase extends LifecycleMBeanBase implements Contained, Valve { protected static final StringManager sm = StringManager.getManager(ValveBase.class); //------------------------------------------------------ Constructor public ValveBase() { this(false); } public ValveBase(boolean asyncSupported) { this.asyncSupported = asyncSupported; } //------------------------------------------------------ Instance Variables /** * Does this valve support Servlet 3+ async requests? */ protected boolean asyncSupported; /** * The Container whose pipeline this Valve is a component of. */ protected Container container = null; /** * Container log */ protected Log containerLog = null; /** * The next Valve in the pipeline this Valve is a component of. */ protected Valve next = null; } 4.1) StandardEngineValve#invoke public final void invoke(Request request, Response response) throws IOException, ServletException { // Select the Host to be used for this Request Host host = request.getHost(); if (host == null) { response.sendError (HttpServletResponse.SC_BAD_REQUEST, sm.getString(\"standardEngine.noHost\", request.getServerName())); return; } if (request.isAsyncSupported()) { request.setAsyncSupported(host.getPipeline().isAsyncSupported()); } // Ask this Host to process this request host.getPipeline().getFirst().invoke(request, response); } - 4.1.1) StandardHostValve#invoke public final void invoke(Request request, Response response) throws IOException, ServletException { // Select the Context to be used for this Request Context context = request.getContext(); if (context == null) { return; } if (request.isAsyncSupported()) { request.setAsyncSupported(context.getPipeline().isAsyncSupported()); } boolean asyncAtStart = request.isAsync(); try { context.bind(Globals.IS_SECURITY_ENABLED, MY_CLASSLOADER); if (!asyncAtStart && !context.fireRequestInitEvent(request.getRequest())) { // Don't fire listeners during async processing (the listener // fired for the request that called startAsync()). // If a request init listener throws an exception, the request // is aborted. return; } // Ask this Context to process this request. Requests that are in // async mode and are not being dispatched to this resource must be // in error and have been routed here to check for application // defined error pages. try { if (!response.isErrorReportRequired()) { context.getPipeline().getFirst().invoke(request, response); } } catch (Throwable t) { ExceptionUtils.handleThrowable(t); container.getLogger().error(\"Exception Processing \" + request.getRequestURI(), t); // If a new error occurred while trying to report a previous // error allow the original error to be reported. if (!response.isErrorReportRequired()) { request.setAttribute(RequestDispatcher.ERROR_EXCEPTION, t); throwable(request, response, t); } } // Now that the request/response pair is back under container // control lift the suspension so that the error handling can // complete and/or the container can flush any remaining data response.setSuspended(false); Throwable t = (Throwable) request.getAttribute(RequestDispatcher.ERROR_EXCEPTION); // Protect against NPEs if the context was destroyed during a // long running request. if (!context.getState().isAvailable()) { return; } // Look for (and render if found) an application level error page if (response.isErrorReportRequired()) { if (t != null) { throwable(request, response, t); } else { status(request, response); } } if (!request.isAsync() && !asyncAtStart) { context.fireRequestDestroyEvent(request.getRequest()); } } finally { // Access a session (if present) to update last accessed time, based // on a strict interpretation of the specification if (ACCESS_SESSION) { request.getSession(false); } context.unbind(Globals.IS_SECURITY_ENABLED, MY_CLASSLOADER); } } - 4.1.1.1) StandardContextValve#invoke public final void invoke(Request request, Response response) throws IOException, ServletException { // Disallow any direct access to resources under WEB-INF or META-INF MessageBytes requestPathMB = request.getRequestPathMB(); if ((requestPathMB.startsWithIgnoreCase(\"/META-INF/\", 0)) || (requestPathMB.equalsIgnoreCase(\"/META-INF\")) || (requestPathMB.startsWithIgnoreCase(\"/WEB-INF/\", 0)) || (requestPathMB.equalsIgnoreCase(\"/WEB-INF\"))) { response.sendError(HttpServletResponse.SC_NOT_FOUND); return; } // Select the Wrapper to be used for this Request Wrapper wrapper = request.getWrapper(); if (wrapper == null || wrapper.isUnavailable()) { response.sendError(HttpServletResponse.SC_NOT_FOUND); return; } // Acknowledge the request try { response.sendAcknowledgement(); } catch (IOException ioe) { container.getLogger().error(sm.getString( \"standardContextValve.acknowledgeException\"), ioe); request.setAttribute(RequestDispatcher.ERROR_EXCEPTION, ioe); response.sendError(HttpServletResponse.SC_INTERNAL_SERVER_ERROR); return; } if (request.isAsyncSupported()) { request.setAsyncSupported(wrapper.getPipeline().isAsyncSupported()); } wrapper.getPipeline().getFirst().invoke(request, response); } - 4.1.1.1.1) （调用Servlet)StandardWrapperValve#invoke StandardWrapperValve 1.- allocate() 用来加载并初始化 Servlet，值的一提的是 Servlet 并不都是单例的，当 Servlet 实现了 SingleThreadModel 接口后，StandardWrapper 会维护一组 Servlet 实例，这是享元模式。当然了 SingleThreadModel 在 Servlet 2.4 以后就弃用了。 2.- createFilterChain() 方法会从 StandardContext 中获取到所有的过滤器，然后将匹配 Request URL 的所有过滤器挑选出来添加到 filterChain 中。 3.- doFilter() 执行过滤链,当所有的过滤器都执行完毕后调用 Servlet 的 service() 方法。 public final void invoke(Request request, Response response) throws IOException, ServletException { // Initialize local variables we may need boolean unavailable = false; Throwable throwable = null; // This should be a Request attribute... long t1=System.currentTimeMillis(); requestCount.incrementAndGet(); StandardWrapper wrapper = (StandardWrapper) getContainer(); Servlet servlet = null; Context context = (Context) wrapper.getParent(); // Check for the application being marked unavailable if (!context.getState().isAvailable()) { response.sendError(HttpServletResponse.SC_SERVICE_UNAVAILABLE, sm.getString(\"standardContext.isUnavailable\")); unavailable = true; } // Check for the servlet being marked unavailable if (!unavailable && wrapper.isUnavailable()) { container.getLogger().info(sm.getString(\"standardWrapper.isUnavailable\", wrapper.getName())); long available = wrapper.getAvailable(); if ((available > 0L) && (available 0L) && (available 0) { context.getLogger().info(log); } } } else { if (request.isAsyncDispatching()) { request.getAsyncContextInternal().doInternalDispatch(); } else { filterChain.doFilter (request.getRequest(), response.getResponse()); } } } } catch (ClientAbortException e) { throwable = e; exception(request, response, e); } catch (IOException e) { container.getLogger().error(sm.getString( \"standardWrapper.serviceException\", wrapper.getName(), context.getName()), e); throwable = e; exception(request, response, e); } catch (UnavailableException e) { container.getLogger().error(sm.getString( \"standardWrapper.serviceException\", wrapper.getName(), context.getName()), e); // throwable = e; // exception(request, response, e); wrapper.unavailable(e); long available = wrapper.getAvailable(); if ((available > 0L) && (available maxTime) maxTime=time; if( time - 4.1.1.1.1.1) StandardWrapper#allocate（创建servlet实例) public Servlet allocate() throws ServletException { // If we are currently unloading this servlet, throw an exception if (unloading) { throw new ServletException(sm.getString(\"standardWrapper.unloading\", getName())); } boolean newInstance = false; // If not SingleThreadedModel, return the same instance every time if (!singleThreadModel) { // Load and initialize our instance if necessary if (instance == null || !instanceInitialized) { synchronized (this) { if (instance == null) { try { if (log.isDebugEnabled()) { log.debug(\"Allocating non-STM instance\"); } // Note: We don't know if the Servlet implements // SingleThreadModel until we have loaded it. instance = loadServlet(); newInstance = true; if (!singleThreadModel) { // For non-STM, increment here to prevent a race // condition with unload. Bug 43683, test case // #3 countAllocated.incrementAndGet(); } } catch (ServletException e) { throw e; } catch (Throwable e) { ExceptionUtils.handleThrowable(e); throw new ServletException(sm.getString(\"standardWrapper.allocate\"), e); } } if (!instanceInitialized) { initServlet(instance); } } } if (singleThreadModel) { if (newInstance) { // Have to do this outside of the sync above to prevent a // possible deadlock synchronized (instancePool) { instancePool.push(instance); nInstances++; } } } else { if (log.isTraceEnabled()) { log.trace(\" Returning non-STM instance\"); } // For new instances, count will have been incremented at the // time of creation if (!newInstance) { countAllocated.incrementAndGet(); } return instance; } } synchronized (instancePool) { while (countAllocated.get() >= nInstances) { // Allocate a new instance if possible, or else wait if (nInstances - 4.1.1.1.1.1.1) StandardWrapper#loadServlet public synchronized Servlet loadServlet() throws ServletException { // Nothing to do if we already have an instance or an instance pool if (!singleThreadModel && (instance != null)) return instance; PrintStream out = System.out; if (swallowOutput) { SystemLogHandler.startCapture(); } Servlet servlet; try { long t1=System.currentTimeMillis(); // Complain if no servlet class has been specified if (servletClass == null) { unavailable(null); throw new ServletException (sm.getString(\"standardWrapper.notClass\", getName())); } InstanceManager instanceManager = ((StandardContext)getParent()).getInstanceManager(); try { servlet = (Servlet) instanceManager.newInstance(servletClass); } catch (ClassCastException e) { unavailable(null); // Restore the context ClassLoader throw new ServletException (sm.getString(\"standardWrapper.notServlet\", servletClass), e); } catch (Throwable e) { e = ExceptionUtils.unwrapInvocationTargetException(e); ExceptionUtils.handleThrowable(e); unavailable(null); // Added extra log statement for Bugzilla 36630: // http://bz.apache.org/bugzilla/show_bug.cgi?id=36630 if(log.isDebugEnabled()) { log.debug(sm.getString(\"standardWrapper.instantiate\", servletClass), e); } // Restore the context ClassLoader throw new ServletException (sm.getString(\"standardWrapper.instantiate\", servletClass), e); } if (multipartConfigElement == null) { MultipartConfig annotation = servlet.getClass().getAnnotation(MultipartConfig.class); if (annotation != null) { multipartConfigElement = new MultipartConfigElement(annotation); } } // Special handling for ContainerServlet instances // Note: The InstanceManager checks if the application is permitted // to load ContainerServlets if (servlet instanceof ContainerServlet) { ((ContainerServlet) servlet).setWrapper(this); } classLoadTime=(int) (System.currentTimeMillis() -t1); if (servlet instanceof SingleThreadModel) { if (instancePool == null) { instancePool = new Stack<>(); } singleThreadModel = true; } initServlet(servlet); fireContainerEvent(\"load\", this); loadTime=System.currentTimeMillis() -t1; } finally { if (swallowOutput) { String log = SystemLogHandler.stopCapture(); if (log != null && log.length() > 0) { if (getServletContext() != null) { getServletContext().log(log); } else { out.println(log); } } } } return servlet; } - 4.1.1.1.1.1.1.1) StandardWrapper#initServlet private synchronized void initServlet(Servlet servlet) throws ServletException { if (instanceInitialized && !singleThreadModel) return; // Call the initialization method of this servlet try { if( Globals.IS_SECURITY_ENABLED) { boolean success = false; try { Object[] args = new Object[] { facade }; SecurityUtil.doAsPrivilege(\"init\", servlet, classType, args); success = true; } finally { if (!success) { // destroy() will not be called, thus clear the reference now SecurityUtil.remove(servlet); } } } else { servlet.init(facade); } instanceInitialized = true; } catch (UnavailableException f) { unavailable(f); throw f; } catch (ServletException f) { // If the servlet wanted to be unavailable it would have // said so, so do not call unavailable(null). throw f; } catch (Throwable f) { ExceptionUtils.handleThrowable(f); getServletContext().log(\"StandardWrapper.Throwable\", f ); // If the servlet wanted to be unavailable it would have // said so, so do not call unavailable(null). throw new ServletException (sm.getString(\"standardWrapper.initException\", getName()), f); } } - 4.1.1.1.1.2) ApplicationFilterFactory#createFilterChain public static ApplicationFilterChain createFilterChain(ServletRequest request, Wrapper wrapper, Servlet servlet) { // If there is no servlet to execute, return null if (servlet == null) return null; // Create and initialize a filter chain object ApplicationFilterChain filterChain = null; if (request instanceof Request) { Request req = (Request) request; if (Globals.IS_SECURITY_ENABLED) { // Security: Do not recycle filterChain = new ApplicationFilterChain(); } else { filterChain = (ApplicationFilterChain) req.getFilterChain(); if (filterChain == null) { filterChain = new ApplicationFilterChain(); req.setFilterChain(filterChain); } } } else { // Request dispatcher in use filterChain = new ApplicationFilterChain(); } filterChain.setServlet(servlet); filterChain.setServletSupportsAsync(wrapper.isAsyncSupported()); // Acquire the filter mappings for this Context StandardContext context = (StandardContext) wrapper.getParent(); FilterMap filterMaps[] = context.findFilterMaps(); // If there are no filter mappings, we are done if ((filterMaps == null) || (filterMaps.length == 0)) return filterChain; // Acquire the information we will need to match filter mappings DispatcherType dispatcher = (DispatcherType) request.getAttribute(Globals.DISPATCHER_TYPE_ATTR); String requestPath = null; Object attribute = request.getAttribute(Globals.DISPATCHER_REQUEST_PATH_ATTR); if (attribute != null){ requestPath = attribute.toString(); } String servletName = wrapper.getName(); // Add the relevant path-mapped filters to this filter chain for (int i = 0; i - 4.1.1.1.1.3) ApplicationFilterChain#doFilter public void doFilter(ServletRequest request, ServletResponse response) throws IOException, ServletException { if( Globals.IS_SECURITY_ENABLED ) { final ServletRequest req = request; final ServletResponse res = response; try { java.security.AccessController.doPrivileged( new java.security.PrivilegedExceptionAction() { @Override public Void run() throws ServletException, IOException { internalDoFilter(req,res); return null; } } ); } catch( PrivilegedActionException pe) { Exception e = pe.getException(); if (e instanceof ServletException) throw (ServletException) e; else if (e instanceof IOException) throw (IOException) e; else if (e instanceof RuntimeException) throw (RuntimeException) e; else throw new ServletException(e.getMessage(), e); } } else { internalDoFilter(request,response); } } - 4.1.1.1.1.3.1) ApplicationFilterChain#internalDoFilter（这里是起个头，后续doFilter是在用户Filter中调用的) private void internalDoFilter(ServletRequest request, ServletResponse response) throws IOException, ServletException { // Call the next filter if there is one if (pos - 4.1.1.1.1.3.1.1) DefaultServlet#service（处理静态资源，如果任何servlet都无法匹配，则转向该servlet) protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { if (req.getDispatcherType() == DispatcherType.ERROR) { doGet(req, resp); } else { super.service(req, resp); } } protected void doGet(HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException { // Serve the requested resource, including the data content serveResource(request, response, true, fileEncoding); } - 4.1.1.1.1.3.1.1.1) DefaultServlet#serveResource 首先会判断要请求的资源是否存在，文件是否可读，之后，根据资源的类型，设置响应头的content-type，判断文件的时间，设置超时时间等，最终是流的读写。 /** * Serve the specified resource, optionally including the data content. * * @param request The servlet request we are processing * @param response The servlet response we are creating * @param content Should the content be included? * @param encoding The encoding to use if it is necessary to access the * source as characters rather than as bytes * * @exception IOException if an input/output error occurs * @exception ServletException if a servlet-specified error occurs */ protected void serveResource(HttpServletRequest request, HttpServletResponse response, boolean content, String encoding) throws IOException, ServletException { boolean serveContent = content; // Identify the requested resource path String path = getRelativePath(request, true); if (debug > 0) { if (serveContent) log(\"DefaultServlet.serveResource: Serving resource '\" + path + \"' headers and data\"); else log(\"DefaultServlet.serveResource: Serving resource '\" + path + \"' headers only\"); } if (path.length() == 0) { // Context root redirect doDirectoryRedirect(request, response); return; } WebResource resource = resources.getResource(path); boolean isError = DispatcherType.ERROR == request.getDispatcherType(); if (!resource.exists()) { // Check if we're included so we can return the appropriate // missing resource name in the error String requestUri = (String) request.getAttribute( RequestDispatcher.INCLUDE_REQUEST_URI); if (requestUri == null) { requestUri = request.getRequestURI(); } else { // We're included // SRV.9.3 says we must throw a FNFE throw new FileNotFoundException(sm.getString( \"defaultServlet.missingResource\", requestUri)); } if (isError) { response.sendError(((Integer) request.getAttribute( RequestDispatcher.ERROR_STATUS_CODE)).intValue()); } else { response.sendError(HttpServletResponse.SC_NOT_FOUND, requestUri); } return; } if (!resource.canRead()) { // Check if we're included so we can return the appropriate // missing resource name in the error String requestUri = (String) request.getAttribute( RequestDispatcher.INCLUDE_REQUEST_URI); if (requestUri == null) { requestUri = request.getRequestURI(); } else { // We're included // Spec doesn't say what to do in this case but a FNFE seems // reasonable throw new FileNotFoundException(sm.getString( \"defaultServlet.missingResource\", requestUri)); } if (isError) { response.sendError(((Integer) request.getAttribute( RequestDispatcher.ERROR_STATUS_CODE)).intValue()); } else { response.sendError(HttpServletResponse.SC_FORBIDDEN, requestUri); } return; } // If the resource is not a collection, and the resource path // ends with \"/\" or \"\\\", return NOT FOUND if (resource.isFile() && (path.endsWith(\"/\") || path.endsWith(\"\\\\\"))) { // Check if we're included so we can return the appropriate // missing resource name in the error String requestUri = (String) request.getAttribute( RequestDispatcher.INCLUDE_REQUEST_URI); if (requestUri == null) { requestUri = request.getRequestURI(); } response.sendError(HttpServletResponse.SC_NOT_FOUND, requestUri); return; } boolean included = false; // Check if the conditions specified in the optional If headers are // satisfied. if (resource.isFile()) { // Checking If headers included = (request.getAttribute( RequestDispatcher.INCLUDE_CONTEXT_PATH) != null); if (!included && !isError && !checkIfHeaders(request, response, resource)) { return; } } // Find content type. String contentType = resource.getMimeType(); if (contentType == null) { contentType = getServletContext().getMimeType(resource.getName()); resource.setMimeType(contentType); } // These need to reflect the original resource, not the potentially // precompressed version of the resource so get them now if they are going to // be needed later String eTag = null; String lastModifiedHttp = null; if (resource.isFile() && !isError) { eTag = resource.getETag(); lastModifiedHttp = resource.getLastModifiedHttp(); } // Serve a precompressed version of the file if present boolean usingPrecompressedVersion = false; if (compressionFormats.length > 0 && !included && resource.isFile() && !pathEndsWithCompressedExtension(path)) { List precompressedResources = getAvailablePrecompressedResources(path); if (!precompressedResources.isEmpty()) { Collection varyHeaders = response.getHeaders(\"Vary\"); boolean addRequired = true; for (String varyHeader : varyHeaders) { if (\"*\".equals(varyHeader) || \"accept-encoding\".equalsIgnoreCase(varyHeader)) { addRequired = false; break; } } if (addRequired) { response.addHeader(\"Vary\", \"accept-encoding\"); } PrecompressedResource bestResource = getBestPrecompressedResource(request, precompressedResources); if (bestResource != null) { response.addHeader(\"Content-Encoding\", bestResource.format.encoding); resource = bestResource.resource; usingPrecompressedVersion = true; } } } ArrayList ranges = null; long contentLength = -1L; if (resource.isDirectory()) { if (!path.endsWith(\"/\")) { doDirectoryRedirect(request, response); return; } // Skip directory listings if we have been configured to // suppress them if (!listings) { response.sendError(HttpServletResponse.SC_NOT_FOUND, request.getRequestURI()); return; } contentType = \"text/html;charset=UTF-8\"; } else { if (!isError) { if (useAcceptRanges) { // Accept ranges header response.setHeader(\"Accept-Ranges\", \"bytes\"); } // Parse range specifier ranges = parseRange(request, response, resource); // ETag header response.setHeader(\"ETag\", eTag); // Last-Modified header response.setHeader(\"Last-Modified\", lastModifiedHttp); } // Get content length contentLength = resource.getContentLength(); // Special case for zero length files, which would cause a // (silent) ISE when setting the output buffer size if (contentLength == 0L) { serveContent = false; } } ServletOutputStream ostream = null; PrintWriter writer = null; if (serveContent) { // Trying to retrieve the servlet output stream try { ostream = response.getOutputStream(); } catch (IllegalStateException e) { // If it fails, we try to get a Writer instead if we're // trying to serve a text file if (!usingPrecompressedVersion && ((contentType == null) || (contentType.startsWith(\"text\")) || (contentType.endsWith(\"xml\")) || (contentType.contains(\"/javascript\"))) ) { writer = response.getWriter(); // Cannot reliably serve partial content with a Writer ranges = FULL; } else { throw e; } } } // Check to see if a Filter, Valve of wrapper has written some content. // If it has, disable range requests and setting of a content length // since neither can be done reliably. ServletResponse r = response; long contentWritten = 0; while (r instanceof ServletResponseWrapper) { r = ((ServletResponseWrapper) r).getResponse(); } if (r instanceof ResponseFacade) { contentWritten = ((ResponseFacade) r).getContentWritten(); } if (contentWritten > 0) { ranges = FULL; } if (resource.isDirectory() || isError || ( (ranges == null || ranges.isEmpty()) && request.getHeader(\"Range\") == null ) || ranges == FULL ) { // Set the appropriate output headers if (contentType != null) { if (debug > 0) log(\"DefaultServlet.serveFile: contentType='\" + contentType + \"'\"); response.setContentType(contentType); } if (resource.isFile() && contentLength >= 0 && (!serveContent || ostream != null)) { if (debug > 0) log(\"DefaultServlet.serveFile: contentLength=\" + contentLength); // Don't set a content length if something else has already // written to the response. if (contentWritten == 0) { response.setContentLengthLong(contentLength); } } if (serveContent) { try { response.setBufferSize(output); } catch (IllegalStateException e) { // Silent catch } InputStream renderResult = null; if (ostream == null) { // Output via a writer so can't use sendfile or write // content directly. if (resource.isDirectory()) { renderResult = render(getPathPrefix(request), resource, encoding); } else { renderResult = resource.getInputStream(); } copy(resource, renderResult, writer, encoding); } else { // Output is via an InputStream if (resource.isDirectory()) { renderResult = render(getPathPrefix(request), resource, encoding); } else { // Output is content of resource if (!checkSendfile(request, response, resource, contentLength, null)) { // sendfile not possible so check if resource // content is available directly byte[] resourceBody = resource.getContent(); if (resourceBody == null) { // Resource content not available, use // inputstream renderResult = resource.getInputStream(); } else { // Use the resource content directly ostream.write(resourceBody); } } } // If a stream was configured, it needs to be copied to // the output (this method closes the stream) if (renderResult != null) { copy(resource, renderResult, ostream); } } } } else { if ((ranges == null) || (ranges.isEmpty())) return; // Partial content response. response.setStatus(HttpServletResponse.SC_PARTIAL_CONTENT); if (ranges.size() == 1) { Range range = ranges.get(0); response.addHeader(\"Content-Range\", \"bytes \" + range.start + \"-\" + range.end + \"/\" + range.length); long length = range.end - range.start + 1; response.setContentLengthLong(length); if (contentType != null) { if (debug > 0) log(\"DefaultServlet.serveFile: contentType='\" + contentType + \"'\"); response.setContentType(contentType); } if (serveContent) { try { response.setBufferSize(output); } catch (IllegalStateException e) { // Silent catch } if (ostream != null) { if (!checkSendfile(request, response, resource, range.end - range.start + 1, range)) copy(resource, ostream, range); } else { // we should not get here throw new IllegalStateException(); } } } else { response.setContentType(\"multipart/byteranges; boundary=\" + mimeSeparation); if (serveContent) { try { response.setBufferSize(output); } catch (IllegalStateException e) { // Silent catch } if (ostream != null) { copy(resource, ostream, ranges.iterator(), contentType); } else { // we should not get here throw new IllegalStateException(); } } } } } Mapper 在Tomcat中，当一个请求到达时，该请求最终由哪个Servlet来处理呢？这个任务是由Mapper路由映射器完成的。Mapper是由Service管理。存储结构 MapElement（基类) protected abstract static class MapElement { public final String name; public final T object; public MapElement(String name, T object) { this.name = name; this.object = object; } } MappedHost protected static final class MappedHost extends MapElement { public volatile ContextList contextList; /** * Link to the \"real\" MappedHost, shared by all aliases. */ private final MappedHost realHost; /** * Links to all registered aliases, for easy enumeration. This field * is available only in the \"real\" MappedHost. In an alias this field * is null. */ private final List aliases; /** * Constructor used for the primary Host * * @param name The name of the virtual host * @param host The host */ public MappedHost(String name, Host host) { super(name, host); realHost = this; contextList = new ContextList(); aliases = new CopyOnWriteArrayList<>(); } } MappedContext protected static final class MappedContext extends MapElement { public volatile ContextVersion[] versions; public MappedContext(String name, ContextVersion firstVersion) { super(name, null); this.versions = new ContextVersion[] { firstVersion }; } } 其中ContextVersion包含了Context下的所有Servlet，有多种映射方式，如精确的map，通配符的map，扩展名的map，如下： protected static final class ContextVersion extends MapElement { public final String path; public final int slashCount; public final WebResourceRoot resources; public String[] welcomeResources; public MappedWrapper defaultWrapper = null; // 精确匹配 public MappedWrapper[] exactWrappers = new MappedWrapper[0]; // 通配符匹配 public MappedWrapper[] wildcardWrappers = new MappedWrapper[0]; // 基于扩展名的匹配 public MappedWrapper[] extensionWrappers = new MappedWrapper[0]; public int nesting = 0; private volatile boolean paused; public ContextVersion(String version, String path, int slashCount, Context context, WebResourceRoot resources, String[] welcomeResources) { super(version, context); this.path = path; this.slashCount = slashCount; this.resources = resources; this.welcomeResources = welcomeResources; } } MappedWrapper protected static class MappedWrapper extends MapElement { public final boolean jspWildCard; public final boolean resourceOnly; public MappedWrapper(String name, Wrapper wrapper, boolean jspWildCard, boolean resourceOnly) { super(name, wrapper); this.jspWildCard = jspWildCard; this.resourceOnly = resourceOnly; } } Mapper 简单地说，Mapper中以数组的形式保存了host, context, wrapper, 且他们在数组中有序的，Mapper可以通过请求的url，通过二分法查找定位到wrapper。 public final class Mapper { private static final Log log = LogFactory.getLog(Mapper.class); private static final StringManager sm = StringManager.getManager(Mapper.class); // ----------------------------------------------------- Instance Variables /** * Array containing the virtual hosts definitions. */ // Package private to facilitate testing // host数组，host里面又包括了context和wrapper数组 volatile MappedHost[] hosts = new MappedHost[0]; /** * Default host name. */ private String defaultHostName = null; private volatile MappedHost defaultHost = null; /** * Mapping from Context object to Context version to support * RequestDispatcher mappings. */ private final Map contextObjectToContextVersionMap = new ConcurrentHashMap<>(); } Mapper#addHost public synchronized void addHost(String name, String[] aliases, Host host) { name = renameWildcardHost(name); MappedHost[] newHosts = new MappedHost[hosts.length + 1]; MappedHost newHost = new MappedHost(name, host); if (insertMap(hosts, newHosts, newHost)) { hosts = newHosts; if (newHost.name.equals(defaultHostName)) { defaultHost = newHost; } if (log.isDebugEnabled()) { log.debug(sm.getString(\"mapper.addHost.success\", name)); } } else { MappedHost duplicate = hosts[find(hosts, name)]; if (duplicate.object == host) { // The host is already registered in the mapper. // E.g. it might have been added by addContextVersion() if (log.isDebugEnabled()) { log.debug(sm.getString(\"mapper.addHost.sameHost\", name)); } newHost = duplicate; } else { log.error(sm.getString(\"mapper.duplicateHost\", name, duplicate.getRealHostName())); // Do not add aliases, as removeHost(hostName) won't be able to // remove them return; } } List newAliases = new ArrayList<>(aliases.length); for (String alias : aliases) { alias = renameWildcardHost(alias); MappedHost newAlias = new MappedHost(alias, newHost); if (addHostAliasImpl(newAlias)) { newAliases.add(newAlias); } } newHost.addAliases(newAliases); } Mapper#addContextVersion public void addContextVersion(String hostName, Host host, String path, String version, Context context, String[] welcomeResources, WebResourceRoot resources, Collection wrappers) { hostName = renameWildcardHost(hostName); MappedHost mappedHost = exactFind(hosts, hostName); if (mappedHost == null) { addHost(hostName, new String[0], host); mappedHost = exactFind(hosts, hostName); if (mappedHost == null) { log.error(\"No host found: \" + hostName); return; } } if (mappedHost.isAlias()) { log.error(\"No host found: \" + hostName); return; } int slashCount = slashCount(path); synchronized (mappedHost) { ContextVersion newContextVersion = new ContextVersion(version, path, slashCount, context, resources, welcomeResources); if (wrappers != null) { addWrappers(newContextVersion, wrappers); } ContextList contextList = mappedHost.contextList; MappedContext mappedContext = exactFind(contextList.contexts, path); if (mappedContext == null) { mappedContext = new MappedContext(path, newContextVersion); ContextList newContextList = contextList.addContext( mappedContext, slashCount); if (newContextList != null) { updateContextList(mappedHost, newContextList); contextObjectToContextVersionMap.put(context, newContextVersion); } } else { ContextVersion[] contextVersions = mappedContext.versions; ContextVersion[] newContextVersions = new ContextVersion[contextVersions.length + 1]; if (insertMap(contextVersions, newContextVersions, newContextVersion)) { mappedContext.versions = newContextVersions; contextObjectToContextVersionMap.put(context, newContextVersion); } else { // Re-registration after Context.reload() // Replace ContextVersion with the new one int pos = find(contextVersions, version); if (pos >= 0 && contextVersions[pos].name.equals(version)) { contextVersions[pos] = newContextVersion; contextObjectToContextVersionMap.put(context, newContextVersion); } } } } } Mapper#addWrapper public void addWrapper(String hostName, String contextPath, String version, String path, Wrapper wrapper, boolean jspWildCard, boolean resourceOnly) { hostName = renameWildcardHost(hostName); ContextVersion contextVersion = findContextVersion(hostName, contextPath, version, false); if (contextVersion == null) { return; } addWrapper(contextVersion, path, wrapper, jspWildCard, resourceOnly); } protected void addWrapper(ContextVersion context, String path, Wrapper wrapper, boolean jspWildCard, boolean resourceOnly) { synchronized (context) { if (path.endsWith(\"/*\")) { // Wildcard wrapper String name = path.substring(0, path.length() - 2); MappedWrapper newWrapper = new MappedWrapper(name, wrapper, jspWildCard, resourceOnly); MappedWrapper[] oldWrappers = context.wildcardWrappers; MappedWrapper[] newWrappers = new MappedWrapper[oldWrappers.length + 1]; if (insertMap(oldWrappers, newWrappers, newWrapper)) { context.wildcardWrappers = newWrappers; int slashCount = slashCount(newWrapper.name); if (slashCount > context.nesting) { context.nesting = slashCount; } } } else if (path.startsWith(\"*.\")) { // Extension wrapper String name = path.substring(2); MappedWrapper newWrapper = new MappedWrapper(name, wrapper, jspWildCard, resourceOnly); MappedWrapper[] oldWrappers = context.extensionWrappers; MappedWrapper[] newWrappers = new MappedWrapper[oldWrappers.length + 1]; if (insertMap(oldWrappers, newWrappers, newWrapper)) { context.extensionWrappers = newWrappers; } } else if (path.equals(\"/\")) { // Default wrapper MappedWrapper newWrapper = new MappedWrapper(\"\", wrapper, jspWildCard, resourceOnly); context.defaultWrapper = newWrapper; } else { // Exact wrapper final String name; if (path.length() == 0) { // Special case for the Context Root mapping which is // treated as an exact match name = \"/\"; } else { name = path; } MappedWrapper newWrapper = new MappedWrapper(name, wrapper, jspWildCard, resourceOnly); MappedWrapper[] oldWrappers = context.exactWrappers; MappedWrapper[] newWrappers = new MappedWrapper[oldWrappers.length + 1]; if (insertMap(oldWrappers, newWrappers, newWrapper)) { context.exactWrappers = newWrappers; } } } } Mapper#find（查找MapElement) // 根据name，查找一个MapElement（host, context, 或者wrapper) /** * Find a map element given its name in a sorted array of map elements. * This will return the index for the closest inferior or equal item in the * given array. */ private static final int find(MapElement[] map, CharChunk name) { return find(map, name, name.getStart(), name.getEnd()); } /** * Find a map element given its name in a sorted array of map elements. * This will return the index for the closest inferior or equal item in the * given array. */ private static final int find(MapElement[] map, CharChunk name, int start, int end) { int a = 0; int b = map.length - 1; // Special cases: -1 and 0 if (b == -1) { return -1; } if (compare(name, start, end, map[0].name) >> 1; int result = compare(name, start, end, map[i].name); if (result == 1) { a = i; } else if (result == 0) { return i; } else { b = i; } if ((b - a) == 1) { int result2 = compare(name, start, end, map[b].name); if (result2 /** * Compare given char chunk with String. * Return -1, 0 or +1 if inferior, equal, or superior to the String. */ private static final int compare(CharChunk name, int start, int end, String compareTo) { int result = 0; char[] c = name.getBuffer(); int len = compareTo.length(); if ((end - start) compareTo.charAt(i)) { result = 1; } else if (c[i + start] (end - start)) { result = -1; } else if (compareTo.length() Mapper#exactFind（精确查找MapElement) private static final > E exactFind(E[] map, String name) { int pos = find(map, name); if (pos >= 0) { E result = map[pos]; if (name.equals(result.name)) { return result; } } return null; } Mapper#map public void map(MessageBytes host, MessageBytes uri, String version, MappingData mappingData) throws IOException { if (host.isNull()) { host.getCharChunk().append(defaultHostName); } host.toChars(); uri.toChars(); internalMap(host.getCharChunk(), uri.getCharChunk(), version, mappingData); } MappingData是Request中的域internalMap（查找host和context) private final void internalMap(CharChunk host, CharChunk uri, String version, MappingData mappingData) throws IOException { if (mappingData.host != null) { // The legacy code (dating down at least to Tomcat 4.1) just // skipped all mapping work in this case. That behaviour has a risk // of returning an inconsistent result. // I do not see a valid use case for it. throw new AssertionError(); } // Virtual host mapping MappedHost[] hosts = this.hosts; MappedHost mappedHost = exactFindIgnoreCase(hosts, host); if (mappedHost == null) { // Note: Internally, the Mapper does not use the leading * on a // wildcard host. This is to allow this shortcut. int firstDot = host.indexOf('.'); if (firstDot > -1) { int offset = host.getOffset(); try { host.setOffset(firstDot + offset); mappedHost = exactFindIgnoreCase(hosts, host); } finally { // Make absolutely sure this gets reset host.setOffset(offset); } } if (mappedHost == null) { mappedHost = defaultHost; if (mappedHost == null) { return; } } } - // 设置host mappingData.host = mappedHost.object; if (uri.isNull()) { // Can't map context or wrapper without a uri return; } uri.setLimit(-1); // Context mapping ContextList contextList = mappedHost.contextList; MappedContext[] contexts = contextList.contexts; int pos = find(contexts, uri); if (pos == -1) { return; } int lastSlash = -1; int uriEnd = uri.getEnd(); int length = -1; boolean found = false; MappedContext context = null; while (pos >= 0) { context = contexts[pos]; if (uri.startsWith(context.name)) { length = context.name.length(); if (uri.getLength() == length) { found = true; break; } else if (uri.startsWithIgnoreCase(\"/\", length)) { found = true; break; } } if (lastSlash == -1) { lastSlash = nthSlash(uri, contextList.nesting + 1); } else { lastSlash = lastSlash(uri); } uri.setEnd(lastSlash); pos = find(contexts, uri); } uri.setEnd(uriEnd); if (!found) { if (contexts[0].name.equals(\"\")) { context = contexts[0]; } else { context = null; } } if (context == null) { return; } mappingData.contextPath.setString(context.name); ContextVersion contextVersion = null; ContextVersion[] contextVersions = context.versions; final int versionCount = contextVersions.length; if (versionCount > 1) { Context[] contextObjects = new Context[contextVersions.length]; for (int i = 0; i } internalMapWrapper（查找Wrapper) private final void internalMapWrapper(ContextVersion contextVersion, CharChunk path, MappingData mappingData) throws IOException { int pathOffset = path.getOffset(); int pathEnd = path.getEnd(); boolean noServletPath = false; int length = contextVersion.path.length(); if (length == (pathEnd - pathOffset)) { noServletPath = true; } int servletPath = pathOffset + length; path.setOffset(servletPath); // Rule 1 -- Exact Match MappedWrapper[] exactWrappers = contextVersion.exactWrappers; internalMapExactWrapper(exactWrappers, path, mappingData); // Rule 2 -- Prefix Match boolean checkJspWelcomeFiles = false; MappedWrapper[] wildcardWrappers = contextVersion.wildcardWrappers; if (mappingData.wrapper == null) { internalMapWildcardWrapper(wildcardWrappers, contextVersion.nesting, path, mappingData); if (mappingData.wrapper != null && mappingData.jspWildCard) { char[] buf = path.getBuffer(); if (buf[pathEnd - 1] == '/') { /* * Path ending in '/' was mapped to JSP servlet based on * wildcard match (e.g., as specified in url-pattern of a * jsp-property-group. * Force the context's welcome files, which are interpreted * as JSP files (since they match the url-pattern), to be * considered. See Bugzilla 27664. */ mappingData.wrapper = null; checkJspWelcomeFiles = true; } else { // See Bugzilla 27704 mappingData.wrapperPath.setChars(buf, path.getStart(), path.getLength()); mappingData.pathInfo.recycle(); } } } if(mappingData.wrapper == null && noServletPath && contextVersion.object.getMapperContextRootRedirectEnabled()) { // The path is empty, redirect to \"/\" path.append('/'); pathEnd = path.getEnd(); mappingData.redirectPath.setChars (path.getBuffer(), pathOffset, pathEnd - pathOffset); path.setEnd(pathEnd - 1); return; } // Rule 3 -- Extension Match MappedWrapper[] extensionWrappers = contextVersion.extensionWrappers; if (mappingData.wrapper == null && !checkJspWelcomeFiles) { internalMapExtensionWrapper(extensionWrappers, path, mappingData, true); } // Rule 4 -- Welcome resources processing for servlets if (mappingData.wrapper == null) { boolean checkWelcomeFiles = checkJspWelcomeFiles; if (!checkWelcomeFiles) { char[] buf = path.getBuffer(); checkWelcomeFiles = (buf[pathEnd - 1] == '/'); } if (checkWelcomeFiles) { for (int i = 0; (i internalMapExactWrapper（URL精确匹配) private final void internalMapExactWrapper (MappedWrapper[] wrappers, CharChunk path, MappingData mappingData) { MappedWrapper wrapper = exactFind(wrappers, path); if (wrapper != null) { mappingData.requestPath.setString(wrapper.name); mappingData.wrapper = wrapper.object; if (path.equals(\"/\")) { // Special handling for Context Root mapped servlet mappingData.pathInfo.setString(\"/\"); mappingData.wrapperPath.setString(\"\"); // This seems wrong but it is what the spec says... mappingData.contextPath.setString(\"\"); mappingData.matchType = MappingMatch.CONTEXT_ROOT; } else { mappingData.wrapperPath.setString(wrapper.name); mappingData.matchType = MappingMatch.EXACT; } } } private static final > E exactFind(E[] map, CharChunk name) { int pos = find(map, name); if (pos >= 0) { E result = map[pos]; if (name.equals(result.name)) { return result; } } return null; } Tomcat类加载器 Tomcat不能直接使用系统的类加载器，必须要实现自定义的类加载器。servlet应该只允许加载WEB-INF/classes目录及其子目录下的类，和从部署的库到WEB-INF/lib目录加载类，实现不同的应用之间的隔离。另一个要实现自定义类加载器的原因是，为了提供热加载的功能。如果WEB-INF/classes或WEB-INF/lib目录下的类发生变化时，Tomcat应该会重新加载这些类。在Tomcat的类加载中，类加载使用一个额外的线程，不断检查servlet类和其他类的文件的时间戳。Tomcat所有类加载器必须实现Loader接口，支持热加载的还需要实现Reloader接口。 Tomcat类加载器 commonLoader、catalinaLoader和sharedLoader是在Tomcat容器初始化时创建的。catalinaLoader会被设置为Tomcat主线程的线程上下文类加载器，并且使用catalinaLoader加载Tomcat容器自身的class。 它们三个都是URLClassLoader类的一个实例，只是它们的类加载路径不一样，在tomcat/conf/catalina.properties配置文件中配置 (common.loader,server.loader,shared.loader).应用隔离 对于每个webapp应用，都会对应唯一的StandContext，在StandContext中会引用WebappLoader，该类又会引用WebappClassLoader，WebappClassLoader就是真正加载webapp的classloader。 WebappClassLoader加载class的步骤如下： 1.- 先检查webappclassloader的缓存是否有该类 2.- 为防止webapp覆盖java se类，尝试用application classloader（应用类加载器)加载 3.- 尝试WebappClassLoader自己加载class 4.- 最后无条件地委托给父加载器 common classloader，加载CATALINA_HOME/lib下的类 5.- 如果都没有加载成功，则抛出ClassNotFoundException异常 WebappClassLoader#loadClass 不同的StandardContext有不同的WebappClassLoader，那么不同的webapp的类加载器就是不一致的。加载器的不一致带来了名称空间不一致，所以webapp之间是相互隔离的。 public Class loadClass(String name) throws ClassNotFoundException { return loadClass(name, false); } public Class loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { if (log.isDebugEnabled()) log.debug(\"loadClass(\" + name + \", \" + resolve + \")\"); Class clazz = null; // Log access to stopped class loader checkStateForClassLoading(name); // (0) Check our previously loaded local class cache clazz = findLoadedClass0(name); if (clazz != null) { if (log.isDebugEnabled()) log.debug(\" Returning class from cache\"); if (resolve) resolveClass(clazz); return clazz; } // (0.1) Check our previously loaded class cache clazz = findLoadedClass(name); if (clazz != null) { if (log.isDebugEnabled()) log.debug(\" Returning class from cache\"); if (resolve) resolveClass(clazz); return clazz; } // (0.2) Try loading the class with the system class loader, to prevent // the webapp from overriding Java SE classes. This implements // SRV.10.7.2 String resourceName = binaryNameToPath(name, false); ClassLoader javaseLoader = getJavaseClassLoader(); boolean tryLoadingFromJavaseLoader; try { // Use getResource as it won't trigger an expensive // ClassNotFoundException if the resource is not available from // the Java SE class loader. However (see // https://bz.apache.org/bugzilla/show_bug.cgi?id=58125 for // details) when running under a security manager in rare cases // this call may trigger a ClassCircularityError. // See https://bz.apache.org/bugzilla/show_bug.cgi?id=61424 for // details of how this may trigger a StackOverflowError // Given these reported errors, catch Throwable to ensure any // other edge cases are also caught tryLoadingFromJavaseLoader = (javaseLoader.getResource(resourceName) != null); } catch (Throwable t) { // Swallow all exceptions apart from those that must be re-thrown ExceptionUtils.handleThrowable(t); // The getResource() trick won't work for this class. We have to // try loading it directly and accept that we might get a // ClassNotFoundException. tryLoadingFromJavaseLoader = true; } // 使用System ClassLoader加载J2SE的类 if (tryLoadingFromJavaseLoader) { try { clazz = javaseLoader.loadClass(name); if (clazz != null) { if (resolve) resolveClass(clazz); return clazz; } } catch (ClassNotFoundException e) { // Ignore } } // (0.5) Permission to access this class when using a SecurityManager if (securityManager != null) { int i = name.lastIndexOf('.'); if (i >= 0) { try { securityManager.checkPackageAccess(name.substring(0,i)); } catch (SecurityException se) { String error = \"Security Violation, attempt to use \" + \"Restricted Class: \" + name; log.info(error, se); throw new ClassNotFoundException(error, se); } } } boolean delegateLoad = delegate || filter(name, true); // (1) Delegate to our parent if requested if (delegateLoad) { if (log.isDebugEnabled()) log.debug(\" Delegating to parent classloader1 \" + parent); try { clazz = Class.forName(name, false, parent); if (clazz != null) { if (log.isDebugEnabled()) log.debug(\" Loading class from parent\"); if (resolve) resolveClass(clazz); return clazz; } } catch (ClassNotFoundException e) { // Ignore } } // (2) Search local repositories if (log.isDebugEnabled()) log.debug(\" Searching local repositories\"); try { clazz = findClass(name); if (clazz != null) { if (log.isDebugEnabled()) log.debug(\" Loading class from local repository\"); if (resolve) resolveClass(clazz); return clazz; } } catch (ClassNotFoundException e) { // Ignore } // (3) Delegate to parent unconditionally if (!delegateLoad) { if (log.isDebugEnabled()) log.debug(\" Delegating to parent classloader at end: \" + parent); try { clazz = Class.forName(name, false, parent); if (clazz != null) { if (log.isDebugEnabled()) log.debug(\" Loading class from parent\"); if (resolve) resolveClass(clazz); return clazz; } } catch (ClassNotFoundException e) { // Ignore } } } throw new ClassNotFoundException(name); } 热部署 后台的定期检查，该定期检查是StandardContext的一个后台线程，会做reload的check，过期session清理等等，这里的modified实际上调用了WebappClassLoader中的方法以判断这个class是不是已经修改。注意到它调用了StandardContext的reload方法。 StandardContext#backgroundProcess public void backgroundProcess() { if (!getState().isAvailable()) return; Loader loader = getLoader(); if (loader != null) { try { loader.backgroundProcess(); } catch (Exception e) { log.warn(sm.getString( \"standardContext.backgroundProcess.loader\", loader), e); } } Manager manager = getManager(); if (manager != null) { try { manager.backgroundProcess(); } catch (Exception e) { log.warn(sm.getString( \"standardContext.backgroundProcess.manager\", manager), e); } } WebResourceRoot resources = getResources(); if (resources != null) { try { resources.backgroundProcess(); } catch (Exception e) { log.warn(sm.getString( \"standardContext.backgroundProcess.resources\", resources), e); } } InstanceManager instanceManager = getInstanceManager(); if (instanceManager != null) { try { instanceManager.backgroundProcess(); } catch (Exception e) { log.warn(sm.getString( \"standardContext.backgroundProcess.instanceManager\", resources), e); } } super.backgroundProcess(); } WebappLoader#backgroundProcess public void backgroundProcess() { if (reloadable && modified()) { try { Thread.currentThread().setContextClassLoader (WebappLoader.class.getClassLoader()); if (context != null) { context.reload(); } } finally { if (context != null && context.getLoader() != null) { Thread.currentThread().setContextClassLoader (context.getLoader().getClassLoader()); } } } } StandardContext#reload Tomcat lifecycle中标准的启停方法stop和start，别忘了，start方法会重新造一个WebappClassLoader并且重复loadOnStartup的过程，从而重新加载了webapp中的类，注意到一般应用很大时，热部署通常会报outofmemory: permgen space not enough之类的，这是由于之前加载进来的class还没有清除而方法区内存又不够的原因 public synchronized void reload() { // Validate our current component state if (!getState().isAvailable()) throw new IllegalStateException (sm.getString(\"standardContext.notStarted\", getName())); if(log.isInfoEnabled()) log.info(sm.getString(\"standardContext.reloadingStarted\", getName())); // Stop accepting requests temporarily. setPaused(true); try { stop(); } catch (LifecycleException e) { log.error( sm.getString(\"standardContext.stoppingContext\", getName()), e); } try { start(); } catch (LifecycleException e) { log.error( sm.getString(\"standardContext.startingContext\", getName()), e); } setPaused(false); if(log.isInfoEnabled()) log.info(sm.getString(\"standardContext.reloadingCompleted\", getName())); } 异步Servlet 入口点是Request#startAsync Request#startAsync（开启异步上下文，之后Tomct回收Worker线程) public AsyncContext startAsync() { return startAsync(getRequest(),response.getResponse()); } public AsyncContext startAsync(ServletRequest request, ServletResponse response) { if (!isAsyncSupported()) { IllegalStateException ise = new IllegalStateException(sm.getString(\"request.asyncNotSupported\")); log.warn(sm.getString(\"coyoteRequest.noAsync\", StringUtils.join(getNonAsyncClassNames())), ise); throw ise; } if (asyncContext == null) { asyncContext = new AsyncContextImpl(this); } asyncContext.setStarted(getContext(), request, response, request==getRequest() && response==getResponse().getResponse()); asyncContext.setTimeout(getConnector().getAsyncTimeout()); return asyncContext; } - 1) AsyncContextImpl#construactor 成员变量 Tomcat工作线程在Request#startAsync之后，把该异步servlet的后续代码执行完毕后，Tomcat工作线程直接就结束了，也就是返回线程池中了，相当于线程根本不会保存记录信息。 public class AsyncContextImpl implements AsyncContext, AsyncContextCallback { private static final Log log = LogFactory.getLog(AsyncContextImpl.class); protected static final StringManager sm = StringManager.getManager(Constants.Package); /* When a request uses a sequence of multiple start(); dispatch() with * non-container threads it is possible for a previous dispatch() to * interfere with a following start(). This lock prevents that from * happening. It is a dedicated object as user code may lock on the * AsyncContext so if container code also locks on that object deadlocks may * occur. */ private final Object asyncContextLock = new Object(); private volatile ServletRequest servletRequest = null; private volatile ServletResponse servletResponse = null; private final List listeners = new ArrayList<>(); private boolean hasOriginalRequestAndResponse = true; private volatile Runnable dispatch = null; private Context context = null; // Default of 30000 (30s) is set by the connector private long timeout = -1; private AsyncEvent event = null; private volatile Request request; private volatile InstanceManager instanceManager; } public AsyncContextImpl(Request request) { if (log.isDebugEnabled()) { logDebug(\"Constructor\"); } this.request = request; } - 2) AsyncContextImpl#setStarted public void setStarted(Context context, ServletRequest request, ServletResponse response, boolean originalRequestResponse) { synchronized (asyncContextLock) { this.request.getCoyoteRequest().action( ActionCode.ASYNC_START, this); this.context = context; this.servletRequest = request; this.servletResponse = response; this.hasOriginalRequestAndResponse = originalRequestResponse; this.event = new AsyncEvent(this, request, response); List listenersCopy = new ArrayList<>(); listenersCopy.addAll(listeners); listeners.clear(); for (AsyncListenerWrapper listener : listenersCopy) { try { listener.fireOnStartAsync(event); } catch (Throwable t) { ExceptionUtils.handleThrowable(t); log.warn(\"onStartAsync() failed for listener of type [\" + listener.getClass().getName() + \"]\", t); } } } } AbstractProcessor#action case ASYNC_START: { asyncStateMachine.asyncStart((AsyncContextCallback) param); break; } AsyncStateMachine#asyncStart synchronized void asyncStart(AsyncContextCallback asyncCtxt) { if (state == AsyncState.DISPATCHED) { state = AsyncState.STARTING; this.asyncCtxt = asyncCtxt; lastAsyncStart = System.currentTimeMillis(); } else { throw new IllegalStateException( sm.getString(\"asyncStateMachine.invalidAsyncState\", \"asyncStart()\", state)); } } 3) AsyncContextImpl#setTimeout public void setTimeout(long timeout) { check(); this.timeout = timeout; request.getCoyoteRequest().action(ActionCode.ASYNC_SETTIMEOUT, Long.valueOf(timeout)); } AsyncContext#complete（结束) public void complete() { if (log.isDebugEnabled()) { logDebug(\"complete \"); } check(); request.getCoyoteRequest().action(ActionCode.ASYNC_COMPLETE, null); } case ASYNC_COMPLETE: { clearDispatches(); if (asyncStateMachine.asyncComplete()) { processSocketEvent(SocketEvent.OPEN_READ, true); } break; } protected void processSocketEvent(SocketEvent event, boolean dispatch) { SocketWrapperBase socketWrapper = getSocketWrapper(); if (socketWrapper != null) { socketWrapper.processSocket(event, dispatch); } } public void processSocket(SocketEvent socketStatus, boolean dispatch) { endpoint.processSocket(this, socketStatus, dispatch); } - 见2.2.1) AbstractEndpoint#processSocket 相当于重新开启一个工作线程，这个工作线程带着SocketWrapper，又来一遍容器的流程，而这一遍的流程，因为Servlet已经处理过，所以会略过servlet的执行直接将后续的处理走完，包括最后response的收尾，对象的清空等等。 但是异步Servlet此时不会重新跑一次Servlet，直接跳到response收尾。 AsyncContext#dispatch（转发) "},"zother5-Java-Interview/五、JVM.html":{"url":"zother5-Java-Interview/五、JVM.html","title":"五、JVM","keywords":"","body":"JVM 运行时数据区 Java运行时数据区有 堆 ，本地方法栈，虚拟机栈，程序计数器，方法区（运行时常量池，属性和方法数据，代码区) Java虚拟机在执行Java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。Java虚拟机所管理的内存将会包括以下集合运行时数据区域： 5.1 程序计数器（线程独享) 程序计数器（Program Counter Register)是一块很小的内存区域，它可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型中，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式实现的，在任何一个确定的时刻，一个处理器都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们称这类内存区域为线程私有的内存。 如果线程正在执行的是Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Native方法， 这个计数器的值为空。此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。5.2 虚拟机栈（线程独享) Java虚拟机栈也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧（StackFrame)用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用到执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。5.3 本地方法栈（线程独享) 本地方法栈（Native Method Stack)与虚拟机栈所发挥的作用是非常相似的，它们之间的区别不过是虚拟机栈为虚拟机执行Java方法（字节码)服务，而本地方法栈为虚拟机使用到的Native方法服务。5.4 Java堆 对大多数应用来说，Java堆是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例。 Java堆是垃圾收集器管理的主要区域。从内存回收的角度来看，由于现在收集器基本都采用分代收集算法，所以Java堆中还可以分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。从内存分配的角度来看，线程共享的Java堆中可能划分出多个线程私有的分配缓冲区（Thread Local Allocation Buffer，TLAB)。 Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可。5.5 方法区 方法区（Method Area)与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆)，目的应该是与Java堆区分开来。 “PermGen space”是方法区。不过方法区和“PermGen space”又有着本质的区别。前者是 JVM 的规范，而后者则是 JVM 规范的一种实现，并且只有 HotSpot 才有 “PermGen space”。 HotSpot虚拟机将GC分代收集拓展至方法区，或者说使用永久代来实现方法区。这样的HotSpot的垃圾收集器可以像管理Java堆一样管理这部分内存，能够省去专门为方法区编写内存管理代码的工作。如果实现方法区属于虚拟机实现细节，不受虚拟机规范约束，但是用永久代实现方法区，并不是一个好主意，因为这样容易遇到内存溢出问题。 垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区就永久存在了。这区域的内存回收目标主要是针对常量池的回收和对类型的卸载。 在Java8中，永久代被删除，方法区的HotSpot的实现为Metaspace元数据区，不放在虚拟机中而放在本地内存中，存储类的元信息； 而将类的静态变量（放在Class对象中)和运行时常量池放在堆中。 为什么？ 1)移除永久代是为融合HotSpot JVM与 JRockit VM而做出的努力，因为JRockit没有永久代，不需要配置永久代。 2)现实使用中易出问题 由于永久代内存经常不够用或发生内存泄露，出现异常java.lang.OutOfMemoryError: PermGen运行时常量池 运行时常量池（Runtime Constant Pool)是方法区的一部分。Class文件中除了有关的描述信息外，还有一项信息是常量池，用于存放编译器生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。一般来说，除了保存Class文件中描述的符号引用外，还会把翻译出来的直接引用也存储在运行时常量池。 运行时常量池相对于Class文件常量池的一个重要特征是具备动态性，Java语言并不要求常量一定只有编译期才能产生，运行期间也可能将新的常量放入池中，比如String类的intern方法。5.6 直接内存 直接内存（Direct Memory)并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域，但这部分内存也被频繁使用。JDK的NIO类，引入了一种基于通道和缓冲区的IO方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在堆中的DirectByteBuffer对象作为这块内存的引用进行操作。这样能在一些场合显著提高性能，避免了在Java堆和Native堆来回复制数据。 直接内存的分配不会受到Java堆大小的限制，但会受到本机总内存的限制。对象 5.7 对象的创建 虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那就执行类加载过程。 在类加载检查通过后，虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来。假设Java堆中内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间那边挪动一块与对象大小相等的距离，这种分配方式称为指针碰撞。如果Java堆中的内存不是规整的，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为空闲列表（Free List)。选择哪种分配方式由Java堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 除如何划分可用空间之外，还有另外一个需要考虑的问题是对象创建在虚拟机中是非常频繁的行为，即使是仅仅修改一个指针所指向的位置，在并发情况下也不是线程安全的，可能出现正在给对象A分配内存，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况。解决这个问题有两个方案，一种是对分配内存空间的动作进行同步处理，另一种是把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲TLAB。哪个线程分配内存，就在哪个线程的TLAB上分配，只有TLAB用完并分配新的TLAB时，才需要同步锁定。 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值。接下来，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何找到类的元数据等信息。这些信息存放在对象的对象头之中。上述工作完成后，从虚拟机的视角来看，一个新的对象已经产生，但从Java程序的视角来看，构造方法还没有执行，字段都还为0。所以执行new指令之后会接着执行构造方法等，这样一个对象才算真正产生出来。5.8 对象的内存布局 在HotSpot虚拟机中，对象在内存中存储的布局可以分为3个区域：对象头（Header)、实例数据（Instance Data)和对齐填充（Padding)。 对象头包括两部分信息，第一部分用于存储对象自身的运行时数据，如哈希码、GC分代年龄、锁状态标志等。对象头的另一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 实例数据是对象真正存储的有效信息，也是在程序代码中所定义的各种类型的字段内容。无论是从父类继承的，还是在子类中定义的，都需要记录下来。相同宽度的字段总是被分配到一起，在这个前提下，在父类中定义的变量会出现在子类之前。 对齐填充并不是必然存在的，它仅仅起着占位符的作用，HotSpot虚拟机的自动内存管理系统要求对象起始地址必须是8字节的整数倍，即对象大小必须是8字节的整数倍，而对象头正好是8字节的整数倍。因此，当对象实例数据部分没有对齐时，就需要对齐填充来补全。5.9 对象的访问定位 Java程序需要通过栈上的Reference数据来操作堆上的具体对象。由于Reference类型在Java虚拟机规范中只规定了一个指向对象的引用，并没有定义这个引用应该通过何种方式来定位、访问堆中对象的具体位置，所以对象访问方式也是取决于虚拟机实现而定的。目标主流的方式有使用句柄和直接指针两种。 如果使用句柄访问的话，那么Java堆中将会划分出一块内存来作为句柄池，Reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据和类型数据各自的具体地址信息。 如果使用直接指针访问，那么Java堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而Reference中存储的直接就是对象地址。 使用句柄来访问的最大好处就是Reference中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而Reference本身不需要修改。 使用直接指针访问方式的最大好处就是速度更快，它节省了一次指针定位的时间开销。 内存溢出与内存泄露 OOM ；方法区OOM时的异常；查看dump 文件，怎么查看，具体命令记得吗，答jstack 具体怎么用的5.10 堆溢出 Java堆用于存储对象实例，只要不断增加对象，并且保证GC Roots到对象之间有可达路径来避免垃圾回收机制清除这些对象，那么在对象数量达到最大堆的容量限制后就会产生OOM异常。 public class HeapOOM { static class OOMObject{ } public static void main(String[] args) { List list = new ArrayList<>(); while(true){ list.add(new OOMObject()); } } } VM Options: -Xms20m -Xmx20m -XX:+HeapDumpOnOutOfMemoryError java.lang.OutOfMemoryError: Java heap space Dumping heap to java_pid15080.hprof ... Heap dump file created [28193498 bytes in 0.125 secs] Exception in thread \"main\" java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf(Arrays.java:3210) at java.util.Arrays.copyOf(Arrays.java:3181) at java.util.ArrayList.grow(ArrayList.java:261) at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:235) at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:227) at java.util.ArrayList.add(ArrayList.java:458) at cn.sinjinsong.se.review.oom.HeapOOM.main(HeapOOM.java:17) 要解决这个区域的异常，一般的手段是通过内存映像分析工具对dump出来的堆转储快照进行分析，重点是确认内存中的对象是否是必要的，也就是要判断是出现来内存泄露还是内存溢出。前者的话要进一步通过工具查看泄露对象到GC Roots的引用链；后者的话可以调大虚拟机的堆参数（-Xms和-Xmx)，或者从代码上检查某些对象生命周期过长等。 5.11 栈溢出（虚拟机栈和本地方法栈) 对于HotSpot来说，虽然-Xoss参数（设置本地方法栈大小)存在，但实际上是无效的，栈容量只由-Xss参数设定。关于虚拟机栈和本地方法栈，在JVM规范中描述了两种异常： 1) 如果线程请求的栈深度大于虚拟机所允许的最大深度，将抛出StackOverflowError异常。 2)如果虚拟机在扩展栈时无法申请到足够的内存空间，将抛出OutOfMemoryError异常。 public class StackSOF { private int stackLength = -1; public void stackLeak() { stackLength++; stackLeak(); } public static void main(String[] args) { StackSOF sof = new StackSOF(); try { sof.stackLeak(); } catch (Throwable e) { System.out.println(\"stack length:\" + sof.stackLength); throw e; } } } -Xss128k（设置栈容量) stack length:998 Exception in thread \"main\" java.lang.StackOverflowError at cn.sinjinsong.se.review.oom.StackSOF.stackLeak(StackSOF.java:10) at cn.sinjinsong.se.review.oom.StackSOF.stackLeak(StackSOF.java:11) ... 操作系统分配给每个进程的内存是有限制的，每个线程分配到的栈容量越大，可以建立的线程数量自然就越少，建立线程时就越容易把剩下的内存耗尽。 如果线程过多导致SOF，可以通过减少最大堆和减少栈容量来换取更多的线程。 public class StackSOFByThread { public void stackLeakByThread() { while(true) { new Thread(() -> { while (true){} }).start(); } } public static void main(String[] args) { new StackSOFByThread().stackLeakByThread(); } } 5.12 方法区溢出 注意Java8下运行时常量池在堆中，所以运行时常量池过大会体现为OOM：heap； 而在此以前是放在永久代中，体现为OOM：PermGen space。 public class RuntimeConstantPoolOOM { public static void main(String[] args) { List list = new ArrayList<>(); int i = 0; while (true) { list.add(String.valueOf(i++).intern()); } } } VM Options: -Xms20m -Xmx20m Exception in thread \"main\" java.lang.OutOfMemoryError: GC overhead limit exceeded at java.lang.Integer.toString(Integer.java:401) at java.lang.String.valueOf(String.java:3099) at cn.sinjinsong.se.review.oom.RuntimeConstantPoolOOM.main(RuntimeConstantPoolOOM.java:15) 方法区还存放Class的相关信息，运行时产生大量的类也会导致方法区（Java8中放在直接内存中)溢出。 public class MetaspaceOOM { public static void main(String[] args) { while(true){ Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(HeapOOM.OOMObject.class); enhancer.setUseCache(false); enhancer.setCallback(new MethodInterceptor() { @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable { return proxy.invokeSuper(obj,args); } }); enhancer.create(); } } } VM Options: -XX:MetaspaceSize=10m -XX:MaxMetaspaceSize=10m Caused by: java.lang.OutOfMemoryError: Metaspace at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) ... 11 more 方法区溢出也是一种常见的内存溢出异常，一个类被GC，判定条件是比较苛刻的。在经常生成大量Class的应用中，需要特别注意类的回收情况。这类场景除了动态代理生成类和动态语言外，还有：大量使用JSP、基于OSGi的应用。 5.13 直接内存溢出 直接内存可以使用-XX:MaxDirectMemorySize指定，如果不指定，则默认与Java堆最大值相同。 虽然使用DirectByteBuffer分配内存也会抛出OOM异常，但它抛出异常时并没有真正向OS申请分配内存，而是通过计算得知内存无法分配，于是手动抛出异常。 真正申请内存的方法是unsafe.allocateMemory()。 public class DirectMemoryOOM { private static final int _1MB = 1024 * 1024; public static void main(String[] args) throws IllegalAccessException { Field unsafeField = Unsafe.class.getDeclaredFields()[0]; unsafeField.setAccessible(true); Unsafe unsafe = (Unsafe) unsafeField.get(null); while(true) { unsafe.allocateMemory(_1MB); } } } VM Options: -XX:MaxDirectMemorySize=10m Exception in thread \"main\" java.lang.OutOfMemoryError at sun.misc.Unsafe.allocateMemory(Native Method) at cn.sinjinsong.se.review.oom.DirectMemoryOOM.main(DirectMemoryOOM.java:19) 5.14 内存泄露 - 1)非静态内部类 - 2)连接未关闭：比如数据库连接（dataSourse.getConnection())，网络连接(socket)和io连接，除非其显式的调用了其close（)方法将其连接关闭，否则是不会自动被GC 回收的。 GC 5.15 对象是否存活 引用计数算法 很多教科书判断对象是否存活的算法是这样的：给对象添加一个引用计数器，每当有一个地方引用它时，计数器就加1；当引用失效时，计数器值就减1；任何时刻计算器为0的对象就是不可能再被使用的。 主流的Java虚拟机中没有选用计数算法来管理内存，最主要的原因是它很难就解决对象之间相互循环引用的问题。 可达性分析算法 主流的商用程序语言的主流实现中，都是称通过可达性分析（Reachability Analysis)来判定对象是否存活的。这个算法的基本思路就是通过一系列的称为GC Roots 的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain)，当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可达的。下图章，对象object5、object6、object7虽然互相有关联，但是它们到GC Roots时不可达的，所以它们将会被判定为可回收的对象。 在Java中，可作为GC Roots的对象包括： - 虚拟机栈中引用的对象 - 方法区中类静态属性引用的对象 - 方法区中常量引用的对象 - 本地方法栈中JNI（一般说的Native方法)引用的对象 finalize 即使在可达性分析中不可达的对象，也并非是非死不可。要真正宣告一个对象死亡，至少要经历两次标记过程：如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记并且进行一次筛选，筛选的条件就是此对象是否有必要执行finalize()方法。当对象没有覆盖finalize()方法，或者finalize()已经被虚拟机调用过，虚拟机将这两种情况视为没有必要执行。 如果这个对象被判为有必要执行finalize()方法，那么这个对象将会放置在一个叫做F-Queue队列之中，并在稍后由一个虚拟机自动建立的、低优先级的Finalizer线程去执行它。这里所谓的执行是指虚拟机会触发这个方法，但并不承诺会等待它运行结束，这样做的原因是，如果一个对象在finalize()方法中执行缓慢，或者发生了死循环，将很可能会导致F-Queue队列中其他对象永久处于等待，甚至导致整个内存回收系统崩溃。finalize()方法是对象逃脱死亡命运的最后一次机会，稍后GC将会对F-Queue中的对象进行第二次小规模的标记，\\如果对象要在finalize()中拯救自己，只要重新与引用链上的任何一个对象建立联系即可，比如把自己this复制给某个类变量或对象的成员变量，那在第二次标记时它将被移出即将回收的集合；如果对象这时候还没有逃脱，那基本上它就真的被回收了。任何一个对象的finalize()方法都只会被系统调用一次，如果对象面临下一次回收，它的finalize()方法不会被再次执行。回收方法区 在方法区（永久代)中进行垃圾收集的性价比较低：在堆中，尤其在新生代中，常规应用进行一次垃圾收集一般可以回收70%~95%的空间，而永久代的垃圾收集效率远低于此。 永久代的垃圾收集主要回收两部分内容：废弃常量和无用的类。回收废弃常量和回收Java堆中的对象类似。以常量池中字面量的回收为例，没有任何String对象引用常量池中的某个字符串常量，这个常量就会被系统清理出常量池。常量池中的其他类、方法、字段的符号引用也与此类似。 判定一个类是否是无用的类的条件比较苛刻，需要同时满足以下三个条件： 1)该类的所有实例都已经被回收 2)加载该类的类加载器已经被回收 3)该类对应的Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 虚拟机可以对满足上述3个条件的无用类进行回收，这里说的仅仅是可以，而不是和对象一样，不适用了就必然会被回收。是否对类回收，HotSpot虚拟机提供了参数进行控制。 在大量使用反射、动态代理、CGLib等ByteCode框架，动态生成JSP以及OSGi这类频繁自定义ClassLoader的场景都需要虚拟机具备类卸载的功能，以保证永久代不会溢出。 5.16 GC算法 标记-清除算法 最基础的收集算法是标记-清除算法（Mark-Sweep)，算法分为标记和清除两个阶段。首先标记处所有需要回收的对象，在标记完成后统一回收所有被标记的对象。他的不足主要有两个：一个是效率问题，标记和清除两个过程的效率都不高；另一个是空间问题，标记清除后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后再程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。复制算法 为了解决效率问题，出现了复制算法。它将可用内存按容量划分为大小相等的两块，每次只是用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存空间，实现简单，运行高效。只是这种算法的代价是将内存缩小为了原来的一半，未免太高了一点。 现在的商业虚拟机都采用这种收集算法来回收新生代。将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次都使用Eden和其中一块Survivor。当回收时，将Eden和Survivor中还存活着的对象一次性地复制到另外一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。HotSpot虚拟机默认Eden和Survivor的大小比例是8：1。当Survivor空间不够用时，需要依赖其他内存（老年代)进行分配担保。 标记-整理算法 复制收集算法在对象存活率较高时，效率就会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，所以老年代一般不能直接选用这种算法。 根据老年代的特点，有人提出一种标记-整理算法（Mark-Compact)，标记过程仍然与标记-清除算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。 分代收集算法 当前商业虚拟机的垃圾收集都采用分代收集算法（Generational Collection)，这种算法是根据对象存活周期的不同将内存划分为适当的几块。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用标记-清除或者标记-整理算法来进行回收。 5.17 Minor Full GC Minor GC 从年轻代空间（包括 Eden 和 Survivor 区域)回收内存被称为 Minor GC。 非常频繁，回收速度较快。 各种Young GC的触发原因都是eden区满了。Full GC 收集整个堆，包括年轻代、老年代、元数据区等所有部分。 速度较慢。 触发原因不确定，因具体垃圾收集器而异。 比如老年代内存不足，ygc出现promotion failure，System.gc()等。 CMS垃圾收集器不能像其他垃圾收集器那样等待年老代机会完全被填满之后再进行收集，需要预留一部分空间供并发收集时的使用。 5.18 HotSpot的垃圾收集器 Java虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，因此不同厂商、不同版本的虚拟机所提供的垃圾收集器都可能有很大差别，并且一般都会提供参数供用户根据自己得到应用特点和要求组合出各个年代所使用的收集器。这里讨论的收集器基于HotSpot虚拟机，这个虚拟机包含的所有收集器如图所示。 (1).Serial垃圾收集器 Serial是最基本、历史最悠久的垃圾收集器，使用复制算法，曾经是JDK1.3.1之前新生代唯一的垃圾收集器。 Serial是一个单线程的收集器，它不仅仅只会使用一个CPU或一条线程去完成垃圾收集工作，并且在进行垃圾收集的同时，必须暂停其他所有的工作线程，直到垃圾收集结束。 Serial垃圾收集器虽然在收集垃圾过程中需要暂停所有其他的工作线程，但是它简单高效，对于限定单个CPU环境来说，没有线程交互的开销，可以 获得最高的单线程垃圾收集效率，因此Serial垃圾收集器依然是java虚拟机运行在Client模式下默认的新生代垃圾收集器。(2).ParNew垃圾收集器 ParNew垃圾收集器其实是Serial收集器的多线程版本，也使用复制算法，除了使用多线程进行垃圾收集之外，其余的行为和Serial收集器完全一样，ParNew垃圾收集器在垃圾收集过程中同样也要暂停所有其他的工作线程。 ParNew收集器默认开启和CPU数目相同的线程数，可以通过-XX:ParallelGCThreads参数来限制垃圾收集器的线程数。 ParNew虽然是除了多线程外和Serial收集器几乎完全一样，但是ParNew垃圾收集器是很多java虚拟机运行在Server模式下新生代的默认垃圾收集器。(3).Parallel Scavenge收集器 Parallel Scavenge收集器也是一个新生代垃圾收集器，同样使用复制算法，也是一个多线程的垃圾收集器，它重点关注的是程序达到一个可控制的吞吐量 （Thoughput，CPU用于运行用户代码的时间/CPU总消耗时间，即吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间))，高吞吐量 可以最高效率地利用CPU时间，尽快地完成程序的运算任务，主要适用于在后台运算而不需要太多交互的任务。 Parallel Scavenge收集器提供了两个参数用于精准控制吞吐量： a.-XX:MaxGCPauseMillis：控制最大垃圾收集停顿时间，是一个大于0的毫秒数。 b.-XX:GCTimeRation：直接设置吞吐量大小，是一个大于0小于100的整数，也就是程序运行时间占总时间的比率，默认值是99，即垃圾收集运行最大1%（1/(1+99))的垃圾收集时间。 Parallel Scavenge是吞吐量优先的垃圾收集器，它还提供一个参数：-XX:+UseAdaptiveSizePolicy，这是个开关参数，打开之后就不需 要手动指定新生代大小(-Xmn)、Eden与Survivor区的比例(-XX:SurvivorRation)、新生代晋升年老代对象年龄 (-XX:PretenureSizeThreshold)等细节参数，虚拟机会根据当前系统运行情况收集性能监控信息，动态调整这些参数以达到最大吞吐 量，这种方式称为GC自适应调节策略，自适应调节策略也是ParallelScavenge收集器与ParNew收集器的一个重要区别。(4).Serial Old收集器 Serial Old是Serial垃圾收集器年老代版本，它同样是个单线程的收集器，使用标记-整理算法，这个收集器也主要是运行在Client默认的java虚拟机默认的年老代垃圾收集器。 在Server模式下，主要有两个用途： a.在JDK1.5之前版本中与新生代的Parallel Scavenge收集器搭配使用。 b.作为年老代中使用CMS收集器的后备垃圾收集方案。 新生代Serial与年老代Serial Old搭配垃圾收集过程图： 新生代Parallel Scavenge收集器与ParNew收集器工作原理类似，都是多线程的收集器，都使用的是复制算法，在垃圾收集过程中都需要暂停所有的工作线程。 新生代Parallel Scavenge/ParNew与年老代Serial Old搭配垃圾收集过程图： (5).Parallel Old收集器 Parallel Old收集器是Parallel Scavenge的年老代版本，使用多线程的标记-整理算法，在JDK1.6才开始提供。 在JDK1.6之前，新生代使用ParallelScavenge收集器只能搭配年老代的Serial Old收集器，只能保证新生代的吞吐量优先，无法保证整体的吞吐量，Parallel Old正是为了在年老代同样提供吞吐量优先的垃圾收集器，如果系统对吞吐量要求比较高，可以优先考虑新生代Parallel Scavenge和年老代Parallel Old收集器的搭配策略。 新生代Parallel Scavenge和年老代Parallel Old收集器搭配运行过程图： (6).CMS收集器（重点) Concurrent mark sweep(CMS)收集器是一种年老代垃圾收集器，其最主要目标是获取最短垃圾回收停顿时间，和其他年老代使用标记-整理算法不同，它使用多线程的标记-清除算法。 最短的垃圾收集停顿时间可以为交互比较高的程序提高用户体验，CMS收集器是Sun HotSpot虚拟机中第一款真正意义上并发垃圾收集器，它第一次实现了让垃圾收集线程和用户线程同时工作。 CMS工作机制相比其他的垃圾收集器来说更复杂，整个过程分为以下4个阶段： a.初始标记：只是标记一下GC Roots能直接关联的对象，速度很快，仍然需要暂停所有的工作线程。 b.并发标记：进行GC Roots跟踪的过程，和用户线程一起工作，不需要暂停工作线程。 c.重新标记：为了修正在并发标记期间，因用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，仍然需要暂停所有的工作线程。 d.并发清除：清除GC Roots不可达对象，和用户线程一起工作，不需要暂停工作线程。 由于耗时最长的并发标记和并发清除过程中，垃圾收集线程可以和用户现在一起并发工作，所以总体上来看CMS收集器的内存回收和用户线程是一起并发地执行。 CMS收集器工作过程： CMS收集器有以下三个不足： a.CMS收集器对CPU资源非常敏感，其默认启动的收集线程数=(CPU数量+3)/4，在用户程序本来CPU负荷已经比较高的情况下，如果还要分出CPU资源用来运行垃圾收集器线程，会使得CPU负载加重。 b.CMS无法处理浮动垃圾(Floating Garbage)，可能会导致Concurrent ModeFailure失败而导致另一次Full GC。由于CMS收集器和用户线程并发运行，因此在收集过程中不断有新的垃圾产生，这些垃圾出现在标记过程之后，CMS无法在本次收集中处理掉它们，只好 等待下一次GC时再将其清理掉，这些垃圾就称为浮动垃圾。 CMS垃圾收集器不能像其他垃圾收集器那样等待年老代机会完全被填满之后再进行收集，需要预留一部分空间供并发收集时的使用，可以通过参数 -XX:CMSInitiatingOccupancyFraction来设置年老代空间达到多少的百分比时触发CMS进行垃圾收集，默认是68%。 如果在CMS运行期间，预留的内存无法满足程序需要，就会出现一次ConcurrentMode Failure失败，此时虚拟机将启动预备方案，使用Serial Old收集器重新进行年老代垃圾回收。 c.CMS收集器是基于标记-清除算法，因此不可避免会产生大量不连续的内存碎片，如果无法找到一块足够大的连续内存存放对象时，将会触发因此 Full GC。CMS提供一个开关参数-XX:+UseCMSCompactAtFullCollection，用于指定在Full GC之后进行内存整理，内存整理会使得垃圾收集停顿时间变长，CMS提供了另外一个参数-XX:CMSFullGCsBeforeCompaction， 用于设置在执行多少次不压缩的Full GC之后，跟着再来一次内存整理。 promotion failure 发生在 young gc 阶段，即 cms 的 ParNewGC。promotion failed是在进行Minor GC时，survivor space放不下、对象只能放入老年代，而此时老年代也放不下造成的； concurrent mode failure是在执行CMS GC的过程中同时有对象要放入老年代，而此时老年代空间不足造成的(7).G1收集器（重点) Garbage first垃圾收集器是目前垃圾收集器理论发展的最前沿成果，相比与CMS收集器，G1收集器两个最突出的改进是： a.基于标记-整理算法，不产生内存碎片。 b.可以非常精确控制停顿时间，在不牺牲吞吐量前提下，实现低停顿垃圾回收。 G1收集器避免全区域垃圾收集，它把堆内存划分为大小固定的几个独立区域，并且跟踪这些区域的垃圾收集进度，同时在后台维护一个优先级列表，每次根据所允许的收集时间，优先回收垃圾最多的区域。 区域划分和优先级区域回收机制，确保G1收集器可以在有限时间获得最高的垃圾收集效率。 5.19 内存分配原则 对象的内存分配，就是在堆上分配，对象主要分配在新生代的Eden区上，如果启动了本地线程分配缓冲，将按线程优先在TLAB上分配。少数情况下也可能直接分配在老年代中，分配的规则不是固定的，其细节取决于当前使用的是哪一种垃圾收集器组合，还有虚拟机中与内存相关的参数设置。 下面会讲解几条最普遍的内存分配原则。对象优先在Eden分配 大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC。如果GC期间虚拟机发现已有的对象全部无法放入Survivor空间，会通过分配担保机制提前转移至老年代中。 大对象直接进入老年代 所谓的大对象是指，需要大量连续内存空间的Java对象，最典型的大对象就是那种很长的字符串以及数组。经常出现大对象容易导致内存还有不少空间就提前触发垃圾收集以获取足够的连续空间来安置它们。长期存活的对象将进入老年代 虚拟机为每个对象定义一个对象年龄（Age)计数器。如果对象在Eden出生并经过第一次Minor GC后仍然存活，并且能够被Survivor容纳的话，将被移动到Survivor空间中，并且对象年龄设为1.对象在Survivor区中每熬过一次Minor GC，年龄就增加1岁，当它的年龄增加到一定程度（默认为15岁)，就将会被晋升到老年代。动态对象年龄判定 虚拟机并不是永远地要求对象的年龄必须达到MaxTenuringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代。空间分配担保 在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那么Minor GC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管这次Minor GC是有风险的；如果小于，或者HandlePromotionFailure设置不允许冒险，那此时也要改为进行一次Full GC。 冒险是指当出现大量对象在Minor GC后仍然存活的情况，就需要老年代进行分配担保，把Survivor区无法容纳的对象直接进入老年代。老年代要进行这样的担保，前提是老年代本身还有容纳这些对象的剩余空间，一共会有多少对象会活下来在实际完成内存回收之前是无法明确知道的，所以只好取之间每一次回收晋升到老年代对象容量的平均大小值作为经验值，与老年代的剩余空间进行比较，决定是否进行Full GC来让老年代腾出更多空间。 取平均值进行比较其实仍然是一种动态概率的手段，依然存在担保失败的情况。如果出现了HandlePromotionFailure失败，那就只好在失败后重新发起一次Full GC。 5.20 GC相关API System.gc 建议JVM进行Full GC,虽然只是建议而非一定,但很多情况下它会触发 Full GC,从而增加Full GC的频率,也即增加了间歇性停顿的次数。强烈影响系建议能不使用此方法就别使用，让虚拟机自己去管理它的内存，可通过通过-XX:+ DisableExplicitGC来禁止RMI调用System.gc。 public static void gc() { Runtime.getRuntime().gc(); } Runtime.gc的底层实现位于Runtime.c文件中 JNIEXPORT void JNICALL Java_java_lang_Runtime_gc(JNIEnv *env, jobject this) { JVM_GC(); } JVM_ENTRY_NO_ENV(void, JVM_GC(void)) JVMWrapper(\"JVM_GC\"); if (!DisableExplicitGC) { Universe::heap()->collect(GCCause::_java_lang_system_gc); } JVM_END 这里有一个DisableExplicitGC参数，默认是false，如果启动JVM时添加了参数-XX:+DisableExplicitGC，那么JVM_GC相当于一个空函数，并不会进行GC。 其中Universe::heap()返回当前堆对象，由collect方法开始执行GC，并设置当前触发GC的条件为_java_lang_system_gc，内部会根据GC条件执行不同逻辑。 JVM的具体堆实现，在Universe.cpp文件中的initialize_heap()由启动参数所设置的垃圾回收算法决定。 堆实现和回收算法对应关系： 1、UseParallelGC：ParallelScavengeHeap 2、UseG1GC：G1CollectedHeap 3、默认或者CMS：GenCollectedHeap 类文件结构 Class类文件的结构 Class文件并不一定定义在文件里，也可以通过类加载器直接生成。 Class文件是一组以8位字节为基础单位的二进制流，各个数据项目严格按照顺序紧凑地排列在Class文件之中，中间没有添加任何分隔符。当遇到需要占用8位字节以上空间的数据项时，则会按照高位在前的方式分割成若干个8位字节进行存储。 Class文件结构采用一种类似于C语言结构体的伪结构来存储数据，这种伪结构只有两种数据类型：无符号数和表。 无符号数属于基本的数据类型，以u1、u2、u4、u8来分别代表1个字节、2个字节、4个字节、8个字节的无符号数，无符号数可以用来描述数字、索引引用、数量值或者按照utf-8编码构成字符串值。 表是由多个无符号数或者其他表作为数据项构成的复合数据类型，所有表都习惯性地以_info结尾。表用于描述有层次关系的复合结构的数据。 无论是无符号数还是表，当需要描述同一类型但数量不定的多个数据时，经常会使用一个前置的容量计数器加若干个连续的数据项的形式，这时称这一系列连续的某一类型的数据为某一类型的集合。 魔数与Class文件的版本 每个Class文件的头4个字节称为魔数，它的唯一作用是确定这个文件是否为一个能被虚拟机接收的Class文件。很多文件存储格式都使用魔数来进行身份识别。魔数的值为0xCAFEBABE。 紧接着魔数的4个字节存储的是Class文件的版本号：第5和第6个字节是此版本号，第7和第8个字节是主版本号。 简单的一段Java代码，后面的内容将以此为例进行讲解： public class TestClass { private int m; public int inc(){ return m+1; } } 常量池 紧接着主次版本号之后的是常量池入口，常量池可以理解为Class文件中的资源仓库，它是Class文件结构中和其他项目关联最多的数据类型，也是占用Class文件空间最大的数据项目之一，同时它还是在Class文件中第一个出现的表类型数据项目。 由于常量池中常量的数量是不固定的，所以在常量池入口需要放置一项u2类型的数据，代表常量池容量计数器（从1开始)。对于其他集合类型，包括接口索引集合、字段表集合、方法表集合等的容量计数都与一般习惯相同，是从0开始的。 常量池中主要存放两大类常量：字面量（Literal)和符号引用（Symbolic References)。字面量比较接近于Java语言层面的常量概念，如字符串、final常量值。而符号引用则属于编译原理方面的概念，包括了下面三类常量： - 类和接口的全限定名 - 字段的名称和描述符 - 方法的名称和描述符 Java代码在javac编译的时候，并没有连接这一步骤，而是在虚拟机加载Class文件的时候进行动态连接。在Class文件中不会保存各个方法、字段的最终内存布局信息，因此这些字段、方法的符号引用不经过运行期转换的话无法得到真正的内存入口地址，也就无法直接被虚拟机使用。当虚拟机运行时，需要从常量池获得对应的符号引用，再在类创建时或运行时解析、翻译到具体的内存地址之中。 常量池中每一项常量都是一个表，在JDK1.7之前有11种不同结构的表结构数据。1.7增加了3种。它们的共同特点是表开始的第一位是一个u1类型的标志位，代表当前这个常量属于哪种常量类型。 访问标志 在常量池结束之后，紧接着的两个字节代表访问标志，这个标志用于识别一些类或接口层次的访问信息，包括：这个Class是类还是接口；是否定义为public类型；是否定义为abstract类型；如果是类的话，是否被声明为final等。 访问标志中一共有16个标志位可以使用，当前只定义了其中8个，没有使用到的标志位一律为0。 类索引、父类索引与接口索引集合 类索引和父类索引都是一个u2类型数据，而接口索引集合是一组u2类型数据的集合，Class文件中由这3项数据来确定这个类的继承关系。类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名。接口索引集合用来描述这个类实现了哪些接口，这些被实现的接口将按implements 语句后的接口顺序从左到右排列在接口索引集合中。 类索引、父类索引和接口索引集合都按顺序排列在访问标志之后，类索引和父类索引用两个u2类型的索引值表示，它们各自指向一个类型为CONSTANT_Class_info的类描述符常量，通过CONSTANT_Class_info类型的常量中的索引值可以找到定义在CONSTANT_Utf8_info类型的常量中的全限定名字符串。 对于接口类型集合，入口的第一项---u2类型的数据为接口计数器，表示索引表的容量。如果该类没有实现任何接口，则该计数器值为0，后面接口的索引表不再占用任何字节。 字段表集合 字段表用于描述接口或者类中声明的变量。字段包括类级变量以及实例级变量，但不包括在方法内部声明的局部变量。可以包括的信息有：作用域（访问权限)、static修饰符、final修饰符、并发可见性、序列化修饰符等。 跟随access_flags标志的是两项索引值：name_index and desciptor_index。它们都是对常量池的引用，分别代表字段的简单名称以及字段和方法的描述符。 全限定名和简单名称：org/fenixsoft/clazz/TestClass是这个类的全限定名。简单名称是指没有类型和参数修饰的方法或者字段名称，这个类的inc（)方法和m字段的简单名称分别是inc和m。 相对于全限定名和简单名称而言，方法和字段的描述符要复杂一些。描述符的作用是用来描述字段的数据类型、方法的参数列表（数量、类型、顺序)和返回值。基本数据类型（byte、char、double…)以及代表无返回值的void类型都用一个大写字母表示，而对象类型则用字符L加对象的全限定名来表示。 对于数组来说，每一维度将使用一个前置的【字符来描述，如定义一个java.lang.String[][]类型的二维数组，将被记录为[[Ljava/lang/String，一个整数数组 int[] 将被记录为[I。 用描述符描述方法时，按照先参数列表，后返回值的顺序描述，参数列表按照参数的严格顺序放在一组小括号之内。比如方法void inc（)的描述符为()V，方法java.lang.String.toString()的描述符为()Ljava/lang/String。 字段表集合首先是一个容量计数器，说明该类的字段表数据个数，然后是access_flags标志，然后是其他标志、 在descriptor_index之后跟随着一个属性表集合用于存储一些额外的信息。 字段表集合不会列出从超类或者父接口继承而来的字段，但有可能列出原本Java代码中不存在的字段，比如内部类会自动添加指向外部类实例的字段。 方法表集合 方法表的结构依次包括了访问标志、名称索引、描述符索引、属性表集合。 方法里面的Java代码，经过编译器编译为字节码指令后，存放在方法属性表中一个名为Code的属性里面，属性表是Class文件格式中最具拓展性的一种数据项目。 与字段表集合对应的，如果父类方法在子类中没有被重写，方法表集合中就不会出现来自父类的方法信息。但同样的，有可能会出现由编译器自动添加的方法，最典型的便是类构造器和实例构造器。 要重载一个方法，除了要有和原方法相同的简单名称外，还必须有一个不同的特征签名。特征签名就是一个方法中各个参数在常量池中的字段符号引用的结婚，也就是返回值不会包含在特征签名中。但是在Class文件格式中，特征签名的范围更大一些，只要描述符不是完全一致的两个方法也可以共存。如果两个方法有相同的名称和特征签名，但返回值不同，那么也是可以合法共存在同一个Class文件中的。 属性表集合 与Class文件中其他的数据项目要求严格的顺序、长度和内容不同，属性表集合的限制宽松一些，不再要求各个属性表具有严格顺序，并且只要不与已有属性名重复，任何人实现的编译器都可以向属性表写入自己定义的属性信息。为了能正确解析Class文件，Java虚拟机规范预定义了9项虚拟机实现应当能识别的属性。（现已增至21项) 以上列出其中的5种。 对于每个属性，它的名称需要从常量池中引用一个CONSTANT_Utf8_info类型的常量来表示，而属性表的结构则是完全自定义的，只需要通过一个u4的长度属性去说明属性值所占用的位数即可。一个符合规则的属性表应该满足下图所定义的结构： 1、Code属性 Code属性出现在方法表的属性集合中，但并非所有的方法表都必须存在这种属性。 max_stack代表了操作数栈深度的最大值。在方法执行的任意时刻，操作数栈都不会超过这个深度。 max_locals代表了局部变量表所需的存储空间，单位是slot，Slot是虚拟机为局部变量分配内存所使用的最小单位。对于byte、char等长度不超过32位的数据类型，每个局部变量占1个Slot，而long和double占2个Slot。方法参数（包括this)、显式异常处理器参数（catch所定义的异常)、方法体中定义的局部变量都需要使用局部变量表来存放。并不是方法中用到了多少个局部变量，就把这些变量所占Slot之和作为max_locals的值，因为局部变量表中的Slot可以重用，当代码执行超过一个局部变量的作用域时，这个局部变量所占的Slot可以被其他局部变量所使用。 Code属性表是Class文件中最重要的一个属性，如果把一个Java程序中的信息分为代码和元数据两部分，那么在整个Class文件中，Code属性用于描述代码，所有的其他数据项目都用于描述元数据。 在任何实例方法中，都可以通过this关键字访问到此方法所属的对象。它的实现就是通过javac编译器变异的时候把对this关键字的访问转变为对一个普通方法参数的访问，然后在虚拟机调用实例方法时自动传入此参数。因此在实例方法的局部变量表中至少会存在一个指向当前对象实例的局部变量，局部变量表中也会预留出第一个Slot位来存放对象实例的引用。 在字节码指令之后的是这个方法的显式异常处理表集合，异常表对Code属性来说并不是必须存在的。 异常表包含4个字段，这些字段的含义是：如果当字节码在第start_pc行到第end_pc之间（不含end_pc)出现了类型为catch_type或者其子类的异常，则转到第handler_pc行继续处理。当catch_type的值为0时，代表任意异常情况都需要转向到handler_pc处进行处理。 异常表实际上是Java代码的一部分，编译器使用异常表而不是简单的跳转命令来实现Java异常及finally处理机制。 2、Exceptions属性 Exceptions属性的作用是列举出方法中可能抛出的受检查异常，也就是方法描述时在throws关键字后面列举的异常。它的结构： 3、LineNumberTable属性 LineNumberTable属性用于描述Java源码行号与字节码行号之间的对应关系，是可选的属性。如果选择不生成LineNumberTable属性，对程序运行的最主要的影响就是当跳出异常时，堆栈中将不会显示出错的行号，并且在调试的时候，也无法按照源码行来设置断点。 4、LocalVariableTable属性 LocalVariableTable属性用于描述栈帧中局部变量表中的变量和Java源码中定义的变量之间的关系，它也是可选的属性。 5、SourceFile属性 SourceFile属性用于记录生成这个Class文件的源码文件名称，也是可选的。 6、ConstantValue属性 ConstantValue属性的作用是通知虚拟机自动为静态变量赋值。只有被static修饰的变量才可以使用这项属性。对于非static类型的变量的赋值是在实例构造方法中进行的；而对于类变量，则有两种方式可以选择：在类构造器中或者使用ConstantValue属性。目前Sun Javac编译器的选择是：如果是常量（static final)，并且这个常量的数据类型是基本类型或者String的话，就生成ConstantValue属性来进行初始化，如果这个变量没有被final修饰，或者并非基本类型或字符串，则将会选择在类构造器中进行初始化。 字节码指令简介 Java虚拟机指令是由一个字节长度的、代表着某种特定操作含义的数字（操作码，Opcode)以及跟随其后的零至多个代表此操作所需参数（操作数，Operands)而构成。由于Java虚拟机采用面向操作数栈而不是寄存器的架构，所以大多数的指令都不包含操作数，只有一个操作码。 1个字节意味着指令集的操作码总数不能超过256条；又由于Class文件格式放弃了编译后代码的操作数长度对齐，这就意味着虚拟机处理那些超过一个字节数据的时候，不得不在运行时从字节中重建出具体数据的结构。放弃了操作数长度对齐，就意味着可以省略很多填充和间隔符号。 Java虚拟机的解释器可以使用下面这个伪代码当做最基本的执行模型来理解： 字节码与数据类型 在Java虚拟机的指令集中，大多数的指令都包含了其操作所对应的数据类型信息。iload指令用于从局部变量表中记载int型的数据到操作数栈中，而fload指令加载的则是float类型的数据。这两条指令的操作在虚拟机内部可能会是由同一段代码来实现的，但在Class文件中它们必须拥有各自独立的操作码。 对于大部分与数据类型相关的字节码指令，它们的操作码助记符中都有特殊的字符来表明专门为哪种数据类型服务，i代表对int类型的数据操作，l代表long等。 大部分的指令都没有支持整数类型byte、char和short，编译器会在编译时或运行时将byte和short类型的数据带符号拓展为相应的int类型的数据。大多数对于boolean、byte、short和char类型的数据的操作，实际上都是使用相应的int类型作为运算类型。 加载和存储指令 加载和存储指令用于将数据在栈帧中的局部变量表和操作数之间来回传输。 尖括号结尾的指令实际上是代表了一组指令，这几组指令都是某个带有一个操作数的通用指令的特殊形式，对于这若干组的特殊指令来说，它们省略掉了显式地操作数，不需要进行取操作数的动作，实际上操作数就隐含在指令中。 运算指令 类型转换指令 尽管数据类型窄化转换可能会发生上限溢出、下限溢出和精度丢失等情况，但是Java虚拟机规范中明确规定数值类型的窄化转换指令永远不可能导致虚拟机抛出运行时异常。 对象创建与访问指令 操作数栈管理指令 控制转换指令 方法调用和返回指令 异常处理指令 在Java程序中显式抛出的操作都由athrow指令来实现。处理异常（catch)不是由字节码指令来实现的，而是采用异常表来完成的。 athrow指令与异常表： public void catchException() { try { throw new Exception(); } catch (Exception var2) { ; } } 字节码： public void catchException(); Code: Stack=2, Locals=2, Args_size=1 0: new #58; //class java/lang/Exception 3: dup 4: invokespecial #60; //Method java/lang/Exception.\"\":()V 7: athrow 8: astore_1 9: return Exception table: from to target type 0 8 8 Class java/lang/Exception 偏移为7的athrow指令，这个指令运作过程大致是首先检查操作栈顶，这时栈顶必须存在一个reference类型的值，并且是java.lang.Throwable的子类（虚拟机规范中要求如果遇到null则当作NPE异常使用)，然后暂时先把这个引用出栈，接着搜索本方法的异常表，找一下本方法中是否有能处理这个异常的handler，如果能找到合适的handler就会重新初始化PC寄存器指针指向此异常handler的第一个指令的偏移地址。接着把当前栈帧的操作栈清空，再把刚刚出栈的引用重新入栈。如果在当前方法中很悲剧的找不到handler，那只好把当前方法的栈帧出栈（这个栈是VM栈，不要和前面的操作栈搞混了，栈帧出栈就意味着当前方法退出)，这个方法的调用者的栈帧就自然在这条线程VM栈的栈顶了，然后再对这个新的当前方法再做一次刚才做过的异常handler搜索，如果还是找不到，继续把这个栈帧踢掉，这样一直到找，要么找到一个能使用的handler，转到这个handler的第一条指令开始继续执行，要么把VM栈的栈帧抛光了都没有找到期望的handler，这样的话这条线程就只好被迫终止、退出了。 上面的异常表只有一个handler记录，它指明了从偏移地址0开始（包含0)，到偏移地址8结束（不包含8)，如果出现了java.lang.Exception类型的异常，那么就把PC寄存器指针转到8开始继续执行。顺便说一下，对于Java语言中的关键字catch和finally，虚拟机中并没有特殊的字节码指令去支持它们，都是通过编译器生成字节码片段以及不同的异常处理器来实现。 同步指令（重点) Java虚拟机可以支持方法级的同步和方法内部一段指令序列的同步，这两种同步结构都是使用管程（Monitor)来支持的。 方法级的同步是隐式的，即无需通过字节码指令来控制，它实现在方法调用和返回操作之中。当方法调用时，调用指令将会检查方法的ACC_SYNCHRONIZED访问标志是否被设置，如果设置了，执行线程就要求先成功持有管程。在方法执行期间，执行线程持有了管程，其他任何线程都无法再获取到同一个管程。如果一个同步方法执行期间抛出了异常，并且在方法内部无法处理此异常，那么这个同步方法所持有的管程将在异常抛到同步方法之外时自动释放。 同步一段指令集序列通常是由synchronized语句块来表示的，Java虚拟机的指令集有monitorenter和monitorexit两条指令来支持synchronized关键字的语义，正确实现synchronized关键字需要Javc编译器和虚拟机两者共同协作支持。 方法中调用过的每一条monitorenter指令都必须执行其对应的monitorexit指令，而无论这个方法是否正常结束。 为了保证在方法异常完成时monitorenter和monitorexit指令异常可以正常配对执行，编译器会自动产生一个异常处理器，这个异常处理器声明可以处理所有的异常，它的目的就是用来执行monitorexit指令。类加载机制 概述 虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。 与那些编译时需要进行连接的语言不同，Java语言里面，类型的加载、连接和初始化都是在程序运行期间完成的，这种策略虽然会令类加载时增加一些性能开销，但是是为Java应用程序提供高度的灵活性，Java里天生可以动态扩展的语言特性就是依赖运行期动态加载和动态连接这个特点实现的。 类加载的时机 类的整个生命周期包括：加载、验证、准备、解析、初始化、使用和卸载。 加载、验证、准备、初始化和卸载这5个阶段的顺序是确定的，类的加载过程必须按照这种顺序开始，而解析阶段在某些情况下可以在初始化阶段之后再开始，这是为了支持Java语言的运行时绑定（动态绑定)。 什么时候开始类加载过程的第一个阶段：加载？ Java虚拟机规范规定有且只有5种情况必须立即对类进行初始化： 1)遇到new、getstatic、putstatic、invokestatic这4条字节码指令时，如果类没有过初始化，则需要先初始化。生成这4条指令的最常见的Java代码场景是：使用new实例化对象时、读取或设置一个类的静态字段时、调用一个类的静态方法时 2)反射 3)如果一个类的父类尚未初始化，那么先触发其父类的初始化 4)main方法所在类 5)java.lang.invoke.MethodHandle实例最后的解析结果REF_getStatic、REF_putStatic、 REF_invokeStatic的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化。 只有直接定义一个静态字段的类才会被初始化，因此通过其子类来引用父类中定义的静态字段，只会触发父类的初始化而不会触发子类的初始化。至于是否要触发子类的加载和验证，在虚拟机规范中并未明确确定，这点取决于虚拟机的具体实现。 当一个类在初始化时，要求其父类全部都已经初始化过了，但是一个接口在初始化时，并不要求其父类接口全部都完成了初始化，只有在真正使用到父接口（如引用接口中定义的常量)的时候才会初始化。 类加载的过程 1) 加载 加载是类加载过程的一个阶段。 在加载阶段，虚拟机需要完成以下3件事情： 1)通过一个类的全限定名获得此类的二进制字节流 2)将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构 3)在内存中生成一个代表这个类的Class对象（HotSpot中放在堆里)，作为方法区这个类的各种数据的访问入口。 加载阶段既可以使用系统提供的引导类加载器来完成，也可以由用户自定义的类加载器，开发人员可以通过定义自己的类加载器去控制字节流的获取方式。 加载阶段和连接阶段的部分内容是交叉进行的 java.lang.Class实例并不负责记录真正的类元数据，而只是对VM内部的InstanceKlass对象的一个包装供Java的反射访问用，InstanceKlass放在方法区（Java8HotSpot中放在元数据区) 2) 验证 验证是连接阶段的第一步，目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 验证阶段是否严谨直接决定了Java虚拟机是否能承受恶意代码的攻击，从执行性能的角度上讲，验证阶段的工作量在虚拟机的类加载子系统中又占了相当大的一部分。 验证阶段大致会完成下面4个阶段的检验动作：文件格式验证、元数据验证、字节码验证、符号引用验证。 1、文件格式验证 验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理。这阶段的验证是基于二进制字节流进行的，只有通过了这个阶段的验证后，字节流才会进入内存的方法区中进行存储，所以后面的3个验证阶段全部是基于方法区的存储结构进行的，不会再直接操作字节流。 2、元数据验证 第二阶段是对字节码描述的数据进行语义分析，以保证其描述的信息符合Java语言规范的要求。 3、字节码验证 第三阶段是整个验证过程中最复杂的一个阶段，主要目的是通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。这个阶段将对类的方法体进行校验分析，保证被校验的类的方法在运行时不会做出危害虚拟机安全的事件。 如果一个方法体通过了字节码校验，也不能说明其一定就是安全的，这里涉及一个停机问题，通过程序去校验程序逻辑是无法做到绝对准确的-----不能通过程序准确地检查出程序是否能在有限的时间之内结束运行。 4、符号引用验证 最后一个阶段发生在虚拟机将符号引用转化为直接引用的时候，这个转化动作将在连接的第三阶段—解析阶段中发生。符号引用可以看做是对类自身以外（常量池中的各种符号引用)的信息进行匹配性校验。 3) 准备 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存都将在方法区中进行分配。这时候进行内存分配的仅包括类变量，而不包括实例变量；这里所说的初始值，是指0值。 如果是static final 常量，那么会被初始化为ConstantValue属性所指定的值。 4) 解析 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。 符号引用：符号引用是一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时无歧义地定位到目标即可。符号引用与虚拟机实现的内存布局无关，引用的目标并不一定都已经加载到内存中。 直接引用：直接引用是可以直接指向目标的指针，相对偏移量或是一个能间接定位到目标的句柄。直接引用是和虚拟机实现的内存布局相关的，同一个符号引用在不同虚拟机实例上翻译出来的直接引用一般不会相同。如果有了直接引用，那么引用的目标必须已经在内存中存在。 对同一个符号引用进行多次解析请求是很常见的事情，除了invokedynamic指令外，虚拟机实现可以对第一次解析的结果进行缓存，从而避免解析动作重复进行。动态（invokedynamic)的含义是必须等到程序实际运行到这条指令的时候，解析动作才能进行。 5) 初始化 类初始化是类加载阶段的最后一步。到了初始化阶段，才真正开始执行类中定义的Java程序代码。 初始化阶段是执行类构造器方法的过程。 类构造器是由编译器自动收集类中的所有类变量的赋值动作和静态代码块中的语句合并产生的，编译器收集的顺序是由语句在源文件中出现的顺序决定的，静态代码块只能访问到定义在静态代码块之间的变量，定义在它之后的变量，在前面的静态代码块可以赋值，但是不能访问。 类构造器与类的构造方法不同，它不需要显式调用父类构造器，虚拟机会保证在子类的类构造器执行之前，父类的类构造器已经执行完毕。 类构造器对于类或接口不是必需的，如果一个类中没有静态代码块，也没有对变量的赋值操作，那么编译器可以不为这个类生成类构造器。 接口中不能使用静态代码块，但是仍然有变量初始化的赋值操作，因此接口和类一样都会生成类构造器。但接口与类不同的是，执行接口的类构造器不需要先执行父接口的类构造器。只有当父接口中定义的变量使用时，父接口才会初始化，另外，接口的实现类在初始化时也一样不会执行接口的类构造器。 虚拟机会保证一个类的类构造器在多线程环境中被正确地加锁、同步，如果多个线程同时去初始化一个类，那么只有一个线程去执行这个类的类构造器，其他线程都需要阻塞等待，直至活动线程执行类构造器完毕。 类的主动引用和被动引用 主动引用（一定会发生类的初始化) new对象 引用静态变量（static非final)和静态方法 反射 main方法所在类 当前类的所有父类 被动引用（不会发生类的初始化) 访问一个类的静态变量，只有真正声明这个静态变量的类才会被初始化 通过数组定义类引用 引用常量（存在方法区的运行时常量池) 类加载器 类加载阶段中的“通过一个类的全限定名来获取描述此类的二进制字节流”这个动作放到Java虚拟机外部去实现，以便让应用程序自己决定如何去获取所需要的类。实现这个动作的代码模块称为类加载器。 类与类加载器 类加载器虽然只用于实现类的加载动作，但它在Java程序中的作用不限于类加载阶段。对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立其在Java虚拟机中的唯一性，每一个类加载器，都拥有一个独立的类名称空间。换句话说，比较两个类是否相等，只有在这两个类是由同一个类加载器（实例)加载的前提下才有意义，否则，即使这两个来源于同一个Class文件，被同一个虚拟机加载，只要加载它们的类加载器不同，那这两个类必定不相等。 这里所指的相等，包括类的Class对象的equals方法等的返回结果，也包括instance of的返回结果。 双亲委派模型 从Java虚拟机角度来讲，只存在两种不同的类加载器：一种是启动类加载器（bootstrap classloader)，这个类加载器由C++语言实现（HotSpot)，是虚拟机自身的一部分；另一种就是所有的其他类加载器，都由Java语言实现，独立于虚拟机外部。并且全继承自java.lang.ClassLoader。 从Java开发人员的角度看，Java程序使用到以下3种系统提供的类加载器： 1)启动类加载器：负责将存放在\\lib目录中的类库加载到虚拟机内存中。启动类加载器无法被Java程序直接引用。 2)扩展类加载器（Extension ClassLoader)：这个加载器负责加载\\lib\\ext目录中的所有类库，开发者可以直接使用扩展类加载器。 3)应用程序类加载器（Application ClassLoader)：或称系统类加载器，负责加载用户classpath下所指定的类库，开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 类加载器之间的层次关系成为类加载器的双亲委派模型（Parents Delegation Model)。双亲委派模型要求除了顶层了启动类加载器，其他的类加载器都应当有自己的父类加载器。这里类加载器之间的父子关系一般不会以继承的关系来实现，而是使用组合的方式来复用父加载器的代码。 双亲委派模型的工作过程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求，子加载器才会尝试自己去加载。 使用双亲委派模型来组织类加载器之间的关系，有一个显而易见的好处就是Java类随它的类加载器一起具备了一种带有优先级的层次关系。 破坏双亲委派模型 双亲委派模型并不是一个强制性的约束模型，而是Java设计者推荐给开发者的类加载器实现方式。比如OSGi环境下，类加载不再是双亲委派模型中的树形结构，而是进一步发展为更加复杂的网状结构，当收到类加载请求，OSGi将按照下面的顺序进行类搜索： 热部署 如果我们希望将java类卸载，并且替换更新版本的java类，该怎么做呢？’ 1、销毁该自定义ClassLoader 2、更新class文件 3、创建新的ClassLoader去加载更新后的class文件。5.21 对象初始化的先后顺序 单个类： 1)类的静态变量清0 2)静态变量赋值，静态代码块（按照编写顺序调用) 3)成员变量清0 4)成员变量赋值，非静态代码块（按照编写顺序调用) 5)构造方法 1、2统称为类的初始化 4、5统称为对象初始化 带有继承时： 1)父类类初始化 2)子类类初始化 3)成员变量清0，包括父类和子类 3)父类对象初始化 4)子类对象初始化 实例一： public class StaticTest { public static void main(String[] args) { staticFunction(); } static StaticTest st = new StaticTest(); static { //静态代码块 System.out.println(\"1\"); } { // 实例代码块 System.out.println(\"2\"); } StaticTest() { // 实例构造器 System.out.println(\"3\"); System.out.println(\"a=\" + a + \",b=\" + b); } public static void staticFunction() { // 静态方法 System.out.println(\"4\"); } int a = 110; // 实例变量 static int b = 112; // 静态变量 /** main 方法属于静态方法，主动引用，开始执行类的初始化：按照编写顺序进行静态变量赋值与静态代码块执行 1)先初始化StaticTest，对象实例化时，因为类已经被加载，所以执行对象初始化，先对成员变量进行初始化（a赋值为0)， 然后按照编写顺序进行非静态变量赋值与非静态代码块执行（打印2，a赋值为110)， 再调用构造方法（打印3，打印a=110，b=0) 2)再执行静态代码块，打印1 3)再赋值b为112, 4)至此类加载完毕，执行main方法，打印4 2 3 a=110,b=0 1 4 */ } 实例二： public class InitializeDemo { private static int k = 1; private static InitializeDemo t1 = new InitializeDemo(\"t1\"); private static InitializeDemo t2 = new InitializeDemo(\"t2\"); private static int i = print(\"i\"); private static int n = 99; static { print(\"静态块\"); } private int j = print(\"j\"); { print(\"构造块\"); } public InitializeDemo(String str) { System.out.println((k++) + \":\" + str + \" i=\" + i + \" n=\" + n); ++i; ++n; } public static int print(String str) { System.out.println((k++) + \":\" + str + \" i=\" + i + \" n=\" + n); ++n; return ++i; } public static void main(String args[]) { new InitializeDemo(\"init\"); } } 1:j i=0 n=0 2:构造块 i=1 n=1 3:t1 i=2 n=2 4:j i=3 n=3 5:构造块 i=4 n=4 6:t2 i=5 n=5 7:i i=6 n=6 8:静态块 i=7 n=99 9:j i=8 n=100 10:构造块 i=9 n=101 11:init i=10 n=102 实例三： class Glyph { void draw() { System.out.println(\"Glyph.draw()\"); } Glyph() { System.out.println(\"Glyph() before draw()\"); draw(); System.out.println(\"Glyph() after draw()\"); } } class RoundGlyph extends Glyph { private int radius = 1; RoundGlyph(int r) { radius = r; System.out.println(\"RoundGlyph.RoundGlyph(), radius = \" + radius); } void draw() { System.out.println(\"RoundGlyph.draw(), radius = \" + radius); } } public class PolyConstructors { public static void main(String[] args) { new RoundGlyph(5); /** * Glyph() before draw() RoundGlyph.draw(), radius = 0 Glyph() after draw() RoundGlyph.RoundGlyph(), radius = 5 */ } } 字节码执行引擎 概述 虚拟机的执行引擎不是直接建立在处理器、硬件、指令集和操作系统层面的，而是由自己实现的，因此可以自行制定指令集与执行引擎的结构体系，并能够执行哪些不被硬件直接支持的指令集格式。 在不同的虚拟机实现里面，执行引擎在执行Java代码的时候可能会有解释执行和编译执行（通过即时编译器产生本地代码)两种选择，也可能两者兼备。但从外观上看起来，所有的Java虚拟机都是一样的：输入的是字节码文件，处理过程是字节码解析的等效过程，输出的是执行过程。 运行时栈帧结构 栈帧（Stack Frame)是用于支持虚拟机进行方法调用和方法执行的数据结构，它是虚拟机运行时数据区中的虚拟机栈的栈元素。栈帧存储了方法的局部变量表、操作数栈、动态链接、返回地址等信息。每一个方法从调用开始至执行完成过程，都对应着一个栈帧在虚拟机栈里面从入栈到出栈的过程。 在编译程序代码时，栈帧中需要多大的局部变量表，多深的操作数栈都已经完全确定了，并且写入到方法表的Code属性之中。因此，一个栈帧需要分配多少内存，不会受到程序运行期变量数据的影响，而仅仅取决于具体的虚拟机实现。 一个线程中的方法调用链可能会很长，很多方法同时处于执行状态。对于执行引擎来说，在活动线程中，只有位于栈顶的栈帧才是有效的，称为当前栈帧，与这个栈帧相关联的方法称为当前方法。执行引擎运行的所有字节码指令都只针对当前栈帧进行操作。 局部变量表 局部变量表（Local Variable Table)是一组变量值存储空间，用于存放方法参数和方法内部定义的局部变量。方法的Code属性的max_locals数据项中确定了该方法所需要分配的局部变量表的最大容量。 局部变量表的容量以变量槽（Slot)为最小单位，每个Slot都应该能存放一个boolean、byte、char、short、Reference等类型的数据，允许Slot的长度可以随着处理器、操作系统或虚拟机的实现不同而发生变化。 一个Slot可以存放一个对象实例的引用，虚拟机能够通过这个引用做到两点：一是从此引用中直接或间接地查找对象在Java堆中的数据存放的起始地址索引，二是此引用中直接或间接地查找到对象所属数据类型在方法区中的存储的类型信息。 局部变量表是线程私有的数据，无论读写两个连续的Slot（long、double)是否为原子操作，都不会引起线程安全问题。 对于64位的数据类型，虚拟机会以高位对齐的方式为其分配两个连续的Slot空间。 虚拟机通过索引定位的方式使用局部变量表，索引值的范围从0开始至局部变量表最大的Slot数量。对于两个相邻的共同存放一个64位数据的两个Slot，不允许采用任何方式单独访问其中的某一个。 在方法执行时，虚拟机是使用局部变量表完成参数值到参数变量列表的传递过程的，如果执行的是实例方法，那局部变量表的第0位索引的Slot默认是用于传递方法所属对象实例的引用，在方法中可以通过关键字this来访问到这个隐含的参数。其他参数则按照参数表顺序排列，占用从1开始的局部变量Slot，参数分配完毕后，再根据方法体内部定义的变量顺序和作用域分配其余的Slot。 局部变量表中的Slot是可以重用的，方法体中定义的变量，其作用域并不一定会覆盖整个方法体，如果当前字节码PC计数器的值已经超出了某个变量的作用域，那整个变量对应的Slot就可以交给其他变量使用。Slot的复用会直接影响到系统的垃圾收集行为。 操作数栈 操作数栈（Operand Stack)是一个后进先出栈。操作数栈的最大深度也是在编译的时候就写入到Code属性的max_stacks数据项中。操作数栈的每一个元素可以是任意的Java数据类型，包括long和double。32位数据类型所占的栈容量为1,64位数据类型所占的栈容量为2。在方法执行的任何时候，操作数栈的深入都不会超过max_stacks。 在概念模型中，两个栈帧作为虚拟机栈的元素，是完全相互独立的。但在大多数虚拟机的实现里都会做一些优化处理，令两个栈帧出现一部分重叠。让下面栈帧的部分操作数栈与上面栈帧的部分局部变量表重叠在一起，这样在进行方法调用时就可以共用一部分数据，无须进行额外的参数复制传递。 Java虚拟机的解释执行引擎称为基于栈的执行引擎，其中的栈就是操作数栈。 动态连接 每个栈帧都包含一个执行运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态连接（Dynamic Linking)。Class文件的常量池存有大量的符号引用，字节码中的方法调用指令就以常量池中指向方法的符号引用作为参数。这些符号引用一部分会在类加载阶段或者第一次使用的时候就转化为直接引用，这种转化称为静态解析。另外一部分将在每一次运行期间转化为直接引用，这部分称为动态连接。 方法返回地址 当一个方法开始执行后，只有两种方式可以退出这个方法。第一种方式是执行引擎遇到任意一个方法返回的字节码指令，这时候可能会有返回值传递给上层的方法调用者，是否有返回值和返回值的类型将根据遇到何种方法返回指令来决定，这种退出方法的方式称为正常完成出口。 另外一种退出方式是，在方法执行过程中遇到了异常，并且这个异常没有在方法体内得到处理，无论是Java虚拟机内部产生的异常，还是代码中使用athrow字节码指令产生的异常，只要在本方法的异常表中没有搜索到匹配的异常处理器，就会导致方法退出，这种退出方法的方式称为异常完成出口。一个方法使用异常完成出口的方式退出，是不会给它的上层调用者产生任何返回值的。 无论何种退出方式，在方法退出之后，都需要返回到方法被调用的位置，程序才能继续执行。一般来说，方法正常退出时，调用者的PC计数器的值可以作为返回地址，栈帧中很可能会保存这个计数器值。而方法异常退出时，返回地址是通过异常处理器确定的，栈帧中一般不会保存这部分信息。 方法退出的过程实际上就等同于把当前栈帧出栈，因此退出可能的操作有：恢复上层方法的局部变量表和操作数栈，把返回值压入调用者栈帧的操作数栈中，调整PC计数器的值以指向方法调用指令后面的一条指令。5.22 方法调用 方法调用并不等同于方法执行，方法调用阶段唯一的任务就是确定被调用方法的版本（即调用哪一个方法)，暂时不涉及方法内部的具体执行过程。Class文件的编译过程不包含传统编译中的连接步骤，一切方法调用在Class文件中存储的都是符号引用，而不是方法在实际运行时内存布局中的入口地址。这个特性使得Java方法调用过程变得复杂，需要在类加载器件，甚至到运行期间才能确定目标方法的直接引用。 解析 符号引用能转为直接引用成立的前提是：方法在程序真正运行之前就有一个可确定的调用版本，并且这个方法的调用版本在运行期是不可改变的。换句话说，调用目标在程序代码写好、编译器进行编译时就必须确定下来。这类方法的调用称为解析。 在Java语言中符合“编译期可知，运行期不可变”这个要求的方法，主要包括静态方法和私有方法，前者和类型直接关联，后者在外部不可被访问，这两种方法的特点决定了它们都不可能通过继承或别的方式重写其他版本，因此它们都适合在类加载阶段进行解析。 只要能被invokestatic和invokespecial指令调用的方法，都可以在解析阶段中确定唯一的调用版本，符合这个条件的有静态方法、私有方法、实例构造器、父类方法4类，它们在类加载的时候就会把符号引用解析为该方法的直接引用。这个方法可以称为非虚方法，与之相反，其他方法称为虚方法（除去final)。 非虚方法也包含被final修饰的方法。虽然final方法是使用invokevirtual指令来调用的，但是由于它无法被覆盖，没有其他版本，所以也无须对方法调用者进行多态选择，又或者说多态选择的结果肯定是位移的。 解析调用一定是个静态过程，在编译期间就完全确定，在类加载的解析阶段就会把涉及的符号引用全部转为可确定的直接引用，不会延迟到运行期再去完成。而分派调用则可能是静态的也可能是动态的，根据分派依据的宗量数可分为单分派和多分派。这两类分派方式的组合就构成了静态单分派、静态多分派、动态单分派、动态多分派这4中分派组合情况。 分派 分派调用过程将会揭示多态性的一些最基本体现，如重载和重写。 1、静态分派 上面代码中的Human称为变量的静态类型（Static Type)，或者叫做外观类型，后面的Man则称为变量的实际类型（Actual Type)，静态类型和实际类型在程序中都可以发生一些变化，区别是静态类型的变化仅仅在使用时发生，变量本身的静态类型不会发生改变，并且最终的静态类型是在编译期可知的；而实际类型变化的结果在运行期才可确定，编译器在编译程序的时候并不知道一个对象的实际类型是什么。 代码中刻意定义了两个静态类型相同但是实际类型不同的变量，但编译器在重载时是通过参数的静态类型而不是实际类型作为判定依据的。并且静态类型是编译器可知的，因此，在编译阶段，Javac编译器会根据参数的静态类型决定使用哪个重载版本，，所以选择了sayHello（Human)作为调用目标，并把这个方法的符号引用写到main（)方法里的两条invokevirtual指令的参数中。 所有依赖静态类型来定位方法执行版本的分派动作被称为静态分派。静态分派的典型应用是方法重载。静态分派发生在编译阶段，因此确定静态分派的动作实际上不是由虚拟机来执行的。另外，编译器虽然能确定出方法的重载版本，但在很多情况下这个重载版本并不是唯一的，往往只能确定一个更加合适的版本。产生这种模糊结论的主要原因是字面量不需要定义，所以字面量没有显式地静态类型，它的静态类型只能通过语言上的规则去理解和推断。 2、动态分派 重写 导致整个现象的原因很明显，是这两个变量的实际类型不同。 以下为字节码 由于invokevitual指令执行的第一步就是 在运行期确定接受者的实际类型，所以两次调用中的invokevirtual指令把常量池中的类方法符号解析到了不同的直接引用上，这个过程就是Java语言中方法重写的本质，我们把这种在运行期根据实际类型确定方法执行版本的分派过程称为动态分派。 3、单分派与多分派 方法的接收者和方法的参数统称为方法的宗量。根据分派基于多少种宗量，可以将分派划分为单分派和多分派两种。单分派是根据一个宗量对目标方法进行选择，多分派则是根据多于一个宗量对目标方法进行选择。 今天的Java语言是一门静态多分派、动态单分派的语言。 4、虚拟机动态分派的实现 由于动态分派是非常频繁的动作，而且动态分派的方法版本选择过程需要运行时在类的方法元数据中搜索合适的目标方法，因此在虚拟机的实际实现中基于性能的考虑，大部分实现都不会真正地进行如何频繁的搜索。最常用的稳定优化的方法就是为类在方法区中建立一个虚方法表，使用虚方法表索引来代替元数据查找以提高性能。 虚方法表中存放着各个方法的实际入口地址。如果某个方法在子类中没有被重写，那子类的虚方法表里面的地址入口和父类相同方法的地址入口是一致的，都指向父类的实现入口。如果子类中重写了这个方法，子类方法表中的地址将会替换为指向子类实现版本的入口地址。 为了程序实现上的方便，具有相同签名的方法，在父类、子类的虚方法表中都应当具有一样的索引编号，这样当类型变换时，仅需要变更查找的方法表，就可以从不同的虚方法表中按索引转换出所需的入口地址。 方法表一般在类加载的连接阶段进行初始化，准备了类的变量初始值后，虚拟机会把该类的方法表也初始化完毕。 虚拟机除了使用方法表之外，在条件允许的情况下，还会使用内联缓存和基于类型继承关系分析技术的守护内联两种非稳定的激进优化手段来获得更高的性能。 基于栈的字节码解释执行引擎 许多Java虚拟机的执行引擎在执行Java代码的时候都有解释执行和编译执行两种选择。 解释执行 Java语言经常被人们定位为解释执行的语言，但当主流的虚拟机都包含了即时编译器后，Class文件中的代码到底会被解释执行还是编译执行，就成了只有虚拟机自己才能准确判断的事情。只有确定了谈论对象是某种具体的Java实现版本和执行引擎运行模式时，谈解释执行还是编译执行才会比较确切。 基于栈的指令集和基于寄存器的指令集 Java编译器输出的指令流，基本上是一种基于栈的指令集架构（Instruction Set Architecture,ISA)，指令流中的指令大部分都是零地址指令，它们依赖操作数栈进行工作。与之相对的另外一套常用的指令集架构是基于寄存器的指令集。 计算1+1： 前者： 后者： 基于栈的指令集主要的优点是可移植，寄存器由硬件直接提供，程序直接依赖这些硬件寄存器则不可避免地要受到硬件的约束。栈架构的指令集还有一些其他的优点，如代码相对更加紧凑、编译器实现更加简单（不需要考虑空间分配，都在栈上操作)等。 栈架构指令集的主要缺点是执行速度相对来说会稍慢一些。 虽然栈架构指令集的代码非常紧凑，但是完成相同功能所需的指令数量一般会比寄存器架构多，因为出栈、入栈操作本身就产生了相当多的指令数量。更重要的是，栈实现在内存之中，频繁的栈访问也就意味着频繁的内存访问，相对于处理器来说，内存始终是执行速度的瓶颈。尽管虚拟机可以采取栈顶缓存的手段，把最常用的操作映射到寄存器中避免直接内存访问，但这也只能是优化措施而不是解决本质问题的方法。由于指令数量和内存访问的原因，所以导致了栈架构指令集的执行速度会相对较慢。 程序编译与代码优化 5.23 字节码的编译过程（前端编译器) Java语言的编译期是一段不确定的操作过程。 1)编译器前端/前端编译器：把java文件转为class文件，比如Sun的Javac 2)编译器后端/后端运行时编译器(JIT Just In Time 编译器)：把字节码转为机器码，比如HotSpot VM的C1、C2编译器 3)静态提前编译器（AOT Ahead Of Time 编译器)：直接把java文件编译为本地机器代码，比如GNU Compiler for the Java（GCJ)。 通常意义上的编译器就是前端编译器，这类编译器对代码的运行效率几乎没有任何优化，把对性能的优化集中到了后端编译器，这样可以使其他语言的class文件也同样能享受到编译器优化所带来的好处。 但是Javac做了很多针对Java语言编码过程中的优化措施来改善程序员的编码风格和提高编码效率，相当多的新的语法特性都是靠前端编译器的语法糖实现的，而非依赖虚拟机的底层改进来实现。 Javac的编译过程大致可以分为三个阶段： 1)解析和填充符号表 2)插入式注解处理器的注解处理 3)语义分析与字节码生成 解析与填充符号表 - 1)解析包括了词法分析和语法分析两个过程。 词法分析是将源代码的字符流变为Token序列； 语法分析是根据Token序列构造抽象语法树AST的过程 2)填充符号表 符号表是由一组符号地址和符号信息构成的表格。 符号表中所登记的信息在编译的不同阶段都要用到。 插入式注解处理器的注解处理 插入式注解处理器可以视为一组编译器的插件，可以读取、修改、添加AST中的任意元素。如果在处理注解期间对AST进行了修改，那么编译器将回到解析与填充符号表的过程重新处理，每一次循环称为一个Record。 语义分析与字节码生成 语义分析的主要任务是对结构上正确的源程序进行上下文有关性质的审查。 语义分析的过程分为Token检查和数据及控制流分析两个阶段。 1)Token检查的内容包括变量使用前是否声明、变量和赋值之间的数据类型能否匹配，还有常量折叠等。 2)数据及控制流分析是对程序上下文逻辑进行更进一步的验证，它可以检查出如程序员局部变量在使用前是否有赋值、方法的每条路径是否都有返回值、是否所有的受检异常都被正确处理等。 3)解语法糖：比如泛型、变长参数、自动装箱/拆箱等 4)字节码生成：不仅仅是把签个各个步骤所生成的信息转化成字节码写到磁盘中，编译器还进行了少量的代码添加和转换工作，比如添加实例构造器()和类构造器()。5.24 后端编译器的优化（JIT) 解决以下几个问题： 1)为何HotSpot虚拟机要使用解释器和编译器并存的架构 2)为何HotSpot虚拟机要实现两个不同的JIT 3)程序何时使用解释器执行，何时使用编译器执行 4)哪些程序代码会被编译为本地代码，如何编译为本地代码 5)如何从外部观察JIT的编译过程和编译结果 编译器与解释器 解释器与编译器各有优势，前者节省内存，后者提高效率。 在整个虚拟机执行架构中，解释器与编译器经常配合工作。 HotSpot虚拟机中内置了两个JIT，分别称为Client Compiler和Server Compiler。在虚拟机中习惯将Client Compiler称为C1，将Server Complier 称为C2。目前主流的HotSpot虚拟机中，默认采用解释器与其中一个编译器直接配合的方式工作，程序使用哪个编译器取决于虚拟机运行的模式。HotSpot虚拟机会根据自身版本与机器硬件性能自动选择运行模式，用户也可以使用-client或者-server参数去强制指定虚拟机运行的模式。 无论采用哪一种编译器，解释器与编译器搭配使用的方式在虚拟机中称为混合模式，用户可以使用参数-Xint强制虚拟机运行于解释模式，这时编译器完全不介入工作；也可以使用参数-Xcomp强制虚拟机运行于编译模式，这时将优先采用编译方式执行程序，但是解释器仍然要在编译无法进行的情况下介入执行过程。 为了在程序启动响应速度与运行效率之间达到最佳平衡，HotSpot虚拟机会逐渐启用分层编译的策略。 第0层：程序解释执行，解释器不开启性能监控功能，可触发第1层编译 第1层：也称为C1编译，将字节码编译为本地代码，进行简单、可靠的优化，如有必须将加入性能监控的逻辑 第2层（或2层以上)：也称为C2编译，也是将字节码编译为本地代码，但是会启动一些编译耗时较长的优化，甚至会根据性能监控信息进行一些不可靠的激进优化。 实时分层编译后，Client Compiler和Server Compiler将会同时工作，很多代码都可能会被多次编译，用Client Compiler获取更高的编译速度，用Server Compile获取更好的编译质量，在解释执行的时候也无需再承担收集性能监控信息的任务。 编译对象与触发条件 在运行过程中被JIT编译的热点代码有两类： 1)被多次调用的方法 2)被多次执行的循环体热点探测 编译器都会以整个方法作为编译对象。这种编译方式因为编译发生在方法执行过程之中，因此形象地成为栈上替换（On Stack Replacement OSR)。 判断一段代码是不是热点代码，是不是需要触发JIT，这样的行为称为热点探测。目前主流的热点探测判定方式有两种： 1)基于采样的热点探测：采用这种方法的虚拟机会周期性地检查各个线程的栈顶，如果发现某些方法经常出现在栈顶，那这些方法就是热点方法。好处是简单高效，还可以很容易得获取方法调用关系，缺点是很难精确地确认一个方法的热度，容易因为受到线程阻塞或别的外界因素的影响而扰乱热点探测。 2)基于计数器的热点探测：采用这种方法的虚拟机会为每个方法建立计数器，统计方法的执行次数，如果执行次数超过一定的阈值就认为它是热点方法。好处是更加精确演进，缺点是实现较为麻烦。 HotSpot采用的第二种方法，因为它为每个方法准备了两类计数器：方法调用计数器和回边计数器。这两个计数器都有一个确定的阈值，当计数器超过阈值溢出了，就会触发JIT编译。 方法调用计数器 当一个方法被调用时，会先检查该方法是否存在被JIT编译过的版本，如果存在，则优先使用编译后的本地代码来执行。如果不存在已被编译过的版本，则将此方法的调用计数器加一，然后判断方法调用计数器与回边计数器之和是否超过方法调用计数器的阈值。如果已超过阈值，那么会向JIT提交一个该方法的代码编译请求。 如果不做任何设置，方法调用计数器统计的并不是方法被调用的总次数，而是一个相对的执行频率，即一段时间内方法被调用的次数。当超过一定的时间限度，如果方法的调用次数仍然不足以让它提交给JIT编译，则这个方法的调用计数器就会被减少一半，这个过程称为方法调用计数器的衰减，而这段时间就称为此方法统计的半衰周期。 回边计数器 回边计数器的作用是统计一个方法中循环体的代码执行次数，在字节码中遇到控制流向后调换的指令称为回边，回边计数器统计的目的就是为了触发OSR编译。 当解释器遇到一条回边指令时，会先查找将要执行的代码片段是否有这已经编译好的版本，如果有，它将会优先执行已编译的代码，否则就把回边计数器的值加一，然后判断方法调用计数器与回边计数器之和是否超过回边计数器的阈值。当超过阈值时，将会提交一个OSR编译请求，并且把回边计数器的值降低一些，以便继续在解释器中执行循环，等待编译器输出编译结果。 Client Compiler（编译速度快) 是一个简单快速的三段式编译器，主要的关注点在于局部性的优化，而放弃了许多耗时较长的全局优化手段。 三段式： 1)第一个阶段，一个平台独立的前端会将字节码构造成一种高级中间代码表示（HIR High-Level Intermediate Representation)。HIR使用静态单分配的形式来表示代码值，这使得一些在HIR的构造过程之中和之后进行的优化动作更容易实现。 在此之前编译器会在字节码上完成一部分基础优化，如方法内联、常量传播等。 2)第二个阶段，一个平台相关的后端从HIR中产生低级中间代码表示（LIR Low-Level Intermediate Representation)，而在此之前会在HIR上完成另外一些优化，如空值检查消除、范围检查消除等。 3)第三个阶段，一个平台相关的后端使用线性扫描算法在LIR上分配寄存器，并在LIR上做窥孔优化，然后产生机器代码。 Server Compiler（编译质量高) 是专门面向服务端的典型应用并为服务端的性能配置特别调整过的编译器，也是一个充分优化过的编译器。它会执行所有经典的优化动作，如无用代码消除、循环展开、循环表达式外提、消除公共子表达式、常量传播、基本块重排序等，还会实施一些与Java语言特性密切相关的优化技术，如范围检查消除、空值检查消除。另外还可能根据解释器或Client Compiler提供的性能监控信息，进行一些不稳定的激进优化，如守护内联、分值预测检测等。编译优化 语言无关的经典优化技术之一：公共子表达式消除 语言相关的经典优化技术之一：数组范围检查消除 最重要的优化技术之一：方法内联 最前沿的优化技术之一：逃逸分析语言相关的优化技术——逃逸分析 分析指针动态范围的方法称之为逃逸分析（通俗点讲，当一个对象的指针被多个方法或线程引用时们称这个指针发生了逃逸)。 逃逸分析并不是直接的优化手段，而是一个代码分析，通过动态分析对象的作用域，为其它优化手段如栈上分配、标量替换和同步消除等提供依据，发生逃逸行为的情况有两种：方法逃逸和线程逃逸。 1、方法逃逸：当一个对象在方法中定义之后，作为参数传递到其它方法中； 2、线程逃逸：如类变量或实例变量，可能被其它线程访问到； 如果不存在逃逸行为，则可以对该对象进行如下优化：同步消除、标量替换和栈上分配。 同步消除 线程同步本身比较耗，如果确定一个对象不会逃逸出线程，无法被其它线程访问到，那该对象的读写就不会存在竞争，则可以消除对该对象的同步锁，通过-XX:+EliminateLocks可以开启同步消除。 标量替换 1、标量是指不可分割的量，如java中基本数据类型和reference类型，相对的一个数据可以继续分解，称为聚合量； 2、如果把一个对象拆散，将其成员变量恢复到基本类型来访问就叫做标量替换； 3、如果逃逸分析发现一个对象不会被外部访问，并且该对象可以被拆散，那么经过优化之后，并不直接生成该对象，而是在栈上创建若干个成员变量； 通过-XX:+EliminateAllocations可以开启标量替换， -XX:+PrintEliminateAllocations查看标量替换情况。 栈上分配 故名思议就是在栈上分配对象，其实目前Hotspot并没有实现真正意义上的栈上分配，实际上是标量替换。 性能监控与故障处理工具 - 如果一个接口调用很慢，原因是，如何定位，没有日志的话：假设一下，复现问题，dump查看内存，查看监控日志 - 如何把java内存的数据全部dump出来 - 在生产线Dump堆分析程序是否有内存及性能问题 - jstack jmap、jconsole 等工具 可视化工具使用；如何线上排查JVM的相关问题？ - JVM线程死锁，你该如何判断是因为什么？如果用VisualVM，dump线程信息出来，会有哪些信息？ - 查看jvm虚拟机里面堆、线程的信息，你用过什么命令？ - 内存泄露如何定位 5.25 JPS：显示所有虚拟机进程 5.26 JConsole：图形化工具，查询JVM中的内存变化情况。 5.27 JVisualVM：图形化工具，分析GC趋势、内存消耗情况 可以分析堆dump文件 5.28 JMap：命令行工具，查看JVM中各个代的内存状况、JVM中对象的内存的占用状况，以及dump整个JVM中的内存信息。 jmap –heap [pid] 整个JVM中内存的状况 jmap –histo [pid] JVM堆中对象的详细占用情况 jmap –dump:format=b,file=文件名 [pid] 将整个JVM内存拷贝到文件中 5.29 JHat 用于分析JVM堆的dump文件：jhat –J-Xmx1024M [file] 可以通过浏览器访问，端口号是7000. 5.30 JStack：看到JVM中线程的运行状况，包括锁的等待、线程是否在运行等。 jstack [pid] jstack [option] pid jstack [option] executable core jstack [option] [server-id@]remote-hostname-or-ip 命令行参数选项说明如下： -l long listings，会打印出额外的锁信息，在发生死锁时可以用jstack -l pid来观察锁持有情况 -m mixed mode，不仅会输出Java堆栈信息，还会输出C/C++堆栈信息（比如Native方法) jstack可以定位到线程堆栈，根据堆栈信息我们可以定位到具体代码，所以它在JVM性能调优中使用得非常多。下面我们来一个实例找出某个Java进程中最耗费CPU的Java线程并定位堆栈信息，用到的命令有ps、top、printf、jstack、grep。 第一步先找出Java进程ID，服务器上的Java应用名称为mrf-center： root@ubuntu:/# ps -ef | grep mrf-center | grep -v grep （或者直接JPS查看进程PID) root 21711 1 1 14:47 pts/3 00:02:10 java -jar mrf-center.jar 第二步 top -H -p pid 用第三个，输出如下： PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 21936 root 20 0 1747m 21m 9404 S 0.0 0.6 0:00.00 java 21937 root 20 0 1747m 21m 9404 S 0.0 0.6 0:00.14 java 21938 root 20 0 1747m 21m 9404 S 0.0 0.6 0:00.00 java 21939 root 20 0 1747m 21m 9404 S 0.0 0.6 0:00.00 java 21940 root 20 0 1747m 21m 9404 S 0.0 0.6 0:00.00 java TIME列就是各个Java线程耗费的CPU时间，CPU时间最长的是线程ID为21742的线程，用 printf \"%x\\n\" 21742 得到21742的十六进制值为54ee，下面会用到。 OK，下一步终于轮到jstack上场了，它用来输出进程21711的堆栈信息，然后根据线程ID的十六进制值grep，如下： root@ubuntu:/# jstack 21711 | grep 54ee \"PollIntervalRetrySchedulerThread\" prio=10 tid=0x00007f950043e000 nid=0x54ee in Object.wait() 可以看到CPU消耗在PollIntervalRetrySchedulerThread这个类的Object.wait()，我找了下我的代码，定位到下面的代码： // Idle wait getLog().info(\"Thread [\" + getName() + \"] is idle waiting...\"); schedulerThreadState = PollTaskSchedulerThreadState.IdleWaiting; long now = System.currentTimeMillis(); long waitTime = now + getIdleWaitTime(); long timeUntilContinue = waitTime - now; synchronized(sigLock) { try { if(!halted.get()) { sigLock.wait(timeUntilContinue); } } catch (InterruptedException ignore) { } } 它是轮询任务的空闲等待代码，上面的sigLock.wait(timeUntilContinue)就对应了前面的Object.wait()。5.31 JStat：JVM统计监测工具 jstat [ generalOption | outputOptions vmid [interval[s|ms] [count]] ] vmid是Java虚拟机ID，在Linux/Unix系统上一般就是进程ID。interval是采样时间间隔。count是采样数目。 比如下面输出的是GC信息，采样时间间隔为250ms，采样数为4： root@ubuntu:/# jstat -gc 21711 250 4 S0C S1C S0U S1U EC EU OC OU PC PU YGC YGCT FGC FGCT GCT 192.0 192.0 64.0 0.0 6144.0 1854.9 32000.0 4111.6 55296.0 25472.7 702 0.431 3 0.218 0.649 192.0 192.0 64.0 0.0 6144.0 1972.2 32000.0 4111.6 55296.0 25472.7 702 0.431 3 0.218 0.649 192.0 192.0 64.0 0.0 6144.0 1972.2 32000.0 4111.6 55296.0 25472.7 702 0.431 3 0.218 0.649 192.0 192.0 64.0 0.0 6144.0 2109.7 32000.0 4111.6 55296.0 25472.7 702 0.431 3 0.218 0.649 S0C、S1C、S0U、S1U：Survivor 0/1区容量（Capacity)和使用量（Used) EC、EU：Eden区容量和使用量 OC、OU：年老代容量和使用量 PC、PU：永久代容量和使用量 YGC、YGT：年轻代GC次数和GC耗时 FGC、FGCT：Full GC次数和Full GC耗时 GCT：GC总耗时5.32 MAT 可视化分析dump文件 Memory Analyzer Tool 性能调优 5.33 参数 堆设置 -Xms:初始堆大小 -Xmx:最大堆大小 -Xmn年轻代大小 -XX:NewRatio=n:设置年轻代和年老代的比值。如:为3，表示年轻代与年老代比值为1：3，年轻代占整个年轻代年老代和的1/4 -XX:SurvivorRatio=n:年轻代中Eden区与两个Survivor区的比值。注意Survivor区有两个。如：3，表示Eden：Survivor=3：2，一个Survivor区占整个年轻代的1/5 栈设置 -Xss 设置每个线程的栈大小元数据区设置 -XX:MetaspaceSize -XX:MaxMetaspaceSize 元数据区的初始大小和最大大小 异常设置 -XX:+HeapDumpOnOutOfMemoryError 使得JVM在产生内存溢出时自动生成堆内存快照（日后再进行分析，写监控脚本，如果发现应用崩溃则重启，并提醒开发人员去查看dump信息) -XX:HeapDumpPath 改变默认的堆内存快照生成路径，可以是相对或者绝对路径 -XX:OnOutOfMemoryError 当内存发生溢出时 执行一串指令 收集器设置 -XX:+UseSerialGC:设置串行收集器 -XX:+UseParallelGC:设置并行收集器 -XX:+UseParalledlOldGC:设置并行年老代收集器 -XX:+UseConcMarkSweepGC:设置并发收集器垃圾回收统计信息 -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:filename并行收集器设置 -XX:ParallelGCThreads=n:设置并行收集器收集时使用的CPU数。并行收集线程数。 -XX:MaxGCPauseMillis=n:设置并行收集最大暂停时间 -XX:GCTimeRatio=n:设置垃圾回收时间占程序运行时间的百分比。公式为1/(1+n)并发收集器设置 -XX:+CMSIncrementalMode:设置为增量模式。适用于单CPU情况。 -XX:ParallelGCThreads=n:设置并发收集器年轻代收集方式为并行收集时，使用的CPU数。并行收集线程数。5.34 调优原则 JVM的内存参数；xmx,xms,xmn,xss参数你有调优过吗，设置大小和原则你能介绍一下吗？；Xss默认大小，在实际项目中你一般会设置多大 对JVM内存的系统级的调优主要的目的是减少GC的频率和Full GC的次数，过多的GC和Full GC是会占用很多的系统资源（主要是CPU)，影响系统的吞吐量。特别要关注Full GC，因为它会对整个堆进行整理。 代大小的调优 避免新生代大小设置过小，过小的话一是minor GC更加频繁，二是有可能导致minor GC对象直接进入老年代，此时如进入老年代的对象占据了老年代剩余空间，则触发Full GC。 避免新生代大小设置过大，过大的话一是老年代变小了，有可能导致Full GC频繁执行，二是minor GC的耗时大幅度增加。 避免Survivor区过小或过大 合理设置新生代存活周期，存活周期决定了新生代的对象经过多少次Minor GC后进入老年代，对应的JVM参数是-XX;MaxTenuringThreshold。 GC策略的调优 串行GC性能太差，在实际场景中使用的主要为并行和并发GC。 由于CMS GC多数动作是和应用并发进行的，确实可以减小GC给应用带来的暂停。 "},"zother5-Java-Interview/八、Mysql.html":{"url":"zother5-Java-Interview/八、Mysql.html","title":"八、Mysql","keywords":"","body":"数据库理论 OLTP与OLAP 8.1 OLTP（关系型数据库) OLTP即联机事务处理，就是我们经常说的关系数据库，意即记录即时的增、删、改、查，就是我们经常应用的东西，这是数据库的基础 对于各种数据库系统环境中大家最常见的OLTP系统，其特点是并发量大，整体数据量比较多，但每次访问的数据比较少，且访问的数据比较离散，活跃数据占总体数据的比例不是太大。对于这类系统的数据库实际上是最难维护，最难以优化的，对主机整体性能要求也是最高的。因为不仅访问量很高，数据量也不小。 针对上面的这些特点和分析，我们可以对OLTP的得出一个大致的方向。 虽然系统总体数据量较大，但是系统活跃数据在数据总量中所占的比例不大，那么我们可以通过扩大内存容量来尽可能多的将活跃数据cache到内存中； 虽然IO访问非常频繁，但是每次访问的数据量较少且很离散，那么我们对磁盘存储的要求是IOPS(Input/Output Operations Per Second，即每秒进行读写操作的次数)表现要很好，吞吐量是次要因素； 并发量很高，CPU每秒所要处理的请求自然也就很多，所以CPU处理能力需要比较强劲； 虽然与客户端的每次交互的数据量并不是特别大，但是网络交互非常频繁，所以主机与客户端交互的网络设备对流量能力也要求不能太弱。 8.2 OLAP（数据分析挖掘) 用于数据分析的OLAP系统的主要特点就是数据量非常大，并发访问不多，但每次访问所需要检索的数据量都比较多，而且数据访问相对较为集中，没有太明显的活跃数据概念。 OLAP即联机分析处理，是数据仓库的核心部心，所谓数据仓库是对于大量已经由OLTP形成的数据的一种分析型的数据库，用于处理商业智能、决策支持等重要的决策信息；数据仓库是在数据库应用到一定程序之后而对历史数据的加工与分析 基于OLAP系统的各种特点和相应的分析，针对OLAP系统硬件优化的大致策略如下： 数据量非常大，所以磁盘存储系统的单位容量需要尽量大一些； 单次访问数据量较大，而且访问数据比较集中，那么对IO系统的性能要求是需要有尽可能大的每秒IO吞吐量，所以应该选用每秒吞吐量尽可能大的磁盘； 虽然IO性能要求也比较高，但是并发请求较少，所以CPU处理能力较难成为性能瓶颈，所以CPU处理能力没有太苛刻的要求； 虽然每次请求的访问量很大，但是执行过程中的数据大都不会返回给客户端，最终返回给客户端的数据量都较小，所以和客户端交互的网络设备要求并不是太高； 此外，由于OLAP系统由于其每次运算过程较长，可以很好的并行化，所以一般的OLAP系统都是由多台主机构成的一个集群，而集群中主机与主机之间的数据交互量一般来说都是非常大的，所以在集群中主机之间的网络设备要求很高。 数据库完整性 8.3 实体完整性 primary key （列级约束和表级约束) 定义主码之后，每当用户程序对基本表插入一条记录或对主码列进行更新操作时，DBMS将会检查 1)检查主码值是否唯一：一种方法是全表扫描，耗时长；DBMS一般在主码上自动建立一个索引，通过索引查找基本表中是否已经存在新的主码值将大大提供效率。 2)检查主码值是否为空 8.4 参照完整性 foreign key references ... 当上述的不一致发生时，系统可以采用以下策略加以处理： 1)拒绝执行（No action) 2)级联操作（Cascade)：当删除或修改被参照表时的一个元组导致与参照表不同时，删除或修改参照表中的所有导致不一致的元组。 3)设置为空值：当删除或修改被参照表时的一个元组导致与参照表不同时，则将参照表中 的所有造成不一致的元组的对应属性设置为空值。 关系查询处理和查询优化 关系数据库系统的查询处理 查询处理是RDBMS执行查询语句的过程，其任务是把用户提交给RDBMS的查询语句转换为高效的查询执行计划。 查询处理步骤 查询处理分为4个阶段：查询分析、查询检查、查询优化和查询执行。 1、查询分析（语法) 对查询语句进行扫描、词法分析、语法分析 2、查询检查（语义) 对合法的查询语句进行语义检查，即根据数据字典中有关的模式定义检查语句中的数据库对象，如关系名、属性名等是否存在和有效。然后进行安全性、完整性检查。检查通过后把SQL语句转换成等价的关系代数表达式。RDBMS一般采用查询树（语法树)来表示拓展的关系代数表达式。 3、查询优化 查询优化就是选择一个高效执行的查询处理策略。 分为代数优化和物理优化 1)代数优化是指关系代数表达式的优化，即按照一定的规则，通过对关系代数表达式进行等价变换，改变代数表达式中操作的次序和组合，使查询执行更高效 2)物理优化是指通过存取路径和底层操作算法的选择进行的优化 选择的依据可以是基于规则的、基于代价的、基于语义的。 4、查询执行 依据优化器得到的执行策略生成查询执行计划，由代码生成器生成执行这个查询计划的代码，然后加以执行，回送查询结果。 实现查询操作的算法示例1、选择操作的实现 （1) 全表扫描方法 (Table Scan) 对查询的基本表顺序扫描，逐一检查每个元组是否满足选择条件，把满足条件的元组作为结果输出 适合小表，不适合大表 （2)索引扫描方法 (Index Scan) 适合于选择条件中的属性上有索引(例如B+树索引或Hash索引) 通过索引先找到满足条件的元组主码或元组指针，再通过元组指针直接在查询的基本表中找到元组 1)全表扫描算法 假设可以使用的内存为M块，全表扫描算法思想： ①- 按照物理次序读Student的M块到内存 ②- 检查内存的每个元组t，如果满足选择条件，则输出t ③- 如果student还有其他块未被处理，重复①和② 1）- 索引扫描算法 [例9.1-C2] SELECT * FROM Student WHERE Sno='201215121' - 假设Sno上有索引(或Sno是散列码) - 算法： - 使用索引(或散列)得到Sno为‘201215121’ 元组的指针 - 通过元组指针在Student表中检索到该学生 [例9.1-C3] SELECT * FROM Student WHERE Sage>20 - 假设Sage 上有B+树索引 - 算法： - 使用B+树索引找到Sage=20的索引项，以此为入口点在B+树的顺序集上得到Sage>20的所有元组指针 - 通过这些元组指针到student表中检索到所有年龄大于20的学生。 [例9.1-C4] SELECT * FROM Student WHERE Sdept='CS' AND Sage>20; - 假设Sdept和Sage上都有索引 - 算法一：分别用索引扫描找到Sdept＝’CS’的一组元组指针和Sage>20的另一组元组指针 - 求这两组指针的交集 - 到Student表中检索 - 得到计算机系年龄大于20的学生 - 算法二：找到Sdept=’CS’的一组元组指针， - 通过这些元组指针到Student表中检索 - 并对得到的元组检查另一些选择条件(如Sage>20)是否满足 - 把满足条件的元组作为结果输出。 当选择率较低时，基于索引的选择算法要优于全表扫描算法。但在某些情况下，例如选择率较高，或者要查找的元组均匀地分布在查找的表中，这是基于索引的选择算法性能不如全表扫描算法。因此除了对表的扫描操作，还要加上对B+树索引的扫描操作，对每一个检索码，从B+树根节点到叶子结点路径上的每个结点都要进行一次IO操作。 2、连接操作的实现 连接操作是查询处理中最耗时的操作之一 本节只讨论等值连接(或自然连接)最常用的实现算法 [例9.2] SELECT * FROM Student, SC WHERE Student.Sno=SC.Sno; 1)嵌套循环算法(nested loop join) - 对外层循环(Student表)的每一个元组(s)，检索内层循环(SC表)中的每一个元组(sc) - 检查这两个元组在连接属性(Sno)上是否相等 - 如果满足连接条件，则串接后作为结果输出，直到外层循环表中的元组处理完为止。 2)排序-合并算法(sort-merge join 或merge join) - 如果连接的表没有排好序，先对Student表和SC表按连接属性Sno排序 - 取Student表中第一个Sno，依次扫描SC表中具有相同Sno的元组 - 当扫描到Sno不相同的第一个SC元组时，返回Student表扫描它的下一个元组，再扫描SC表中具有相同Sno的元组，把它们连接起来 - 重复上述步骤直到Student 表扫描完 Student表和SC表都只要扫描一遍 如果两个表原来无序，执行时间要加上对两个表的排序时间 对于大表，先排序后使用排序-合并连接算法执行连接，总的时间一般仍会减少3)索引连接(index join)算法 步骤： ① 在SC表上已经建立属性Sno的索引。 ② 对Student中每一个元组，由Sno值通过SC的索引查找相应的SC元组。 ③ 把这些SC元组和Student元组连接起来 循环执行②③，直到Student表中的元组处理完为止 只有表2需要索引 4)Hash Join算法 - 把连接属性作为hash码，用同一个hash函数把Student表和SC表中的元组散列到hash表中。 - 划分阶段（Build) - 对包含较少元组的表(如Student表)进行一遍处理 - 把它的元组按hash函数分散到hash表的桶中 - 试探阶段（Probe) - 对另一个表(SC表)进行一遍处理 - 把SC表的元组也按同一个hash函数（hash码是连接属性)进行散列 - 把SC元组与桶中来自Student表并与之相匹配的元组连接起来 将小表转为哈希表，用表1的匹配字段用哈希函数映射到哈希表 上面hash join算法前提：假设两个表中较小的表在第一阶段后可以完全放入内存的hash桶中 关系数据库系统的查询优化 查询优化在关系数据库系统中有着非常重要的地位 关系查询优化是影响关系数据库管理系统性能的关键因素 由于关系表达式的语义级别很高，使关系系统可以从关系表达式中分析查询语义，提供了执行查询优化的可能性 查询优化概述 关系系统的查询优化 是关系数据库管理系统实现的关键技术又是关系系统的优点所在 减轻了用户选择存取路径的负担 非关系系统 用户使用过程化的语言表达查询要求，执行何种记录级的操作，以及操作的序列是由用户来决定的 用户必须了解存取路径，系统要提供用户选择存取路径的手段，查询效率由用户的存取策略决定 如果用户做了不当的选择，系统是无法对此加以改进的 查询优化的优点 1、用户不必考虑如何最好地表达查询以获得较好的效率 2、系统可以比用户程序的“优化”做得更好 （1) 优化器可以从数据字典中获取许多统计信息，而用户程序则难以获得这些信息。 （2)如果数据库的物理统计信息改变了，系统可以自动对查询重新优化以选择相适应的执行计划。在非关系系统中必须重写程序，而重写程序在实际应用中往往是不太可能的。 （3)优化器可以考虑数百种不同的执行计划，程序员一般只能考虑有限的几种可能性。 （4)优化器中包括了很多复杂的优化技术，这些优化技术往往只有最好的程序员才能掌握。系统的自动优化相当于使得所有人都拥有这些优化技术。 关系数据库管理系统通过某种代价模型计算出各种查询执行策略的执行代价，然后选取代价最小的执行方案 查询优化的总目标 选择有效的策略 求得给定关系表达式的值 使得查询代价最小(实际上是较小) 一个实例 一个关系查询可以对应不同的执行方案，其效率可能相差非常大。 [例9.3] 求选修了2号课程的学生姓名。 SELECT Student.Sname FROM Student, SC WHERE Student.Sno=SC.Sno AND SC.Cno=’2’ 假定学生-课程数据库中有1000个学生记录，10000个选课记录 选修2号课程的选课记录为50个 第一种情况： 1、计算笛卡尔积 算法： 1)在内存中尽可能多地装入某个表(如Student表)的若干块，留出一块存放另一个表(如SC表)的元组。 2)把SC中的每个元组和Student中每个元组连接，连接后的元组装满一块后就写到中间文件上 3)从SC中读入一块和内存中的Student元组连接，直到SC表处理完。 4)再读入若干块Student元组，读入一块SC元组 5)重复上述处理过程，直到把Student表处理完 2、作选择操作 依次读入连接后的元组，按照选择条件选取满足要求的记录 假定内存处理时间忽略。读取中间文件花费的时间(同写中间文件一样)需读入106块。 若满足条件的元组假设仅50个，均可放在内存。 3、作投影操作 把第2步的结果在Sname上作投影输出，得到最终结果 第一种情况下执行查询的总读写数据块 第二种情况： 1、计算自然连接 1)执行自然连接，读取Student和SC表的策略不变，总的读取块数仍为2100块 2)自然连接的结果比第一种情况大大减少，为104个元组 3)写出数据块= 103 块 2、读取中间文件块，执行选择运算，读取的数据块= 103 块 3、把第2步结果投影输出。 第二种情况下执行查询的总读写数据块=2100+ 103 +103 其执行代价大约是第一种情况的488分之一 第三种情况： 1、先对SC表作选择运算，只需读一遍SC表，存取100块，因为满足条件的元组仅50个，不必使用中间文件。 2、读取Student表，把读入的Student元组和内存中的SC元组作连接。也只需读一遍Student表共100块。 3、把连接结果投影输出 第三种情况总的读写数据块=100+100 其执行代价大约是第一种情况的万分之一，是第二种情况的20分之一 - 假如SC表的Cno字段上有索引 - 第一步就不必读取所有的SC元组而只需读取Cno=‘2’的那些元组(50个) - 存取的索引块和SC中满足条件的数据块大约总共3～4块 - 若Student表在Sno上也有索引 - 不必读取所有的Student元组 - 因为满足条件的SC记录仅50个，涉及最多50个Student记录 - 读取Student表的块数也可大大减少 有选择和连接操作时，先做选择操作，这样参加连接的元组就可以大大减少，这是代数优化 在Q3中SC表的选择操作算法有全表扫描或索引扫描，经过初步估算，索引扫描方法较优。 对于Student和SC表的连接，利用Student表上的索引，采用索引连接代价也较小，这就是物理优化。 代数优化 关系代数表达式等价变换规则 代数优化策略：通过对关系代数表达式的等价变换来提高查询效率 关系代数表达式的等价：指用相同的关系代替两个表达式中相应的关系所得到的结果是相同的 两个关系表达式E1和E2是等价的，可记为E1≡E2 常用的代数变换规则： 1.连接、笛卡尔积交换律 连接、笛卡尔积的结合律 3.投影的串接定律 减少IO次数 选择的串接定律 合并条件 选择与投影操作的交换律 先选择后投影效率更高 选择与笛卡尔积的交换律 减少IO次数 选择与并的分配律 选择与差运算的分配律 选择对自然连接的分配律 投影与笛卡尔积的分配律 投影与并的分配律 查询树的启发式优化 典型的启发式规则 （1)选择运算应尽可能先做 在优化策略中这是最重要、最基本的一条。 （2)把投影运算和选择运算同时进行 如有若干投影和选择运算，并且它们都对同一个关系操作，则可以在扫描此关系的同时完成所有的这些运算以避免重复扫描关系。 （3) 把投影同其前或其后的双目运算结合起来，没有必要为了去掉某些字段而扫描一遍关系。 （4) 把某些选择同在它前面要执行的笛卡尔积结合起来成为一个连接运算，连接特别是等值连接运算要比同样关系上的笛卡尔积省很多时间。 （5) 找出公共子表达式 如果这种重复出现的子表达式的结果不是很大的关系，并且从外存中读入这个关系比计算该子表达式的时间少得多 则先计算一次公共子表达式并把结果写入中间文件是合算的。 当查询的是视图时，定义视图的表达式就是公共子表达式的情况 遵循这些启发式规则，应用9.3.1的等价变换公式来优化关系表达式的算法。 算法：关系表达式的优化 输入：一个关系表达式的查询树 输出：优化的查询树 方法： （1)利用等价变换规则4把形如σF1∧F2∧…∧Fn(E)变换为 σF1(σF2(…(σFn(E))…))。（分开选择条件) （2)对每一个选择，利用等价变换规则4～9尽可能把它移到树的叶端。 （选择先做) （3)对每一个投影利用等价变换规则3，5，10，11中的一般形式尽可能把它移向树的叶端。 注意： 等价变换规则3使一些投影消失或使一些投影出现 规则5把一个投影分裂为两个，其中一个有可能被移向树的叶端 （4)利用等价变换规则3～5，把选择和投影的串接合并成单个选择、单个投影或一个选择后跟一个投影，使多个选择或投影能同时执行，或在一次扫描中全部完成 （选择投影一起做) （5)把上述得到的语法树的内节点分组。 每一双目运算()和它所有的直接祖先为一组(这些直接祖先是(σ，π运算)。 如果其后代直到叶子全是单目运算，则也将它们并入该组 但当双目运算是笛卡尔积(×)，而且后面不是与它组成等值连接的选择时，则不能把选择与这个双目运算组成同一组 示例： 1)把SQL语句转换成查询树 为了使用关系代数表达式的优化法，假设内部表示是关 系代数语法树，则上面的查询树如图 2)对查询树进行优化 利用规则4、6把选择σSC.Cno=‘2’移到叶端，图9.4查询树便转换成下图优化的查询树。这就是9.2.2节中Q3的查询树表示。 物理优化 代数优化改变查询语句中操作的次序和组合，不涉及底层的存取路径 对于一个查询语句有许多存取方案，它们的执行效率不同， 仅仅进行代数优化是不够的 物理优化就是要选择高效合理的操作算法或存取路径，求得优化的查询计划 物理优化方法 1)基于规则的启发式优化 启发式规则是指那些在大多数情况下都适用，但不是在每种情况下都是最好的规则。 2)基于代价估算的优化 优化器估算不同执行策略的代价，并选出具有最小代价的执行计划。 3)两者结合的优化方法： 常常先使用启发式规则，选取若干较优的候选方案，减少代价估算的工作量 然后分别计算这些候选方案的执行代价，较快地选出最终的优化方案 基于启发式规则的存取路径选择优化(定性) 1.选择操作的启发式规则 对于小关系，使用全表顺序扫描，即使选择列上有索引 对于大关系，启发式规则有： 1)对于选择条件是“主码＝值”的查询 查询结果最多是一个元组，可以选择主码索引 一般的RDBMS会自动建立主码索引 2)对于选择条件是“非主属性＝值”的查询，并且选择列上有索引 要估算查询结果的元组数目 如果比例较小( 3)对于选择条件是属性上的非等值查询或者范围查询，并且选择列上有索引 要估算查询结果的元组数目 如果比例较小( 4)对于用AND连接的合取选择条件 如果有涉及这些属性的组合索引，优先采用组合索引扫描方法 如果某些属性上有一般的索引，可以用索引扫描方法 通过分别查找满足每个条件的指针，求指针的交集 通过索引查找满足部分条件的元组，然后在扫描这些元组时判断是否满足剩余条件 其他情况：使用全表顺序扫描 5)对于用OR连接的析取选择条件，一般使用全表顺序扫描 2.连接操作的启发式规则 1)如果2个表都已经按照连接属性排序：选用排序-合并算法 2)如果一个表在连接属性上有索引，选用索引连接算法 3)如果上面2个规则都不适用，其中一个表较小，选用Hash join算法 4)可以选用嵌套循环方法，并选择其中较小的表，确切地讲是占用的块数(b)较少的表，作为外表(外循环的表) 理由： 设连接表R与S分别占用的块数为Br与Bs 连接操作使用的内存缓冲区块数为K 分配K-1块给外表 如果R为外表，则嵌套循环法存取的块数为Br+BrBs/(K-1) 显然应该选块数小的表作为外表 基于代价估算的优化(定量) 启发式规则优化是定性的选择，适合解释执行的系统。因为解释执行的系统，优化开销包含在查询总开销之中。 编译执行的系统中查询优化和查询执行是分开的，因此可以采用精细复杂一些的基于代价的优化方法 1.统计信息 基于代价的优化方法要计算查询的各种不同执行方案的执行代价，它与数据库的状态密切相关 优化器需要的统计信息 （1)对每个基本表 ①- 该表的元组总数(N) ②- 元组长度(l) ③- 占用的块数(B) ④- 占用的溢出块数(BO) （2)对基表的每个列 ①- 该列不同值的个数(m) ②- 列最大值 ③- 最小值 ④- 列上是否已经建立了索引 ⑤- 哪种索引(B+树索引、Hash索引、聚集索引) ⑥- 可以计算选择率(f) 如果不同值的分布是均匀的，f＝1/m 如果不同值的分布不均匀，则要计算每个值的选择率， f＝具有该值的元组数/N （3)对索引 ①- 索引的层数(L) ②- 不同索引值的个数 ③- 索引的选择基数S(有S个元组具有某个索引值) ④- 索引的叶结点数(Y) 2.代价估算示例 1)全表扫描算法的代价估算公式 如果基本表大小为B块，全表扫描算法的代价 cost＝B 如果选择条件是“码＝值”，那么平均搜索代价 cost＝B/2 （可能是第一块也可能是最后一块) 2)索引扫描算法的代价估算公式 如果选择条件是“码＝值”，则采用该表的主索引 若为B+树，层数为L，需要存取B+树中从根结点到叶结点L块，再加上基本表中该元组所在的那一块，所以cost=L+1 如果选择条件涉及非码属性 若为B+树索引，选择条件是相等比较，S是索引的选择基数(有S个元组满足条件) 满足条件的元组可能会保存在不同的块上，所以(最坏的情况)cost=L+S 如果比较条件是＞，＞＝，＜，＜＝操作 假设有一半的元组满足条件，就要存取一半的叶结点 通过索引访问一半的表存储块 cost=L+Y/2+B/2 如果可以获得更准确的选择基数，可以进一步修正Y/2与B/2 3)嵌套循环连接算法的代价估算公式 嵌套循环连接算法的代价 cost＝Br+BrBs/(K-1) 如果需要把连接结果写回磁盘 cost＝Br+Br Bs/(K-1)+(FrsNrNs)/Mrs 其中Frs为连接选择性(join selectivity)，表示连接结果元组数的比例 Mrs是存放连接结果的块因子，表示每块中可以存放的结果元组数目 4)排序-合并连接算法的代价估算公式 如果连接表已经按照连接属性排好序，则cost＝Br+Bs+(FrsNrNs)/Mrs 如果必须对文件排序还需要在代价函数中加上排序的代价 对于包含B个块的文件排序的代价大约是 (2B)+(2B*log2B) 小结： 查询处理是关系数据库管理系统的核心，查询优化技术是查询处理的关键技术 本章主要内容 查询处理过程 查询优化 代数优化 物理优化 查询执行 比较复杂的查询，尤其是涉及连接和嵌套的查询 不要把优化的任务全部放在RDBMS上 应该找出RDBMS的优化规律，以写出适合RDBMS自动优化的SQL语句 对于RDBMS不能优化的查询需要重写查询语句，进行手工调整以优化性能 事务与数据库恢复技术 事务处理技术包括数据库恢复技术和并发控制技术。 数据库恢复机制和并发控制机制是DBMS的重要组成部分。8.5 事务的基本概念 1.事务：是用户定义的一个数据库操作序列，这些操作要么全做，要么全不做，是一个不可分割的工作单位。 事务和程序比较 - 在关系数据库中，一个事务可以是一条或多条SQL语句,也可以包含一个或多个程序。 - 一个程序通常包含多个事务 显式定义方式： begin transaction .... commit/rollback 示例： begin transaction select * from teacher; update teacher set title=null where tno=‘101’; select * from teacher; rollback; select * from teacher; 隐式方式 当用户没有显式地定义事务时，DBMS按缺省规定自动划分事务 AutoCommit事务是SQL Server默认事务方式, 2.事务的特性(ACID特性) 1.- 原子性（atomicity)：一个事务是一个不可分割的工作单位，事务中包括的诸操作要么都做，要么都不做； 2.- 一致性（consistency)：事务必须使数据库从一个一致性状态变成另一个一致性状态； 3.- 隔离性（isolation)：一个事务的执行不能被其他事务干扰； 4.- 持续性（durability)：也称永久性，指一个事务一旦提交，它对数据库中数据的改变就应该是永久性的． 8.6 数据库恢复概述 - 故障是不可避免的 - 系统故障：计算机软、硬件故障 - 人为故障：操作员的失误、恶意的破坏等。 - 数据库的恢复 - 把数据库从错误状态恢复到某一已知的正确状态(亦称为一致状态或完整状态) 8.7 故障的种类 1、事务内部的故障： 1)有的是可以通过事务程序本身发现的 2)有的是非预期的，不能由应用程序处理（如运算溢出、死锁等) 以后，事务故障仅指非预期的故障 事务故障的恢复：UNDO撤销 2、系统故障 造成系统停止运转的任何事件，使得系统要重新启动。（硬件错误、断电等) 影响正在运行的所有事务，但不破坏数据库。此时内存，尤其是数据库缓冲区中的内容全部丢失，所有运行事务非正常终止。 恢复： 1)未提交的事务：UNDO撤销所有未完成的事务 2)已提交的事务，但缓冲区内容未完全写入磁盘：REDO重做所有已提交的事务 3、介质故障 系统故障称为软故障，介质故障称为硬故障（外存故障，如磁盘损坏) 恢复：装入数据库发生介质故障前某个时刻的数据副本 REDO自此时开始的所有成功事务，将这些事务已提交的结果重新写入数据库 4、计算机病毒 计算机病毒是一种人为的故障或破坏，是一些恶作剧者研制的一种计算机程序。 可以繁殖和传播，并造成对计算机系统包括数据库的危害。 总之：各类故障，对数据库的影响有两种可能性 - 一是数据库本身被破坏 - 二是数据库没有被破坏，但数据可能不正确，这是由于事务的运行被非正常终止造成的。 8.8 恢复的实现技术 - 恢复操作的基本原理：冗余 - 恢复机制涉及的两个关键问题 1）- 如何建立冗余数据 •- 数据转储（backup) •- 登录日志文件（logging) 2）- 如何利用这些冗余数据实施数据库恢复 数据转储 - 数据转储定义： 转储是指DBA将整个数据库复制到其他存储介质上保存起来的过程，备用的数据称为后备副本或后援副本 - 如何使用 1）- 数据库遭到破坏后可以将后备副本重新装入 2）- 重装后备副本只能将数据库恢复到转储时的状态 - 转储方法 1）- 静态转储与动态转储 2）- 海量转储与增量转储 静态转储： 1)定义：在系统中无事务运行时进行的转储操作。转储开始的时刻数据库处于一 致性状态，而转储不允许对数据库的任何存取、修改活动。静态转储得到的一定是一个数据一致性的副本。 2)优点：实现简单 3)缺点：降低了数据库的可用性 转储必须等待正运行的用户事务结束才能进行；新的事务必须等待转储结束才能执行 动态转储： 1)定义：转储期间允许对数据库进行存取或修改。转储和用户事务可以并发执行。 2)优点：不用等待正在运行的用户事务结束；不会影响新事务的运行。 3)实现：必须把转储期间各事务对数据库的修改活动登记下来，建立日志文件 后备副本加上日志文件就能把数据库恢复到某一时刻的正确状态。 海量转储： 1)定义：每次转储全部数据库 2)特点：从恢复角度，使用海量转储得到的后备副本进行恢复更方便一些。 增量转储： 1)定义：每次只转储上一次转储后更新过的数据 2)特点：如果数据库很大，事务处理又十分频繁，则增量转储方式更实用更有效。 日志文件 1、什么是日志文件 日志文件(log)是用来记录事务对数据库的更新操作的文件 2、日志文件的格式 1)以记录为单位： 日志文件中需要登记的内容包括： ①- 各个事务的开始标记(BEGIN TRANSACTION) ②- 各个事务的结束标记(COMMIT或ROLLBACK) ③- 各个事务的所有更新操作 以上均作为日志文件中的一个日志记录 每个日志记录的内容： ①- 事务标识（标明是哪个事务) ②- 操作类型（插入、删除或修改) ③- 操作对象（记录内部标识) ④- 更新前数据的旧值（对插入操作而言，此项为空值) ⑤- 更新后数据的新值（对删除操作而言, 此项为空值) 2)以数据块为单位 日志记录内容包括： 事务标识（标明是哪个事务) 被更新的数据块 3、日志文件的作用： - 进行事务故障恢复 - 进行系统故障恢复 - 协助后备副本进行介质故障恢复 1)事务故障恢复和系统故障恢复必须用日志文件 2)在动态转储方式中必须建立日志文件，后备副本和日志文件结合起来才能有效地恢复数据库 3)静态转储方式中也可以建立日志文件（重新装入后备副本，然后利用日志文件把已完成的事务进行重做，对未完成事务进行撤销) 4、登记日志文件： - 基本原则 - 登记的次序严格按并行事务执行的时间次序 - 必须先写日志文件，后写数据库 为什么要先写日志文件？ 1)写数据库和写日志文件是两个不同的操作，在这两个操作之间可能发生故障 2)如果先写了数据库修改，而在日志文件中没有登记下这个修改，则以后就无法恢复这个修改了 3)如果先写日志，但没有修改数据库，按日志文件恢复时只不过是多执行一次不必要的UNDO操作，并不会影响数据库的正确性8.9 恢复策略 事务故障的恢复 - 事务故障：事务在运行至正常终止点前被终止 - 恢复方法 - 由恢复子系统应利用日志文件撤消（UNDO)此事务已对数据库进行的修改 - 事务故障的恢复由系统自动完成，对用户是透明的，不需要用户干预 - 事务故障的恢复步骤 反向扫描文件日志，查找该事务的更新操作。 对该事务的更新操作执行逆操作。即将日志记录中“更新前的值” 写入数据库。 - 插入操作， “更新前的值”为空，则相当于做删除操作 - 删除操作，“更新后的值”为空，则相当于做插入操作 - 若是修改操作，则相当于用修改前值代替修改后值 继续反向扫描日志文件，查找该事务的其他更新操作，并做同样处理。 如此处理下去，直至读到此事务的开始标记，事务故障恢复就完成了。系统故障的恢复 - 系统故障造成数据库不一致状态的原因 - 未完成事务对数据库的更新已写入数据库 - 已提交事务对数据库的更新还留在缓冲区没来得及写入数据库 - 恢复方法 - 1. Undo 故障发生时未完成的事务 - 2. Redo 已完成的事务 - 系统故障的恢复由系统在重新启动时自动完成，不需要用户干预 - 系统故障的恢复步骤 正向扫描日志文件 - 重做(REDO) 队列: 在故障发生前已经提交的事务 - 这些事务既有BEGIN TRANSACTION记录，也有COMMIT记录 - 撤销 (Undo)队列: 故障发生时尚未完成的事务 - 这些事务只有BEGIN TRANSACTION记录，无相应的COMMIT记录 对撤销(Undo)队列事务进行撤销(UNDO)处理 - 反向扫描日志文件，对每个UNDO事务的更新操作执行逆操作 对重做(Redo)队列事务进行重做(REDO)处理 - 正向扫描日志文件，对每个REDO事务重新执行登记的操作介质故障的恢复 - 恢复步骤 - 1.重装数据库 •- 装入最新的后备副本，使数据库恢复到最近一次转储时的一致性状态。 –- 对于静态转储的数据库副本，装入后数据库即处于一致性状态 –- 对于动态转储的数据库副本，还须同时装入转储时刻的日志文件副本，利用恢复系统故障的方法（即REDO+UNDO)，才能将数据库恢复到一致性状态。 - 2. 装入有关的日志文件副本，重做已完成的事务。 •- 首先扫描日志文件，找出故障发生时已提交的事务的标识，将其记入重做队列。 •- 然后正向扫描日志文件，对重做队列中的所有事务进行重做处理。 - 介质故障的恢复需要DBA介入 - DBA的工作 - 重装最近转储的数据库副本和有关的各日志文件副本 - 执行系统提供的恢复命令，具体的恢复操作仍由DBMS完成8.10 具有检查点的数据恢复 - 利用日志技术进行数据库恢复存在两个问题 - 搜索整个日志将耗费大量的时间 - REDO处理：事务实际上已经执行，又重新执行，浪费了大量时间 - 具有检查点（checkpoint)的恢复技术 - 在日志文件中增加检查点记录（checkpoint) - 增加重新开始文件，并让恢复子系统在登录日志文件期间动态地维护日志 - 检查点记录的内容 - 建立检查点时刻所有正在执行的事务清单 - 这些事务最近一个日志记录的地址 - 重新开始文件的内容 - 记录各个检查点记录在日志文件中的地址 - 动态维护日志文件的方法 周期性地执行如下操作：建立检查点，保存数据库状态。 具体步骤是： 1.将当前日志缓冲区中的所有日志记录写入磁盘的日志文件上 2.在日志文件中写入一个检查点记录 3.将当前数据缓冲区的所有数据记录写入磁盘的数据库中 4.把检查点记录在日志文件中的地址写入一个重新开始文件 - 使用检查点方法可以改善恢复效率 - 当事务T在一个检查点之前提交 - T对数据库所做的修改一定都已写入数据库 - 写入时间是在这个检查点建立之前或在这个检查点建立之时 - 在进行恢复处理时，没有必要对事务T执行REDO操作 - 使用检查点的恢复步骤 - 1.从重新开始文件中找到最后一个检查点记录在日志文件中的地址，由该地址在日志文件中找到最后一个检查点记录 - 2.由该检查点记录得到检查点建立时刻所有正在执行的事务清单ACTIVE-LIST •- 建立两个事务队列 –- UNDO-LIST –- REDO-LIST •- 把ACTIVE-LIST暂时放入UNDO-LIST队列，REDO队列暂为空 - 3.从检查点开始正向扫描日志文件，直到日志文件结束 •- 如有新开始的事务Ti，把Ti暂时放入UNDO-LIST队列 •- 如有提交的事务Tj，把Tj从UNDO-LIST队列移到REDO-LIST队列 - 4.对UNDO-LIST中的每个事务执行UNDO操作 对REDO-LIST中的每个事务执行REDO操作 8.11 数据库镜像 为避免硬盘介质出现故障影响数据库的可用性，许多DBMS提供了数据库映像（mirror)功能用于数据库恢复。 将整个数据库或其中的关键数据复制到另一个磁盘上，每当主数据库更新时，DBMS自动把更新后的数据复制过去，由DBMS自动保证镜像数据与主数据库的一致性。一旦出现介质故障，可由镜像磁盘继续提供使用，同时DBMS自动利用磁盘数据进行数据库的恢复，不需要关闭系统和重装数据库副本。 在没有出现故障时，数据库镜像还可以用于并发操作，即当一个用户对数据库加排它锁修改数据时，其他用户可以读镜像数据库上的数据，而不必等待该用户释放锁。 由于数据库镜像是通过复制数据实现的，频繁地赋值数据自然会降低系统运行效率。因此在实际应用中用户往往只选择对关键数据和日志文件进行镜像。 小结： - 如果数据库只包含成功事务提交的结果，就说数据库处于一致性状态。保证数据一致性是对数据库的最基本的要求。 - 事务是数据库的逻辑工作单位 - DBMS保证系统中一切事务的原子性、一致性、隔离性和持续性 - DBMS必须对事务故障、系统故障和介质故障进行恢复 - 恢复中最经常使用的技术：数据库转储和登记日志文件 - 恢复的基本原理：利用存储在后备副本、日志文件和数据库镜像中的冗余数据来重建数据库 - 常用恢复技术 - 事务故障的恢复 - UNDO - 系统故障的恢复 - UNDO + REDO - 介质故障的恢复 - 重装备份并恢复到一致性状态 + REDO - 提高恢复效率的技术 - 检查点技术 - 可以提高系统故障的恢复效率 - 可以在一定程度上提高利用动态转储备份进行介质故障恢复的效率 - 镜像技术 - 镜像技术可以改善介质故障的恢复效率 并发控制 多用户数据库：允许多个用户同时使用的数据库（订票系统) 不同的多事务执行方式： 1.串行执行：每个时刻只有一个事务运行，其他事务必须等到这个事务结束后方能运行。 2.交叉并发方式： 单处理机系统中，事务的并发执行实际上是这些并行事务的并行操作轮流交叉运行（不是真正的并发，但是提高了系统效率) 3.同时并发方式： 多处理机系统中，每个处理机可以运行一个事务，多个处理机可以同时运行多个事务，实现多个事务真正的并行运行 并发执行带来的问题： 多个事务同时存取同一数据（共享资源) 存取不正确的数据，破坏事务一致性和数据库一致性8.12 并发控制概述 并发操作带来的数据不一致性包括 1)丢失修改（lost update) 2)不可重复读（non-repeatable read) 3)读脏数据（dirty read) 记号:W（x)写数据x R（x)读数据x 并发控制机制的任务： 1)对并发操作进行正确的调度 2)保证事务的隔离性 3)保证数据库的一致性 并发控制的主要技术 1)封锁（locking)（主要使用的) 2)时间戳（timestamp) 3)乐观控制法（optimistic scheduler) 4)多版本并发控制（multi-version concurrency control ，MVCC) 8.13 封锁 封锁：封锁就是事务T在对某个数据对象（例如表、记录等)操作之前，先向系统发出请求，对其加锁。加锁后事务T就对该数据对象有了一定的控制，在事务T释放它的锁之前，其它的事务不能更新此数据对象 确切的控制由封锁的类型决定 基本的封锁类型有两种：排它锁（X锁，exclusive locks)、共享锁（S 锁，share locks) 排它锁又称写锁，对A加了排它锁之后，其他事务不能对A加 任何类型的锁（排斥读和写) 共享锁又称读锁，对A加了共享锁之后，其他事务只能对A加S锁，不能加X锁（只排斥写) 8.14 封锁协议 在运用X锁和S锁对数据对象加锁时，需要约定一些规则：封锁协议（Locking Protocol) 何时申请X锁或S锁 持锁时间、何时释放 对封锁方式制定不同的规则，就形成了各种不同的封锁协议。 常用的封锁协议：三级封锁协议 三级封锁协议在不同程度上解决了并发问题，为并发操作的正确调度提供一定的保证。 1、一级封锁协议 事务T在修改数据R之前，必须先对其加X锁，直到事务结束（commit/rollback)才释放。 一级封锁协议可以防止丢失修改 如果是读数据，不需要加锁的，所以它不能保证可重复读和不读“脏”数据。 2、 二级封锁协议 在一级封锁协议的基础（写要加X锁，事务结束释放)上，增加事务T在读入数据R之前必须先对其加S锁，读完后即可释放S锁。（读要加S锁，读完即释放) 二级封锁协议除了可以防止丢失修改，还可以防止读脏数据 由于读完数据即释放S锁，不能保证不可重复读 3、三级封锁协议： 在一级封锁协议基础上增加事务T在读取数据R之前必须先对其加S锁，直到事务结束后释放。 三级封锁协议除了可以防止丢失修改和读脏数据外，还防止了不可重复读 三级封锁协议的主要区别是什么操作需要申请锁，何时释放锁。封锁协议越高，一致性程度越高。 8.15 饥饿和死锁 饥饿 饥饿：事务T1封锁了数据R，事务T2又请求封锁R，于是T2等待。T3也请求封锁R，当T1释放了R上的封锁之后，系统首先批准了T3的请求，T2仍然等待。 T4又请求封锁R，当T3释放了R上的封锁之后系统又批准了T4的请求……T2有可能永远等待，这就是饥饿的情形 避免饥饿的方法：先来先服务 当多个事务请求封锁同一数据对象时，按请求封锁的先后次序对这些事务排队 该数据对象上的锁一旦释放，首先批准申请队列中第一个事务获得锁。 死锁 死锁：事务T1封锁了数据R1， T2封锁了数据R2。 T1又请求封锁R2，因T2已封锁了R2，于是T1等待T2释放R2上的锁。 接着T2又申请封锁R1，因T1已封锁了R1，T2也只能 等待T1释放R1上的锁。 这样T1在等待T2，而T2又在等待T1，T1和T2两个事务永远不能结束，形成死锁。 解决死锁的方法：预防、诊断和解除 1、死锁的预防 产生死锁的原因是两个或多个事务都已经封锁了一些数据对象，然后又都请求对已被其他事务封锁的数据对象加锁，从而出现死等待。 预防死锁发生就是破坏产生死锁的条件 方法 1)一次封锁法： 要求每个事务必须一次将所有要使用的数据全部加锁，否则就不能继续执行。 存在的问题：降低系统的并发度；难以实现精确确定封锁对象 2)顺序封锁法： 预先对数据对象规定一个封锁顺序，所有事务都按这个顺序实施封锁。 存在的问题： 维护成本：数据库系统中的封锁对象极多，并且在不断地变化 难以实现：很难实现确定每一个事务要封锁哪些对象 DBMS普通采用的诊断并解除死锁的方法 2、死锁的诊断和解除 方法：超时法和事务等待图法 1)超时法：如果一个事务的等待时间超过了规定的时限，就认为发生了死锁 优点：实现简单 缺点：误判死锁；时限若设置太长，死锁发生后不能及时发现。 2)事务等待图法：用事务等待图动态反映所有事务的等待情况事务 等待图是一个有向图G=(T，U)，T为结点的集合，每个结点表示正运行的事务， U为边的集合，每条边表示事务等待的情况。若T1等待T2，则T1、T2之间划一条有向边，从T 1指向T2。 并发控制子系统周期性地（比如每隔数秒)生成事务等待图，检测事务。如果发现图中存在回路，则表示系统中出现了死锁。 解除死锁：并发控制子系统选择一个处理死锁代价最小的事务，将其撤销。 释放该事务持有的所有的锁，使其他事务能够继续运行下去。 8.16 并发调度的可串行性 什么样的调度是正确的？串行调度是正确的。 （执行结果等价于串行调度的调度也是正确的，这样的调度称为可串行化调度。)可串行化调度 定义：多个事务的并发执行是正确的，当且仅当其结果与按某一次序串行地执行这些事务时的结果相同，称这种调度策略为可串行化调度（serializable)。 可串行性是并发事务正确调度的准则。按这个准则规定，一个给定的并发调度，当且仅当它是可串行化的，才认为是正确调度。 冲突可串行化调度 判断可串行化调度的充分条件 冲突操作：不同的事务对同一个数据的读写和写写操作。 不同事务的冲突操作和同一事务的两个操作是不能交换的。 Ri（x)和Wj（x)不可交换，Wi（x)和Wj（x)不可交换 冲突可串行化调度： 一个调度Sc在保证冲突操作的次序不变的情况下，通过交换两个事务不冲突操作的次序得到另一个调度Sc’，如果Sc’是串行的，称调度Sc为冲突可串行化的调度。 若一个调度是冲突可串行化，则一定是可串行化的。冲突可串行化调度是可串行化调度的充分条件而非必要条件，同样存在不满足冲突可串行化调度的可串行化调度。 8.17 两段锁协议 DBMS的并发控制机制必须提供一定的手段来保证调度是可串行化的。目前DBMS普遍采用两段锁协议（TwoPhase Locking，简称2PL)的方法来显示并发调度的可串行性。 两段锁协议是指所有事务必须分两个阶段对数据对象进行加锁和解锁。 1)在对任何数据进行读写操作以前，首先要申请并获得对该数据的锁。 2)在释放一个锁之后，事务不再申请和获得其他任何的锁。 “两段”锁的含义：事务分为两个阶段 第一阶段是获得封锁，也称为扩展阶段 事务可以申请获得任何数据对象上的任何类型的锁，但是不能释放任何锁 第二阶段是释放封锁，也称为收缩阶段 事务可以释放任何数据对象上的任何类型的锁，但是不能再申请任何锁 符合两段锁协议的可串行化调度示例： 事务遵守两段锁协议是可串行化调度的充分条件，而不是必要条件。 若并发事务都遵守两段锁协议，则对这些事务的任何并发调度策略都是可串行化的 若并发事务的一个调度是可串行化的，不一定所有事务都符合两段锁协议 两段锁协议与防止死锁的一次封锁法 一次封锁法要求每个事务必须一次将所有要使用的数据全部加锁，否则就不能继续执行，因此一次封锁法遵守两段锁协议 但是两段锁协议并不要求事务必须一次将所有要使用的数据全部加锁，因此遵守两段锁协议的事务可能发生死锁 8.18 封锁的粒度 封锁对象的大小称为封锁粒度（granularity)。 封锁的对象可以是逻辑单元（属性值、属性值集合、元组、关系、索引项、数据库)，也可以是物理单元（页、物理记录)。 选择封锁粒度原则： 封锁粒度和系统的并发度和并发控制的开销密切相关 封锁的粒度越大，数据库所能够封锁的数据单元就越少，并发度就越低，系统开销也 越小； 封锁的粒度越小，并发度较高，但系统开销也就越大 多粒度封锁 如果在一个系统中同时支持多种封锁粒度供不同的事务选择，这种封锁方法称为多粒度封锁。（multiple granularity locking) 选择封锁粒度应该同时考虑封锁开销和并发度两个因素，适当选择封锁粒度以求得最优的效果。 需要处理多个关系的大量元组的用户事务：以数据库为封锁单位 需要处理大量元组的用户事务：以关系为封锁单元 只处理少量元组的用户事务：以元组为封锁单位 多粒度树 以树形结构来表示多级封锁粒度。根结点是整个数据库，表示最大的数据粒度，叶结点表示最小的数据粒度 多粒度封锁协议：允许多粒度树中的每个节点被独立地加锁，对一个节点加锁意味着这个节点的所有子节点也被加以同样类型的锁。因此，在多粒度封锁中一个数据对象可能以显式封锁和隐式封锁两种方式封锁。 • 显式封锁：直接加到数据对象上的封锁 • 隐式封锁：该数据对象没有独立加锁，是由于其上级结点加锁而使该数据对象加上了锁 • 显式封锁和隐式封锁的效果是一样的 系统检查封锁冲突时要检查显式封锁，还要检查隐式封锁 例如事务T要对关系R1加X锁，系统必须搜索其上级结点数据库、关系R1，还要搜索R1的下级结点，即R1中的每一个元组。如果其中某一个数据对象已经加了不相容锁，则T必 须等待。 对某个数据对象加锁，系统要检查该数据对象上有无显式封锁与之冲突；再检查其所有上级节点，看本事务的显式封锁是否与该数据对象上的隐式封锁（由于上级节点已加的封锁造成的)冲突；还要检查其所有下级节点，看它们的显式封锁是否与本事务的隐式封锁（将加到下级节点的封锁)冲突。 这种检查方法效率较低，引入一种新的锁，意向锁。有了意向锁，DBMS就无须逐个检查下一级节点的显式封锁。 意向锁 意向锁：如果对一个节点加意向锁，则可说明该节点的下层节点正在被加锁；对任一节点加锁时，必须先对它的上层节点加意向锁。 例如，对任一元组加锁时，必须先对它所在的数据库和关系加意向锁。 三种常用的意向锁：意向共享锁（Intent Share Lock，IS锁)；意向排它锁（Intent Exclusive Lock，IX锁)；共享意向排它锁（Share Intent Exclusive Lock，SIX锁)。 1、IS锁 如果对一个数据对象加IS锁，表示它的子节点拟加S锁。 例如：事务T1要对R1中某个元组加S锁，则要首先对关系R1和数据库加IS锁 2、IX锁 如果对一个数据对象加IX锁，表示它的子节点拟加X锁。 例如：事务T1要对R1中某个元组加X锁，则要首先对关系R1和数据库加IX锁 3、SIX锁 如果对一个数据对象加SIX锁，表示对它加S锁，再加IX锁，即SIX = S + IX。 例如：对某个表加SIX锁，则表示该事务要读整个表（所以要对该表加S锁)，同 时会更新个别元组（所以要对该表加IX锁) 意向锁的强度： 锁的强度是指它对其他锁的排斥程度。一个事务在申请封锁时以强锁代替弱锁是安全的，反之则不然。 具有意向锁的多粒度封锁方法 申请封锁时应该按自上而下的次序进行 释放封锁时则应该按自下而上的次序进行 优点： 1)提高了系统并发度 2)减少了加锁和解锁的开销 在实际的DBMS产品中得到广泛应用。 8.19 其他并发控制机制 并发控制的方法除了封锁技术外，还有时间戳方法、乐观控制法和多版本并发控制。 时间戳方法：给每一个事务盖上一个时标，即事务开始的时间。每个事务具有唯一的时间戳，并按照这个时间戳来解决事务的冲突操作。如果发生冲突操作，就回滚到具有较早时间戳的事务，以保证其他事务的正常执行，被回滚的事务被赋予新的时间戳被从头开始执行。 乐观控制法认为事务执行时很少发生冲突，所以不对事务进行特殊的管制，而是让它自由执行，事务提交前再进行正确性检查。如果检查后发现该事务执行中出现过冲突并影响了可串行性，则拒绝提交并回滚该事务。又称为验证方法 多版本控制是指在数据库中通过维护数据对象的多个版本信息来实现高效并发的一种策略。范式（避免数据冗余和操作异常) 8.20 函数依赖 A->B A和B是两个属性集，来自同一关系模式，对于同样的A属性值，B属性值也相同8.21 平凡的函数依赖 X->Y，如果Y是X的子集8.22 非平凡的函数依赖 X->Y，如果Y不是X的子集8.23 部分函数依赖 X->Y，如果存在W->Y，且W⊂X8.24 传递函数依赖 在R(U)中，如果X→Y(非平凡函数依赖,完全函数依赖)，Y→Z， 则称Z对X传递函数依赖。 记为：XZ8.25 super key&candidate key&primary key&主属性&非主属性 super key：在关系中能唯一标识元素的属性集 candidate key或key：不含有多余属性的super key primary key：在candidate key 中任选一个 candidate key中X决定所有属性的函数依赖是完全函数依赖 包含在任何一个candidate key中的属性 ，称为主属性 不包含在candidate key中的属性称为非主属性8.26 1NF 列不可分 列不可分8.27 2NF 消除了非主属性对键的部分函数依赖 在关系T上有函数依赖集F，F+是F的闭包。 F满足2NF，当且仅当 每个非平凡的函数依赖X->A(F+),A是单个非主属性，要求X不是任何key的真子集（有可能是super key，也有可能是非key)。 8.28 3NF 消除了非主属性对键的传递函数依赖 F满足3NF，当且仅当 每个非平凡的函数依赖X->A(F+),A是单个非主属性，要求X是T的super key。 8.29 BCNF 消除了主属性对键的部分函数依赖和传递函数依赖 F满足BCNF，当且仅当 每个非平凡的函数依赖X->A(F+),A是单个属性，要求X是T的super key。 对于F+中 的任意一个X->A，如果A是单个属性，且A不在X中，那么X一定是T的super key。 反范式（减少连接，提高查询效率) 8.30 Pattern1：合并1对1关系 例：学院给老师配车，车少人多，车完全参与，人部分参与 car car_id car_name 1 c1 2 c2 3 c3 teacher teacher_id teacher_name 1 t1 2 t2 3 t3 4 t4 合并后 car_and_teacher car_id car_name teacher_id teacher_name 1 c1 1 t1 2 c2 2 t2 3 c3 3 t3 NULL NULL 4 t4 问题：会产生大量空值，若两边都部分参与则不能合并； 部分参与为大部分参与时比较适合Pattern18.31 Pattern2：1对N关系中复制非键属性以减少连接 两表连接时复制非键属性以减少连接 例：查询学生以及所在学院名，可以在学生表中不仅存储学院id，并且存储学院名 faculty fid fname 1 f1 student sid sname fid fname 1 s1 1 f1 维护时： 1)如果在UI中，只允许用户进行选择，不能自行输入，保证输入一致性 2)如果是程序员，对于类似学院名这种一般不变的代码表，在修改时直接对两张表都进行修改；如果经常变化，则可以加一个触发器。8.32 Pattern3：1对N关系中复制外键以减少连接 把另一张表的主键复制变成外键 应用后： 8.33 Pattern4：N对N关系中复制属性，把两张表中经常需要的内容复制到中间关系表中以减少连接 例： 8.34 Pattern5：引入重复值 通常对于一个多值属性，值不太多，且不会经常变，可以在表中建立多个有关此属性的列 address1 | address2 | address3 | address48.35 Pattern6：建立提取表 为了解决查询和更新之间不可调和的矛盾，可以将更新和查询放在两张表中，从工作表中提取查询表，专门用于查询。只适用于查询实时性不高的情况。8.36 Pattern7：分表 水平拆分 垂直拆分 MySQL使用 MySQL特点 MySQL是一个关系型数据库管理系统，开发者为瑞典MySQL AB公司。目前MySQL被广泛地应用在互联网行业。由于其体积小、速度快、总体拥有成本低，尤其是开放源码这一特点，许多互联网公司选择了MySQL作为后端数据库。2008年MySQL被Sun公司收购，2010年甲骨文成功收购Sun公司。 MySQL数据库的优点： 1、多语言支持：MySQL为C、C++、Python、Java、Perl、PHP、Ruby等多种编程语言提供了API，访问和使用方便。 2、可以移植性好：MySQL是跨平台的。 3、免费开源。 4、高效：MySQL的核心程序采用完全的多线程编程。 5、支持大量数据查询和存储：MySQL可以承受大量的并发访问数据类型 8.37 数值类型 整数类型： 实数类型： 定点数：DECIMAL和NUMERIC类型在MySQL中视为相同的类型。它们用于保存必须为确切精度的值。 DECIMAL(M,D)，其中M表示十进制数字总的个数，D表示小数点后面数字的位数。  - 如果存储时，整数部分超出了范围（如上面的例子中，添加数值为1000.01)，MySql就会报错，不允许存这样的值。 - 如果存储时，小数点部分若超出范围，就分以下情况： - 若四舍五入后，整数部分没有超出范围，则只警告，但能成功操作并四舍五入删除多余的小数位后保存。如999.994实际被保存为999.99。 - 若四舍五入后，整数部分超出范围，则MySql报错，并拒绝处理。如999.995和-999.995都会报错。 M的默认取值为10，D默认取值为0。如果创建表时，某字段定义为decimal类型不带任何参数，等同于decimal(10,0)。带一个参数时，D取默认值。 M的取值范围为1~65，取0时会被设为默认值，超出范围会报错。 D的取值范围为0~30，而且必须 所以，很显然，当M=65，D=0时，可以取得最大和最小值。 浮点数类型：float，double和real。他们定义方式为：FLOAT(M,D) 、 REAL(M,D) 、 DOUBLE PRECISION(M,D)。 “(M,D)”表示该值一共显示M位整数，其中D位位于小数点后面 FLOAT和DOUBLE中的M和D的取值默认都为0，即除了最大最小值，不限制位数。 M取值范围为0~255。FLOAT只保证6位有效数字的准确性，所以FLOAT(M,D)中，M D取值范围为0~30，同时必须 内存中，FLOAT占4-byte（1位符号位 8位表示指数 23位表示尾数)，DOUBLE占8-byte（1位符号位 11位表示指数 52位表示尾数)。 浮点数比定点数类型存储空间少，计算速度快，但是不够精确。 因为需要计算额外的空间和计算开销，所以应该尽量只在对小数进行精确计算时 才使用DECIMAL。但在数据量比较大的情况下，可以考虑使用BIGINT代替DECIMAL，将需要存储的货币单位根据小数的位数乘以相应的倍数即可。 BIT数据类型可用来保存位字段值。BIT(M)类型允许存储M位值。M范围为1~64，默认为1。 BIT其实就是存入二进制的值，类似010110。 如果存入一个BIT类型的值，位数少于M值，则左补0. 如果存入一个BIT类型的值，位数多于M值，MySQL的操作取决于此时有效的SQL模式： 如果模式未设置，MySQL将值裁剪到范围的相应端点，并保存裁减好的值。 如果模式设置为traditional(“严格模式”)，超出范围的值将被拒绝并提示错误，并且根据SQL标准插入会失败。 MySQL把BIT当做字符串类型，而非数字类型。 8.38 字符串类型 字符串类型指CHAR、VARCHAR、BINARY、VARBINARY、BLOB、TEXT、ENUM和SET。CHAR & VARCHAR CHAR和VARCHAR类型声明的长度表示你想要保存的最大字符数。例如，CHAR(30)可以占用30个字符。默认长度都为255。 CHAR列的长度固定为创建表时声明的长度。长度可以为从0到255的任何值。当保存CHAR值时，在它们的右边填充空格以达到指定的长度。当检索到CHAR值时，尾部的空格被删除掉，所以，我们在存储时字符串右边不能有空格，即使有，查询出来后也会被删除。在存储或检索过程中不进行大小写转换。 所以当char类型的字段为唯一值时，添加的值是否已经存在以不包含末尾空格（可能有多个空格)的值确定，比较时会在末尾补满空格后与现已存在的值比较。 VARCHAR列中的值为可变长字符串。长度可以指定为0到65,535之间的值（实际可指定的最大长度与编码和其他字段有关，比如，MySql使用utf-8编码格式，大小为标准格式大小的2倍，仅有一个varchar字段时实测最大值仅21844，如果添加一个char(3)，则最大取值减少3。整体最大长度是65,532字节)。 同CHAR对比，VARCHAR值保存时只保存需要的字符数，另加一个字节来记录长度(如果列声明的长度超过255，则使用两个字节)。 VARCHAR值保存时不进行填充。当值保存和检索时尾部的空格仍保留，符合标准SQL。 如果分配给CHAR或VARCHAR列的值超过列的最大长度，则对值进行裁剪以使其适合。如果被裁掉的字符不是空格，则会产生一条警告。如果裁剪非空格字符，则会造成错误(而不是警告)并通过使用严格SQL模式禁用值的插入。 BINARY & VARBINARY BINARY和VARBINARY类型类似于CHAR和VARCHAR类型，但是不同的是，它们存储的不是字符串，而是二进制串。所以它们没有编码格式，并且排序和比较基于列值字节的数值值。 当保存BINARY值时，在它们右边填充0x00(零字节)值以达到指定长度。取值时不删除尾部的字节。比较时所有字节很重要（因为空格和0x00是不同的，0x00 对于VARBINARY，插入时不填充字符，选择时不裁剪字节。比较时所有字节很重要。 BLOB & TEXT BLOB是一个二进制大对象，可以容纳可变数量的数据。有4种BLOB类型：TINYBLOB、BLOB、MEDIUMBLOB和LONGBLOB。它们只是可容纳值的最大长度不同。 有4种TEXT类型：TINYTEXT、TEXT、MEDIUMTEXT和LONGTEXT。这些对应4种BLOB类型，有相同的最大长度和存储需求。 BLOB列被视为二进制字符串。TEXT列被视为字符字符串，类似BINARY和CHAR。 在TEXT或BLOB列的存储或检索过程中，不存在大小写转换。 未运行在严格模式时，如果你为BLOB或TEXT列分配一个超过该列类型的最大长度的值，值被截取以保证适合。如果截掉的字符不是空格，将会产生一条警告。使用严格SQL模式，会产生错误，并且值将被拒绝而不是截取并给出警告。 在大多数方面，可以将BLOB列视为能够足够大的VARBINARY列。同样，可以将TEXT列视为VARCHAR列。 BLOB和TEXT在以下几个方面不同于VARBINARY和VARCHAR： 当保存或检索BLOB和TEXT列的值时不删除尾部空格。(这与VARBINARY和VARCHAR列相同)。 比较时将用空格对TEXT进行扩充以适合比较的对象，正如CHAR和VARCHAR。 对于BLOB和TEXT列的索引，必须指定索引前缀的长度。对于CHAR和VARCHAR，前缀长度是可选的。 BLOB和TEXT列不能有默认值。 BLOB或TEXT对象的最大大小由其类型确定，但在客户端和服务器之间实际可以传递的最大值由可用内存数量和通信缓存区大小确定。你可以通过更改max_allowed_packet变量的值更改消息缓存区的大小，但必须同时修改服务器和客户端程序。 每个BLOB或TEXT值分别由内部分配的对象表示。 它们（TEXT和BLOB同)的长度： Tiny：最大长度255个字符(2^8-1) BLOB或TEXT：最大长度65535个字符(2^16-1) Medium：最大长度16777215个字符(2^24-1) LongText 最大长度4294967295个字符(2^32-1) 实际长度与编码有关，比如utf-8的会减半。 当BLOB和TEXT值太大时，InnoDB会使用专门的外部存储区域来进行存储，此时单个值在行内需要1~4个字节存储一个指针，然后在外部存储区域存储实际的值。 MySQL会BLOB和TEXT进行排序与其他类型是不同的：它只对每个类的最前max_sort_length字节而不是整个字符串进行排序。 MySQL不能将BLOB和TEXT列全部长度的字符串进行索引，也不能使用这些索引消除排序。 ENUM 使用枚举代替字符串类型 MySQL在存储枚举时非常紧凑，会根据列表值的数量压缩到一个或两个字节中。MySQL在内部将每个值在列表中的位置保存为整数，并且在表的.frm文件中保存“数组——字符串”映射关系的查找表。 枚举字段是按照内部存储的整数而不是定义的字符串进行排序的； 由于MySQL把每个枚举值都保存为整数，并且必须通过查找才能转换为字符串，所以枚举列有一定开销。在特定情况下，把CHAR/VARCHAR列与枚举列进行JOIN可能会比直接关联CHAR/VARCHAR更慢。 8.39 时间和日期类型 DATE, DATETIME, 和TIMESTAMP类型 这三者其实是关联的，都用来表示日期或时间。 当你需要同时包含日期和时间信息的值时则使用DATETIME类型。MySQL以'YYYY-MM-DD HH:MM:SS'格式检索和显示DATETIME值。支持的范围为'1000-01-01 00:00:00'到'9999-12-31 23:59:59'。 当你只需要日期值而不需要时间部分时应使用DATE类型。MySQL用'YYYY-MM-DD'格式检索和显示DATE值。支持的范围是'1000-01-01'到 '9999-12-31'。 TIMESTAMP类型同样包含日期和时间，范围从'1970-01-01 00:00:01' UTC 到'2038-01-19 03:14:07' UTC。 TIME值的范围可以从'-838:59:59'到'838:59:59'。小时部分会因此大的原因是TIME类型不仅可以用于表示一天的时间(必须小于24小时)，还可能为某个事件过去的时间或两个事件之间的时间间隔(可以大于24小时，或者甚至为负) 两者的存储方式不一样 对于TIMESTAMP，它把客户端插入的时间从当前时区转化为UTC（世界标准时间)进行存储。查询时，将其又转化为客户端当前时区进行返回。 而对于DATETIME，不做任何改变，基本上是原样输入和输出。 YEAR类型是一个单字节类型用于表示年。 MySQL以YYYY格式检索和显示YEAR值。范围是1901到2155。 逻辑架构 MySQL的特点体现在其存储引擎的架构上。 插件式的存储引擎架构将查询处理和其他的系统任务以及数据的存储提取相分离，这种架构可以让用户根据业务需求和实际需要选择合适的存储引擎。 8.40 连接层（管理客户端的连接，维护线程池) 最上层是一些客户端和连接服务，引入了线程池的概念；实现基于SSL的安全连接 每个客户端都会在服务器进程中拥有一个线程，这个连接的查询只会在这个单独的线程中执行。 当客户端连接到MySQL服务器时，服务器需要对其进行认证。如果使用了SSL安全套接字的方式连接，还会使用X.509证书认证。一旦客户端连接成功，服务器会继续验证该客户端是否具有执行某个特定查询的权限。 8.41 服务器（与具体存储引擎解耦，服务器通过API与存储引擎进行通信) - SQL接口 - SQL分析与优化 - 存储过程 - 触发器 - 视图 MySQL会解析查询，并创建内部数据结构（解析树)，然后对其进行各种优化，包括重写查询、决定表的读取顺序，以及选择合适的索引等。 优化器并不关心表使用的是什么存储引擎，但存储引擎对优化查询是有影响的。优化器会请求存储引擎提供容量或某个具体操作的开销信息，以及表数据的统计信息等。 对于SELECT语句，在解析查询前，服务器会先检查查询缓存，如果能够在其中找到对应的查询，服务器就不再执行查询解析、优化和执行的整个过程，而是直接返回查询缓存中的结果集。 8.42 存储引擎层（负责数据的存储和存取) 存储引擎层，存储引擎真正的负责了MySQL中数据的存储和提取。 存储引擎API包含了几十个底层函数，用于执行诸如“开启一个事务”或者“根据主键提取一行记录”等操作。但存储引擎不会去解析SQL（InnoDB是一个例外，它会解析外键定义，因为服务器没有实现该功能)，不同存储引擎之间也不会相互通信，而只是简单地响应上层服务器的请求。 8.43 存储层（将数据存储到文件系统上) 数据存储层，主要是将数据存储在运行于裸设备的文件系统之上，并完成与存储引擎的交互。 存储引擎 MyISAM崩溃后无法安全恢复（由于不支持事务)约束 - 主键约束 ：不允许重复记录，避免数据冗余 - 外键约束：保证本事务所关联的其他事务是存在的（主键表中的这个字段) - check约束：限制某一个值在某一个范围之内 check（) （)内是关系表达式和逻辑表达式的嵌套 注意逻辑运算符是not and or - default约束：确定默认值(可以更改) 保证事务的某个属性一定会有一个值 有默认值的话如果不想对其更改，可以用insert对其他字段进行赋值，跳过有默认值的字段 但是不能在整体insert的时候跳过这个字段 - unique约束 唯一键：唯一的值不可重复，但允许为空 就是该记录的这个值不会有重复的值 unique和not null 可以组合使用，顺序任意 注意空值可以写为null，注意空值的这个值也不能重复，只能有一条记录的这个字段可以是空值 （而oracle中可以允许多个有唯一键的记录为空值) - not null约束 要求用户必须为该字段赋一个值，否则出错 如果非空的话必须赋值，不能采用部分insert的办法来跳过对这个字段的赋值 不写not null/null 的话默认就是允许有空值，如果没赋值的话字段的值默认是null null和default 关系：都允许不对某字段进行赋值，但是结果不同，一个是空值，另一个是默认值 create table student2 ( stu_id int primary key, stu_name nvarchar(20) unique not null, stu_sal int check(stu_sal >= 1000 and stu_sal stu_sex nchar(1) default '男' ) insert into student values(1,'啦啦',1800,'男') ok insert into student values(2,'啦啦',1800,'男') error insert into student values(2,'嘿嘿',2400,null) error insert into student values(null,'嘿嘿',1200,null) ok 主键和唯一键的关系： 不要用业务逻辑字段当做主键，应添加一个没有任何实际意义的字段（代理主键)当做主键 一般是主键（或者唯一键)作为其他表的外键。 如果业务逻辑字段的信息修改，则会影响其他表 查询效率低（数字、编号效率高) 这个业务逻辑字段修改时，因为这个主键同时充当多个其他表的外键，所以也要一并修改，十分麻烦 将有实际业务含义的、不能重复的、不是主键的一个字段作为唯一键 MySQL常用函数 8.44 文本处理函数 Left(x,len) – 返回串左边的字符（长度为len) Right(x,len) Length(x) – 返回串的长度 Locate(x,sub_x) – 找出串的一个子串 SubString(x, from, to) – 返回字串的字符 Lower(x) Upper(x) LTrim(x) RTrim(x) Soundex(x) – 读音（用于发音匹配) SELECT cust_name, cust_contact FROM customers WHERE Soundex(cust_contact) = Soundex(‘Y Lie’); 8.45 日期和时间处理函数 日期和时间采用相应的数据类型和特殊的格式存储，以便可以快速和有效的排序或过滤，节省物理存储空间. 一般，应用程序不使用用来存储日期和时间的格式，因此日期和时间函数总是被用来读取、统计和处理这些函数. 常用日期和时间处理函数： AddDate() – 增加一个日期（天，周等) AddTime() – 增加一个时间（时，分等) CurDate() – 返回当前日期 CurTime() – 返回当前时间 Date() – 返回日期时间的日期部分 DateDiff() – 计算两个日期之差 Date_Add() – 日期运算函数 Date_Format() – 返回一个格式化的日期或时间串 Day() – 返回一个日期的天数部分 DayOfWeek() – 返回日期对应的星期几 Hour() – 返回一个时间的小时部分 Minute() – 返回一个时间的分钟部分 Second() – 返回一个时间的秒部分 Month() – 返回一个日期的月部分 Now() – 返回当前日期和时间 Time() – 返回一个日期时间的时间部分 Year() – 返回一个日期的年份部分 日期首选格式： yyyy-mm-dd; 如2005-09-01 检索某日期下的数据： SELECT cust_id, order_num FROM orders WHERE Date(order_date) = ‘2005-09-01’; 检索某月或日期范围内的数据： SELECT cust_id, order_num FROM orders WHERE Year(order_date) = 2005 AND Month(order_date) = 9; – or SELECT cust_id, order_num FROM orders WHERE date(order_date) BETWEEN ‘2005-09-01’ AND ‘2005-09-30’; 8.46 数值处理函数 代数、三角函数、几何运算等 常用数值处理函数： abs(); cos(); exp(); mod()（取余); Pi(); Rand(); Sin(); Sqrt(); Tan(); 视图 视图是虚拟的表，与包含数据的表不同，视图只包含使用时动态检索数据的查询,主要是用于查询。 8.47 为什么使用视图 - 重用sql语句 - 简化复杂的sql操作，在编写查询后，可以方便地重用它而不必知道他的基本查询细节。 - 使用表的组成部分而不是整个表。 - 保护数据。可以给用户授予表的特定部分的访问权限而不是整个表的访问权限。 - 更改数据格式和表示。视图可返回与底层表的表示和格式不同的数据。 注意： - 在视图创建之后，可以用与表基本相同的方式利用它们。可以对视图执行select操作，过滤和排序数据，将视图联结到其他视图或表，甚至能添加和更新数据。 - 重要的是知道视图仅仅是用来查看存储在别处的数据的一种设施。视图本身不包含数据，因此它们返回的数据时从其他表中检索出来的。在添加和更改这些表中的数据时，视图将返回改变过的数据。 - 因为视图不包含数据，所以每次使用视图时，都必须处理查询执行时所需的任一检索。如果你使用多个联结和过滤创建了复杂的视图或者嵌套了视图，可能会发现性能下降得很厉害。因此，在部署使用了大量视图的应用前，应该进行测试。 8.48 视图的规则和限制 - 与表一样，视图必须唯一命名； - 可以创建任意多的视图； - 为了创建视图，必须具有足够的访问权限。这些限制通常由数据库管理人员授予。 - 视图可以嵌套，可以利用从其他视图中检索数据的查询来构造一个视图。 - Order by 可以在视图中使用，但如果从该视图检索数据select中也是含有order by，那么该视图的order by 将被覆盖。 - 视图不能索引，也不能有关联的触发器或默认值 - 视图可以和表一起使用 8.49 视图的创建 - 利用create view 语句来进行创建视图 - 使用show create view viewname；来查看创建视图的语句 - 用drop view viewname 来删除视图 - 更新视图可以先drop在create，也可以使用create or replace view。 8.50 视图的更新 视图是否可以更新，要视情况而定。 通常情况下视图是可以更新的，可以对他们进行insert，update和delete。更新视图就是更新其基表(视图本身没有数据)。如果你对视图进行增加或者删除行，实际上就是对基表进行增加或者删除行。 但是，如果MySQL不能正确的确定更新的基表数据，则不允许更新(包括插入和删除)，这就意味着视图中如果存在以下操作则不能对视图进行更新：(1)分组(使用group by 和 having )；(2)联结；(3)子查询；(4)并；(5)聚集函数;(6)dictinct;(7)导出(计算)列。【注意：基于5.0版本的规则，不排除后续变化】存储过程 存储过程就是为了以后的使用而保存的一条或者多条MySQL语句的集合。可将视为批文件，虽然他们的作用不仅限于批处理。8.51 为什么使用储存过程？ 1.通过把处理封装在容易使用的单元中，简化复杂的操作； 2.由于不要求反复建立一系列处理步骤，保证了数据的完整性。如果所有开发人员和应用程序都使用同一(实验和测试)存储过程，则所使用的代码都是相同的。这一点的延伸就是防止错误。需要执行的步骤越多，出错的可能性就越大，防止错误保证了数据的一致性。 3.简化对变动的管理，如果表名。列名或者业务逻辑等有变化，只需要更改存储过程的代码。使用它的人员甚至不需要知道这些变化。这一点延伸就是安全性，通过存储过程限制对基数据的访问减少了数据讹误的机会。 4.提高性能。因为使用存储过程比使用单独的sql语句更快。 5.存在一些只能用在单个请求的MySQL元素和特性，存储过程可以使用他们来编写功能更强更灵活的代码 综上： 三个主要的好处：简单、安全、高性能。 两个缺陷： 1、存储过程的编写更为复杂，需要更高的技能更丰富的经验。 2、可能没有创建存储过程的安全访问权限。许多数据库管理员限制存储过程的 创建权限，允许使用，不允许创建。8.52 执行存储过程 Call关键字：Call接受存储过程的名字以及需要传递给他的任意参数。存储过程可以显示结果，也可以不显示结果。 CREATE PROCEDURE productpricing() BEGIN SELECT AVG( prod_price) as priceaverage FROM products; END; 创建名为productpricing的储存过程。如果存储过程中需要传递参数，则将他们在括号中列举出来即可。括号必须有。BEGIN和END关键字用来限制存储过程体。上述存储过程体本身是一个简单的select语句。注意这里只是创建存储过程并没有进行调用。 储存过程的使用： Call productpring()； 8.53 使用参数的存储过程 一般存储过程并不显示结果，而是把结果返回给你指定的变量上。 变量：内存中一个特定的位置，用来临时存储数据。 MySQL> CREATE PROCEDURE prod( out pl decimal(8,2), out ph decimal(8,2), out pa decimal(8,2) ) begin select Min(prod_price) into pl from products; select MAx(prod_price) into ph from products; select avg(prod_price) into pa from products; end; call PROCEDURE(@pricelow,@pricehigh,@priceaverage); select @pricelow; select @pricehigh; select @pricelow,@pricehigh,@priceaverage; 解释： 此存储过程接受3个参数，pl存储产品最低价，ph存储产品最高价，pa存储产品平均价。每个参数必须指定类型，使用的为十进制，关键字OUT 指出相应的参数用来从存储过程传出一个值(返回给调用者)。 MySQL支持in(传递给存储过程)、out(从存储过程传出，这里所用)和inout(对存储过程传入和传出)类型的参数。存储过程的代码位于begin和end语句内。他们是一系列select语句，用来检索值。然后保存到相对应的变量(通过INTO关键字)。 存储过程的参数允许的数据类型与表中使用的类型相同。注意记录集是不被允许的类型，因此，不能通过一个参数返回多个行和列，这也是上面为什么要使用3个参数和3条select语句的原因。 调用：为调用此存储过程，必须指定3个变量名。如上所示。3个参数是存储过程保存结果的3个变量的名字。调用时，语句并不显示任何数据，它返回以后可以显示的变量(或在其他处理中使用)。 注意：所有的MySQL变量都是以@开头。 CREATE PROCEDURE ordertotal( IN innumber int, OUT outtotal decimal(8,2) ) BEGIN SELECT Sum(item_price * quantity) FROM orderitems WHERE order_num = innumber INTO outtotal; end // CALL ordertotal(20005,@total); select @total; // 得到20005订单的合计 CALL ordertotal(20009,@total); select @total; //得到20009订单的合计 8.54 带有控制语句的存储过程 CREATE PROCEDURE ordertotal( IN onumber INT, IN taxable BOOLEAN, OUT ototal DECIMAL(8,2) )COMMENT 'Obtain order total, optionally adding tax' BEGIN -- declear variable for total DECLARE total DECIMAL(8,2); -- declear tax percentage DECLARE taxrate INT DEFAULT 6; -- get the order total SELECT Sum(item_price * quantity) FROM orderitems WHERE order_num = onumber INTO total; -- IS this taxable? IF taxable THEN -- yes ,so add taxrate to the total SELECT total+(total/100*taxrate)INTO total; END IF; -- finally ,save to out variable SELECT total INTO ototal; END; 在存储过程中我们使用了DECLARE语句，他们表示定义两个局部变量，DECLARE要求指定变量名和数据类型。它也支持可选的默认值(taxrate默认6%)，因为后期我们还要判断要不要增加税，所以，我们把SELECT查询的结果存储到局部变量total中，然后在IF 和THEN的配合下，检查taxable是否为真，然后在真的情况下，我们利用另一条SELECT语句增加营业税到局部变量total中，然后我们再利用SELECT语句将total(增加税或者不增加税的结果)保存到总的ototal中。 COMMENT关键字 上面的COMMENT是可以给出或者不给出，如果给出，将在SHOW PROCEDURE STATUS的结果中显示。 触发器 在某个表发生更改时自动处理某些语句，这就是触发器。 触发器是MySQL响应delete 、update 、insert 、位于begin 和end语句之间的一组语句而自动执行的一条MySQL语句。其他的语句不支持触发器。 8.55 创建触发器 在创建触发器时，需要给出4条语句（规则)： 唯一的触发器名； 触发器关联的表； 触发器应该响应的活动； 触发器何时执行(处理之前或者之后) Create trigger 语句创建 触发器 CREATE TRIGGER newproduct AFTER INSERT ON products FOR EACH ROW SELECT 'Product added' INTO @info; CREATE TRIGGER用来创建名为newproduct的新触发器。触发器可以在一个操作发生前或者发生后执行，这里AFTER INSERT 是指此触发器在INSERT语句成功执行后执行。这个触发器还指定FOR EACH ROW ， 因此代码对每个插入行都会执行。文本Product added 将对每个插入的行显示一次。 注意： 1、触发器只有表才支持，视图，临时表都不支持触发器。 2、触发器是按照每个表每个事件每次地定义，每个表每个事件每次只允许一个触发器，因此，每个表最多支持六个触发器(insert，update，delete的before 和after)。 3、单一触发器不能与多个事件或多个表关联，所以，你需要一个对insert和update 操作执行的触发器，则应该定义两个触发器。 4、触发器失败：如果before 触发器失败，则MySQL将不执行请求的操作，此外，如果before触发器或者语句本身失败，MySQL则将不执行after触发器。8.56 触发器类别 INSERT触发器 是在insert语句执行之前或者执行之后被执行的触发器。 1、在insert触发器代码中，可引入一个名为new的虚拟表，访问被插入的行； 2、在before insert触发器中，new中的值也可以被更新(允许更改被插入的值)； 3、对于auto_increment列，new在insert执行之前包含0，在insert执行之后包含新的自动生成值 CREATE TRIGGER neworder AFTER INSERT ON orders FOR EACH ROW SELECT NEW.order_num; 创建一个名为neworder的触发器，按照AFTER INSERT ON orders 执行。在插入一个新订单到orders表时，MySQL生成一个新的订单号并保存到order_num中。触发器从NEW.order_num取得这个值并返回它。此触发器必须按照AFTER INSERT执行，因为在BEFORE INSERT语句执行之前，新order_num还没有生成。对于orders的每次插入使用这个触发器总是返回新的订单号。DELETE触发器 Delete触发器在delete语句执行之前或者之后执行。 1、在delete触发器的代码内,可以引用一个名为OLD的虚拟表，用来访问被删除的行。 2、OLD中的值全为只读，不能更新。 CREATE TRIGGER deleteorder BEFORE DELETE ON orders FOR EACH ROW BEGIN INSERT INTO archive_orders(order_num,order_date,cust_id) values (OLD.order_num,OLD.order_date,OLD.cust_id); END; CREATE TABLE archive_orders( order_num int(11) NOT NULL AUTO_INCREMENT, order_date datetime NOT NULL, cust_id int(11) NOT NULL, PRIMARY KEY (order_num), KEY fk_orders1_customers1 (cust_id), CONSTRAINT fk_orders1_customers1 FOREIGN KEY (cust_id) REFERENCES customers (cust_id) ) ENGINE=InnoDB AUTO_INCREMENT=20011 DEFAULT CHARSET=utf8 在任意订单被删除前将执行此触发器，它使用一条INSERT 语句将OLD中的值(要被删除的订单) 保存到一个名为archive_orders的存档表中(为实际使用这个例子，我们需要用与orders相同的列创建一个名为archive_orders的表) 使用BEFORE DELETE触发器的优点(相对于AFTER DELETE触发器来说)为，如果由于某种原因，订单不能存档，delete本身将被放弃。 我们在这个触发器使用了BEGIN和END语句标记触发器体。这在此例子中并不是必须的，只是为了说明使用BEGIN END 块的好处是触发器能够容纳多条SQL 语句(在BEGIN END块中一条挨着一条)。 UPDATE触发器 在update语句执行之前或者之后执行 1、在update触发器的代码内,可以引用一个名为OLD的虚拟表，用来访问以前(UPDATE语句之前)的值，引用一个名为NEW的虚拟表访问新更新的值。 2、在BEFORE UPDATE触发器中，NEW中的值可能也被用于更新(允许更改将要用于UPDATE语句中的值) 3、OLD中的值全为只读，不能更新。 CREATE TRIGGER updatevendor BEFORE UPDATE ON vendors FOR EACH ROW SET NEW.vend_state = Upper(NEW.vemd_state); 保证州名缩写总是大写(不管UPFATE语句中是否给出了大写)，每次更新一行时，NEW.vend_state中的值(将用来更新表行的值)都用Upper(NEW.vend_state)替换。8.57 总结 1、通常before用于数据的验证和净化(为了保证插入表中的数据确实是需要的数据) 也适用于update触发器。 2、与其他DBMS相比，MySQL 5中支持的触发器相当初级，未来的MySQL版本中估计会存在一些改进和增强触发器的支持。 3、创建触发器可能需要特殊的安全访问权限，但是触发器的执行时自动的，如果insert，update，或者delete语句能够执行，则相关的触发器也能执行。 4、用触发器来保证数据的一致性(大小写，格式等)。在触发器中执行这种类型的处理的优点就是它总是进行这种处理，而且透明的进行，与客户机应用无关。 5、触发器的一种非常有意义的使用就是创建审计跟踪。使用触发器，把更改(如果需要，甚至还有之前和之后的状态)记录到另外一个表是非常容易的。 6、MySQL触发器不支持call语句，无法从触发器内调用存储过程。 MySQL索引 b 树和 hash 索引应用场合 区别 主键索引和普通索引的区别 聚簇索引在底层怎么实现的，数据和关键字是怎么存的 复合索引 复合索引要把那个字段放最前，为什么 为啥MySQL索引要用B+树而MongoDB用B树？8.58 索引使用的基本原则 最经常查询的列上建立聚簇索引以提高查询效率 一个基本表最多只建立一个聚簇索引 经常更新的列不宜建立聚簇索引 主键和唯一键会自动创建索引8.59 索引分类——从数据结构角度 B-树,B+树,B*树 B/B+树是一种多级索引组织方法，是适合于组织存放在外存的大型磁盘文件的一种树状索引结构。其中用得比较多的是B+树。多路查找树 m叉查找树 内结点：非叶节点；外结点：叶节点 定义： 1)每个内结点至多有m个孩子和m-1个键值 2)具有n个键值的结点有n+1个孩子 3)有p个键值的结点：C0 K1 C1 K2 ,,, Kp Cp Ci是指针域 Ki是数据域 4)键值有序（从左到右 由小到大) 5)满足查找树的要求 C0所在子树的所有键值 高度与结点关系 m叉查找树的高度为h，则其 h B-树 平衡的m叉查找树 定义：B树首先是一棵多路查找树 1)根节点至少有两个孩子 2)所有非叶结点（除根节点)至少有 ceil(m/2) 个孩子 3)所有叶结点都在同一层，叶结点总数 = 键值总数 +1 因此一个结点的孩子数在 [ceil(m/2),m] 之间 随机查找的磁盘访问次数最多为树的高度 B+树 定义： 1)树中每个非叶结点最多有m个孩子 2)根节点至少有2个孩子 3)除根节点外，每个非叶结点至少有ceil(m/2)个孩子 4)有n个孩子的结点有n-1个键值 5)所有叶节点在同一层，包含了所有键值和指向相应数据对象的指针，键值升序 6)每个叶节点中的孩子数允许大于m。假设叶节点可容纳的最多键值数为m1，则指向数据对象的指针数为m1，孩子数n应满足 ceil(m1/2) 通常在B+树上有两个头指针，一个指向根结点（进行随机搜索)，一个指向关键字最小的叶结点（进行顺序搜索)。 随机查找key时每次所需要的磁盘I/O次数等于B+树的高度 B+树与B树的比较 组织方式不一样 B+树：所有有效的索引关键字值都必须存储在叶结点中，其内部结点中的键值只用于索引项的查找定位。 B树：有效的索引关键字值可以出现在B树的任意一个结点中。 因此： B+树：所有关键字的查找速度基本一致 B树：依赖于查找关键字所在结点的层次叶结点不同 B+树中叶节点间增加链表指针，提供对索引关键字的顺序扫描功能；叶节点的个数未必符合m叉查找树的要求，它依赖于键值字节数和指针字节数，为m1阶。为什么B+比B树更适合实际应用中操作系统的文件索引和数据库索引 1) B+的磁盘读写代价更低 B+的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说IO读写次数也就降低了。 2) B+树的查询效率更加稳定 由于非叶结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 3)树的遍历效率较高 数据库索引采用B+树的主要原因是 B树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。正是为了解决这个问题，B+树应运而生。B+树只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作（或者说效率太低)B*树 与B+树的区别： 1)定义了非叶子结点键值个数至少为(2/3)*m，即块的最低使用率为2/3 （代替B+树的1/2)； 2)为非叶结点也增加链表指针 B*树分配新结点的概率比B+树要低，空间使用率更高 MySQL中的B+树适用场景 InnoDB存储引擎使用的是B+树。 B+树为对如下类型的查询有效： 1)全值匹配：和索引中的所有列进行匹配 2)匹配最左前缀：只使用索引的第一列或前几列 3)匹配列前缀：只匹配某一列的值的开头部分 4)匹配范围值 5)精确匹配某一列并范围匹配另外一列 6)覆盖索引/只访问索引的查询 一般来说，如果B+树可以按照某种方式查找到值，那么也可以按照这种方式用于排序。如果ORDER BY子句满足前面列出的几种查询类型，则这个索引也可以满足对应的排序需求。 下面是一些关于B+树索引的限制： 1)如果不是按照索引的最左列开始查找，则无法使用索引 2)不能跳过索引中的列 3)如果查询中有某个列的范围查询，则其右边所有列都无法使用索引优化查找 Hash索引 哈希索引就是采用一定的哈希算法，把键值换算成新的哈希值，检索时不需要类似B+树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快。 只有精确匹配索引所有列的查询才有效！ 在MySQL中，只有Memory引擎显式支持Hash索引。 限制： 1)哈希索引只包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行（无法使用覆盖索引)。不过，访问内存中的行的速度很快。 2)哈希索引数据并不是按照索引值顺序存储的，所以无法进行排序 3)哈希索引不支持部分索引列匹配查找。比如建立复合哈希索引(A,B)，无法仅使用A使用哈希索引去查询 4)不支持范围查询，仅支持等值查询 5)哈希冲突严重时，索引维护的代码很高。B树索引与Hash索引比较 1)如果是等值查询，那么哈希索引明显有绝对优势，因为只需要经过一次算法即可找到相应的键值；当然了，这个前提是，键值都是唯一的。如果键值不是唯一的，就需要先找到该键所在位置，然后再根据链表往后扫描，直到找到相应的数据； 如果是范围查询检索，这时候哈希索引就毫无用武之地了，因为原先是有序的键值，经过哈希算法后，有可能变成不连续的了，就没办法再利用索引完成范围查询检索； 2)哈希索引也没办法利用索引完成排序，以及like ‘xxx%’ 这样的部分模糊查询（这种部分模糊查询，其实本质上也是范围查询)； 3)哈希索引也不支持多列联合索引的最左匹配规则； 4)B+树索引的关键字检索效率比较平均，在有大量重复键值情况下，哈希索引的效率是极低的，因为存在所谓的哈希碰撞问题。 8.60 索引分类——从物理存储角度 聚簇索引 InnoDB的聚簇索引实际上在同一个结构中保存了B+树索引和数据行。 当表有聚簇索引时，它的数据行实际上存放在索引的叶子页中。聚簇表示数据行和相邻的键值紧紧地存储在一起。因为无法同时把数据行存储在两个不同的地方，所以一个表只能有一个聚簇索引。 InnoDB通过主键聚簇数据。 每张表都会有一个聚簇索引。聚簇索引是一级索引。 聚簇索引一般是主键；没有主键，就是第一个唯一键；没有唯一键，就是隐藏ID。 聚簇索引以外的所有索引都称为二级索引。在InnoDB中，二级索引中的每条记录都包含该行的主键列，以及为二级索引指定的列。 InnoDB使用这个主键值来搜索聚簇索引中的行。 聚簇索引的优点： 1)可以将相关数据保存在一起，只需一次IO就可以取出相邻的数据 2)数据访问更快，因为索引和数据保存在同一个B+树中 3)使用覆盖索引扫描的查询可以直接使用叶节点中的主键值 缺点： 1)插入速度严重依赖于插入顺序。按照主键的顺序插入是加载数据到InnoDB表中速度最快的方式。但如果不是按照主键顺序加载数据，那么在加载完成后最好使用OPTIMIZE TABLE命令重新组织一下表 2)更新聚簇索引列的代价很高，因为会强制InnoDB将每个被更新的行移动到新的位置 3)插入新行或者更新主键导致需要移动行的时候，可能面临页分裂的问题。当行的主键值要求必须将这一行插入到某个已满的页中时，存储引擎会将该页分裂成两个页面来容纳该行，这就是一次页分裂操作。页分裂会导致表占用更多的磁盘空间。 4)可能导致全表扫描变慢，尤其是行比较稀疏，或者由于页分裂导致数据存储不连续的时候 5)二级索引（非聚簇索引)可能会更大， 因为在二级索引的叶子节点包含了引用行的主键值。这样的策略减少了当出现行移动或者页分裂时二级索引的维护工作。 6)二级索引访问需要两次B树索引查找，而不是一次。因为二级索引中叶子节点保存的是行的主键值，要找到数据行，还需要拿主键值到聚簇索引中进行一次查找。 对于InnoDB，自适应哈希索引能够减少这样的重复工作。 非聚簇索引 8.61 索引分类——从逻辑角度 主键索引 索引列的值必须唯一，并且不允许有空值唯一索引 与普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值（注意和主键不同)。如果是组合索引，则列值的组合必须唯一普通索引 最基本的索引，它没有任何限制复合索引 全文索引 在相同的列上同时创建全文索引和基于值的B树索引不会有冲突，全文索引适用于MATCH AGAINST操作，而不是普通的WHERE条件操作。 FULLTEXT索引仅可用于 MyISAM 表；他们可以从CHAR、VARCHAR或TEXT列中作为CREATE TABLE语句的一部分被创建，或是随后使用ALTER TABLE 或CREATE INDEX被添加空间索引（R-Tree) 空间索引：空间索引是对空间数据类型的字段建立的索引，MYSQL中的空间数据类型有4种，分别是GEOMETRY、POINT、LINESTRING、POLYGON MyISAM表支持空间索引，可以用作地理数据存储。和B树索引不同，这类索引无须前缀查询。空间索引会从所有维度来索引数据。查询时，可以有效地使用任意维度来组合查询。MySQL中的GIS支持并不完善，做的比较好的关系数据库是PostgreSQL的PostGIS。 8.62 索引的特殊应用 InnoDB AUTO_INCREMENT 如果正在使用InnoDB表并且没有什么数据需要聚集，那么可以定义一个代理键作为主键，这种主键的数据应该与应用无关，最简单的方法是使用AUTO_INCREMENT自增列。这样可以保证数据行是按顺序写入的，对于根据主键做关联操作的性能也会更好。 最好避免随机的聚簇索引，特别是对于IO密集型应用，比如UUID，它使得聚簇索引的插入变得完全随机，这是最坏的情况，使得数据没有任何聚集特性。 如果主键的值是顺序的，那么InnoDB会把每一条记录都存储在上一条记录的后面。当达到页的最大填充因子时，下一条记录就会写入新的页中。一旦数据按照这种顺序的方式加载，主键页就会近似于被顺序的记录填满，这也正是所期望的结果。 使用UUID作为主键的缺点： 1)写入的目标页可能已经刷到磁盘上并从缓存中移除，或者还没有被加载到缓存中，InnoDB在插入之前不得不先找到并从磁盘读取目标页到内存中，这将导致大量的随机iO 2)因为写入是乱序的，InnoDB不得不频繁地做页分裂操作，以便为新的行分配空间。页分裂会导致移动大量数据，一次插入最少修改三个页而不是一个页。 3)由于频繁的页分裂，页会变得稀疏并被不规则填充，所以最终数据会有碎片。 在把这些随机值载入到聚簇索引后，也许需要做一次OPTIMIZE TABLE来重建表并优化页的填充。 使用InnoDB时应该尽可能地按主键顺序插入数据，并且尽可能地使用单调增加的聚簇值来插入新行。 顺序主键的缺点是什么？ 对于高并发工作负载，在InnoDB中按主键顺序插入可能会造成明显的争用。主键的上界会成为热点。因为所有的插入都在这里，所以并发插入可能导致锁竞争。另一个热点可能是AUTO_INCREMENT锁机制，可能需要重新设计表或应用。 AUTO-INC锁是当向使用含有AUTO_INCREMENT列的表中插入数据时需要获取的一种特殊的表级锁 在最简单的情况下，如果一个事务正在向表中插入值，则任何其他事务必须等待对该表执行自己的插入操作，以便第一个事务插入的行的值是连续的。 innodb_autoinc_lock_mode配置选项控制用于自动增量锁定的算法。 它允许您选择如何在可预测的自动递增值序列和插入操作的最大并发性之间进行权衡。 innodb会在内存里保存一个计数器用来记录auto_increment的值，当插入一个新行数据时，就会用一个表锁来锁住这个计数器，直到插入结束。如果一行一行的插入数据则没有什么问题，但是如果大量的并发插入就废了，表锁会引起SQL堵塞，不但影响效率，而且可能会瞬间达到max_connections而崩溃。 InnoDB提供了一个可配置的锁定机制，可以显著提高使用AUTO_INCREMENT列向表中添加行的SQL语句的可伸缩性和性能。 要对InnoDB表使用AUTO_INCREMENT机制，必须将AUTO_INCREMENT列定义为索引的一部分，以便可以对表执行相当于索引的SELECT MAX（ai_col)查找以获取最大列值。 通常，这是通过使列成为某些表索引的第一列来实现的。 下面介绍AUTO_INCREMENT锁定模式的行为，对不同AUTO_INCREMENT锁定模式设置的使用含义，以及InnoDB如何初始化AUTO_INCREMENT计数器。 插入类型 1)simple inserts simple inserts指的是那种能够事先确定插入行数的语句，比如INSERT/REPLACE INTO 等插入单行或者多行的语句，语句中不包括嵌套子查询。此外，INSERT INTO … ON DUPLICATE KEY UPDATE这类语句也要除外。 2)bulk inserts bulk inserts指的是事先无法确定插入行数的语句，比如INSERT/REPLACE INTO … SELECT, LOAD DATA等。 3)mixed-mode inserts 指的是simple inserts类型中有些行指定了auto_increment列的值，有些行没有指定，比如： INSERT INTO t1 (c1,c2) VALUES (1,’a’), (NULL,’b’), (5,’c’), (NULL,’d’); 另外一种mixed-mode inserts是 INSERT … ON DUPLICATE KEY UPDATE这种语句，可能导致分配的auto_increment值没有被使用。 innodb_autoinc_lock_mode 配置 innodb_autoinc_lock_mode=0（traditional lock mode) 传统的auto_increment机制。这种模式下所有针对auto_increment列的插入操作都会加AUTO-INC锁，分配的值也是一个个分配，是连续的，正常情况下也不会有间隙（当然如果事务rollback了这个auto_increment值就会浪费掉，从而造成间隙)。innodb_autoinc_lock_mode=1（consecutive lock mode) 这种情况下，针对bulk inserts才会采用AUTO-INC锁这种方式，而针对simple inserts，则直接通过分析语句，获得要插入的数量，然后一次性分配足够的auto_increment id，只会将整个分配的过程锁住。。当然，如果其他事务已经持有了AUTO-INC锁，则simple inserts需要等待. 针对Mixed-mode inserts：直接分析语句，获得最坏情况下需要插入的数量，然后一次性分配足够的auto_increment id，只会将整个分配的过程锁住。 保证同一条insert语句中新插入的auto_increment id都是连续的，语句之间是可能出现auto_increment值的空隙的。比如mixed-mode inserts以及bulk inserts中都有可能导致一些分配的auto_increment值被浪费掉从而导致间隙。innodb_autoinc_lock_mode=2（interleaved lock mode) 这种模式下任何类型的inserts都不会采用AUTO-INC锁，性能最好。这种模式是来一个分配一个，而不会锁表，只会锁住分配id的过程，和innodb_autoinc_lock_mode = 1的区别在于，不会预分配多个。但是在replication中当binlog_format为statement-based时（简称SBR statement-based replication)存在问题，因为是来一个分配一个，这样当并发执行时，“Bulk inserts”在分配时会同时向其他的INSERT分配，会出现主从不一致（从库执行结果和主库执行结果不一样)，因为binlog只会记录开始的insert id。 可能会在同一条语句内部产生auto_increment值间隙。不同模式下间隙情况 simple inserts 针对innodb_autoinc_lock_mode=0,1,2，只有在一个有auto_increment列操作的事务出现回滚时，分配的auto_increment的值会丢弃不再使用，从而造成间隙。bulk inserts innodb_autoinc_lock_mode=0,由于一直会持有AUTO-INC锁直到语句结束，生成的值都是连续的，不会产生间隙。 innodb_autoinc_lock_mode=1，这时候一条语句内不会产生间隙，但是语句之间可能会产生间隙。 innodb_autoinc_lock_mode=2，如果有并发的insert操作，那么同一条语句内都可能产生间隙。mixed-mode inserts 这种模式下针对innodb_autoinc_lock_mode的值配置不同，结果也会不同，当然innodb_autoinc_lock_mode=0时时不会产生间隙的，而innodb_autoinc_lock_mode=1以及innodb_autoinc_lock_mode=2是会产生间隙的。 另外注意的一点是，在master-slave这种架构中，复制如果采用statement-based replication这种方式，则innodb_autoinc_lock_mode=0或1才是安全的。而如果是采用row-based replication或者mixed-based replication，则innodb_autoinc_lock_mode=0,1,2都是安全的。覆盖索引 如果一个索引包含了所有需要查询字段的值，就称为覆盖索引。 覆盖索引的优点： 1)索引条目远少于数据行大小，如果只需要读取索引，则MySQL就会极大地减少数据访问了，这对缓存的负载非常重要，因为这种情况下响应时间大部分花费在数据拷贝上。覆盖索引对IO密集型应用也有帮助，因为索引比数据更小，更容易全部放入内存中。 2)因为索引是按照列值顺序存储的，对于IO密集型的范围查询会比随机从磁盘读取每一行数据的IO次数会少得多。 3)InnoDB的二级索引在叶节点中保存了行的主键值，如果二级索引是覆盖索引，则可以避免对主键聚簇索引的二次查询。 不是所有类型的索引都可以成为覆盖索引。覆盖索引必须要存储索引列的值，而哈希索引、空间索引和全文索引都不存储索引列的值，所以MySQL只能使用B树索引做覆盖索引。 当发起一个索引覆盖查询时，在EXPLAIN的Extra列可以看到Using index的信息。 InnoDB的二级索引的叶子节点都包含了主键的值，这意味着InnoDB的二级索引可以有效利用这些额外的主键列来覆盖查询。 使用索引进行排序 MySQL有两种可以生成有序的结果：通过排序操作；按索引顺序扫描。如果EXPLAIN出来的type列的值为index，则说明MySQL使用了索引顺序扫描来做排序。 扫描索引本身是很快的，但如果索引不能覆盖查询所需的全部列，那就不得不每扫描一条索引记录就都回表查询一次对应的行。这基本上都是随机IO，因此按索引顺序读取数据的速度通常要比顺序地全表扫描要慢，尤其是在IO密集型的工作负载时。 只有当索引的列顺序和ORDER BY子句的顺序完全一致，并且所有列的排序方向（降序或升序，索引默认是升序)都一样时，MySQL才可以使用索引来对结果做排序。如果查询需要关联多张表，则只有当ORDER BY子句引用的字段全部为第一张表时，才能使用索引做排序。 ORDER BY子句和查找型索引的限制是一样的，都需要满足索引的最左前缀的要求。 有一种情况下ORDER BY子句可以不满足索引的最左前缀的要求，就是前导列为常量的时候。 前缀压缩索引 MyISAM通过前缀压缩来减少索引的大小，从而让更多的索引可以放入内存中。默认只压缩字符串，但通过参数调整也能对整数进行压缩。 MyISAM压缩每个索引块的方法时，先完全保存索引块的第一个值，然后将其他值和第一个值进行比较得到相同前缀的字节数和剩余的不同后缀部分，把这部分存储起来即可。 压缩块使用更少的情况，代价是某些操作可能更慢。因为每个值的压缩前缀都依赖前面的值，所以MyISAM查找时无法在索引块使用二分查找而只能从头开始扫描。冗余和重复索引 冗余索引：MySQL允许在相同列上创建多个索引。MySQL需要单独维护重复的索引，并且优化器在优化查询时也需要逐个地进行考虑，这会影响性能。 重复索引是指在相同的列上按照相同的顺序创建的相同类型的索引，应该避免这样创建重复索引，发现以后也应该立即移除。 冗余索引和重复索引有一些不同。如果创建了索引(A,B)，又创建了索引(A)就是冗余索引，索引(A,B)也可以当做索引(A)来使用。但是如果再创建索引(B,A)，就不是冗余索引。另外，其他不同类型的索引也不会是B树索引的冗余索引。 冗余索引通常发生在为表添加新索引的时候。例如，有人可能会增加一个新的索引(A,B)而不是扩展已有的索引(A)，还有一种情况是将一个索引扩展为(A,PK)，对于InnoDB而言PK已经包含在二级索引中了，所以这也是冗余的。 大多数情况下都不需要冗余索引，应该尽量扩展已有的索引而不是创建新索引。但也有时候出于性能方面的考虑需要冗余索引，因为扩展已有的索引会导致其变得太大，从而影响其他使用该索引的查询的性能。 例如，现在在整数列上有一个索引，需要额外增加一个很长的VARCHAR列来扩展该索引，那性能可能会急剧下降。 可以使用一些工具来找出冗余和重复的索引。 索引重用 现有索引(A,B,C)，如果要使用索引，那么where中必须写为A=a and B = b and C = c。如果没有对B的筛选，还想使用索引，怎么绕过最左前缀匹配呢？ 假设B是一个选择性很低的列，只有b1和b2两种取值，那么查询可以写为A = a and B in(b1,b2) and C = c。避免多个范围条件 对于范围条件查询，MySQL无法再使用范围列后面的其他索引列了，但是对于多个等值条件查询（in ...)则没有这个限制。 假设有索引(A,B)，查询条件为 A > a and B 优化limit 延迟关联，使用覆盖索引 8.63 适合建索引的情况 主键 连接中频繁使用的列 在某一范围内频繁搜索的列和按排列顺序频繁搜索的列8.64 不适合建索引的情况 很少或从来不在查询中引用的列 只有两个或很少几个值的列 以bit text image 数据类型定义的列 数据行数很少的小表8.65 索引优点 1)大大减少了服务器需要扫描的数据量 2)帮助服务器减少排序和临时表（group by和order by都可以使用索引，因为索引有序) 3)可以将随机IO变为顺序IO（覆盖索引)8.66 索引缺点 创建索引要花费时间，占用存储空间 减慢数据修改速度8.67 索引失效 CREATE TABLE staffs ( id INT PRIMARY KEY AUTO_INCREMENT, NAME VARCHAR (24) NOT NULL DEFAULT '' COMMENT '姓名', age INT NOT NULL DEFAULT 0 COMMENT '年龄', pos VARCHAR (20) NOT NULL DEFAULT '' COMMENT '职位', add_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '入职时间' ) CHARSET utf8 COMMENT '员工记录表' ; ALTER TABLE staffs ADD INDEX idx_staffs_nameAgePos(name, age, pos); 1、全值匹配 EXPLAIN SELECT * FROM staffs WHERE NAME = 'July'; EXPLAIN SELECT * FROM staffs WHERE NAME = 'July' AND age = 25; EXPLAIN SELECT * FROM staffs WHERE NAME = 'July' AND age = 25 AND pos = 'dev'; 2、最佳左前缀法则 如果索引了多列，要遵守最佳左前缀法则，指的是从索引的最左边的列开始并且不跳过索引中的列。 查询时就按照建索引的顺序进行筛选 EXPLAIN SELECT * FROM staffs WHERE age = 25 AND pos = 'dev'; EXPLAIN SELECT * FROM staffs WHERE pos = 'dev'; 3、在索引上使用表达式 - 索引列上使用了表达式，如where substr(a, 1, 3) = 'hhh'，where a = a + 1，表达式是一大忌讳，再简单MySQL也不认。 有时数据量不是大到严重影响速度时，一般可以先查出来，比如先查所有有订单记录的数据，再在程序中去筛选 哪怕是该字段没有建立索引，但不能保证以后不在这个字段上建立索引，所以可以这么说：不要在任何字段上进行操作。 4、range 类型查询字段后面的索引无效 最后一次只用到了两个索引 此时可以建一个只含前两个字段的索引5、尽量使用覆盖索引 6、使用不等于时索引失效 7、is (not) null 时索引失效 如果没有值，可以使其等于一个默认值，这样就可以利用到索引了。8、like 以通配符开头会导致全表扫描 9、varchar 类型不加单引号索引失效 不加单引号会出现类型转换，此时索引失效 10、使用or时索引失效 所以要少用or 8.68 总结 假设index(a,b,c) Where语句 索引是否被使用 where a = 3 Y,使用到a where a = 3 and b = 5 Y,使用到a，b where a = 3 and b = 5 and c = 4 Y,使用到a,b,c where b = 3 | where b = 3 and c = 4 | where c = 4 N where a = 3 and c = 5 使用到a， 但是C不可以，中间断了 where a = 3 and b > 4 and c = 7 使用到a和b， c在范围之后，断了 where a = 3 and b like 'kk%' and c = 4 同上 MySQL查询分析工具 8.69 慢查询日志 MySQL的慢查询日志是MySQL提供的一种日志记录，它用来记录在MySQL中响应时间超过阈值的语句，具体指运行时间超过long_query_time值的SQL，则会被记录到慢查询日志中。long_query_time的默认值为10，意思是运行10S以上的语句。默认情况下，Mysql数据库并不启动慢查询日志，需要我们手动来设置这个参数，当然，如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。慢查询日志支持将日志记录写入文件，也支持将日志记录写入数据库表。 slow_query_log ：是否开启慢查询日志，1表示开启，0表示关闭。 slow-query-log-file：新版（5.6及以上版本)MySQL数据库慢查询日志存储路径。可以不设置该参数，系统则会默认给一个缺省的文件host_name-slow.log long_query_time ：慢查询阈值，当查询时间多于设定的阈值时，记录日志。 log_queries_not_using_indexes：未使用索引的查询也被记录到慢查询日志中（可选项)。 log_output：日志存储方式。log_output='FILE'表示将日志存入文件，默认值是'FILE'。log_output='TABLE'表示将日志存入数据库，这样日志信息就会被写入到mysql.slow_log表中。MySQL数据库支持同时两种日志存储方式，配置的时候以逗号隔开即可，如：log_output='FILE,TABLE'。日志记录到系统的专用日志表中，要比记录到文件耗费更多的系统资源，因此对于需要启用慢查询日志，又需要能够获得更高的系统性能，那么建议优先记录到文件。 在实际生产环境中，如果要手工分析日志，查找、分析SQL，显然是个体力活，MySQL提供了日志分析工具mysqldumpslow。 s, 是表示按照何种方式排序 c: 访问计数 l: 锁定时间 r: 返回记录 t: 查询时间 al:平均锁定时间 ar:平均返回记录数 at:平均查询时间 -t, 是top n的意思，即为返回前面多少条的数据； -g, 后边可以写一个正则匹配模式，大小写不敏感的； 比如: 得到返回记录集最多的10个SQL。 mysqldumpslow -s r -t 10 /database/mysql/mysql06_slow.log 得到访问次数最多的10个SQL mysqldumpslow -s c -t 10 /database/mysql/mysql06_slow.log 得到按照时间排序的前10条里面含有左连接的查询语句。 mysqldumpslow -s t -t 10 -g “left join” /database/mysql/mysql06_slow.log 另外建议在使用这些命令时结合 | 和more 使用 ，否则有可能出现刷屏的情况。 mysqldumpslow -s r -t 20 /mysqldata/mysql/mysql06-slow.log | more8.70 explain explain SQL分析 每个列代表什么含义（关于优化级别 ref 和 all，什么时候应该用到index却没用到，关于extra列出现了usetempory 和 filesort分别的原因和如何着手优化等) 各字段解释：id - 1)id相同，表示执行顺序从上到下 where 条件从右往左读取 2)id不同，如果是子查询，id的序号会递增，id越大优先级越高，越先被执行 primary 是主查询 subquery是子查询 3)id有相同的，也有不同的，同时存在 id相同的可以被认为是一组，从上往下顺序执行 在所有组中，id值越大，优先级越高，越先执行。 Derived：衍生的 select_type table 显示这一行的数据是来自哪一张表 type（重要) type显示的是访问类型，是较为重要的一个指标，结果值从最好到最坏依次是： system > const > eq_ref > ref > fulltext > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL ， 一般来说，得保证查询至少达到range级别，最好能达到ref。 possible_keys 显示可能应用在这张表中的索引，一个或多个。 查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询实际使用 理论上可以被用上的key 实际使用到的索引 key_len ref 由key_len可知t1表的idx_col1_col2被充分使用，col1匹配t2表的col1，col2匹配了一个常量，即 'ac' 用到了多少个字段上的索引，ref就会有几个（大部分情况) 或者可以根据key_len的倍数来判断使用了几个字段上的索引 rows rows越少越好 extra 前两个最重要： 1、Using fileSort - 建立索引的作用1)查询2)排序 如果排序字段没有索引，那么可能会产生filesort文件排序，降低效率。 2、临时表Using temporary 如果数据量很大，使用临时表效率会很低。3、Using Index 覆盖索引（Covering Index),一说为索引覆盖。 理解方式一:就是select的数据列只用从索引中就能够取得，不必读取数据行，MySQL可以利用索引返回select列表中的字段，而不必根据索引再次读取数据文件,换句话说查询列要被所建的索引覆盖。 理解方式二:索引是高效找到行的一个方法，但是一般数据库也能使用索引找到一个列的数据，因此它不必读取整个行。毕竟索引叶子节点存储了它们索引的数据;当能通过读取索引就可以得到想要的数据，那就不需要读取行了。一个索引包含了(或覆盖了)满足查询结果的数据就叫做覆盖索引。 注意： 如果要使用覆盖索引，一定要注意select列表中只取出需要的列，不可select *， 因为如果将所有字段一起做索引会导致索引文件过大，查询性能下降。 4、Impossible where explain实例1 - 第一行（执行顺序4)：id列为1，表示是union里的第一个select，select_type列的primary表 示该查询为外层查询，table列被标记为，表示查询结果来自一个衍生表，其中derived3中3代表该查询衍生自第三个select查询，即id为3的select。【select d1.name......】 - 第二行（执行顺序2)：id为3，是整个查询中第三个select的一部分。因查询包含在from中，所以为derived。【select id,name from t1 where other_column=''】 - 第三行（执行顺序3)：select列表中的子查询select_type为subquery，为整个查询中的第二个select。【select id from t3】 - 第四行（执行顺序1)：select_type为union，说明第四个select是union里的第二个select，最先执行【select name,id from t2】 - 第五行（执行顺序5)：代表从union的临时表中读取行的阶段，table列的表示用第一个和第四个select的结果进行union操作。【两个结果union操作】 explain实例2（单表) CREATE TABLE IF NOT EXISTS article ( id INT(10) UNSIGNED NOT NULL PRIMARY KEY AUTO_INCREMENT, author_id INT(10) UNSIGNED NOT NULL, category_id INT(10) UNSIGNED NOT NULL, views INT(10) UNSIGNED NOT NULL, comments INT(10) UNSIGNED NOT NULL, title VARBINARY(255) NOT NULL, content TEXT NOT NULL ); 查询 category_id 为 1 且 comments 大于 1 的情况下,views 最多的 article_id。 EXPLAIN SELECT id,author_id FROM article WHERE category_id = 1 AND comments > 1 ORDER BY views DESC LIMIT 1; 结论：很显然,type 是 ALL,即最坏的情况。Extra 里还出现了 Using filesort,也是最坏的情况。优化是必须的。 开始优化： 1.1 新建索引+删除索引 ALTER TABLE article ADD INDEX idx_article_ccv ( category_id , comments, views ); create index idx_article_ccv on article(category_id,comments,views); DROP INDEX idx_article_ccv ON article 1.2 第2次EXPLAIN EXPLAIN SELECT id,author_id FROM article WHERE category_id = 1 AND comments >1 ORDER BY views DESC LIMIT 1; EXPLAIN SELECT id,author_id FROM article WHERE category_id = 1 AND comments =3 ORDER BY views DESC LIMIT 1 结论： type 变成了 range,这是可以忍受的。但是 extra 里使用 Using filesort 仍是无法接受的。 但是我们已经建立了索引,为啥没用呢? 这是因为按照 BTree 索引的工作原理, 先排序 category_id, 如果遇到相同的 category_id 则再排序 comments,如果遇到相同的 comments 则再排序 views。 当 comments 字段在联合索引里处于中间位置时, 因comments > 1 条件是一个范围值(所谓 range), MySQL 无法利用索引再对后面的 views 部分进行检索,即 range 类型查询字段后面的索引无效。 1.3 删除第一次建立的索引 DROP INDEX idx_article_ccv ON article; 1.4 第2次新建索引 ALTER TABLE article ADD INDEX idx_article_cv ( category_id , views ) ; create index idx_article_cv on article(category_id,views); 1.5 第3次EXPLAIN EXPLAIN SELECT id,author_id FROM article WHERE category_id = 1 AND comments > 1 ORDER BY views DESC LIMIT 1; 结论：可以看到,type 变为了 ref,Extra 中的 Using filesort 也消失了,结果非常理想。 DROP INDEX idx_article_cv ON article;explain实例3（两表) CREATE TABLE IF NOT EXISTS class ( id INT(10) UNSIGNED NOT NULL AUTO_INCREMENT, card INT(10) UNSIGNED NOT NULL, PRIMARY KEY (id) ); CREATE TABLE IF NOT EXISTS book ( bookid INT(10) UNSIGNED NOT NULL AUTO_INCREMENT, card INT(10) UNSIGNED NOT NULL, PRIMARY KEY (bookid) ); class： book： 下面开始explain分析 EXPLAIN SELECT * FROM class LEFT JOIN book ON class.card = book.card; 结论：type 有All 添加索引优化 ALTER TABLE book ADD INDEX Y ( card); 第2次explain EXPLAIN SELECT * FROM class LEFT JOIN book ON class.card = book.card; 可以看到第二行的 type 变为了 ref,rows 也变成了优化比较明显。 这是由左连接特性决定的。LEFT JOIN 条件用于确定如何从右表搜索行,左边一定都有, 所以右边是我们的关键点,一定需要建立索引。 左外连接索引建右表 删除旧索引 + 新建 + 第3次explain DROP INDEX Y ON book; ALTER TABLE class ADD INDEX X (card); EXPLAIN SELECT * FROM class LEFT JOIN book ON class.card = book.card; 然后来看一个右连接查询: 优化较明显。这是因为 RIGHT JOIN 条件用于确定如何从左表搜索行,右边一定都有,所以左边是我们的关键点,一定需要建立索引。 EXPLAIN SELECT * FROM class RIGHT JOIN book ON class.card = book.card; DROP INDEX X ON class; ALTER TABLE book ADD INDEX Y (card); 右连接，基本无变化 EXPLAIN SELECT * FROM class RIGHT JOIN book ON class.card = book.card;explain实例3（三表) phone： ALTER TABLE phone ADD INDEX z ( card); ALTER TABLE book ADD INDEX Y ( card);#上一个case建过一个同样的 EXPLAIN SELECT * FROM class LEFT JOIN book ON class.card=book.card LEFT JOIN phone ON book.card = phone.card; 后 2 行的 type 都是 ref 且总 rows 优化很好,效果不错。因此索引最好设置在需要经常查询的字段中。 ================================================================================== 【结论】 Join语句的优化 尽可能减少Join语句中的NestedLoop的循环总次数；“永远用小结果集驱动大的结果集”。 优先优化NestedLoop的内层循环； 保证Join语句中被驱动表上Join条件字段已经被索引； 当无法保证被驱动表的Join条件字段被索引且内存资源充足的前提下，不要太吝惜JoinBuffer的设置； 8.71 show profile 是否支持 show variables like 'profiling'; 开启功能 查看结果 show profiles; 诊断SQL 一般性建议 8.72 习题 【建表语句】 create table test03( id int primary key not null auto_increment, c1 char(10), c2 char(10), c3 char(10), c4 char(10), c5 char(10) ); 【建索引】 create index idx_test03_c1234 on test03(c1,c2,c3,c4); show index from test03; 问题：我们创建了复合索引idx_test03_c1234 ,根据以下SQL分析下索引使用情况？ explain select * from test03 where c1='a1'; explain select * from test03 where c1='a1' and c2='a2'; explain select * from test03 where c1='a1' and c2='a2' and c3='a3'; explain select * from test03 where c1='a1' and c2='a2' and c3='a3' and c4='a4'; 1) explain select * from test03 where c1='a1' and c2='a2' and c3='a3' and c4='a4'; 4 2) explain select * from test03 where c1='a1' and c2='a2' and c4='a4' and c3='a3'; 4 原因是MySQL的optimizer会进行优化，将查询语句调整为索引的顺序 3) explain select * from test03 where c1='a1' and c2='a2' and c3>'a3' and c4='a4'; 3 4) explain select * from test03 where c1='a1' and c2='a2' and c4>'a4' and c3='a3'; 4 SQL的优化器也调整为1,2,3,4的顺序 5) explain select * from test03 where c1='a1' and c2='a2' and c4='a4' order by c3; c3作用在排序而不是查找 因为有c3的索引，所以没有出现using filesort 2 无filesort 6) explain select * from test03 where c1='a1' and c2='a2' order by c3; 因为有c3的索引，所以没有出现using filesort 2 无filesort 7) explain select * from test03 where c1='a1' and c2='a2' order by c4; 出现了filesort 2 有filesort 8) 8.1 explain select * from test03 where c1='a1' and c5='a5' order by c2,c3; 只用c1一个字段索引，但是c2、c3用于排序,无filesort 1 无filesort 8.2 explain select * from test03 where c1='a1' and c5='a5' order by c3,c2; 出现了filesort，我们建的索引是1234，它没有按照顺序来，3 2 颠倒了 1 有filesort 9) explain select * from test03 where c1='a1' and c2='a2' order by c2,c3; 10) 10.1 explain select * from test03 where c1='a1' and c2='a2' and c5='a5' order by c2,c3; 用c1、c2两个字段索引，但是c2、c3用于排序,无filesort 10.2 explain select * from test03 where c1='a1' and c2='a2' and c5='a5' order by c3,c2; 本例有常量c2的情况，和8.2对比 where中添加了了c2，此时c2对应的是常量，所以order by c3,c2 真正起作用的只有c3 10.3 explain select * from test03 where c1='a1' and c5='a5' order by c3,c2; 1 有filesort 11) explain select * from test03 where c1='a1' and c4='a4' group by c2,c3; 1 无filesort group by 也会默认进行排序 12) explain select * from test03 where c1='a1' and c4='a4' group by c3,c2; Using where; Using temporary; Using filesort 1 有filesort 因为group by 顺序与索引顺序不同，所以会产生临时表，并排序 分组之前会先排序 MySQL性能优化 性能优化可以理解为在一定工作负载下尽可能地降低响应时间。 性能优化 不等于 提升QPS，这其实仅仅是吞吐量的优化。吞吐量的提升可以看做性能优化的副产品。因为每条查询执行时间变短，因此可以让服务器每秒执行更多的查询。 如果目标是降低响应时间，那么就需要测量时间花在什么地方。没有测量就没有调优。 一旦掌握并实践面向响应时间的优化方法，就会发现需要不断地对系统进行性能剖析（profiling)。性能剖析分为两个步骤：测量任务所花费的时间；对结果进行统计和排序，把重要的任务排在前面。 MySQL的profile将最重要的任务展示在前面，但有时候没显示出来的信息也很重要。比如： 值得优化的查询：一些只占总响应时间比重很小的查询是不值得优化的。 异常情况：某些任务即使没有出现在profile输出的前面也需要优化。比如非常影响用户体验的某些任务，即使执行次数较少。 MySQL查询优化 从效果上第一条影响最大，后面越来越小。 ① SQL语句及索引的优化 ② 数据库表结构的优化 ③ 系统配置的优化 ④ 硬件的优化8.73 慢查询基础：优化数据访问 查询性能低下最基本的原因是访问的数据太多。某些查询可能不可避免地需要筛选大量数据，但这并不常见。大部分性能低下的查询都可以通过减少访问的数据量的方式进行优化。对于低效的查询，我们发现提供下面两个步骤来分析总是很有效。 1、确认应用程序是否在检索大量超过需要的数据，通常是访问了太多的行，但有时候也可能是访问了太多的列。 2、确认MySQL服务器层是否在分析大量超过需要的数据行。是否向数据库请求了不需要的数据 1)查询不需要的记录：尽量使用LIMIT来获取所需的数据，而非取出全部数据然后在Application中获取某些行。 2)多表关联时返回全部列 3)总是取出全部列 4)重复查询相同的数据：使用缓存MySQL是否在扫描额外的记录 对于MySQL，最简单的衡量查询开销的三个指标如下： 1)响应时间 2)扫描的行数 3)返回的行数 响应时间 响应时间是两个部分的和：服务时间和排队时间。服务时间是指数据库处理这个查询真正花了多少时间。排队时间是指服务器因为等待某些资源而没有真正执行查询的时间——等待IO或等待锁。 在不同类型的应用压力下，响应时间并没有什么一致的规律或者公式。响应时间既可能是一个问题的结果也可能是一个问题的原因。 当你看到一个查询的响应时间的时候，首先需要问问自己，这个响应时间是否是一个合理的值。可以采用快速上限估计法来估算查询的响应时间：了解这个查询需要哪些索引以及它的执行计划是什么，然后计算大概需要多少个顺序和随机IO，再用其乘以在具体硬件条件下一次IO的消耗时间，最后把这些消耗都加起来，就可以获得一个大概参考值来判断当前响应时间是不是一个合理的值。扫描的行数和返回的行数 分析查询时，查看该查询扫描的行数是非常有帮助的。这在一定程度上能够说明该查询找到需要的数据的效率高不高。 对于找出那些糟糕的查询，这个指标可能还不够完美，因为并不是所有的行的访问代价都是相同的。较短的行的访问速度更快，内存中的行也比磁盘中的行的访问速度要快得多。 理想情况下扫描的行数和返回的行数应该是相同的。一般扫描的行数对返回的行数的比率很小，一般在1:1和1:10之间。扫描的行数和访问类型 在评估查询开销的时候，需要考虑一下从表中找到某一行数据的成本。MySQL有好几种访问方式可以查找并返回一行结果。有些访问方式可能需要扫描很多行才能返回一行结果，也有些访问方式可能无需扫描就能返回结果。 在EXPLAIN语句中的type列反映了访问类型。访问类型有全表扫描、索引扫描、范围扫描、唯一索引、常数引用等，速度由慢到快。 如果查询没有办法找到合适的访问类型，那么解决的最好方法通常就是增加一个合适的索引。 一般MytSQL能够使用如下三种方式应用WHERE条件，从好到坏依次为： 1)在索引中使用WHERE条件来过滤不匹配的记录，这是在存储引擎层完成的 2)使用覆盖索引（Extra中Using index)来返回记录，直接从索引中过滤不需要的记录并返回命中的结果，这是在MySQL服务器层完成的，但无需再回表查询记录 3)从数据表中返回数据，然后过滤不满足条件的记录（Extra中Using Where)，这是在MySQL服务器层完成的，MySQL需要先从数据表读出记然后过滤。 MySQL不会告诉我们生成结果实际上需要扫描多少行数据（例如关联查询结果返回的一条记录通过是由多条记录组成)，而只会告诉我们生成结果时一共扫描了多少行数据。扫描的行数中大部分都很可能是被WHERE条件过滤掉的，对最终的结果集没有贡献。 如果发现查询需要扫描大量的数据但只返回少数的行，那么通过可以尝试下面的技巧去优化它： 1)使用覆盖索引 2)改变表结构，例如使用单独的汇总表 3)重写这个复杂的查询，让MySQL优化器能够以更优化的方式执行这个查询8.74 重构查询的方式 一个复杂查询还是多个简单查询 传统实现中总是强调数据库层完成尽可能多的工作，因为以前认为网络通信、查询解析和优化是一件代价很高的事情。但是这样的想法对MySQL并不适用，MySQL从设计上让连接和断开连接都很轻量级，在返回一个小的查询结果方面很高效。 MySQL内部每秒能够扫描内存中上百万行数据，相比之下，MySQL响应数据给客户端就慢得多了。在其他条件都相同时，使用尽可能少的查询当然是很好的。但是有时候，将一个大查询分解为多个小查询是很有必要的。切分查询 有时候对于一个大查询我们需要分而治之，将大查询切分成小查询，每个查询功能完全一样，只完成一小部分，每次只返回一部分查询结果。 比如删除旧的数据，分批删除效率会高很多。 分解关联查询 很多高性能的应用都会对关联查询进行分解。可以对每一张表进行一次单表查询，然后将结果在应用程序中进行关联。 这样做的好处有： 1)让缓存的效率更高。许多应用程序可以方便地缓存单表查询对应的结果对象。对MySQL的查询缓存来说，如果关联中的某张表发生了变化，那么就无法使用查询缓存了，而拆分后，如果某张表很少改变，那么基于该表的查询就可以重复利用缓存了。 2)将查询分解后，执行单个查询可以减少锁的竞争 3)在应用层做关联，可以更容易对数据库进行拆分 4)查询本身效率也可能会有所提升。比如使用IN来代替JOIN，可以让MySQL按照ID顺序进行查询。 5)可以减少冗余记录的查询。在数据库中做关联查询可能需要重复地访问一部分数据。 6)相当于在应用中实现了哈希关联，而不是使用MySQL的嵌套循环关联。8.75 优化特定类型的查询 JOIN 优化 1)确保ON或者Using子句上的列有索引，在创建索引的时候就要考虑到关联的顺序。当表A和表B用列c关联时，如果优化器的关联顺序是B、A，那么就不需要在B表的对应列上建索引。一般情况下只需要在关联顺序的第二个表的相应列上创建索引。 2)确保任何的GROUP BY和ORDER BY中的表达式只涉及到一个表中的列，这样MySQL才有可能使用索引来优化这个过程 小表驱动大表 两张表连接，类似于二重循环 外层的表应该是小表，内层的应该是大表 虽然总的遍历次数是一样的，但是频繁切换数据表是影响效率的（IO次数)，应该尽可能减少切换表的次数。 A in B： for b in B: for a in A: if a == b: putIntoResultSet() A exists B： for a in A: for b in B: if a == b: putIntoResultSet() 所以，如果A是小表，B是大表时 如果用in，那么是B in A 如果用exists，那么是A exists B order by优化 尽量使用index方式排序，遵照索引的最佳左前缀 CREATE TABLE tblA( id int primary key not null auto_increment, age INT, birth TIMESTAMP NOT NULL ); CREATE INDEX idx_A_ageBirth ON tblA(age,birth); 排序时使用的字段的顺序最好与index建立的顺序相同 如果字段顺序不同，那么也会出现filesort MySQL支持二种方式的排序，FileSort和Index，Index效率高. 它指MySQL扫描索引本身完成排序。FileSort方式效率较低。 order a,b where a = xxx order by b 非索引列的filesort算法 问题： 在sort_buffer中单路排序比双路排序要多占用很多空间，因为单路排序是把所有字段都取出, 所以有可能取出的数据的总大小超出了sort_buffer的容量，导致每次只能取sort_buffer容量大小的数据，进行排序（创建tmp文件，多路合并)，排完再取sort_buffer容量大小，再排……从而多次I/O。 本来想省一次I/O操作，反而导致了大量的I/O操作，反而得不偿失。 优化策略 分组时也是需要order by1. Order by时select * 是一个大忌 只取出需要的字段， 这点非常重要。在这里的影响是： 1.1 当Query的字段大小总和小于max_length_for_sort_data 而且排序字段不是 TEXT|BLOB 类型时，会用改进后的算法——单路排序， 否则用老算法——多路排序。 1.2 两种算法的数据都有可能超出sort_buffer的容量，超出之后，会创建tmp文件进行合并排序，导致多次I/O，但是用单路排序算法的风险会更大一些,所以要提高sort_buffer_size。2. 尝试提高 sort_buffer_size 不管用哪种算法，提高这个参数都会提高效率，当然，要根据系统的能力去提高，因为这个参数是针对每个进程的 3. 尝试提高 max_length_for_sort_data 提高这个参数， 会增加用改进算法的概率。但是如果设的太高，数据总容量超出sort_buffer_size的概率就增大，明显症状是高的磁盘I/O活动和低的处理器使用率.总结 group by 优化 当无法使用索引时，GROUP BY使用两种策略来完成：使用临时表或者文件排序来做分组。 当不遵照最佳左前缀，order by会出现filesort，而group by会出现临时表和filesortlimit 优化 当偏移量非常大的时候，比如limit 1000,20 这样的查询，这时MySQL需要查询10020条记录然后只返回最后20条，这样的代价非常高。要优化这种查询，要么在页面中限制分页数量，要么优化大偏移的性能。 一个简单的办法是使用覆盖索引（延迟关联) 如果使用书签记录上次取数据的位置，那么下次就可以直接从该书签记录的位置开始扫描。 假设主键递增： 通过判断id的范围来分页 select id,my_sn from big_data where id>5000000 limit 10; 也得到了分页的数据，但是我们发现如果id不是顺序的，也就是如果有数据删除过的话，那么这样分页数据就会不正确，这个是有缺陷的。UNION优化 MySQL总是通过创建并填充临时表的方式来执行UNION查询。因此很多优化策略在UNION查询中都没法很好使用。经常需要手工将WHERE、LIMIT、ORDER BY等子句下推到UNION的各个子查询中，以便优化器可以充分利用这些条件进行优化。 MySQL实现层次模型 8.76 邻接模型 属性：id,id_parent,other fields 兄弟结点无序 特点： 1)DML节点效率高，查询性能最高 2)只支持单父节点 3)递归实现 4)删除子树较难8.77 物化路径模型 属性：materialized_path,other fields PathID(1,1.1,1.2,1.1.1) 使用层次式的路径，明确地标识出来，一般用字符串存储路径，允许兄弟节点有序 特点： 1)查询编写容易 2)计算由路径导出的层次不方便 3)会产生重复记录问题 4)查询性能中等8.78 嵌套集合模型 属性：left_num,right_num,other fields 每一个节点都有一个left_num和一个right_num。某节点的后代的left_num和right_num都会在该节点的left_num和right_num范围内。 特点： 1)查找子节点容易，但是无法实现缩排 2)适合DFS 3)DML开销大，查询性能最低 分区分库分表 分区（针对表) 简介 数据表的物理存储拆分为多个文件 分区表是一个独立的逻辑表，其底层由多个物理子表组成。实现分区的代码实际上是对一组底层表的句柄对象（Handler Object)的封装。对分区表的请求，都会通过句柄对象转化成对存储引擎的接口调用。所以分区对于SQL层来说是一个完全封装底层实现的黑盒子，对应用是透明的。 MySQL实现分区表的方式——对底层表的封装——意味着索引也是按照分区的子表定义的，而没有全局索引。 MySQL在创建表时使用PARTITION BY子句定义每个分区存放的数据。在执行查询的时候，优化器会根据分区定义过滤那些没有我们需要数据的分区，这样查询就无须扫描所有分区——只需要查找包含需要数据的分区即可。 分区的一个目的是将数据按照一个较粗的粒度分在不同的表中。这样做可以将相关的数据存放在一起，另外，如果想一次批量删除整个分区的数据也会变得很方便。 分区非常适合在以下场景： 1)表非常大以至于无法全部放在内存中，或者只在表的最后部分有热点数据，其他均为历史数据 2)分区表的数据更容易维护。（批量删除数据->清除整个分区) 3)分区表的数据可以分布在不同的物理设备上，从而高效地利用多个硬件设备 4)可以使用分区表来避免某些特殊的瓶颈。、比如InnoDB的单个索引的互斥访问，ext3文件系统的inode锁竞争。 5)还可以备份和恢复独立的分区 分区表也有一些限制： 1)一个表最多只能有1024个分区 2)如果分区字段有主键或者唯一索引，那么所有主键列和唯一索引列都必须包含进来 3)分区表中无法使用外键索引 原理 存储引擎管理分区的各个底层表和管理普通表一样，所有的底层表都必须使用相同的存储引擎，分区表的索引只是在各个底层表上各自加一个完全相同的索引。从存储引擎的角度，底层表和一个普通表没有任何不同。select 分区层先打开并锁住所有的底层表，优化器先判断是否可以过滤部分分区，然后再调用对应的存储引擎接口访问各个分区的数据。insert 当写入一条记录时，分区层先打开并锁住所有的底层表，然后确定哪个分区接收这条记录，再将记录写入对应底层表。delete 当删除一条记录时，分区层先打开并锁住所有的底层表，然后确定数据对应的分区，最后对相应底层表进行删除操作。update 当更新一条记录时，分区层先打开并锁住所有的底层表，MySQL先确定需要更新的记录在哪个分区，然后取出数据并更新，再判断更新后的数据应该放在哪个分区，最后对底层表进行写入操作，并对原数据所在的底层表进行删除操作。 虽然每个操作都会先打开并锁住所有的底层表，但这并不是分区表在处理过程中是锁住全表的，如果存储引擎能够自己实现行级锁，则会在分区层释放对应表锁，比如InnoDB，这个加锁和解锁的过程与普通InnoDB上的查询类似 分区类型 Range分区 原理 MySQL将会根据指定的拆分策略，把数据放在不同的表文件上。相当于在文件上被拆成了小块.但是,对外给客户的感觉还是一张表，是透明的。案例 CREATE TABLE tbl_new( id INT NOT NULL PRIMARY KEY, title VARCHAR(20) NOT NULL DEFAULT '' )ENGINE MYISAM CHARSET utf8 PARTITION BY RANGE(id)( PARTITION t0 VALUES LESS THAN(10), PARTITION t1 VALUES LESS THAN(20), PARTITION t2 VALUES LESS THAN(MAXVALUE) ); 0~10放在t0 10~20放在t1 20放在t2 如果要查询id在20以上的，那么会直接去t2分区查找 如果插入的记录的id在20以上，那么会插入到t2分区 物理文件： 可以看出，普通的InnoDB引擎的表是一个frm和一个ibd文件 分区之后的MyIasm引擎的表有一个frm和par文件，此外每个分区还有一个myi和myd文件。 frm：表的结构信息 par：表的分区信息 myi：表的索引信息 myd：表的数据信息 range的字段未必一定是id List分区 原理 MySQL中的LIST分区在很多方面类似于RANGE分区。和按照RANGE分区一样，每个分区必须明确定义。它们的主要区别在于， LIST分区中每个分区的定义和选择是基于某列的值从属于一个值列表集中的一个值， 而RANGE分区是从属于一个连续区间值的集合。 案例 create table area( id INT NOT NULL PRIMARY KEY, region varchar(20) )engine myisam charset utf8; insert into area values(1,'bj'); insert into area values(2,'sh'); insert into area values(3,'gz'); insert into area values(4,'sz'); 这个area的值是确定的 create table user ( uid int not null, userName varchar(20), area_id int )engine myisam charset utf8 partition by list(area_id) ( partition bj values in (1), partition sh values in (2), partition gz values in (3), partition sz values in (4) ); User： 其他分区 Hash分区 Key分区 子分区 分区表如何应用于大数据量 数据量超大时，肯定不能去全表扫描，并且B树索引也无法起作用，除非是覆盖索引。这正是分区要做的事情。理解分区时可以将其当做索引的最初形态，以代价非常小的方式定位到需要的数据在哪一片区域。在这篇区域中，可以做顺序扫描，可以建索引，还可以将数据都缓存到内存。 为了保证大数据量的可扩展性，一般有下面两个策略： 1)全量扫描数据，不要加任何索引 可以使用简单的分区方式存放表，不要任何索引，根据分区的规则大致定位需要的数据位置。只要能够使用WHERE条件，将需要的数据限制在少数分区中，则效率是很高的。 2)索引数据，并分离热点 如果数据有明显的热点，而且除了这部分数据，其他数据很少被访问到，那么可以将这部分热点数据单独放在一个分区中，让这个分区的数据能够有机会都缓存在内存中。这样查询就可以只访问一个很小的分区表，能够使用索引，也能够有效地使用缓存。 分区表的陷阱NULL值会使分区过滤无效 分区的表达式的值可以是NULL；第一个分区是一个特殊分区，如果表达式的值为NULL或非法制，记录都会被存放到第一个分区。WHERE查询时即使看起来可以过滤到只有一个分区，但实际会检查两个分区，即第一个分区。最好是设置分区的列为NOT NULL。分区列和索引列不匹配 如果定义的索引列和分区列不匹配，会导致索引无法进行分区过滤。 假设在列a上定义了索引，而在列b上进行分区。因为每个分区都有其独立的索引，所以扫描b上的索引就需要扫描每一个分区内对应的索引。 选择分区的成本可能很高 尤其是范围分区，对于回答“这一行属于哪个分区”、“这些符合查询条件的行在哪些分区”这样的问题的成本可能会非常高。其他的分区类型，比如键分区和哈希分区，就没有这样的问题。 在批量插入时问题尤其严重。其他限制 1)每个分区都必须使用同样的存储引擎 2)分区函数中可以使用的函数和表达式也有一些限制 3)某些存储引擎不支持分区 4)对应MyISAM表，使用分区表时需要打开更多的文件描述符。有可能出现茶瓯go文件描述符限制的问题。查询优化 对于访问分区表来说，很重要的一点是要在WHERE条件中加入分区列，有时候即使看似多余的也要带上，这样就可以让优化器能够过滤无须访问的分区。 使用EXPLAIN PARTITION可以观察优化器是否执行而来分区过滤。 分库（针对库) 简介 一个库里表太多了，导致了海量数据，系统性能下降，把原本存储于一个库的表拆分存储到多个库上，通常是将表按照功能模块、关系密切程度划分出来，部署到不同库上。 将一个数据库里的表拆分到多个数据库（主机)中，形成数据库集群 比如分为一个静态信息库（基本没有写入)和一个业务相关的库（频繁写入) 为什么要分库 数据库集群环境后都是多台slave,基本满足了读取操作; 但是写入或者说大数据、频繁的写入操作对master性能影响就比较大， 这个时候，单库并不能解决大规模并发写入的问题。 优点 减少增量数据写入时的锁对查询的影响。 由于单表数量下降，常见的查询操作由于减少了需要扫描的记录，使得单表单次查询所需的检索行数变少，减少了磁盘IO，时延变短。 但是它无法解决单表数据量太大的问题。 分表（针对表) 简介 水平拆分（行) 类似于Range分区 一张表有很多数据时，将数据分到多张表中 MySQL单表的容量不超过500W（300W就需要拆分)，否则建议水平拆分 垂直拆分（列) 比如有些表会有大量的属性 将一些相关的属性拆分到一张单独的表 垂直分表， 通常是按照业务功能的使用频次，把主要的、热门的字段放在一起做为主要表； 然后把不常用的，按照各自的业务属性进行聚集，拆分到不同的次要表中；主要表和次要表的关系一般都是一对一的。 冷数据放到主要表中，热数据放到次要表中 使用 切分策略：需要DBA参与研究 导航路由：查询时，怎么导航到哪一张表 开源方案 MySQL Fabric（官方) 网址：http://www.MySQL.com/products/enterprise/fabric.html MySQL Fabric 是一个用于管理 MySQL 服务器群的可扩展框架。该框架实现了两个特性 — 高可用性 (HA) 以及使用数据分片的横向扩展 官方推荐，但是2014年左右才推出，是真正的分表，不是代理的(不同于MySQL-proxy)。 未来很有前景，目前属于测试阶段还没大规模运用于生产,期待它的升级。 Atlas（奇虎360) Atlas是由 Qihoo 360, Web平台部基础架构团队开发维护的一个基于MySQL协议的数据中间层项目。 它在MySQL官方推出的MySQL-Proxy 0.8.2版本的基础上，修改了大量bug，添加了很多功能特性。目前该项目在360公司内部得到了广泛应用，很多MySQL业务已经接入了Atlas平台，每天承载的读写请求数达几十亿条。 主要功能： 读写分离 从库负载均衡 IP过滤 SQL语句黑白名单 自动分表，只支持单库多表，不支持分布式分表，同理，该功能我们可以用分库来代替，多库多表搞不定。 网址： https://github.com/Qihoo360/Atlas TDDL（阿里) 江湖外号：头都大了 淘宝根据自己的业务特点开发了TDDL（Taobao Distributed Data Layer )框架，主要解决了分库分表对应用的透明化以及异构数据库之间的数据复制，它是一个基于集中式配置的 jdbc datasource实现，具有主备，读写分离，动态数据库配置等功能。 TDDL所处的位置（tddl通用数据访问层，部署在客户端的jar包，用于将用户的SQL路由到指定的数据库中)： 淘宝很早就对数据进行过分库的处理， 上层系统连接多个数据库，中间有一个叫做DBRoute的路由来对数据进行统一访问。DBRoute对数据进行多库的操作、数据的整合，让上层系统像操作 一个数据库一样操作多个库。但是随着数据量的增长，对于库表的分法有了更高的要求，例如，你的商品数据到了百亿级别的时候，任何一个库都无法存放了，于是 分成2个、4个、8个、16个、32个……直到1024个、2048个。好，分成这么多，数据能够存放了，那怎么查询它？这时候，数据查询的中间件就要能够承担这个重任了，它对上层来说，必须像查询一个数据库一样来查询数据，还要像查询一个数据库一样快（每条查询在几毫秒内完成)，TDDL就承担了这样一 个工作。在外面有些系统也用DAL（数据访问层) 这个概念来命名这个中间件。 系出名门，淘宝诞生。功能强大，阿里开源（部分) 主要优点： 1.数据库主备和动态切换 2.带权重的读写分离 3.单线程读重试 4.集中式数据源信息管理和动态变更 5.剥离的稳定jboss数据源 6.支持MySQL和oracle数据库 7.基于jdbc规范，很容易扩展支持实现jdbc规范的数据源 8.无server,client-jar形式存在，应用直连数据库 9.读写次数,并发度流程控制，动态变更 10.可分析的日志打印,日志流控，动态变更 TDDL必须要依赖diamond配置中心（diamond是淘宝内部使用的一个管理持久配置的系统，目前淘宝内部绝大多数系统的配置，由diamond来进行统一管理，同时diamond也已开源)。 TDDL动态数据源使用示例说明：http://rdc.taobao.com/team/jm/archives/1645 diamond简介和快速使用：http://jm.taobao.org/tag/diamond%E4%B8%93%E9%A2%98/ TDDL源码：https://github.com/alibaba/tb_tddl TDDL复杂度相对较高。当前公布的文档较少，只开源动态数据源，分表分库部分还未开源，还需要依赖diamond，不推荐使用。 MySQL Proxy（官方) 官网提供的，小巧精干型的，但是能力有限，对于大数据量的分库分表无能为力，适合中小型的互联网应用，基本上MySQL-proxy - master/slave就可以构成一个简单版的读写分离和负载均衡 总结 分库分表演变过程 单库多表--->读写分离主从复制--->垂直分库，每个库又可以带着slave--->继续垂直分库，极端情况单库单表--->分区(变相的水平拆分表，只不过是单库的)--->水平分表后再放入多个数据库里，进行分布式部署 - 单库多表 - 读写分离主从复制 - 垂直分库（每个库又可以带salve) - 继续垂直分库，理论上可以到极端情况，单库单表 - 分区（partition是变相的水平拆分，只不过是单库内进行) - 终于到水平分表，后续放入多个数据库里，进行分布式部署，终极method。 - 但是理论上OK，实际上中间的各种通信、调度、维护和编码要求，更加高。 分库分表后的难题 - 分布式事务的问题，数据的完整性和一致性问题。 - 数据操作维度问题：用户、交易、订单各个不同的维度，用户查询维度、产品数据分析维度的不同对比分析角度。 - 跨库联合查询的问题，可能需要两次查询 - 跨节点的count、order by、group by以及聚合函数问题，可能需要分别在各个节点上得到结果后在应用程序端进行合并 - 额外的数据管理负担，如：访问数据表的导航定位 - 额外的数据运算压力，如：需要在多个节点执行，然后再合并计算 - 程序编码开发难度提升，没有太好的框架解决，更多依赖业务看如何分，如何合，是个难题。 不到最后一步不要轻易水平分表 主从复制 8.79 复制概述 复制解决的基本问题是让一台服务器的数据与其他服务器保持同步。一台主库的数据可以同步到多台备库上，备库本身也可以被配置成另外一台服务器的主库。 MySQL支持两种复制方式：基于行的复制和基于语句的复制。这两种方式都是通过在主库上记录binlog，在备库重放日志的方式来实现异步的数据复制。这意味着，在同一时间点备库上的数据可能与主库存在不一致，并且保证主备之间的延迟。 复制通常不会增加主库的开销，主要是启用binlog带来的开销，但出于备份或及时从崩溃中恢复的目的，这点开销也是必要的。除此之外，每个备库也会对主库增加一些负载（网络IO)，尤其当备库请求从主库读取旧的binlog时，可能会造成更高的IO开销。 通过复制可以将读操作指向备库来获得更好的读扩展，但对于写操作，除非设计得当，否则并不适合通过复制来扩展写操作。复制解决的问题 数据分布 可以在不同地址位置来分布数据备份，例如不同的数据中心，即使在不稳定的网络环境下，远程复制也可以工作。负载均衡 可以将读操作分布到多个服务器上，实现对读密集型应用的优化，并且实现方便。备份 对于备份来说，复制是一项很有意义的技术补充，但复制既不是备份，也不能取代备份。高可用和故障切换 复制能够帮助应用程序避免MySQL单点失败，一个包含复制的设计良好的故障切换系统能够显著地缩短宕机实现。复制如何工作 1、在主库上把数据更改记录在binlog中（这些记录称为二进制日志事件) 2、备库将主库上的日志复制到自己的中继日志中 3、备库读取中继日志中的事件，将其重放到备库数据之上 8.80 复制原理 基于语句的复制 主库会记录那些造成数据更改的查询，当备库读取并重放这些事件时，实际上只是把主库上执行过的SQL再执行一遍。 优点：1)实现简单2)binlog中的事件更加紧凑 问题： 1)同一条SQL在主库和备库上执行的时间可能稍微或很不相同，因此在传输的binlog中，除了SQL，还有一些元数据，比如时间戳 2)一些无法被正确复制的SQL，存储过程、触发器 3)更新必须是串行的，这需要更多的锁基于行的复制 会将实际数据记录在binlog中。 好处：1)可以正确地复制每一行，一些语句可以被更加有效地复制 2)复制更加高效（但也视情况而定)比较 理论上基于行的复制方式整体上更有效，并且在实际应用中也适用于大多数场景。8.81 复制拓扑 可以在任意个主库和备库之间建立复制，只有一个限制：每一个备库只能有一个主库。 每个备库必须有一个唯一的服务器ID；一个主库可以有多个备库；如果打开了log_slave_updates选项，一个备库可以把其主库上的数据变化传播到其他备库。 1)一主库多备库 2)主动-主动模式下的主主复制 / 双主复制 每一个都被配置成对方的主库和备库。 这种配置最大的问题是如何解决冲突，比如两台服务器同时修改一行记录，或同时往两台服务器上向一个包含AUTO_INCREMENT列的表里插入数据。 使用起来非常麻烦 3)主动-被动模式下的主主复制 主要区别在于其中的一台服务器是只读的被动服务器。 这种方式使得反复切换主动和被动服务器非常方便，因为服务器的配置是对称的，这使得故障转移和故障恢复很容易。 设置主动-被动的主主复制在某种意义上类似于创建一个热备份，但是可以使用这个备份来提高性能。比如用它来执行读操作、备份、离线维护以及升级。真正的热备份是做不了这些事情的，然而，你不会获得比单台服务器更好的写性能。 4)拥有备库的主主结构 为每个主库都增加一个备库，增加了冗余，可以将读查询分配到备库上。 5)环形复制：环形结构可以有三个或更多的主库，每个服务器都是它之前的服务器的备库，是在它之后的服务器的主库。 环形结构没有双主结构的一些优点，比如对称配置和简单的故障转移，并完全依赖于环上的每一个可用结点，这大大增加了整个系统失效的几率。 可以为每个节点增加备库的方式来减少环形复制的风险。 8.82 复制和容量规划 为什么复制无法扩展写操作 糟糕的服务容量比例的根本原因是不能像分发读操作那样把写操作等同地分发到更多服务器上。 分区是唯一可以扩展写入的方法（分库分表？) 备库什么时候开始延迟 为了预测在将来的某个时间点会发生什么，可以人为地之制造延迟，然后看多久备库能赶上主库。规划冗余容量 在构建一个大型应用时，有意让服务器不被充分使用，这应该是一种聪明并且划算的方式，尤其是在使用复制的时候，有多余容量的服务器可以更好地处理负载尖峰，也有更多能力处理慢速查询和维护工作（OPTIMIZE TABLE)，并且能更好跟上复制。8.83 复制管理和维护 监控复制 在主库上，可以使用SHOW MASTER STATUS 来查看当前主库的binlog位置和配置，还可以查看主库当前有哪些binlog是在磁盘上的。测量备库延迟 虽然SHOW SLAVE STATUS输出的Seconds_behind_master列理论上显示了备库的延时，但由于各种各样的原因，并不总是准确的： 1)Seconds_behind_master是通过将备库服务器当前时间戳与binlog中事件的时间戳相对比得到的，所以只有在执行事件时才能报告延迟 2)如果备库复制线程没有运行，就会报延迟为NULL 3)一些错误（网络不稳定)可能中断复制/停止复制线程，但Seconds_behind_master将显示为0而不是显示错误 4)即使备库线程正在运行，备库有时候无法计算延时 5)一个大事务可能会导致延迟波动。 可以使用一些其他方法来监控备库延迟，比如heartbeat record，这是一个在主库上每秒更新一次的时间戳。为了计算延时，可以直接用备库当前的时间戳减去心跳记录的值。 确定主备是否一致 主备经常会因为MySQL的bug、网络中断、服务器崩溃导致不一致。 应该经常性地检查主备是否一致。可以使用一些工具去计算表和数据的checksum。从主库重新同步备库 传统方法是关闭备库，然后重新从主库复制一份数据。但当数据量很大时，如果能够找出并修复不一致的数据，要比从其他服务器上重新复制数据要有效的多。 最简单的办法是使用mysqldump转储受影响的数据并重新导入。如果数据没有发生变化，这种方法会很好，可以在主库上将表锁住然后转储，再等待备库赶上主库，然后将数据导入到备库中。缺点是在一个繁忙的服务器上可能行不通，另外在备库上通过非复制的方式改变数据可能不够安全。改变主库 只需在备库中简单地使用CHANGE MASTER TO命令，并指定合适的值。整个过程最难的是获取新主库上合适的binlog的位置，这样备库才可以从和老主库相同的逻辑位置开始复制。8.84 复制的问题和解决方案 数据损坏或丢失 意外关闭服务器时可能会遇到的情况： 1)主库意外关闭：在崩溃前没有将最后几个binlog事件刷新到磁盘中，备库IO线程因此一直处于读不到尚未写入磁盘的事件的状态中。当主库重新启动时，备库将重连到主库逼格再次尝试去读该事件，但主库会告诉备库没有这个binlog偏移量。 解决方法是指定备库从下一个binlog的开头读日志，但一些日志事件将永久地丢失。可以通过在主库开启sync_binlog来避免事件丢失。 2)备库意外关闭：如果使用的都是InnoDB表，可以在重启后观察MySQL错误日志。InnoDB在恢复过程中会打印出它的恢复点的binlog坐标。可以使用这个值来决定备库指向胡库的偏移量。使用非事务型表 基于语句的复制通过能够很好地处理非事务表。但是当对非事务型表的更新发生错误时，就可能导致主库和备库的数据不一致。如果使用的是MyIASM表，在关闭MySQL之前需要确保已经运行了STOP SLAVE，否则服务器在关闭时会kill所有正在运行的查询。事务型存储引擎则没有这个问题，如果使用的是事务型表，失败的更新会在主库上回滚并且不会记录到binlog中。混合事务型和非事务型表 如果使用的是事务型存储引擎，只有在事务提交时才会查询记录到binlog中。因此如果事务回滚，MySQL就不会记录这条查询，也就不会在备库重放。 但是如果混合使用事务型和非事务型表，并且发生了一次回滚，MySQL能够回滚事务型表的更新，但非事务型表就会被永久地更新了。 防止该问题的唯一办法是避免混合使用事务型和非事务型表。如果遇到这个问题，唯一的解决办法是忽略错误，并重新同步相关的表。 基于行的复制不会受这个问题影响。不确定语句 当使用基于语句的复制模式时，如果通过不确定的方式更改数据可能会导致主备不一致。 基于行的复制则没有上述限制。主库和备库使用不同的存储引擎 当使用基于语句的复制方式时，如果备库使用了不同的存储引擎，则可能造成一条查询在主库和备库上的执行结果不同。备库发生数据改变 基于语句的复制方式前提是确保备库上有和主库相同的数据，因此不应该允许对备库数据的任何更改。 唯一的解决办法是重新从主库同步数据。 不唯一的服务器ID 未定义的服务器ID 对未复制数据的依赖性 丢失的临时表 不复制所有的更新 InnoDB加锁读引起的锁争用 过大的复制延迟 来自主库过大的包 受限制的复制带宽 磁盘空间不足 复制的局限性 高可用解决方案 scale-up 向上扩展/垂直扩展：购买更多性能更强的硬件。容易达到性能瓶颈。 scale-out 向外扩展：复制、拆分、数据分片 比如按业务分库；单表分区8.85 脑裂问题 在心跳失效的时候，就发生了脑裂（split-brain)。 （ 一种常见的脑裂情况可以描述如下)比如正常情况下，（集群中的)NodeA 和 NodeB 会通过心跳检测以确认对方存在，在通过心跳检测确认不到对方存在时，就接管对应的（共享) resource 。如果突然间，NodeA 和 NodeB 之间的心跳不存在了（如网络断开)，而 NodeA 和 NodeB 事实上却都处于 Active 状态，此时 NodeA 要接管 NodeB 的 resource ，同时 NodeB 要接管 NodeA 的 resource ，这时就是脑裂（split-brain)。 脑裂（split-brain)会 引起数据的不完整性 ，并且可能会对服务造成严重影响 。 起数据的不完整性主要是指，集群中节点（在脑裂期间)同时访问同一共享资源，而此时并没有锁机制来控制针对该数据访问，那么就存在数据的不完整性的可能。 对付 HA 系统“裂脑”的对策 ，目前我所了解的大概有以下几条： 1)添加冗余的心跳线。例如双“心跳线”。尽量减少“脑裂”发生机会。 2)启用磁盘锁。正在服务一方锁住共享磁盘，“脑裂”发生时，让对方完全“抢不走”共享磁盘资源。但使用锁磁盘也会有一个不小的问题，如果占用共享盘的一方不主动解锁，另一方就永远得不到共享磁盘。现实中假如服务节点突然死机或崩溃，就不可能执行解锁命令。后备节点也就接管不了共享资源和应用服务。于是有人在 HA 系统中设计了“智能”锁。即正在服务的一方只在发现“心跳线”全部断开（察觉不到对端)时才启用磁盘锁。平时就不上锁了。 3)设置仲裁机制。例如设置参考 IP（如网关IP)Monitor，当心跳线完全断开时，2 个节点都各自 ping 一下 参考 IP ，不通则表明断点就出在本端，不仅“心跳线”断了、对外提供“服务”的本端网络链路也路断了，即使启动（或继续)应用服务也没有用了，那就主动放弃竞争，让能够 ping 通参考 IP 的一端去起服务。更保险一些，ping 不通参考 IP 的一方干脆就自我重启，以彻底释放有可能还占用着的那些共享资源。8.86 解决方案 LVS+Keepalived+MySQL（有脑裂问题？但似乎很多人推荐这个) DRBD+Heartbeat+MySQL（有一台机器空余？Heartbeat切换时间较长？有脑裂问题？)MySQL Proxy（不够成熟与稳定？使用了Lua？是不是用了他做分表则可以不用更改客户端逻辑？) MySQL Cluster （社区版不支持INNODB引擎？商用案例不足？) MySQL + MHA （如果配上异步复制，似乎是不错的选择，又和问题？) MySQL + MMM （似乎反映有很多问题，未实践过，谁能给个说法)8.87 MHA MHA（Master High Availability)目前在MySQL高可用方面是一个相对成熟的解决方案，它由日本DeNA公司youshimaton（现就职于Facebook公司)开发，是一套优秀的作为MySQL高可用性环境下故障切换和主从提升的高可用软件。在MySQL故障切换过程中，MHA能做到在0~30秒之内自动完成数据库的故障切换操作，并且在进行故障切换的过程中，MHA能在最大程度上保证数据的一致性，以达到真正意义上的高可用。 该软件由两部分组成： MHA Manager（管理节点)和MHA Node（数据节点)。MHA Manager可以单独部署在一台独立的机器上管理多个master-slave集群，也可以部署在一台slave节点上。MHA Node运行在每台MySQL服务器上，MHA Manager会定时探测集群中的master节点，当master出现故障时，它可以自动将最新数据的slave提升为新的master，然后将所有其他的slave重新指向新的master。整个故障转移过程对应用程序完全透明。 在MHA自动故障切换过程中，MHA试图从宕机的主服务器上保存二进制日志，最大程度的保证数据的不丢失，但这并不总是可行的。例如，如果主服务器硬件故障或无法通过ssh访问，MHA没法保存二进制日志，只进行故障转移而丢失了最新的数据。使用MySQL 5.5的半同步复制，可以大大降低数据丢失的风险。MHA可以与半同步复制结合起来。如果只有一个slave已经收到了最新的二进制日志，MHA可以将最新的二进制日志应用于其他所有的slave服务器上，因此可以保证所有节点的数据一致性。 目前MHA主要支持一主多从的架构，要搭建MHA,要求一个复制集群中必须最少有三台数据库服务器，一主二从，即一台充当master，一台充当备用master，另外一台充当从库，因为至少需要三台服务器， - （1)从宕机崩溃的master保存二进制日志事件（binlog events); - （2)识别含有最新更新的slave； - （3)应用差异的中继日志（relay log)到其他的slave； - （4)应用从master保存的二进制日志事件（binlog events)； - （5)提升一个slave为新的master； - （6)使其他的slave连接新的master进行复制； 8.88 MMM MMM(Master-Master replication managerfor Mysql，Mysql主主复制管理器)是一套灵活的脚本程序，基于perl实现，用来对mysql replication进行监控和故障迁移，并能管理mysql Master-Master复制的配置(同一时间只有一个节点是可写的)。 mmm_mond：监控进程，负责所有的监控工作，决定和处理所有节点角色活动。此脚本需要在监管机上运行。 mmm_agentd：运行在每个mysql服务器上的代理进程，完成监控的探针工作和执行简单的远端服务设置。此脚本需要在被监管机上运行。 mmm_control：一个简单的脚本，提供管理mmm_mond进程的命令。 mysql-mmm的监管端会提供多个虚拟IP（VIP)，包括一个可写VIP，多个可读VIP，通过监管的管理，这些IP会绑定在可用mysql之上，当某一台mysql宕机时，监管会将VIP迁移至其他mysql。 在整个监管过程中，需要在mysql中添加相关授权用户，以便让mysql可以支持监理机的维护。授权的用户包括一个mmm_monitor用户和一个mmm_agent用户，如果想使用mmm的备份工具则还要添加一个mmm_tools用户。 优点：高可用性，扩展性好，出现故障自动切换，对于主主同步，在同一时间只提供一台数据库写操作，保证的数据的一致性。 缺点：Monitor节点是单点，可以结合Keepalived实现高可用。 压力测试 sysbench是一个模块化的、跨平台、多线程基准测试工具，主要用于评估测试各种不同系统参数下的数据库负载情况。关于这个项目的详细介绍请看：http://sysbench.sourceforge.net。 它主要包括以下几种方式的测试： 1、cpu性能 2、磁盘io性能 3、调度程序性能 4、内存分配及传输速度 5、POSIX线程性能 6、数据库性能(OLTP基准测试) 目前sysbench主要支持 MySQL,pgsql,oracle 这3种数据库。 一、安装 首先，在 http://sourceforge.net/projects/sysbench 下载源码包。 接下来，按照以下步骤安装： tar zxf sysbench-0.4.8.tar.gz cd sysbench-0.4.8 ./configure && make && make install strip /usr/local/bin/sysbench 以上方法适用于 MySQL 安装在标准默认目录下的情况，如果 MySQL 并不是安装在标准目录下的话，那么就需要自己指定 MySQL 的路径了。比如我的 MySQL 喜欢自己安装在 /usr/local/mysql 下，则按照以下方法编译： ./configure --with-mysql-includes=/usr/local/mysql/include --with-mysql-libs=/usr/local/mysql/lib && make && make install 当然了，用上面的参数编译的话，就要确保你的 MySQL lib目录下有对应的 so 文件，如果没有，可以自己下载 devel 或者 share 包来安装。 另外，如果想要让 sysbench 支持 pgsql/oracle 的话，就需要在编译的时候加上参数 --with-pgsql 或者 --with-oracle 这2个参数默认是关闭的，只有 MySQL 是默认支持的。 二、开始测试 编译成功之后，就要开始测试各种性能了，测试的方法官网网站上也提到一些，但涉及到 OLTP 测试的部分却不够准确。在这里我大致提一下： 1、cpu性能测试 sysbench --test=cpu --cpu-max-prime=20000 run cpu测试主要是进行素数的加法运算，在上面的例子中，指定了最大的素数为 20000，自己可以根据机器cpu的性能来适当调整数值。 2、线程测试 sysbench --test=threads --num-threads=64 --thread-yields=100 --thread-locks=2 run 3、磁盘IO性能测试 sysbench --test=fileio --num-threads=16 --file-total-size=3G --file-test-mode=rndrw prepare sysbench --test=fileio --num-threads=16 --file-total-size=3G --file-test-mode=rndrw run sysbench --test=fileio --num-threads=16 --file-total-size=3G --file-test-mode=rndrw cleanup 上述参数指定了最大创建16个线程，创建的文件总大小为3G，文件读写模式为随机读。 4、内存测试 sysbench --test=memory --memory-block-size=8k --memory-total-size=4G run 上述参数指定了本次测试整个过程是在内存中传输 4G 的数据量，每个 block 大小为 8K。 5、OLTP测试 sysbench --test=oltp --mysql-table-engine=myisam --oltp-table-size=1000000 \\ --mysql-socket=/tmp/mysql.sock --mysql-user=test --mysql-host=localhost \\ --mysql-password=test prepare 上述参数指定了本次测试的表存储引擎类型为 myisam，这里需要注意的是，官方网站上的参数有一处有误，即 --mysql-table-engine，官方网站上写的是 --mysql-table-type，这个应该是没有及时更新导致的。另外，指定了表最大记录数为 1000000，其他参数就很好理解了，主要是指定登录方式。测试 OLTP 时，可以自己先创建数据库 sbtest，或者自己用参数 --mysql-db 来指定其他数据库。--mysql-table-engine 还可以指定为 innodb 等 MySQL 支持的表存储引擎类型。 容灾备份 8.89 为什么要备份 - 1)灾难恢复 - 2)人们改变刑法 - 3)审计 - 4)测试 8.90 设计备份方案 建议： 1)在生产实践中，对于大数据量来说，物理备份还必需的：逻辑备份太慢并受到资源限制，从逻辑备份中恢复需要很长时间。基于快照的备份是最好的选择。对于较小的数据库，逻辑备份可以很好地胜任。 2)保留多个备份集 3)定期从逻辑备份/物理备份中抽取数据进行恢复测试 4)保存binlog以用于基于故障时间点的回复 5)完全不借助备份工具本身来监控本分和备份的过程，需要另外验证备份是否正常 6)通过演练整个恢复过程来测试备份和恢复，测算恢复所需要的资源在线备份还是离线备份 如果可能，关闭MySQL做备份是最简单安全的，也是所有获取一致性副本中最好的，而且损坏或不一致的风险最小。 由于一致性的需要，对服务器进行在线备份仍然会有明显的服务中断。 最大的权衡是备份时间和备份负载，可以牺牲其一以增强另外一个。 逻辑备份还是物理备份 有两种主要的方法来备份MySQL数据：逻辑备份（导出)和直接复制原始文件的物理备份。 逻辑备份有如下优点： 1)是可以用编辑器或grep、sed查看和操作的普通文件，当需要回复数据或者只查看数据时都非常有帮助 2)恢复非常简单 3)可以通过网络来备份和恢复 4)可以在不能访问底层文件系统的系统中使用 5)非常灵活，因为mysqldump可以接受很多选项。 6)与存储引擎无关。 7)有助于避免数据损坏。 缺点： 1)必须由数据库服务器完成生成逻辑备份的工作，要是要更多的CPU周期 2)在某些场景下比数据库文件本身更大 3)无法保证导出后再还原出来的一定是同样的数据 4)效率较低 最大的缺点是从MySQL中导出数据和通过SQL语句将其加载回去的开销。如果使用逻辑备份，测试恢复需要的时间将非常重要。 物理备份有如下好处： 1)只需要将需要的文件复制到其他地方即可完成备份，不需要额外的工作 2)恢复更加简单，并且更快 缺点： 1)原始文件通常比相应的逻辑备份要大得多 2)物理备份不总是可以跨平台、操作系统和MySQL版本。 简单地说，逻辑备份不易出错，物理备份更加简单高效 备份什么 非显著数据，比如binlog和redo log 代码 复制配置 服务器配置 选定的操作系统文件存储引擎和一致性 数据一致性：InnoDB的MVCC可以帮到我们。开始一个事务，转储一组相关的表，然后提交事务。只要在服务器上使用RR事务隔离级别，并且没有任何DDL，就一定会有完美的一致性，以及基于时间点的数据快照，且在备份过程中不会阻塞任何后续的工作。也可以用mysqldump来获得InnoDB表的一致性逻辑备份。可能会导致一个非常长的事务，在某些负载下会导致开销大到不可接受。 文件一致性：InnoDB如果检测到数据不一致或损坏，会记录错误日志乃至让服务器崩溃。 8.91 管理和备份binlog 服务器的binlog是备份的最重要因素之一，它们对于基于时间点的恢复是必需的，并且通常比数据要小，所以更容易进行频繁的备份。如果有某个时间点的数据备份和所有从那时以后的binlog，就可以重放自从上次全备以来的binlog并前滚所有的变更。 8.92 备份数据 生成逻辑备份 SQL导出 SQL导出是很多人所熟悉的，因为它们是mysqldump默认的方式。 缺点： 1)Schema和数据存储在一起 2)巨大的SQL语句 3)单个巨大的文件符号分隔文件备份 可以使用SQL命令SELECT INTO OUTFILE以符号分隔文件格式创建数据的逻辑备份（mysqldumo的—tab选项)。符号分隔文件包含以ASCII展示的原始数据，没有SQL、注释和列名。 比起SQL导出文件，符号分隔文件要更紧凑且更易于用命令行工具操作，最大的优点是备份和还原速度更快。 LOAD DATA INFILE方法可以加载数据到表中。文件系统快照 文件系统快照是一种非常好的在线备份方法，支持快照的文件系统能够瞬间创建用来备份的内容一致的进行。支持快照的文件系统和设备包括FreeBSD的文件系统、ZFS文件系统、LVM、以及许多的SAN系统和文件存储解决方案。 LVM快照 8.93 从备份中恢复 SQL 分页 带日期 要加上nextkey锁，语句该怎么写 各种join like%..%为什么会扫描全表？遵循什么原则？ sql语句各种条件的执行顺序，如select， where， order by， group by8.94 执行顺序 for human： for machine：SQL解析器执行顺序 from on join where group by having select distinct order by limit 8.95 连接 7种： - 内连接 - 左外连接 - 右外连接 - 全外连接（内连接+左外连接+右外连接) - 左外连接 – 内连接 - 右外连接 – 内连接 - 全外连接 – 内连接 实际的SQL： emp:8 dept:5 笛卡尔积/交叉连接 40 内连接 7 左外连接 8 右外连接 8 全外连接 MySQL不支持，一种替代做法是： 左外连接 – 内连接 1 右外连接 – 内连接 1 全外连接 – 内连接 2 MySQL底层实现 查询处理与查询优化过程 MySQL的查询流程大致是： MySQL客户端通过协议与MySQL服务器建立连接，发送查询语句，先检查查询缓存，如果命中，直接返回结果，否则进行语句解析 有一系列预处理，比如检查语句是否写正确了，然后是查询优化（比如是否使用索引扫描，如果是一个不可能的条件，则提前终止)，生成查询计划，然后查询引擎启动，开始执行查询，从底层存储引擎调用API获取数据，最后返回给客户端。怎么存数据、怎么取数据，都与存储引擎有关。 MySQL客户端-->MySQL服务器-->缓存-->查询检查-->查询优化-->执行查询 8.96 查询执行的基础 1、客户端发送一条查询给服务器 2、服务器先检查查询缓存，如果命中了缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段 3、服务器进行SQL解析、预处理，再由优化器生成对应的执行计划 4、MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询 5、将结果返回给客户端 MySQL C/S通信协议 MySQL客户端和服务器之间的通信协议是半双工的，这意味着，在任何一个时刻，要么是由服务器向客户端发送数据，要么是由客户端向服务器发送数据，这两个动作不能同时发生。所以，我们无法也无需将一个消息切成小块独立来发送。 这种协议让MySQL通信简单快速，但是也从很多地方限制了MySQL，一个明显的限制是，这意味着没法进行流量控制。一旦一端开始发送消息，另一端要接收完整个消息才能响应它。 客户端用一个单独的数据包将查询传给服务器，这也是为什么当查询的语句很长的时候，参宿max_allowed_packet就特别重要的。一旦客户端发送了请求，它能做的事情就只是等待结果了。 相反的，一般服务器响应给用户的数据通常很多，由多个数据包组成。当服务器开始响应客户端请求时，客户端必须完整地接收整个返回结果，而不能简单地只取前面几条结果，然后让服务器停止发送数据。这种情况下，客户端若接受完整的记过，然后取前面几条需要的记过，或者接收完几条结果后就粗暴地断开连接，都不是好主意，这也是在必要的时候一定要在查询中加上LIMIT限制的原因。 多数连接MySQL的库函数都可以获得全部结果集并缓存到内存里，还可以逐行获取需要的数据。默认一般是获得全部结果集并缓存到内存中。MySQL通常需要等所有的数据都已经发送给客户端才能释放这条查询所占用的资源，所以接收全部结果并缓存通过可以减少服务器的压力，让查询能够早点结束，早点释放相应的资源。 查询缓存 在解析一个查询语句之前，如果查询缓存是打开的，那么MySQL会优先检查这个查询是否命中查询缓存中的数据。这个检查是通过一个对大小写敏感的哈希查找实现的。查询和缓存中的查询即使只有一个字节不同，那也不会匹配查询结果，这种情况下查询就会进入下一阶段的处理。 如果当前的查询恰好命中了查询缓存，那么在返回查询结果之前MySQL会检查一次用户权限。这仍然是无须解析查询SQL语句的，因为在查询缓存中已经存放了当前查询需要访问的表信息。如果权限没有问题，MySQL会跳过所有其他阶段，直接从缓存中拿到结果并返回给客户端。这种情况下，查询不会被解析，不用生成执行计划，不会被执行。查询优化处理 语法解析器和预处理 首先，MySQL通过对关键字将SQL语句进行解析，并生成一棵对应的解析树。MySQL解析器将使用MySQL语法规则验证和解析查询。 预处理器则根据一些MySQL规则进一步检查解析树是否合法，下一步预处理器会验证权限。查询优化器 现在语法数被认为是合法的了，将由优化器将其转化为执行计划。一条查询可以有很多种执行方式，最后都返回相同的结果。优化器的作用就是找到这其中最好的执行计划。 MySQL使用基于成本的优化器，它将尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。 成本的最小单位是随机读取一个4K数据页的成本。 可以通过查询当前会话的Last_query_cost的值来得知MySQL计算的当前查询的成本。 成本是根据一系列的统计信息计算得来的：每个表或者索引的页面个数、索引的基数（索引中不同值的数量)、索引和数据行的长度、索引分布情况。优化器在评估成本的时候并不考虑任何层面的缓存，它假设读取任何数据都需要一次磁盘IO。 与很多种原因会导致MySQL优化器选择错误的执行计划： 1)统计信息不准确 2)执行计划中的成本估算不等于实际执行的成本 3)MySQL并不按照执行时间最短来选择的，而是基于成本 4)不考虑其他并发执行的查询 5)并不是任何时候都是基于成本的优化，有时也会基于一些固定的规则 6)不会考虑不受其控制的操作的成本（存储过程、用户自定义函数的成本) MySQL的查询优化器是使用了很多优化策略来生成一个最优的执行计划。优化策略可以简单地分为两种，一种是静态优化，一种是动态优化。 静态优化可以直接对解析树进行分析，并完成优化。静态优化在第一次完成后就一直有效，即使使用不同的参数重复执行查询也不会发送变化，可以认为这是一种编译时优化。 相反，动态优化则和查询的上下文有关，需要在每此查询的时候都重新评估，可以认为这是运行时优化。 MySQL能够处理的优化类型： 1)重新定义关联表的顺序 2)将外连接转为内连接 3)使用等价变换规则 4)优化count、min、max 5)预估并转化为常数表达式 6)覆盖索引扫描 7)子查询优化 8)提前终止查询（比如limit) 9)等值传播 10)列表IN()的比较：MySQL会将IN()列表中的数据先进行排序，然后通过二分查找的方式来确定列表中的值是否满足条件，这是一个O(logn)复杂度的操作，等价地转为OR查询的复杂度为O(n)。 数据和索引的统计信息 在服务器层有查询优化器，却没有保存数据和索引的统计信息。统计信息由存储引擎实现，不同的存储引擎可能会存储不同的统计信息。 因为服务器层没有任何统计信息，所以MySQL查询优化器在生成查询的执行计划时，需要向存储引擎获取相应的统计信息。MySQL如何执行关联查询 MySQL认为任何一个查询都是一次关联，并不仅仅是一个查询需要用到两个表匹配才叫关联。 当前MySQL关联执行的策略很简单，MySQL对任何关联都执行嵌套循环关联操作。 不过不是所有的查询都可以转换成上面的形式，比如全外连接，这大概也是MySQL并不支持全外连接的原因。执行计划 和很多其他关系数据库不同，MySQL并不会生成查询字节码来执行查询。MySQL生成查询的一棵指令树，然后通过存储引擎执行完成这颗指令树并返回结果。 MySQL总是对一张表开始嵌套循环、回溯完成所有表关联。 比如四表关联： 关联查询优化器 MySQL优化器最重要的一部分就是关联查询优化，它决定了多个表关联时的顺序。通过多表关联的时候，可以有多种不同的关联顺序来获得相同的执行结果。关联查询优化器通过评估不同顺序时的成本来选择一个代价最小的关联顺序。 比如嵌套循环关联时将小表（或者说读取的数据页较小的表)放在最外层。排序优化 当不能通过索引生成排序结果的时候，MySQL需要自己进行排序，如果数据量小则在内存中进行，如果数据量大则需要使用磁盘，MySQL将这个过程统一称为filesort文件排序。 如果需要排序的数据量小于排序缓冲区，MySQL使用内存进行快速排序操作。如果内存不够排序，那么MySQL会先将数据分块，对每个独立的块使用快速排序，并将各个块的排序结果存放在磁盘上，然后将各个排好序的块进行合并，最后返回排序结果。 MySQL有两种排序算法： 1)两次传输排序：读取行指针和需要排序的字段，对其进行排序，然后再根据排序结果读取锁需要的数据行。缺点是会产生大量随机IO，数据传输成本高。 2)单次传输排序：先读取查询所需要的所有列，然后再根据给定列进行排序，最后直接返回排序结果。缺点是会占用大量的空间。 当查询需要所有列的总长度不超过max_length_for_sort_data时，MySQL使用单次传输排序。 在关联查询时如果需要排序，MySQL会分两种情况来处理这样的文件排序。如果ORDER BY子句中的所有列都来自关联的第一个表，那么MySQL在关联处理第一个表的时候就会进行文件排序。如果是这样，那么EXPLAIN时会显示Extra字段有“Using filesort”。除此之外的其他情况，MySQL都会先将关联的结果存放到一个临时表中，然后在所有的关联都结束后，再进行文件排序。此时EXPLAIN会显示Extra字段有“Using temporary；Using filesort”。如果有LIMIT的话，LIMIT也会在排序之后应用，所以即使需要返回较少的数据，临时表和需要排序的数据量仍然会非常大。 查询执行引擎 MySQL的查询执行引擎会根据执行计划来完成整个查询，执行计划是一个数据结构。 相对于查询优化阶段，查询执行阶段不是那么复杂：MySQL只是简单地根据执行计划给出的指令逐步执行。在根据执行计划逐步执行的过程中，有大量的操作需要通过调用存储引擎实现的接口来完成，这些接口就是我们成为“handler API”的皆苦，实际上，MySQL在优化阶段就为每个表都创建了一个handler实例，优化器根据这些实例的接口可以获取表的相关信息。 存储引擎接口有着非常丰富的功能，但底层接口却只有几十个，这些接口像搭积木一样能够完成查询的大部分操作。返回结果给客户端 查询执行的最后一个阶段是将结果返回给客户端，即使查询不需要返回结果集给客户端，MySQL仍然会返回这个查询的一些信息，如该查询影响到的行数， 如果查询可以被缓存，那么MySQL在这个阶段也会将结果存放在查询缓存中。 MySQL将结果集返回客户端是一个增量、逐步返回的过程。这样处理有两个好处：服务器无需存储太多的结果，也不会因为要返回太多结果而消耗太多内存。另外这样的处理也让MySQL客户端第一时间获得返回的结果。 结果集中的每一行都会以一个满足MySQL C/S通信协议的封包发送，再通过TCP协议进行传输，在传输过程中，可能对MySQL的封包进行缓存然后批量传输。 8.97 MySQL查询优化器的局限性 关联子查询 MySQL的子查询实现的非常糟糕，最糟糕的一类查询是WHERE中包含IN()的子查询。 并不是所有关联子查询性能都很差。建议通过一些测试来判断使用哪种写法速度会更快。 UNION的限制 有时候MySQL无法将限制条件从外层下推到内层，这使得原本能够限制部分返回结果的条件无法应用到内层查询的优化上。 索引合并优化 当WHERE子句中包含多个复杂条件的时候，MySQL能够访问单个表的多个索引以合并和交叉过滤的方式来定位需要查找的行。 并行执行 MySQL无法利用多核特性来并行执行查询。 松散索引扫描 MySQL不支持松散索引扫描，也就无法不连续的方式扫描一个索引。通常，MySQL的索引扫描需要先定义一个起点和终点，即使需要的数据只是这段索引中很少数的几个，MySQL仍需要扫描这段索引中的每一个条目。最大值和最小值优化 在同一个表上查询和更新 存储实现 每个数据库对应一个子目录，每张表对应子目录下的一个与表同名的.frm文件，它保存了表的定义。 如果是MyIASM引擎，那么表数据存放在.myd文件，表索引存放在.myi文件。 如果是InnoDB引擎，那么表数据和索引文件都放在.ibd文件。InnoDB 简介 从MySQL5.5.8开始，InnoDB存储引擎是默认的存储引擎。InnoDB存储引擎将数据放在一个逻辑的表空间中，这个表空间就像黑盒一样由InnoDB存储引擎自身进行管理。 InnoDB通过MVCC来获得高并发性，并且实现了SQL标准的四种隔离级别，默认为可重复读。同时，使用一种被称为next-key lock的策略来避免幻读。除此之外，InnoDB存储引擎还提供了插入缓冲、两次写、自适应哈希索引、预读等高性能和高可用的概念。 对于表中数据的存储，InnoDB存储引擎采用了聚集的方式，因此每张表的存储都是按主键的顺序进行存放。如果没有显式地在表定义时指定主键，InnoDB存储引擎会为每一行生成一个6字节的ROWID，并以此作为主键。 InnoDB 体系结构 innoDB存储引擎有多个内存块，可以认为这些内存块组成了一个大的内存池，负责： 1)维护所有线程需要访问的多个内部数据结构 2)缓存磁盘上的数据，方便快速读取，同时在对磁盘文件的数据修改之前在这里缓存 3)redo log缓冲。 后台线程的主要作用是负责刷新内存池中的数据，保证缓冲池中的内存缓存的是最近的数据。此外将已修改的数据文件刷新到磁盘文件，同时保证在数据库发生异常的情况下InnoDB能恢复到正常运行状态。 8.98 组件 后台线程 1、Master Thread：负责将缓冲池只能怪的数据异步刷新到磁盘，保证数据的一致性，包括脏页的刷新、合并插入缓冲、UNDO页的回收、 2、IO Thread InnoDB大量使用AIO来处理写IO请求，这样可以极大提高数据库的性能。而IO Thread的工作是负责这些IO 请求的回调处理。 3、Purge（清除) Thread 事务被提交后，其所使用的undo log可能不再需要，因此需要PurgeThread来回收已经使用并分配的undo页。内存 1、缓冲池：InnoDB存储引擎是基于磁盘存储的，并将其中的记录按照页的方式进行管理。缓冲池简单来说就是一块内存取与，通过内存的速度来弥补磁盘速度较慢对数据库性能的影响。 在数据库中进行读取页的操作，首先将从磁盘读到的页存放在缓存池中，这个过程称为将页“FIX”在缓冲池中，下一次再读相同的页时，首先判断该页是否在缓冲池中。若在缓冲池中，称该页在缓冲池中被命中，直接读取该页。 对于磁盘中页的修改操作，则首先修改在缓冲池中的页，然后再以一定的频率刷新到磁盘上。回写是通过一种称为Checkpoint的机制实现的。 在InnoDB存储引擎中，缓冲池中页的大小默认为16KB，使用LRU算法对缓冲池进行管理，并且有一定优化，新读取到的页，不是进入首部，而是放入到LRU列表的midpoint位置。 2、redo log缓冲 InnoDB存储引擎首先将redo log信息先放入到这个缓冲区，然后按一定频率将其刷新到redo log文件。默认为8MB大小。 在下列三种情况下会将redo log buffer中的内存刷新到redo log文件中： 1)Master Thread每一秒将刷新一次 2)每个事务提交时刷新 3)redo log buffer剩余空间小于一半时，刷新 Checkpoint 8.99 事务日志 redo log（保证事务持久性 物理日志) 事务日志即redo log。 InnoDB使用日志来减少提交事务时的开销。因为日志中已经记录了事务，就无须在每个事务提交时把缓冲区中的脏块刷新到磁盘中。事务修改的数据和索引通常会映射到表空间的随机位置，所以刷新这些变更到磁盘需要很多随机IO。 InnoDB用日志把随机IO变成顺序IO，一旦日志安全写到磁盘，事务就持久化了，即使变更还没写到数据文件。如果一些糟糕的事情发生，InnoDB可以重放日志并且恢复已经提交的事务。 当然，InnoDB最后还是要把变更写到数据文件，因为日志有固定大小。InnoDB的日志是环形方式写的，当写到此值的尾部，会重新跳转到开头继续写，但不会覆盖还没应用到数据文件的日志记录。 InnoDB使用一个后台线程智能地刷新这些变更到数据文件。这个线程可以批量组合写入，使得数据写入更顺序，以提高效率。实际上，事务日志把数据文件的随机IO转换成几乎顺序的日志文件和数据文件IO，把刷新操作转移到后台使查询可以更快完成，并且缓和查询高峰时IO系统的压力。 InnoDB变更任何数据，会写一条变更记录到内存日志缓冲区。在缓冲满的时候、事务提交的时候、或者每一秒钟，InnoDB都会刷新缓冲区的内容到磁盘日志文件——无论上述三个条件哪个先达到。 把日志缓冲写到日志文件和把日志刷新到持久化存储是有区别的，在大部分操作系统中，把缓冲写到日志只是简单地把数据从InnoDB的内存缓冲区转移到了操作系统的缓冲区，并没有真正的持久化。 当数据库对数据做修改的时候，需要把数据页从磁盘读到buffer pool中，然后在buffer pool中进行修改，那么这个时候buffer pool中的数据页就与磁盘上的数据页内容不一致，称buffer pool的数据页为dirty page 脏数据，如果这个时候发生非正常的DB服务重启，那么这些数据还在内存，并没有同步到磁盘文件中（注意，同步到磁盘文件是个随机IO，较慢)，也就是会发生数据丢失，如果这个时候，能够在有一个文件，当buffer pool 中的data page变更结束后，把相应修改记录记录到这个文件（注意，记录日志是顺序IO)，那么当DB服务发生crash的情况，恢复DB的时候，也可以根据这个文件的记录内容，重新应用到磁盘文件，数据保持一致。 Innodb将所有对页面的修改操作写入一个专门的文件（顺序IO，很快)。redo log在磁盘上作为一个独立的文件存在，即Innodb的log文件。 8.100 逻辑存储结构 InnoDB把数据保存在表空间内，本质上由一个或多个磁盘文件组成的虚拟文件系统。InnoDB的表空间不只是存储表和索引，它还保存了undo log、插入缓冲、双写缓冲，以及其他内部数据结构。 表空间又由段、区、页组成。页有时也称为块。表空间 表空间可以看做InnoDB存储引擎逻辑结构的最高层，所有的数据都存放在表空间中。在默认情况下InnoDB存储引擎中有一个共享表空间ibdata1，即所有数据都存放在这个表空间里。如果用户启用了参数innodb_file_per_table，则每张表内的数据可以单独放在一个表空间中，存放的只是数据、索引和插入缓冲bitmap页，其他数据，如回滚信息、插入缓冲索引页、系统事务信息、二次写缓冲等还是存放在原来的共享表空间中。 独立表空间 | 共享表空间 独立表空间：每个表都会生成以独立的文件方式来存储，每个表都一个.frm的描述文件，还有一个.ibd文件。其中这个文件包括了单独一个表的数据及索引内容，默认情况下它的存储在mysql指定的目录下。 独立表空间优缺点： 优点： 每个表都有自己独立的表空间；每个表的数据和索引都会存储在各个独立的表空间中；可以实现 单表 在不同的数据进行迁移；表空间可以回收（除了drop table操作，表空不能自己回收)；drop table 操作自动回收表空间，如果对统计分析或是日值表，删除大量数据后可以通过 ：alter table tablename engin=innodb进行回缩不用的空间；对于使用inodb-plugin的innodb使用truncate table会使用空间收缩；对于使用独立表空间，不管怎么删除 ，表空间的碎片都不会太严重。 缺点： 单表增加过大，如超过100G。对于单表增长过大的问题，如果使用共享表空间可以把文件分开，但有同样有一个问题，如果访问的范围过大同样会访问多个文件，一样会比较慢。对于独立表空间也有一个解决办法是：使用分区表，也可以把那个大的表空间移动到别的空间上然后做一个连接。其实从性能上出发，当一个表超过100个G有可能响应也是较慢了，对于独立表空间还容易发现问题早做处理。 共享表空间：某一个数据库所有的表数据、索引保存在一个单独的表空间中，而这个表空间可以由很多个文件组成，一张表可以跨多个文件存在。默认这个共享表空间的文件路径在data目录下，默认的文件名为 bata1,初始化为10M。 共享表空间优缺点 优点：可以将表空间分成多个文件存放在各个磁盘上（表空间文件大小不受表大小的限制，如一个表可以分布在不同的文件上)，数据和文件放在一起方便管理。 缺点：所有的数据和索引存放到一个文件中，将来会是一个很大的文件，虽然可以把一个大文件分成多个小文件，但是多个表及索引在表空间中混合存储，这样对一个表做了大量删除操作后表空间将有大量的空隙，特别是对统计分析、日志系统这类应用最不适合用共享表空间。 共享表空间分配后不能回缩：当出现临时建索引或是创建一个临时表的操作表空间扩大后，就是删除相关的表也没办法回缩那部分空间了 如何开启独立表空间？ 查看是否开启独立表空间： mysql> show variables like '%per_table'; +-----------------------+-------+ | Variable_name | Value | +-----------------------+-------+ | innodb_file_per_table | OFF | +-----------------------+-------+ 设置开启： 在my.cnf文件中[mysqld] 节点下添加innodb_file_per_table=1段 表空间是由各个段组成的，比如数据段、索引段、回滚段等。 数据段即为B+树的叶子节点，索引段即为B+树的非索引节点。区 区是由连续页组成的空间，每个区大小为1MB。默认情况下一页大小为16KB，一个区中有64个连续的页。 页 在InnoDB存储引擎中，默认每个页的大小为16KB，也可以进行重新设置。 在InnoDB存储引擎中，常见的页类型有： 1)数据页 2)undo页 3)系统页 4)事务数据页 5)插入缓冲bitmap页 6)插入缓冲空闲列表页 等等行 每个页允许存放16KB/2 – 200 行的记录，即7992行记录。8.101 InnoDB 特性 两次写 Double Write InnoDB使用双写缓冲来避免页没写完整所导致的数据损坏。当一个磁盘写操作不能完整地完成时，不完整的页写入就可能发生。 双写缓冲是表空间的一个特殊的保留区域，在一些连续的块中足够保存100个页。本质上是一个最近写回的页面的备份拷贝。当InnoDB从缓冲池刷新页面到磁盘时，首先把他们刷新到双写缓冲中，然后再把它们写到其所属的数据区域中，这样可以保证每个页面的写入都是原子并且持久的。 如果有一个不完整的页写到了双写缓冲，原始的页依然会在磁盘上它的真实位置。当InnoDB恢复时，它将用原始页面替换掉双写缓冲中的损坏页面。然而，如果双写缓冲成功写入，但写到页的真实位置失败了，InnoDB在恢复时将使用双写缓冲中的拷贝来替换。InnoDB知道什么时候页面损坏了，因为每个页面在尾部都有校验值。校验值是最后写到页面的冬休，所以如果页面的内容跟校验值不匹配，说明这个页面是损坏的。因此，在恢复的时候，InnoDB只需要读取双写缓冲中每个页面并且验证校验值，如果一个页面的校验值不对，就从它的原始位置读取这个页面。 插入缓冲 Insert Buffer 自适应哈希索引 异步IO 刷新邻接页 InnoDB 数据组织方式与索引分类 InnoDB存储引擎的数据组织方式，是聚簇索引表：完整的记录，存储在主键索引中，通过主键索引，就可以获取记录所有的列。 当在表上定义PRIMARY KEY时，InnoDB将它用作聚簇索引。尽量为每个表定义一个主键。如果没有逻辑的唯一且非空的列或一组列，则添加一个新的AUTO_INCREMENT列，其值将自动填入。 如果没有为表定义一个PRIMARY KEY，那么MySQL将定位第一个UNIQUE索引，其中所有的键列都是非NULL，InnoDB将它用作聚簇索引。 如果该表没有PRIMARY KEY或合适的UNIQUE索引，则InnoDB会在包含行ID值的合成列内部生成一个名为GEN_CLUST_INDEX的隐藏聚簇索引。这些行按照InnoDB分配给这个表中的行的ID进行排序。行ID是一个6字节的字段，随着新行的插入而单调递增。因此，由行ID排序的行在物理上处于插入顺序。 因此，每张表都会有一个聚簇索引。聚簇索引是一级索引。 聚簇索引以外的所有索引都称为二级索引。在InnoDB中，二级索引中的每条记录（叶子)都包含该行的主键列，以及为二级索引指定的列。 InnoDB使用这个主键值来搜索聚簇索引中的行。 聚簇索引一般是主键；没有主键，就是第一个唯一键；没有唯一键，就是隐藏ID。 锁与事务实现原理 概述 所有的存储引擎都以自己的方式实现了锁机制，服务器层完全不了解存储引擎中的锁实现。但服务器层也会使用各种有效的表锁来实现不同的目的。 对于MySQL而言，事务机制更多是靠底层的存储引擎实现的，在服务器层面只有表锁。支持事务的InnoDB存储引擎实现了行锁、gap锁、next-key锁。 分类 按操作类型分 读锁和写锁 按数据操作粒度分 表锁和行锁 MySQL对锁提供了多种选择。每种MySQL存储引擎都可以实现自己的锁策略和锁粒度。在存储引擎的设计中，锁管理是一个非常重要的决定。将锁粒度固定在某个级别，可以为某些特定的应用场景提供更好的性能，但同时也会失去对另外一些应用场景的良好支持。好在MySQL支持多个存储引擎的架构，所以不需要单一的通用解决方案。 MyISAM表锁 特点 偏向MyISAM存储引擎，开销小，加锁快，无死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低 在特定的场景中，表锁也可能有良好的性能。比如，READ LOCAL表锁支持某些类型的并发写操作；另外，写锁也比读锁有更高的优先级，因此一个写锁请求可以会被插入到读锁队列的前面。 尽管存储引擎可以管理自己的锁，服务器还是会使用各种有效的表锁来实现不同的目的。比如服务器在ALTER TABLE时使用表锁，而忽略存储引擎的锁机制。 MyISAM在读表前自动对表加读锁，在写表前自动对表加写锁。 案例 mylock： 手动增加表锁： lock table table1 read/write , table2 read/write ,... 显示加过锁的表： show open tables; 释放表锁： unlock tables; In_use为1表示已经被加锁。 加读锁 左边是用户1，先给mylock加了读锁；右边是用户2，尝试给mylock加写锁，无法获得，处于阻塞状态。 左边是用户1，给mylock加了读锁，由于读锁是共享锁，所以用户1和用户2都可以查询。 session_1 session_2 获得表mylock的READ锁定 连接终端 当前session可以查询该表记录 其他session也可以查询该表的记录 当前session不能查询其它没有锁定的表 其他session可以查询或者更新未锁定的表 当前session中插入或者更新读锁锁定的表都会提示错误： 其他session插入或者更新锁定表会一直等待获得锁： 释放锁 Session2获得锁，插入操作完成： 用户A给表A加了读锁之后，只能读表A，不能写表A（报错)，也不能读写其他表（报错)。 此时用户B可以读表A，可以读写其他表，但是写表A时会出现阻塞（未报错)，直至用户A释放表A的锁之后才解除阻塞，执行命令。 加写锁 session_1 session_2 获得表mylock的WRITE锁定 连接终端 当前session对锁定表的查询+更新+插入操作都可以执行： 其他session对锁定表的查询被阻塞，需要等待锁被释放： 释放锁 Session2获得锁，查询返回： 总结 MyISAM在执行查询语句（SELECT)前，会自动给涉及的所有表加读锁，在执行增删改操作前，会自动给涉及的表加写锁。 MySQL的表级锁有两种模式： 锁类型 可否兼容 读锁 写锁 读锁 是 是 否 写锁 是 否 否 结论： 结合上表，所以对MyISAM表进行操作，会有以下情况： 1、对MyISAM表的读操作（加读锁)，不会阻塞其他进程对同一表的读请求，但会阻塞对同一表的写请求。只有当读锁释放后，才会执行其它进程的写操作。 2、对MyISAM表的写操作（加写锁)，会阻塞其他进程对同一表的读和写操作，只有当写锁释放后，才会执行其它进程的读写操作。 简而言之，就是读锁会阻塞写，但是不会堵塞读。而写锁则会把读和写都堵塞。 分析 【看看哪些表被加锁了】 MySQL>show open tables; 【如何分析表锁定】 可以通过检查table_locks_waited和table_locks_immediate状态变量来分析系统上的表锁定： SQL：show status like 'table%'; 这里有两个状态变量记录MySQL内部表级锁定的情况，两个变量说明如下： Table_locks_immediate：产生表级锁的次数，表示可以立即获取锁的查询次数，每立即获取锁值加1 ； Table_locks_waited：出现表级锁定争用而发生等待的次数(不能立即获取锁的次数，每等待一次锁值加1)，此值高则说明存在着较严重的表级锁争用情况； 此外，Myisam的读写锁调度是写优先，这也是myisam不适合做写为主表的引擎。因为写锁后，其他线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永远阻塞。 InnoDB行锁 特点 - 锁粒度小，并发度高；开销大，加锁慢，会出现死锁 - 支持事务 分析 【如何分析行锁定】 通过检查InnoDB_row_lock状态变量来分析系统上的行锁的争夺情况 MySQL>show status like 'innodb_row_lock%'; 对各个状态量的说明如下： Innodb_row_lock_current_waits：当前正在等待锁定的数量； Innodb_row_lock_time：从系统启动到现在锁定总时间长度； Innodb_row_lock_time_avg：每次等待所花平均时间； Innodb_row_lock_time_max：从系统启动到现在等待最长的一次所花的时间； Innodb_row_lock_waits：系统启动后到现在总共等待的次数； 对于这5个状态变量，比较重要的主要是 Innodb_row_lock_time_avg（等待平均时长)， Innodb_row_lock_waits（等待总次数) Innodb_row_lock_time（等待总时长)这三项。 尤其是当等待次数很高，而且每次等待时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手指定优化计划。 优化建议 8.102 事务隔离级别 锁出现的问题： - 脏读（读-DB结果不一致)：在一个事务中，读取其他事务未提交的数据，其他事务回滚后，导致读到的数据与数据库中的数据不一致； - 不可重复读（读-读结果不一致)：一个事务中多次读取相同记录结果不一致（另一事务对该记录进行增改删)； - 幻读（读-写，用写来验证读，结果不一致)：一个事务中读取某个范围内的记录，另一个事务在该范围内插入新的记录，虽然直接查询读取不到，但在插入同PK（同另一个事务插入记录的PK)时会冲突，并且更新范围记录时会同时更新另一个事务新插入的记录。插入同PK和更新范围记录虽然是写，但是在写之前也是要读的，所以也算在读到不同的记录里面了。 事务隔离级别： - 读未提交（都不能避免) 事务中的数据即使没有提交，也会对其他事务可见； - 读已提交（可避免脏读，提交读，可以立即读到其他事务提交的数据)：一个事务从开始直接提交之前，所做的任何修改对其他事务都是不可见的； - 可重复读（可避免脏读、不可重复读，快照读，一致性读)：一个事务中多次读取相同的记录，结果是一致的； 如果使用select ... for update、lock in share mode才会避免幻读，在第二次读的时候便可读到其他事务更新的数据（相当于破坏了可重复读，但是不会出现幻影)。 InnoDB使用MVCC来实现可重复读（也可实现读已提交)，但没有解决幻读问题； 另外，InnoDB提供了这样的机制：在默认的可重复读的隔离级别里，可以使用加锁读去查询最新的数据。这个加锁读使用到的机制就是next-key locks。 - 串行化（都可避免)： 幻读示例1——插入同PK Session_1 Session_2 开启事务（隔离级别为可重复读) 开启事务（隔离级别为可重复读) 范围查询表 插入PK为n的记录，并提交 再次范围查询表，同上次查询结果一致，没有看到PK为n的记录（体现了可重复读)插入PK为n的记录，报错：Duplicate key for key ‘PRIMARY’（出现了幻读，因为根据上次查询的结果，本不应该存在PK为n的记录的)提交 幻读示例2——范围更新 Session_1 Session_2 开启事务（隔离级别为可重复读) 开启事务（隔离级别为可重复读) 范围查询表，共n条记录 在Session_1的范围内插入记录，并提交 再次范围查询表，同上次查询结果一致，没有看到Session_2插入的新的记录（体现了可重复读)该范围内记录全部更新，最终更新了n+1条记录，包括Session_2插入的新的记录（出现了幻读，因为根据之前查询的结果只有n条记录的)提交 解决幻读示例3——排他锁 Session_1 Session_2 开启事务（隔离级别为可重复读) 开启事务（隔离级别为可重复读) 范围查询表，共n条记录，并使用select...for update 在Session_1的范围内插入记录，阻塞。 再次范围查询表，同上次查询结果一致，没有看到Session_2插入的新的记录（体现了可重复读)整体更新该范围内的记录，最终更新了n条记录（没有幻读)提交 解除阻塞 8.103 事务隔离级别的实现 读未提交 无锁读已提交 MVCC可重复读 MVCC只工作在REPEATABLE READ和READ COMMITED隔离级别下。 MVCC最大的作用是: 实现了非阻塞的读操作,写操作也只锁定了必要的行.可序列化 读加共享锁，写加排他锁，读写互斥。使用的悲观锁的理论。 8.104 MVCC MVCC 在MySQL 中的实现依赖的是 undo log 与 read view。undo log（保证事务原子性->事务回滚) undo log是为回滚而用，具体内容就是copy事务前的数据库内容（行)到undo buffer，在适合的时间把undo buffer中的内容刷新到磁盘。undo buffer与redo buffer一样，也是环形缓冲，但当缓冲满的时候，undo buffer中的内容会也会被刷新到磁盘； 与redo log不同的是，磁盘上不存在单独的undo log文件。 Undo记录默认记录在系统表空间（ibdata)中，从MySQL 5.6开始，Undo使用的表空间可以分离为独立的Undo log文件。 在Innodb当中，INSERT操作在事务提交前只对当前事务可见，Undo log在事务提交后即会被删除，因为新插入的数据没有历史版本，所以无需维护Undo log。而对于UPDATE、DELETE，则需要维护多版本信息。 在InnoDB当中，UPDATE和DELETE操作产生的Undo log都属于同一类型：update_undo。（update可以视为insert新数据到原位置，delete旧数据，undo log暂时保留旧数据) UNDO内部由多个回滚段组成，即 Rollback segment，一共有128个，保存在ibdata系统表空间中，分别从resg slot0 - resg slot127，每一个resg slot，也就是每一个回滚段，内部由1024个undo segment 组成。 回滚段（rollback segment)分配如下： slot 0 ，预留给系统表空间； slot 1- 32，预留给临时表空间，每次数据库重启的时候，都会重建临时表空间； slot33-127，如果有独立表空间，则预留给UNDO独立表空间；如果没有，则预留给系统表空间； 回滚段中除去32个提供给临时表事务使用，剩下的 128-32=96个回滚段，可执行 96*1024 个并发事务操作，每个事务占用一个 undo segment slot，注意，如果事务中有临时表事务，还会在临时表空间中的 undo segment slot 再占用一个 undo segment slot，即占用2个undo segment slot。如果错误日志中有：Cannot find a free slot for an undo log。则说明并发的事务太多了，需要考虑下是否要分流业务。 回滚段（rollback segment )采用 轮询调度的方式来分配使用，如果设置了独立表空间，那么就不会使用系统表空间回滚段中undo segment，而是使用独立表空间的，同时，如果回顾段正在 Truncate操作，则不分配。rollback segment（为了提高并发度) 在Innodb中，undo log被划分为多个段，具体某行的undo log就保存在某个段中，称为回滚段。可以认为undo log和回滚段是同一意思。row 最基本row中包含一些额外的存储信息 DATA_TRX_ID，DATA_ROLL_PTR，DB_ROW_ID，DELETE BIT。 - 6字节的DATA_TRX_ID 标记了最新更新这行记录的transaction id，每处理一个事务，其值自动+1 - 7字节的DATA_ROLL_PTR 指向当前记录项的rollback segment的undo log记录，找之前版本的数据就是通过这个指针 - 6字节的DB_ROW_ID，当由innodb自动产生聚簇索引时，聚簇索引包括这个DB_ROW_ID的值，否则聚簇索引中不包括这个值.，这个用于索引当中 - DELETE BIT位用于标识该记录是否被删除，这里的不是真正的删除数据，而是标志出来的删除。真正意义的删除是在commit的时候。 更新一行的过程： begin->用排他锁锁定该行->记录redo log->记录undo log->修改当前行的值，写事务编号，回滚指针指向undo log中的修改前的行read view 在innodb中，创建一个新事务的时候，innodb会将当前系统中的活跃事务列表（trx_sys->trx_list)创建一个副本（read view)，副本中保存的是系统当前不应该被本事务看到的其他事务id列表。当用户在这个事务中要读取该行记录的时候，innodb会将该行当前的版本号（trx_id)与该read view进行比较。 简单来说，Read View记录读开始时，所有的活动事务，这些事务所做的修改对于Read View是不可见的。除此之外，所有其他的小于创建Read View的事务号的所有记录均可见 新建事务(当前事务)与正在commit 的事务不在活跃事务列表中。 函数：read_view_sees_trx_id。 read_view中保存了当前全局的事务的范围： 【low_limit_id 最迟的事务id， up_limit_id 最早的事务id】 当行记录的事务ID小于当前系统的最早活动id，就是可见的。 　　if (trx_id up_limit_id) { 　　　　return(TRUE); 　　} 当行记录的事务ID大于等于当前系统的最迟活动id，就是不可见的。 　　if (trx_id >= view->low_limit_id) { 　　　　return(FALSE); 　　} 当行记录的事务ID在活动范围之中时，判断是否在活动列表中，如果在就不可见，如果不在就是可见的。 　　for (i = 0; i 　　　　trx_id_t view_trx_id 　　　　　　= read_view_get_nth_trx_id(view, n_ids - i - 1); 　　　　if (trx_id 　　　　return(trx_id != view_trx_id); 　　　　} 　　} 不同隔离级别下read_view的生成规则 读已提交 在每次语句执行的过程中，都关闭read_view, 重新在row_search_for_MySQL函数中创建当前的一份read_view。 这样就可以根据当前的全局事务链表创建read_view的事务区间，实现read committed隔离级别。可重复读 在repeatable read的隔离级别下，创建事务trx结构的时候，就生成了当前的global read view。 使用trx_assign_read_view函数创建，一直维持到事务结束，这样就实现了repeatable read隔离级别。 update - 1)事务1更改该行的各字段的值 当事务1更改该行的值时，会进行如下操作： - 用排他锁锁定该行 - 记录redo log - 把该行修改前的值Copy到undo log，即上图中下面的行 - 修改当前行的值，填写事务编号，使回滚指针指向undo log中的修改前的行 2)事务2修改该行的值 与事务1相同，此时undo log，中有有两行记录，并且通过回滚指针连在一起。 因此，如果undo log一直不删除，则会通过当前记录的回滚指针回溯到该行创建时的初始内容，所幸的时在Innodb中存在purge线程，它会查询那些比现在最老的活动事务还早的undo log，并删除它们，从而保证undo log文件不至于无限增长。 当事务正常提交时InnoDB只需要更改事务状态为COMMIT即可，不需做其他额外的工作，而Rollback则稍微复杂点，需要根据当前回滚指针从undo log中找出事务修改前的版本，并恢复。如果事务影响的行非常多，回滚则可能会变的效率不高，根据经验每事务行数在1000～10000之间，Innodb效率还是非常高的。很显然，Innodb是一个COMMIT效率比Rollback高的存储引擎。 select 查询时会将行的事务id与read_view中的活动事务列表进行匹配。 记录可见，且Deleted bit = 0；当前记录是可见的有效记录。 记录可见，且Deleted bit = 1；当前记录是可见的删除记录。此记录在本事务开始之前，已经删除。InnoDB MVCC 与 理想 MVCC的区别 一般我们认为MVCC有下面几个特点： 每行数据都存在一个版本，每次数据更新时都更新该版本 修改时Copy出当前版本随意修改，各个事务之间无干扰 保存时比较版本号，如果成功（commit)，则覆盖原记录；失败则放弃copy（rollback) 就是每行都有版本号，保存时根据版本号决定是否成功，有乐观锁的味道 而Innodb的实现方式是： 事务以排他锁的形式修改原始数据 把修改前的数据存放于undo log，通过回滚指针与主数据关联 修改成功（commit)啥都不做，失败则恢复undo log中的数据（rollback) 二者最本质的区别是，当修改数据时是否要排他锁定，如果锁定了还算不算是MVCC？ Innodb的实现真算不上MVCC，因为并没有实现核心的多版本共存，undo log中的内容只是串行化的结果，记录了多个事务的过程，不属于多版本共存。但理想的MVCC是难以实现的，当事务仅修改一行记录使用理想的MVCC模式是没有问题的，可以通过比较版本号进行回滚；但当事务影响到多行数据时，理想的MVCC就无能为力了。 比如，如果Transaciton1执行理想的MVCC，修改Row1成功，而修改Row2失败，此时需要回滚Row1，但因为Row1没有被锁定，其数据可能又被Transaction2所修改，如果此时回滚Row1的内容，则会破坏Transaction2的修改结果，导致Transaction2违反ACID。 理想MVCC难以实现的根本原因在于企图通过乐观锁代替两阶段提交。修改两行数据，但为了保证其一致性，与修改两个分布式系统中的数据并无区别，而两阶段提交是目前这种场景保证一致性的唯一手段。二段提交的本质是锁定，乐观锁的本质是消除锁定，二者矛盾，故理想的MVCC难以真正在实际中被应用，Innodb只是借了MVCC这个名字，提供了读的非阻塞而已。 8.105 InnoDB锁分类 record lock - 1)InnoDB里的行锁（record lock)是索引记录的锁 - 2)record lock锁住的永远是索引，而非记录本身，即使该表上没有任何索引，那么innodb会在后台创建一个隐藏的聚集主键索引，那么锁住的就是这个隐藏的聚集主键索引。所以说当一条sql没有走任何索引时，那么将会在每一条聚簇索引后面加X锁，这个类似于表锁，但原理上和表锁应该是完全不同的。 - 3)若多个物理记录对应同一个索引，若同时访问，也会出现锁冲突 - 4)当表有多个索引时，不同事务可以使用不同的索引锁住不同的行。 如果走的是聚簇索引，那么会锁住聚簇索引； 如果走的是二级索引，那么会同时锁住二级索引和聚簇索引 5)即便在条件中使用了索引字段，但是否使用索引来检索数据是由MySQL通过判断不同执行计划的代价来决定的，如果MySQL认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下InnoDB将使用表锁，而不是行锁。因此，在分析锁冲突时，别忘了检查SQL的执行计划，以确认是否真正使用了索引。gap lock 锁定一个范围的记录,但不包括记录本身。锁加在未使用的空闲空间上,可能是两个索引记录之间，也可能是第一个索引记录之前或最后一个索引之后的空间. 示例： create table test(id int,v1 int,v2 int,primary key(id),key idx_v1(v1))Engine=InnoDB DEFAULT CHARSET=UTF8; 该表的记录如下： +----+------+------+ | id | v1 | v2 | +----+------+------+ | 1 | 1 | 0 | | 2 | 3 | 1 | | 3 | 4 | 2 | | 5 | 5 | 3 | | 7 | 7 | 4 | | 10 | 9 | 5 | 间隙锁（Gap Lock)一般是针对非唯一索引而言的，test表中的v1（普通索引，非唯一索引)字段值可以划分的区间为： （-∞,1) （1,3) （3,4) （4,5) （5,7) （7,9) （9, +∞) 假如要更新v1=7的数据行，那么此时会在索引idx_v1对应的值，也就是v1的值上加间隙锁，锁定的区间是（5,7)和（7,9)。同时找到v1=7的数据行的主键索引和非唯一索引，对key加上锁。next-key lock 行锁与间隙锁组合起来用就叫做Next-Key Lock。锁定一个范围，并且锁定记录本身。对于行的查询，都是采用该方法，主要目的是解决幻读的问题。 InnoDB工作在可重复读隔离级别下，并且会以Next-Key Lock的方式对数据行进行加锁，这样可以有效防止幻读的发生。Next-Key Lock是行锁和间隙锁的组合，当InnoDB扫描索引记录的时候，会首先对索引记录加上行锁（Record Lock)，再对索引记录两边的间隙加上间隙锁（Gap Lock)。加上间隙锁之后，其他事务就不能在这个间隙修改或者插入记录。 例如一个索引有10,11,13,和20这四个值，那么该索引表可能被next-key locking的区间： （负无穷，10) 【10,11) 【11,13) 【13,20) 【20,正无穷) 如果包含唯一索引，那么会对其进行优化，降级为Record Lock。 但是如果唯一索引是复合索引，而查询仅是最左前缀，则仍会使用next-key lock。 示例： 意向锁 innodb的意向锁主要用户多粒度的锁并存的情况。比如事务A要在一个表上加S锁，如果表中的一行已被事务B加了X锁，那么该锁的申请也应被阻塞。如果表中的数据很多，逐行检查锁标志的开销将很大，系统的性能将会受到影响。为了解决这个问题，可以在表级上引入新的锁类型来表示其所属行的加锁情况，这就引出了“意向锁”的概念。举个例子，如果表中记录1亿，事务A把其中有几条记录上了行锁了，这时事务B需要给这个表加表级锁，如果没有意向锁的话，那就要去表中查找这一亿条记录是否上锁了。如果存在意向锁，那么假如事务Ａ在更新一条记录之前，先加意向锁，再加Ｘ锁，事务B先检查该表上是否存在意向锁，存在的意向锁是否与自己准备加的锁冲突，如果有冲突，则等待直到事务Ａ释放，而无须逐条记录去检测。事务Ｂ更新表时，其实无须知道到底哪一行被锁了，它只要知道反正有一行被锁了就行了。 说白了意向锁的主要作用是处理行锁和表锁之间的矛盾，能够显示“某个事务正在某一行上持有了锁，或者准备去持有锁” gap lock的危害 【什么是间隙锁】 当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”， 【危害】 因为Query执行过程中通过过范围查找的话，他会锁定整个范围内所有的索引键值，即使这个键值并不存在。 间隙锁有一个比较致命的弱点，就是当锁定一个范围键值之后，即使某些不存在的键值也会被无辜的锁定，而造成在锁定的时候无法插入锁定键值范围内的任何数据。在某些场景下这可能会对性能造成很大的危害 Session_1 Session_2 阻塞产生，暂时不能插入 commit; 阻塞解除，完成插入 8.106 InnoDB加锁分析 一致性非锁定读（快照读，无锁，读不会阻塞，也不会阻塞其他事务读写) 一致性非锁定读基于MVCC，实现了非阻塞读，读不加锁。 1)在read committed隔离级别下： 一致性非锁定读总是读取被锁定行的最新一份快照数据. 产生了不可重复读的问题. 2)在repeatable read 事务隔离级别下： 一致性非锁定读总是读取事务开始时的行数据版本. 解决不可重复读的问题 一致性锁定读（有锁，读可能阻塞，会阻塞其他事务写) 一致性锁定读就是select ... for update 和 select ... in shard mode，读可能会被阻塞，因为是加了锁的。 SELECT ... LOCK IN SHARE MODE（其他事务可读，可加共享锁，不可加排他锁，不可写) 在扫描到的任何索引记录上加共享的next-key lock，还有聚簇索引加排它锁 保证读到的是最新的数据（参见幻读)，并且保证其他事务无法修改正在读的数据，事务完毕后解锁。但是自己不一定能够修改数据，因为有可能其他的事务也对这些数据加了共享锁。 SELECT ... FOR UPDATE（其他事务可读，不可加共享锁&排他锁，不可写) 在扫描到的任何索引记录上加排它的next-key lock，还有聚簇索引加排它锁 保证读到的是最新的数据（参见幻读)，并且保证同一个事务的读-改-写是一个原子性的操作，事务完毕后解锁。当前读 select * from table where ? lock in share mode; select * from table where ? for update; insert into table values (…); update table set ? where ?; delete from table where ?; 所有以上的语句，都属于当前读，读取记录的最新版本。并且读取之后，还需要保证其他并发事务不能修改当前记录，对读取记录加锁。其中，除了第一条语句，对读取记录加S锁 (共享锁)外，其他的操作，都加的是X锁 (排它锁)。 InnoDB 中的加锁，不仅要对where中走的索引加锁，还会对主键聚簇索引加锁。 具体加锁情况要根据事务隔离级别和where中走的索引情况具体分析。 为什么将 插入/更新/删除 操作，都归为当前读？ 更新步骤： 一个Update操作的具体流程。当Update SQL被发给MySQL后，MySQL Server会根据where条件，读取第一条满足条件的记录，然后InnoDB引擎会将第一条记录返回，并加锁 (current read)。待MySQL Server收到这条加锁的记录之后，会再发起一个Update请求，更新这条记录。一条记录操作完成，再读取下一条记录，直至没有满足条件的记录为止。因此，Update操作内部，就包含了一个当前读。同理，Delete操作也一样。Insert操作会稍微有些不同，简单来说，就是Insert操作可能会触发Unique Key的冲突检查，也会进行一个当前读。 注：根据上图的交互，针对一条当前读的SQL语句，InnoDB与MySQL Server的交互，是一条一条进行的，因此，加锁也是一条一条进行的。先对一条满足条件的记录加锁，返回给MySQL Server，做一些DML操作；然后在读取下一条加锁，直至读取完毕。 两段锁协议 在事务执行过程中，随时都可以锁定，锁只有在执行commit或rollback时才会释放，并且所有的锁都是在同一时刻被释放。 除了DML时隐式加排他锁外，读的时候也可以显式加锁，比如select ... in shard mode 和 select ... for update。 MySQL也支持lock tables和unlock tables语句，这是在服务器层实现的，和存储引擎无关。它们有自己的用途，但并不能代替事务处理，如果应用需要用到事务，还是应该选择事务型存储引擎。 并且显式使用lock tables会与事务中使用的锁产生冲突，因此建议不要显式地使用lock tables。 锁与事务隔离级别中的当前读 前面讲事务隔离级别的实现注重于快照读，这里主要讲的是当前读，也就是一致性非锁定读+DML的锁实现。 读已提交：当前读时，对读到的记录加行锁 可重复读：当前读时，对读到的记录加行锁，同时对读取的范围加间隙锁。 可序列化：不区分当前读与快照读，所有的读都是当前读，读加读锁，写加写锁。 案例 DML+select...for update RC+where走聚簇索引 聚簇索引加行级排他锁RC+where走二级索引（包括唯一索引和非唯一索引) 满足条件的记录的二级索引加排他锁，聚簇索引加排他锁RC+where无索引 在聚簇索引上全表加排他锁。为了效率考量，MySQL做了优化，对于不满足条件的记录，会在判断后放锁，最终持有的，是满足条件的记录上的锁，但是不满足条件的记录上的加锁/放锁动作不会省略。同时，优化也违背了2PL的约束。RR+where走聚簇索引 聚簇索引加排他锁RR+where走唯一索引 满足条件的记录的唯一索引加排他锁，聚簇索引加排他锁 RR+where走非唯一索引 GAP锁锁住的位置，也不是记录本身，而是两条记录之间的GAP。 三个GAP锁：[6,c]与[10,b]间，[10,b]与[10,d]间，[10,d]与[11,f] Insert操作，如insert [10,aa]，首先会定位到[6,c]与[10,b]间，然后在插入前，会检查这个GAP是否已经被锁上，如果被锁上，则Insert不能插入记录。因此，通过第一遍的当前读，不仅将满足条件的记录锁上 (X锁)，同时还是增加3把GAP锁，将可能插入满足条件记录的3个GAP给锁上，保证后续的Insert不能插入新的id=10的记录，也就杜绝了同一事务的第二次当前读，出现幻读的情况。 对于每条满足条件的记录，会先加非唯一索引上的排他锁，加GAP上的GAP锁，然后加聚簇索引上的排他锁。 RR+where无索引 在聚簇索引上全表加排他锁，同时会锁上聚簇索引上的所有GAP。 MySQL也做了一些优化，就是所谓的semi-consistent read。semi-consistent read开启的情况下，对于不满足查询条件的记录，MySQL会提前放锁。针对上面的这个用例，就是除了记录[d,10]，[g,10]之外，所有的记录锁都会被释放，同时不加GAP锁。semi-consistent read如何触发：要么是read committed隔离级别；要么是Repeatable Read隔离级别，同时设置了 innodb_locks_unsafe_for_binlog 参数。Serializable MVCC并发控制降级为Lock-Based CC，读也加读锁。 8.107 死锁 死锁的发生与否，并不在于事务中有多少条SQL语句，死锁的关键在于：两个(或以上)的Session加锁的顺序不一致。而使用本文上面提到的，分析MySQL每条SQL语句的加锁规则，分析出每条语句的加锁顺序，然后检查多个并发SQL间是否存在以相反的顺序加锁的情况，就可以分析出各种潜在的死锁情况，也可以分析出线上死锁发生的原因。 8.108 只读事务 Innodb将所有的事务对象维护在链表上，通过trx_sys来管理，在5.6中，最明显的变化就是事务链表被拆分成了两个链表： 一个是只读事务链表：ro_trx_list，其他非标记为只读的事务对象放在链表rw_trx_list上； 这种分离，使得读写事务链表足够小，创建readview 的MVCC快照的速度更快； binlog binlog是MySQL Server层记录的日志， redo log是InnoDB存储引擎层的日志。 两者都是记录了某些操作的日志(不是所有)自然有些重复（但两者记录的格式不同)。 选择binlog日志作为replication主要原因是MySQL的特点就是支持多存储引擎，为了兼容绝大部分引擎来支持复制这个特性。8.109 格式 binlog有三种格式：Statement、Row以及Mixed。从安全性来看，ROW（最安全)、MIXED（不推荐)、STATEMENT（不推荐)。 –基于SQL语句的复制(statement-based replication,SBR)， –基于行的复制(row-based replication,RBR)， –混合模式复制(mixed-based replication,MBR)。Statement 每一条会修改数据的sql都会记录在binlog中。在5.6.24中默认格式。 优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。 缺点：由于记录的只是执行语句，为了这些语句能在slave上正确运行，因此还必须记录每条语句在执行的时候的一些相关信息，以保证所有语句能在slave得到和在master端执行时候相同 的结果。另外MySQL 的复制,像一些特定函数功能，slave可与master上要保持一致会有很多相关问题。 ps：相比row能节约多少性能与日志量，这个取决于应用的SQL情况，正常同一条记录修改或者插入row格式所产生的日志量还小于Statement产生的日志量，但是考虑到如果带条件的update操作，以及整表删除，alter表等操作，ROW格式会产生大量日志，因此在考虑是否使用ROW格式日志时应该跟据应用的实际情况，其所产生的日志量会增加多少，以及带来的IO性能问题。 Row 5.1.5版本的MySQL才开始支持row level的复制,它不记录sql语句上下文相关信息，仅保存哪条记录被修改。 优点： binlog中可以不记录执行的sql语句的上下文相关的信息，仅需要记录那一条记录被修改成什么了。所以rowlevel的日志内容会非常清楚的记录下每一行数据修改的细节。而且不会出现某些特定情况下的存储过程，或function，以及trigger的调用和触发无法被正确复制的问题。 缺点:所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容。 ps:新版本的MySQL中对row level模式也被做了优化，并不是所有的修改都会以row level来记录，像遇到表结构变更的时候就会以statement模式来记录，如果sql语句确实就是update或者delete等修改数据的语句，那么还是会记录所有行的变更。 Mixed 从5.1.8版本开始，MySQL提供了Mixed格式，实际上就是Statement与Row的结合。 在Mixed模式下，一般的语句修改使用statment格式保存binlog，如一些函数，statement无法完成主从复制的操作，则采用row格式保存binlog，MySQL会根据执行的每一条具体的sql语句来区分对待记录的日志形式，也就是在Statement和Row之间选择一种。 8.110 binlog与redo log的区别 1)首先，binlog会记录所有与MySQL数据库有关的日志记录，包括InnoDB、MyISAM、Heap等其他存储引擎的日志。而InnoDB存储引擎的redo log日志只记录有关该引擎本身的事务日志。 2)其次，记录的内容不同。无论用户将二进制日志文件记录的格式设为STATEMENT还是ROW，又或是MIXED，其记录的都是关于一个事务的具体操作内容，即该日志是逻辑日志。而InnoDB存储引擎的重做日志是关于每个页（Page)的更改的物理情况。 3)此外，写入的时间也不同。二进制日志文件仅在事务提交后进行写入，即只写磁盘一次，不论这时该事务多大。而在事务进行的过程中，却不断有redo 条目（redo entry)被写入到重做日志文件中。 "},"zother7-JavaInterview/dataStructures-algorithms/算法面试真题汇总.html":{"url":"zother7-JavaInterview/dataStructures-algorithms/算法面试真题汇总.html","title":"算法面试真题汇总","keywords":"","body":"算法题 作者：office多多 链接：https://www.nowcoder.com/discuss/374134?type=0&order=0&pos=67&page=1 写代码，类似高考成绩，一个表中有很多数据（无序的），给你一个成绩，查出在表中的排名 找出这两个链表是否有相交的点 判断链表有没有环，环起点在哪儿。 手撕topk，时间复杂度是多少。 写个算法，实现抢红包随机获取金额的过程参考 链表反转 两数之和（leetcode第一题～、～） 判断一个字符串是否为另一个字符串子串（暴力写的） 股票最大利润 实现单链表前后交叉排序：1,2,3,4,5,6 变成 1,4,2,5,3,6 因式分解 有序二叉树，一种遍历方法使之有序，中序遍历。 非递归实现先序遍历 找无序数组中第k个数（一开始说用堆实现、后来我又想着用快排的partation实现） 算法题：从字符串S变到T，插入消耗2、删除消耗2、替换消耗3、求最小消耗 算法题：两个栈实现一个队列（实现push、pop、count三个函数）（简单） strcpy的实现 给出两个链表，找出相同的链接。a->b->c->d->f、b1->a1->c1->d->f 二叉树的遍历方式，手写先序遍历(参考代码：https://www.cnblogs.com/anzhengyu/p/11083568.html) 两个字符串的最长公共子串(参考代码：https://www.cnblogs.com/anzhengyu/p/11166708.html) 查找二叉树最大深度 二叉树遍历 写代码判断IP地址(https://blog.csdn.net/u014259820/article/details/78833196?utm_source=distribute.pc_relevant.none-task) 在字符串中找出不重复字符的个数 找出两个只出现一次的数字，其余的数字都出现了两次 给n元钱，m个人，写个随机分钱的函数 两个栈实现一个队列 给个数组求连续子序列最大和 写一个程序；给一个数组，a【2 -2 3 3 6 -9 7】输出a【2 -2 3 -9 3 6 7】输入正负数都有数组，输出数组正负交替出现，多的那一类都放在后面； 给定一个数组 输出和为k的两个数的位置 a【2 7 3 5 11】k=9 输出 0 1 https://leetcode-cn.com/problems/two-sum/solution/liang-shu-zhi-he-by-leetcode-2/ 算法题：实现两个String字符串寻找最大公共子字符串 让写一个洗牌的函数，写完问我为啥那样写、再写一个打印牌的函数，问我洗完牌之后345不连在一起的概率 如何模拟一副扑克牌的洗牌过程 查找字符串中重复的子串，并输出重复的次数 https://blog.csdn.net/zouheliang/article/details/80649584 判断是否为平衡二叉树 找出一个字符串的最长不重复子串(https://www.cnblogs.com/linghu-java/p/9037262.html) 数组 数组求并集（利用set集合的元素唯一性） 一个数组中有正数和负数，找出来和最大的子数组 16瓶水中有1瓶水有毒，小白鼠喝了有毒的水1个小时后会死，一个小白鼠可以喝多瓶水，一瓶水也可以被多个小白鼠喝，现在给1个小时时间，最少需要几只小白鼠能够判断出来14瓶水是无毒的？ 小白鼠问题。16瓶正常水，1瓶毒水，小白鼠喝下毒水后一小时死亡，只给一小时时间，最少用多少只小白鼠可以检测出14瓶正常水？ 链表 二叉树 动态规划 最长公共子序列 dp[i][j]：表示子串A[0...i]（数组长度为n）和子串B[0...j]（数组长度为m）的最长公共子序列 当A[i] == B[j]时，dp[i][j] = dp[i-1][j-1] + 1; 否则，dp[i][j] = max(dp[i-1][j], dp[i][j-1]); 最优解为dp[n-1][m-1]; JAVA代码实现： import java.util.Scanner; public class Main { public static void main(String[] args) { Scanner scanner = new Scanner(System.in); while (scanner.hasNextLine()) { String str1 = scanner.nextLine().toLowerCase(); String str2 = scanner.nextLine().toLowerCase(); System.out.println(findLCS(str1, str1.length(), str2, str2.length())); } } public static int findLCS(String A, int n, String B, int m) { int[][] dp = new int[n + 1][m + 1]; for (int i = 0; i dp[i][j - 1] ? dp[i - 1][j] : dp[i][j - 1]; } } } return dp[n][m]; } } 最长回文子串 https://blog.csdn.net/wolfGuiDao/article/details/104590791 最长上升子序列 https://blog.csdn.net/zangdaiyang1991/article/details/94555112 最长上升子串 https://blog.csdn.net/qq_41706331/article/details/90271543 递归 不用循环找出最大值 数组 字符串 Tip：本来有很多我准备的资料的，但是都是外链，或者不合适的分享方式，所以大家去公众号回复【资料】好了。 现在免费送给大家，在我的公众号 好好学java 回复 资料 即可获取。 有收获？希望老铁们来个三连击，给更多的人看到这篇文章 1、老铁们，关注我的原创微信公众号「好好学java」，专注于Java、数据结构和算法、微服务、中间件等技术分享，保证你看完有所收获。 2、给俺一个 star 呗，可以让更多的人看到这篇文章，顺便激励下我继续写作，嘻嘻。 "},"zother7-JavaInterview/dataStructures-algorithms/算法题目难点题目总结.html":{"url":"zother7-JavaInterview/dataStructures-algorithms/算法题目难点题目总结.html","title":"算法题目难点题目总结","keywords":"","body":"智力题 作者：office多多 链接：https://www.nowcoder.com/discuss/374134?type=0&order=0&pos=67&page=1 10个堆，每堆10个苹果，其中9个堆里苹果是50g/个，一个堆里苹果是40g/个，有一杆秤只能称一次，所称重量为x,求40g苹果所在堆。 将堆编码为1-10；然后每堆拿出k个，最后少了k*10克，则知道是第几堆的苹果。 5L和6L水桶，得到三升水。 1、6L的水桶装满水，倒满5L的桶。 2、将5L桶里的水倒了，将6L桶里剩余的1L放入5L桶。 3、6L的桶装满水，倒满5L桶里，6L桶里还剩2L(6-4)水。 4、 将5L桶里的水倒了，将6L桶里剩余的2L水放入5L桶里。 5、将6L桶装满水，倒满5L的桶，这时6L的桶里还剩3L水。 两个一小时蚊香怎么得到15分钟的记时 同时点燃AB两只蚊香，其中A蚊香点燃两头，等A蚊香烧完后（30分钟），点燃B蚊香的另一头。 4分钟沙漏和7分钟沙漏怎么漏出9分钟 4分钟的和7分钟的同时开始，4分钟的完后又倒过来开始。7分钟的沙漏完后立马倒过来，（4分钟的沙漏还剩1分钟）。等4分钟的沙漏完后，将7分钟的又立马倒过来，等漏完就是9分钟。 八个球，其中有一个是其余球重量的1.5倍，有什么方案找出来 3次。 AB两边，一边放4个，如果A重，则把B端的去了，A端的4个分成AB端一边两个。在把轻的那端去了，重的那端的2个分成一边一个，重的那个球就找出来了。 桌上100个球，每次可以拿一到五个, 现在我们两个人依次拿球，你先拿，使用怎样的拿球策略，可以使你最终能拿到最后一个球？ 第一次拿四个，后来每个你拿球的时候只要保证剩下的球是6的倍数就行了如果他拿n个球 ，你就拿6-n个球。 有10个石头，每人每次可以拿1-2个，轮流拿，最后一个拿的人算输，有什么必赢的方案。 对方先拿、保证两个人每一轮回拿满3个（对方拿一个，我拿两个、对方拿两个，我拿一个）。 一亿数据获取前1000个最大值 https://zhuanlan.zhihu.com/p/73233544 把一亿个数字的前100个 首先放入数组。 然后把最小值放在ary[0]。然后再循环100到一亿之间的。 每次循环判断当前数字是否大于ary[0]当大于时，当前数字放入ary[0] 并再次重构数组最小值进入ary[0]以此类推 。当循环完这一亿个数字后。 最大的前100个数字就出来了。 经典智力题：飞机加油问题 左神总结 左神直通BAT算法笔记（基础篇）-上 左神直通BAT算法笔记（基础篇）-下 左神直通BAT算法（进阶篇）-上 左神直通BAT算法（进阶篇）-下 剑指offer解析-上（Java实现） 剑指offer解析-下（Java实现） leetcode经典题目 https://blog.csdn.net/zangdaiyang1991/article/details/89338652 红黑树 原来手写红黑树可以这么简单 链表 链表思路总结 https://www.jianshu.com/p/91acadc982ff 1 将单向链表按某值划分成左边小、中间相等、右边大的形式（p59） 如果要求不按照原来的顺序排列 1、荷兰国旗问题解决。 如果要求按照原来的顺序排列 2、将原链表划分为三个链表，然后再连接起来。 2 复制含有随机指针节点的链表（p64） 1、 用hash表，但是空间复杂度为n。 map.put(cur,new Node(cur.value)); 2、不用hash表，空间复杂度为1. 将链表复制为：1->1'->2->2'->3->3'->null 1）可以设置每一个节点的复制节点 2）设置rand节点：例如，1的rand节点为3，1的复制节点为1'，那么1'应该指向3'，如何找到3'呢？令1'.next=3.next。 3）将节点和复制节点拆分。 3 两个单链表生成相加链表（p66） 将两个链表代表的整数相加之后的数，再成为一个新的链表。 1、使用栈结构，空间复杂度为n 用两个栈保存两个链表的值，然后弹出相加，如果有进位，用ca记录，最后成为一个链表。 Node node = null; whlie(!s1.isEmpty() || !s2.isEmpty()){ n1 = s1.isEmpty() ? 0 : s1.pop(); n2 = s2.isEmpty() ? 0 : s2.pop(); n = n1+n2; //*******头插法：倒着连接链表********** pre = node node = new Node(n%10); node.next = pre; ca = n/10; } if(ca == 1){ pre = node; node = new Node(1); node.next = pre; } return node; 2、将两个链表逆序，然后相加即可。 public Node reverseList(Node head){ Node pre = null; Node next = null; while(head != null){ next = head.next; head.next = pre; pre = head; head = next; } return pre; } 4 将单链表的每k个节点之间逆序 1、使用栈结构，每个k个节点入栈，然后将这个k个节点连接，注意节点之间的连接与头节点的设置。 每k个节点之间的栈连接 public Node resign(Stack stack,Node left,Node right){ Node cur = stack.pop(); if(left != null){ left.next = cur; } Node next = null; while(!stack.isEmpty()){ next = stack.pop(); cur.next = next; cur = next; } cur.next = right; return cur; } 2、使用单链表逆序 public void resign(Node left,Node start,Node end,Node right){ Node pre = start; Node cur = start.next; Node next = null; while(cur != null){ next = cur.next; cur.next = pre; pre = cur; cur = next; } if(left != null){ left.next = end; } start.next = right; } 5 将搜索二叉树转换为双向链表 一般链表的题目都可以使用容器（栈、队列）来辅助完成。同时，一般需要cur、pre节点辅助连接。 1、使用队列收集遍历到的节点，然后再连接为双向链表。 双向链表连接Node pre = head; pre.left = null; Node cur = null; while(!queue.isEmpty()){ cur = queue.poll(); pre.right = cur; cur.left = pre; pre = cur; } pre.right = null; return head; 二叉树 二叉树递归调用题解 1 二叉树遍历问题（P94） 递归遍历模板 public void pre\b(Node head){ if(head == null){ return; } System.out.println(head.value + \" \"); pre(head.left); pre(head.right); } 非递归遍历 前序遍历 用栈来保存信息，但是遍历的时候，是：先输出根节点信息，然后压入右节点信息，然后再压入左节点信息。 public void pre\b(Node head){ if(head == null){ return; } Stack stack = new Stack<>(); stack.push(head); while(!stack.isEmpty()){ head = stack.poll(); System.out.println(head.value + \" \"); if(head.right != null){ stack.push(head.right); } if(head.left != null){ stack.push(head.left); } } System.out.println(); } 中序遍历 中序遍历的顺序是左中右，先一直左节点遍历，并压入栈中，当做节点为空时，输出当前节点，往右节点遍历。 public void inorder(Node head){ if(head == null){ return; } Stack stack = new Stack<>(); stack.push(head); while(!stack.isEmpty() || head != null){ if(head != null){ stack.push(head); head = head.left } else { head = stack.poll(); System.out.println(head.value + \" \"); head = head.right; } } System.out.println(); } 后序遍历 用两个栈来实现，压入栈1的时候为先左后右，栈1弹出来就是中右左，栈2收集起来就是左右中。 遍历变形题：根据后续数组重建搜索二叉树（P148） 根据搜索二叉树性质，最后一个元素是头节点，比后序数组最后一个元素小的数组会在数组左边，比数组最后一个元素值大的数组在数组的右边。所以，我们需要找到后续数组中，小的数组的中的最后一个元素和大的数组中的第一个元素。 遍历变形题：判断一颗二叉树是否为搜索二叉树和完全二叉树（P150） 搜索二叉树：中序遍历，递增。 完全二叉树：层次遍历二叉树，有右孩子没有左孩子，为false，如果当前节点并不是左右孩子都有，那么后面的节点都必须是叶子节点。 2 打印二叉树的边界节点（P100） 经验：二叉树的大多数问题都是用递归来解决的，所以，碰到二叉树的问题，一定得想递归版本！ 递归的收集左右边界信息 public void setMap(Node head,int level,int[][] map){ if(head == null){ return; } map[l][0] = map[l][0] == null ? h : map[l][0]; map[l][1] = h; setMap(head.left,level+1,map); setMap(head.right,level+1,map); } 递归获取树的高度 public int getHeight(Node head,int level){ if(head == null){ return level; } return Math.max(getHeight(head.left,level+1),getHeight(head.right,level+1)); } 递归打印叶子节点（非左右边界节点） public int printLeafNotInMap(Node head,int level,int[][] map){ if(head == null){ return; } if(head.left == null && head.right == null && head != map[level][0] && head != map[level][1]){ System.out.println(head.value + \" \"); } printLeafNotInMap(head.left,level+1,map); printLeafNotInMap(head.right,level+1,map); } 3 在二叉树中找到累加和为指定值的最长路径长度（P119） 思路： 用一个 hashmap (key是累加和sum，value是当前层level) 保存每一步的累加和sum，例如，当到第三层的第一个节点的时候，当前的累加和sum的值，如果sum在hashmap中不存在，保存到hashmap中。然后递归的遍历求每个sum，同时，求最长路径长度Math.max(level-map.get(sum-k)，max)。 这个思路也可以用到数组中的累加和问题。 4 找到二叉树中的最大搜索二叉子树（P121） 思路 这个题目是二叉树中很多题目中的套路：树形dp。 解题步骤 1、列出所有可能性，比如，这个题目的可能性有：左子树上的可能性、右子树的可能性、左右子树整体的可能性。 2、根据第一步的可能性，确定需要的信息，比如，这个题目，我们需要左子树的满足的最大搜索树头结点leftMaxBSTHead，左子树的最大搜索树的大小leftBSTSize，左子树的最大值max。 3、合并第二步的信息，写出信息结构。 public class ReturnType{ public Node maxBSTHead; public int maxBSTSize; public int min; public int max; public ReturnType(Ndoe maxBSTHead,int maxBSTSize,int min,int max){ this.maxBSTHead = maxBSTHead; this.maxBSTSize - maxBSTSize; this.min = min; this.max = max; } } 4、设计递归函数，以及设计base case。 树形dp题目：判断二叉树是否是平衡二叉树（P146） 平衡二叉树的左右子树高度相差不能超过1，所以，我们需要的信息有height、isBST（是否是平衡二叉树） 树形dp题目：二叉树节点的最大距离问题（P168） 树形dp题目：排队的最大快乐值（P169） 保存yes_X_max：X来的情况。 保存no_X_max：X不来的情况。 返回结构public class ReturnType{ public int yesHeadMax; public int noHeadMax; } 5 二叉树的按层打印与ZigZag打印（P132） 本来按层打印只需要利用queue宽度遍历即可，但是，因为这里需要换行，所以，需要添加额外的辅助变量来换行的操作。 换行需要的变量：last表示正在打印的当前行的最右节点，nlast表示下一行的最右节点。 ZigZag打印：需要用到LinkedList双端队列。 原则1：从左往右打印，头部弹出元素打印，如果有孩子，孩子尾部加入队列。 原则2：从右往左打印，尾部弹出元素打印，如果有孩子，孩子头部加入队列。 6 调整搜索二叉树中两个错误的节点（P137） 搜索二叉树的中序遍历应该为递增的，找到两个节点，过程为：第一个错误节点为第一次降序时较大的节点，第二个错误的节点是最后一次降序时较小的节点。 7 判断t1树中是否有与t2树拓扑结构完全相同的子树（P144） 序列化二叉树，然后判断子串即可。 8 在二叉树中找到一个节点的后继节点（P153） 最简单方法 利用node的parent节点找到头结点，然后中序遍历，当前节点的后一个节点就是。 思路 这种题目都是有一个parent节点指向父节点的，一般会有最优解，减少时间复杂度。 情况1：如果node有右子树，那么后继节点就是右子树上最左边的节点。 情况2：如果没有右子树。 1）如果是当前节点的父节点的左孩子，则当前节点就是后继节点。 2）如果是当前节点的额父节点的右孩子，则向上寻找，假设向上寻找移动到的节点为s，s的父节点为p，如果发现s是p的左孩子，那么p就是后继节点。 public Node getNextNode(Node node){ if(node == null){ return node ; } if(node.right != null){ //寻找右节点的最左节点 return getLeftMost(node.right); } else { Node parent = node.parent; while(parent != null && parent.left != node){ node = parent; parent = node.parent; } return parent; } } 变形题：在二叉树中找到两个节点的最近公共祖先（P155） 方法1：利用后序遍历，递归方法。 1）如果cur等于null，或者等于o1或者o2，那么返回cur。 2）如果cur的left和right都为空，那么返回null，说明没有。 3）如果cur的left和right都不为空，那么cur就是。 4）如果cur的left和right一个为空，一个不为空，返回不是空的节点即可。 方法2：用hashmap给每个节点设置父节点。 利用hashmap，然后将o1节点往上遍历到节点，并把每个节点放入集合A中，然后o2往上遍历到头节点，只要遍历的节点发现在集合A中，那么这个节点就是公共祖先节点。 9 统计完全二叉树的节点数（P176） 1）找到右子树的最左节点，如果到达最后一层，说明左子树为满二叉树，节点数为：2^(h-1)，递归右子树bs(node.right,level+1,h)，所以为：2^(h-1)+bs(node.right,level+1,h)。 2）找到右子树的最左节点，如果没有到达最后一层，说明右子树为满二叉树，所以为：2^(h-level-1)+bs(node.left,level+1,h)。 未解决问题 P173、P172、神级遍历方法 动态规划 背包九讲 https://www.cnblogs.com/jbelial/articles/2116074.html https://www.bilibili.com/video/av33930433/ 动态规划思路总结 https://www.jianshu.com/p/d0950aa36b43 ACM动态规划总结 https://blog.csdn.net/cc_again/article/details/25866971 leetcode动态规划总结（不错） https://www.cnblogs.com/JasonBUPT/p/11868558.html 1 动态规划空间压缩解题思路（P232、P188） 思路 1、用一维数组代替二维数组，降低空间复杂度 2、需要记录的值： 必须值：一维数组的第一个值（累加值） 可能值：一维数组的上一行的前一个值（i-1）、一维数组的上一行的当前值（i） 3、初始化第一行的数据 4、有了相关值之后，从左到右，从上到下遍历。 tips（P185、P198、P203、P232、P235） 1、一般一位变量是：更新的维度（更短的维度），同时，需要求出哪个是长字符串、短字符串 2、字符串的题目循环时：1-n 3、如果是3个方向a[i-1][j-1]、a[i-1][j]、a[i][j-1]来更新值，那么循环时，需要一个变量pre记录左上角a[i-1][j-1]的值，并且循环时，需要记录初始值（P198） 4、一般题目根据循环i、j直接将二维根据变量替换成一维的即可 5、如果需要比较当前dp[i][j]的值的大小，则需要用temp变量记录dp[i][j]的值，并且将当前值替换成temp（P232） 2 动态规划解题思路 1）找到base case 2）找到依赖的位置有哪些 通常依赖的位置 dp[i][j]、dp[i-1][j]、dp[i][j-1]、dp[i-1][j-1]。 累加：依赖某一行、某一列、行和列 方法 方法1：如果题目简单，通过题目直接找出依赖。 方法2：如果题目复杂，写出递归版本，然后，根据递归版本，用普通的值代入，找出依赖哪些值。 3 递归方法解题思路（P204、P238、P240、P245、P249） 1、确定递归函数变量有哪些？ 2、解决局部问题，剩下的问题递归解决，写出递归模板。 3、写出base case。 换钱的最少货币数（P189） 1、process(arr,int i,int aim) 2、process(arr,i+1,aim-k*arr[i]) 机器人到达指定位置的方法数（P192、P199） 1、process(int N,int cur,int rest,int P) 2 2.1）当前位置在1时 process(N,2,rest-1,P) 2.2）当前位置在N时 process(N,N-1,rest-1,P) 2.3）当前位置在i时 process(N,cur-1,rest-1,P)+process(N,cur+1,rest-1,P) 4 字符串动态规划总结 最长递增子序列（P210） 求出dp数组 dp[i] = {dp[j]+1(0 根据dp数组求出最长递增子序列 1、找到dp数组中最大值及最大值的index 2、从最大值开始，从右往左遍历，如果dp[index-1] == dp[index]且arr[index-1]这个字符满足要求，遍历完所有的字符 最长公共子序列问题（P220） 求出dp数组dp[i][j] = dp[i-1][j] dp[i][j] = dp[i][j-1] dp[i][j] = dp[i-1][j-1] + 1 求以上三者的最大值即可。 根据dp数组求出最长公共子序列 如果是来自dp[i-1][j-1]方向的，则是满足要求的字符。 最长公共子串（p223） 求dp数组 如果当前两个字符串的字符相等，则dp[i][j] = dp[i-1][j-1] + 1，否则dp[i][j] = 0 根据dp数组求公共子串 只需要找到最大的dp中的值max，然后往前遍历max次即可，收集这些值。 最小编辑代价（P230） dp[i][j]代表str1的字符到i的子串和str1的字符到j的子串的代价 如果当前字符串相等dp[i][j] = dp[i-1][j-1] 如果当前字符串不相等dp[i][j] = dp[i-1][j-1] + rc//加上一个代替的代价 字符串1和字符串2少一个的情况dp[i][j] = dp[i-1][j] //str1删除一个字符的代价dc dp[i][j] = dp[i][j-1] //str1插入一个字符的代价ic 字符串交错组成（P233） 位置（i，j）的情况如下： dp[i-1][j]代表能否被str1[0...i-2]和str2[0...j-1]交错组成，如果可以，且str1[i-1]==aim[i+j-1]，则dp[i][j]==true dp[i][j-1]代表能否被str1[0...i-1]和str2[0...j-2]交错组成，如果可以，且str2[j-1]==aim[i+j-1]，则dp[i][j]==true 其他情况都是false 数组中最长连续序列（P248，不需要用动态规划） 用hashmap保存key为数组中的值，value为当前值所在的子序列的长度len，每次遍历一个值的时候就更新，并且只需要更新子序列的开始值和结束值这两个边界即可。 递归 LeetCode刷题总结-递归篇 leetcode刷题总结之递归 LeetCode——有关递归的相关题目 递归三部曲套路 二分查找 https://www.jianshu.com/p/c471d87c0847 数组 无序（正数）、有序数组求target数组（P380、P382、） 思路 可以采用双指针的方法进行求解（类似滑动窗口）。 left：当值大于target时，left++; right：当值小于target时，right++； 未排序数组中累加和为指定值的最长子数组系列问题（P384） 思路 用一个map记录（sum，j），key的sum为：从arr最左边开始累加最早出现的sum值，value的j为：sum值最早出现的位置。 用map.get(sum-k)得到长度len; 子数组最大累加和（子矩阵最大累加和）（P397、P398） 思路 当cur当前累加和小于0时，清零，重新累加，用max记录最大值。 二分查找系列 在数组中找到局部最小的位置（P401） 思路：只要确定在二分两侧的某一侧肯定存在你要找的内容，就可以使用二分查找，并不是一定要有序数组。 数组中的最大累乘积（P402） 思路：记录当前位置的最大值和最小值 Tip：本来有很多我准备的资料的，但是都是外链，或者不合适的分享方式，所以大家去公众号回复【资料】好了。 现在免费送给大家，在我的公众号 好好学java 回复 资料 即可获取。 有收获？希望老铁们来个三连击，给更多的人看到这篇文章 1、老铁们，关注我的原创微信公众号「好好学java」，专注于Java、数据结构和算法、微服务、中间件等技术分享，保证你看完有所收获。 2、给俺一个 star 呗，可以让更多的人看到这篇文章，顺便激励下我继续写作，嘻嘻。 "},"zother7-JavaInterview/dataStructures-algorithms/递归套路总结.html":{"url":"zother7-JavaInterview/dataStructures-algorithms/递归套路总结.html","title":"递归套路总结","keywords":"","body":"链接：http://39.96.217.32/blog/4# 前言 相信不少同学和我一样，在刚学完数据结构后开始刷算法题时，遇到递归的问题总是很头疼，而一看解答，却发现大佬们几行递归代码就优雅的解决了问题。从我自己的学习经历来看，刚开始理解递归思路都很困难，更别说自己写了。 我一直觉得刷算法题和应试一样，既然是应试就一定有套路存在。在刷题中，我总结出了一套解决递归问题的模版思路与解法，用这个思路可以秒解很多递归问题。 递归解题三部曲 何为递归？程序反复调用自身即是递归。 我自己在刚开始解决递归问题的时候，总是会去纠结这一层函数做了什么，它调用自身后的下一层函数又做了什么…然后就会觉得实现一个递归解法十分复杂，根本就无从下手。 相信很多初学者和我一样，这是一个思维误区，一定要走出来。既然递归是一个反复调用自身的过程，这就说明它每一级的功能都是一样的，因此我们只需要关注一级递归的解决过程即可。 实在没学过啥绘图的软件，就灵魂手绘了一波，哈哈哈勿喷。 如上图所示，我们需要关心的主要是以下三点： 整个递归的终止条件。 一级递归需要做什么？ 应该返回给上一级的返回值是什么？ 因此，也就有了我们解递归题的三部曲： 找整个递归的终止条件：递归应该在什么时候结束？ 找返回值：应该给上一级返回什么信息？ 本级递归应该做什么：在这一级递归中，应该完成什么任务？ 一定要理解这3步，这就是以后递归秒杀算法题的依据和思路。 但这么说好像很空，我们来以题目作为例子，看看怎么套这个模版，相信3道题下来，你就能慢慢理解这个模版。之后再解这种套路递归题都能直接秒了。 例1：求二叉树的最大深度 先看一道简单的Leetcode题目： Leetcode 104. 二叉树的最大深度 题目很简单，求二叉树的最大深度，那么直接套递归解题三部曲模版： 找终止条件。 什么情况下递归结束？当然是树为空的时候，此时树的深度为0，递归就结束了。 找返回值。 应该返回什么？题目求的是树的最大深度，我们需要从每一级得到的信息自然是当前这一级对应的树的最大深度，因此我们的返回值应该是当前树的最大深度，这一步可以结合第三步来看。 本级递归应该做什么。 首先，还是强调要走出之前的思维误区，递归后我们眼里的树一定是这个样子的，看下图。此时就三个节点：root、root.left、root.right，其中根据第二步，root.left和root.right分别记录的是root的左右子树的最大深度。那么本级递归应该做什么就很明确了，自然就是在root的左右子树中选择较大的一个，再加上1就是以root为根的子树的最大深度了，然后再返回这个深度即可。 具体Java代码如下： class Solution { public int maxDepth(TreeNode root) { //终止条件：当树为空时结束递归，并返回当前深度0 if(root == null){ return 0; } //root的左、右子树的最大深度 int leftDepth = maxDepth(root.left); int rightDepth = maxDepth(root.right); //返回的是左右子树的最大深度+1 return Math.max(leftDepth, rightDepth) + 1; } } 当足够熟练后，也可以和Leetcode评论区一样，很骚的几行代码搞定问题，让之后的新手看的一脸懵逼(这道题也是我第一次一行代码搞定一道Leetcode题)： class Solution { public int maxDepth(TreeNode root) { return root == null ? 0 : Math.max(maxDepth(root.left), maxDepth(root.right)) + 1; } } 例2：两两交换链表中的节点 看了一道递归套路解决二叉树的问题后，有点套路搞定递归的感觉了吗？我们再来看一道Leetcode中等难度的链表的问题，掌握套路后这种中等难度的问题真的就是秒：Leetcode 24. 两两交换链表中的节点 直接上三部曲模版： 找终止条件。 什么情况下递归终止？没得交换的时候，递归就终止了呗。因此当链表只剩一个节点或者没有节点的时候，自然递归就终止了。 找返回值。 我们希望向上一级递归返回什么信息？由于我们的目的是两两交换链表中相邻的节点，因此自然希望交换给上一级递归的是已经完成交换处理，即已经处理好的链表。 本级递归应该做什么。 结合第二步，看下图！由于只考虑本级递归，所以这个链表在我们眼里其实也就三个节点：head、head.next、已处理完的链表部分。而本级递归的任务也就是交换这3个节点中的前两个节点，就很easy了。 附上Java代码： class Solution { public ListNode swapPairs(ListNode head) { //终止条件：链表只剩一个节点或者没节点了，没得交换了。返回的是已经处理好的链表 if(head == null || head.next == null){ return head; } //一共三个节点:head, next, swapPairs(next.next) //下面的任务便是交换这3个节点中的前两个节点 ListNode next = head.next; head.next = swapPairs(next.next); next.next = head; //根据第二步：返回给上一级的是当前已经完成交换后，即处理好了的链表部分 return next; } } 例3：平衡二叉树 相信经过以上2道题，你已经大概理解了这个模版的解题流程了。 那么请你先不看以下部分，尝试解决一下这道easy难度的Leetcode题（个人觉得此题比上面的medium难度要难）：Leetcode 110. 平衡二叉树 我觉得这个题真的是集合了模版的精髓所在，下面套三部曲模版： 找终止条件。 什么情况下递归应该终止？自然是子树为空的时候，空树自然是平衡二叉树了。 应该返回什么信息： 为什么我说这个题是集合了模版精髓？正是因为此题的返回值。要知道我们搞这么多花里胡哨的，都是为了能写出正确的递归函数，因此在解这个题的时候，我们就需要思考，我们到底希望返回什么值？ 何为平衡二叉树？平衡二叉树即左右两棵子树高度差不大于1的二叉树。而对于一颗树，它是一个平衡二叉树需要满足三个条件：它的左子树是平衡二叉树，它的右子树是平衡二叉树，它的左右子树的高度差不大于1。换句话说：如果它的左子树或右子树不是平衡二叉树，或者它的左右子树高度差大于1，那么它就不是平衡二叉树。 而在我们眼里，这颗二叉树就3个节点：root、left、right。那么我们应该返回什么呢？如果返回一个当前树是否是平衡二叉树的boolean类型的值，那么我只知道left和right这两棵树是否是平衡二叉树，无法得出left和right的高度差是否不大于1，自然也就无法得出root这棵树是否是平衡二叉树了。而如果我返回的是一个平衡二叉树的高度的int类型的值，那么我就只知道两棵树的高度，但无法知道这两棵树是不是平衡二叉树，自然也就没法判断root这棵树是不是平衡二叉树了。 因此，这里我们返回的信息应该是既包含子树的深度的int类型的值，又包含子树是否是平衡二叉树的boolean类型的值。可以单独定义一个ReturnNode类，如下： class ReturnNode{ boolean isB; int depth; //构造方法 public ReturnNode(boolean isB, int depth){ this.isB = isB; this.depth = depth; } } 本级递归应该做什么。 知道了第二步的返回值后，这一步就很简单了。目前树有三个节点：root，left，right。我们首先判断left子树和right子树是否是平衡二叉树，如果不是则直接返回false。再判断两树高度差是否不大于1，如果大于1也直接返回false。否则说明以root为节点的子树是平衡二叉树，那么就返回true和它的高度。 具体的Java代码如下： class Solution { //这个ReturnNode是参考我描述的递归套路的第二步：思考返回值是什么 //一棵树是BST等价于它的左、右俩子树都是BST且俩子树高度差不超过1 //因此我认为返回值应该包含当前树是否是BST和当前树的高度这两个信息 private class ReturnNode{ boolean isB; int depth; public ReturnNode(int depth, boolean isB){ this.isB = isB; this.depth = depth; } } //主函数 public boolean isBalanced(TreeNode root) { return isBST(root).isB; } //参考递归套路的第三部：描述单次执行过程是什么样的 //这里的单次执行过程具体如下： //是否终止?->没终止的话，判断是否满足不平衡的三个条件->返回值 public ReturnNode isBST(TreeNode root){ if(root == null){ return new ReturnNode(0, true); } //不平衡的情况有3种：左树不平衡、右树不平衡、左树和右树差的绝对值大于1 ReturnNode left = isBST(root.left); ReturnNode right = isBST(root.right); if(left.isB == false || right.isB == false){ return new ReturnNode(0, false); } if(Math.abs(left.depth - right.depth) > 1){ return new ReturnNode(0, false); } //不满足上面3种情况，说明平衡了，树的深度为左右俩子树最大深度+1 return new ReturnNode(Math.max(left.depth, right.depth) + 1, true); } } 一些可以用这个套路解决的题 暂时就写这么多啦，作为一个高考语文及格分，大学又学了工科的人，表述能力实在差因此啰啰嗦嗦写了一大堆，希望大家能理解这个很好用的套路。 下面我再列举几道我在刷题过程中遇到的也是用这个套路秒的题，真的太多了，大部分链表和树的递归题都能这么秒，因为树和链表天生就是适合递归的结构。 我会随时补充，正好大家可以看了上面三个题后可以拿这些题来练练手，看看自己是否能独立快速准确的写出递归解法了。 Leetcode 101. 对称二叉树 Leetcode 111. 二叉树的最小深度 Leetcode 226. 翻转二叉树：这个题的备注是最骚的。Mac OS下载神器homebrew的大佬作者去面试谷歌，没做出来这道算法题，然后被谷歌面试官怼了：”我们90％的工程师使用您编写的软件(Homebrew)，但是您却无法在面试时在白板上写出翻转二叉树这道题，这太糟糕了。” Leetcode 617. 合并二叉树 Leetcode 654. 最大二叉树 Leetcode 83. 删除排序链表中的重复元素 Leetcode 206. 翻转链表 "},"zother7-JavaInterview/interview/已投公司情况.html":{"url":"zother7-JavaInterview/interview/已投公司情况.html","title":"已投公司情况","keywords":"","body":" 公司 官网链接 牛客链接 投递情况 美团 https://campus.meituan.com/resume-edit 二面 快手 https://zhaopin.kuaishou.cn/recruit/e/#/official/my-apply/ https://www.nowcoder.com/discuss/405019?type=post&order=time&pos=&page=2 hr面 1113903029@qq.com 蘑菇街 http://job.mogujie.com/#/candidate/perfectInfo offer 虎牙 不匹配 远景 https://campus.envisioncn.com/ 笔试 阿里钉钉 https://campus.alibaba.com/myJobApply.htm?saveResume=yes&t=1584782560963 https://www.nowcoder.com/discuss/368915?type=0&order=0&pos=25&page=3 一面 阿里新零售 https://www.nowcoder.com/discuss/374171?type=0&order=0&pos=35&page=1 https://www.nowcoder.com/discuss/372118?type=0&order=0&pos=80&page=2 深信服 https://www.nowcoder.com/discuss/369399?type=0&order=0&pos=40&page=6 CVTE https://www.nowcoder.com/discuss/368463?type=0&order=0&pos=87&page=3 已投 奇安信 https://www.nowcoder.com/discuss/365961?type=0&order=0&pos=102&page=6 已投 携程 http://recruitment.ctrip.com/#/leftIntern https://www.nowcoder.com/discuss/378021?type=post&order=time&pos=&page=3 encore2106@163.com 小米 https://app.mokahr.com/m/candidate/applications/deliver-query/xiaomi https://www.nowcoder.com/discuss/375898?type=0&order=0&pos=12&page=5 https://www.nowcoder.com/discuss/377763?type=7 已投 拼多多 https://pinduoduo.zhiye.com/Portal/Apply/Index https://www.nowcoder.com/discuss/393350?type=post&order=time&pos=&page=8 已投 腾讯 https://join.qq.com/center.php https://www.nowcoder.com/discuss/377813?type=post&order=time&pos=&page=1 offer 猿辅导 https://app.mokahr.com/m/candidate/applications/deliver-query/fenbi https://www.nowcoder.com/discuss/375610?type=0&order=0&pos=95&page=2 已投 斗鱼 https://app.mokahr.com/m/candidate/applications/deliver-query/douyu https://www.nowcoder.com/discuss/375180?type=0&order=0&pos=158&page=1 笔试 淘宝技术部 https://www.nowcoder.com/discuss/374655?type=0&order=0&pos=165&page=6 字节跳动 https://job.bytedance.com/user https://job.bytedance.com/referral/pc/position/application?lightning=1&token=MzsxNTg0MTU2NDIxMDIzOzY2ODgyMjg1NzI1Mjk3MjI4ODM7MA/profile/ https://www.nowcoder.com/discuss/381888?type=post&order=time&pos=&page=2 已投 陌陌 来自内推军 已投 网易 http://gzgame.campus.163.com/applyPosition.do?&lan=zh https://www.nowcoder.com/discuss/373132?type=post&order=create&pos=&page=1 一姐 已投 百度 https://talent.baidu.com/external/baidu/index.html#/individualCenter https://www.nowcoder.com/discuss/376515?type=post&order=time&pos=&page=1 已投 京东 http://campus.jd.com/web/resume/resume_index?fxType=0 https://www.nowcoder.com/discuss/372978?type=post&order=time&pos=&page=4 已投 爱奇艺 科大讯飞 度小满 https://www.nowcoder.com/discuss/387950?type=post&order=time&pos=&page=13 已投 "},"zother7-JavaInterview/interview/自我介绍和项目介绍.html":{"url":"zother7-JavaInterview/interview/自我介绍和项目介绍.html","title":"自我介绍和项目介绍","keywords":"","body":"自我介绍 面试官，您好，我叫XXX，我是XXX21届应届毕业生，在校的专业是计算机技术。 研究生加入实验室后，在研一这一年中，做了一个####项目，然后在去年的5月份到9月份，去了####实习了几个月，由于那里做的项目不是Java的，所以在简历中没有体现。 从大学开始学习计算机专业之后，对技术挺有热情的，喜欢记录自己学过的技术，分享技术，在CSDN博客平台，通过这几年的时间，陆陆续续写了400多篇博客了，同时也获得了CSDN博客专家的称号，博客访问量有150W了。 最后，我通过这几年的学习，对于计算机的基础知识掌握的还可以，对于计算机网络，Java相关的技术，数据库等方面有深入学习，我也特别想去贵公司，自己也特别适合这个岗位，谢谢您给我这个机会！ 项目一介绍 这个项目项目基于SSM框架开发，采用前后端分离，主要为电厂开发一套人员管理及人事审批的OA系统，实现电厂人员管理、电源审批、栈道审批、工器具审批等功能。 在项目中除了用到了ssm技术之外，还有activiti工作流框架，数据库使用的是mysql，缓存框架使用的是redis，权限控制框架使用的是shiro。 在这个项目中我担任的角色是项目负责人，负责整个项目的管理工作，同时，我还负责了以下几块工作： 1、搭建项目环境 2、实现用户的单点登录系统SSO 3、项目的权限控制 4、开发电源管理和栈道管理模块 5、系统性能调优 项目二介绍 项目基于SSM框架开发的综合性电商网站，实现会员商城浏览下单、商家入驻商城出售商品、管理员后台管理商品、订单、搜索及会员等功能，同时，系统可以发起抢购活动功能。 在项目中用到的技术主要包括ssm框架，数据库用的是mysql，缓存用的是redis，消息队列用的是rocketMQ，微服务分布式项目用到了dubbo框架，对于处理分布式的一些问题，用到了zookeeper框架。 在这个项目中我主要责任是 1、搭建dubbo的分布式的项目（怎么分层） 2、利用redis提供缓存服务 3、利用RocketMQ消息队列做消息处理 4、对数据库性能进行调优 5、开发抢购活动功能模块（业务需求） 北京项目介绍 这个项目的背景是，现在深空探索的研究越来越热，这个项目就是研究及开发脉冲星导航的相关问题，而脉冲星导航的能够解决深空探索航天器的位置问题，这个就是北京项目的研究目标。 脉冲星是一颗稳定的中子星，能够发出稳定的脉冲波，通过接收脉冲星发出来的脉冲波，可以确定航天器在太空中的位置。 而我在北京的工作是： 1、开发一个软件 2、实现相关算法对航天器上的部件精度进行控制 3、航天器相关的数据进行数据显示 4、近地卫星轨道5星编队仿真实现 2019年5月–2019年9月，在####实习，参加了脉冲星导航的项目开发工作。项目的背景是，现在深空探索的研究越来越热，这个项目就是研究及开发脉冲星导航的相关问题，而脉冲星导航的能够解决深空探索航天器的位置问题，这个就是项目的研究目标。 负责工作： 1、开发一个脉冲星导航仿真软件 2、实现相关算法对航天器上的部件精度进行控制 3、航天器相关的数据进行数据显示 4、近地卫星轨道5星编队仿真实现 个人收获： 通过这个项目的开发，学习到了许多以前没有接触到的航天知识，对于这方面的知识，个人有了很大的提高，同时，由于时间关系，开发的时间非常紧张，对于自己的有了很大的锻炼，不管是从抗压能力还是学习能力，都是得到了一定得锻炼的。 Tip：本来有很多我准备的资料的，但是都是外链，或者不合适的分享方式，所以大家去公众号回复【资料】好了。 现在免费送给大家，在我的公众号 好好学java 回复 资料 即可获取。 有收获？希望老铁们来个三连击，给更多的人看到这篇文章 1、老铁们，关注我的原创微信公众号「好好学java」，专注于Java、数据结构和算法、微服务、中间件等技术分享，保证你看完有所收获。 2、给俺一个 star 呗，可以让更多的人看到这篇文章，顺便激励下我继续写作，嘻嘻。 "},"zother7-JavaInterview/interview-experience/各大公司面经.html":{"url":"zother7-JavaInterview/interview-experience/各大公司面经.html","title":"各大公司面经","keywords":"","body":"阿里 阿里社招四面（分布式） 笔试两道算法题 https://www.nowcoder.com/discuss/389676?type=post&order=time&pos=&page=1 https://www.nowcoder.com/discuss/394432?type=post&order=time&pos=&page=1 钉钉一面 1、final、finnaly 2、重写重载 3、jvm的fullGC排查问题 4、多线程ArrayBlockingQueue、LinkedBlockingQueue的源码分析，如果用100w数据进行插入，哪个更快（Array） 5、linux：如果查找一个文件中的一个指定的字符串出现的数量、如何查找被占用的端口、如何查看cpu的load（使用率） 6、数据库的一个表有name、id、gender、age字段，设置一个联合索引id、name、gender，查询100w的数据用name做where查询，会不会全表查询 7、git：git如何合并分支、git add、git commit、git push的区别。 8、maven：如何查找重复的jar包问题 9、单点登录如何做的，如果保证安全性 腾讯 pcg一面 1、接口设计原则 2、mvc设计原则，如何设计 3、微服务rpc调用接口如何设计 4、微服务接口调用出现问题，如何设计接口，使得更好定位bug 5、springboot、spring、springcloud的区别 6、数据库如何设计 7、数据量大的时候，数据库表应该怎么设计，怎么插入、怎么查询，提高效率，单表的数据量能支持多大 单表数据量1000W 8、spring AOP 9、spring的动态代理，区别，静态代理，动态代理，cglib 10、dubbo设计架构 11、解耦合的设计模式有哪些？应用场景，却别是什么？ 观察者模式等 https://blog.csdn.net/liman65727/article/details/79762475 12、JVM垃圾回收的过程 13、多线程如何实现，区别，怎么设计多线程 14、多线程与多进程的应用场景，多进程的应用场景 15、list和set的区别，集合其他相关，时间复杂度等等 16、tcp如何保证可靠性传输，滑动窗口 https://blog.csdn.net/liuchenxia8/article/details/80428157 https://juejin.im/post/5c9f1dd651882567b4339bce 17、linux的alias命令 18、linux的递归查询文件 19、回文字符串，最长回文字符串 20、reverse方法时间复杂度 pcg二面 1、tcp可靠性 2、项目 3、不包含重复的最长连续子串 pcg三面 1、项目 2、长连接短连接，应用场景 3、不用循环查找最大值 百度 滴滴 头条 拼多多 美团 一面 1、多线程相关（如何实现线程、线程池的相关参数） 2、hashmap和hashtable的区别，hashmap的getSize方法需要加锁吗 3、concurrenthashmap的put的过程 4、jvm内存模型、垃圾回收器 5、哪些可以作为GCRoot 6、new一个对象的过程 https://www.cnblogs.com/JackPn/p/9386182.html 7、fullGC的原因，如何排查 8、cpu内存使用过高如何排查 9、spring的AOP 10、spring的IOC 11、redis的删除策略 12、redis如何实现数据一致性 13、dubbo的原理 14、dubbo如何实现远程调用 15、项目介绍 16、单点登录 17、数据库如何调优 18、联合索引abc，哪些情况会出现索引失效 19、分布式项目请求操作经历的过程 二面 1、场景题：4亿用户，访问10个商品，保证等概率，且每个用户每次显示同样的3个商品，不能用数据库，不能用文件保存，不能用redis等框架 思路：用concurrentHashMap，分段锁，然后用多台机器，再限流，保证数据一致性 2、项目介绍 小米 网易 华为 快手 Tip：本来有很多我准备的资料的，但是都是外链，或者不合适的分享方式，所以大家去公众号回复【资料】好了。 现在免费送给大家，在我的公众号 好好学java 回复 资料 即可获取。 有收获？希望老铁们来个三连击，给更多的人看到这篇文章 1、老铁们，关注我的原创微信公众号「好好学java」，专注于Java、数据结构和算法、微服务、中间件等技术分享，保证你看完有所收获。 2、给俺一个 star 呗，可以让更多的人看到这篇文章，顺便激励下我继续写作，嘻嘻。 "},"zother7-JavaInterview/interview-experience/面试常见知识.html":{"url":"zother7-JavaInterview/interview-experience/面试常见知识.html","title":"面试常见知识","keywords":"","body":"阿里 linux进程结构 linux文件系统 linux统计文件数量 spingioc，springaop中的cglib动态 shell脚本 webLogic知道不？ 与tomcat区别？ webservice HashMap链表转红黑树是怎么实现的 进程通信 散列表（哈希表） Java中pv操作实现时用的类 说一下TCP/IP的状态转移 说一下NIO 线程通信 写过异步的代码吗 拜占庭算法的理解？ 红黑树么，在插入上有什么优化？ resetful风格 分布式rpc调度过程中要注意的问题 hashmap并发读写死循环问题 快排时间复杂度？最好什么情况，最坏什么情况？有什么改进方案？ HashMap get和put源码，为什么红黑而非平衡树？ 分布式系统CAP理论，重点解释分区容错性的意义 对虚拟内存的理解 三个线程如何实现交替打印ABC 线程池中LinkedBlockingQueue满了的话，线程会怎么样 HashMap和ConcurrentHashMap哪个效率更高？为什么？ mybatis如何进行类型转换 myql间歇锁的实现原理 future的底层实现异步原理 rpc原理 多个服务端上下线怎么感知 降级处理hystrix了解过么 redis的热点key问题 一致性哈希 ClassLoader原理和应用 注解的原理 腾讯 百度 滴滴 头条 拼多多 美团 小米 网易 华为 Tip：本来有很多我准备的资料的，但是都是外链，或者不合适的分享方式，所以大家去公众号回复【资料】好了。 现在免费送给大家，在我的公众号 好好学java 回复 资料 即可获取。 有收获？希望老铁们来个三连击，给更多的人看到这篇文章 1、老铁们，关注我的原创微信公众号「好好学java」，专注于Java、数据结构和算法、微服务、中间件等技术分享，保证你看完有所收获。 2、给俺一个 star 呗，可以让更多的人看到这篇文章，顺便激励下我继续写作，嘻嘻。 "},"zother7-JavaInterview/interview-experience/面试常见问题分类汇总.html":{"url":"zother7-JavaInterview/interview-experience/面试常见问题分类汇总.html","title":"面试常见问题分类汇总","keywords":"","body":"Java基础 反射在jvm层面的实现 https://www.jianshu.com/p/b6cb4c694951 mysql语句分别会加什么锁 https://blog.csdn.net/iceman1952/article/details/85504278 若hashcode方法永远返回1会产生什么结果 https://blog.csdn.net/cnq2328/article/details/50436175 jvm的方法区存什么？ https://www.jianshu.com/p/10584345b10a Class.forName和ClassLoader的区别 https://blog.csdn.net/qq_27093465/article/details/52262340 java对象信息分配 https://blog.csdn.net/u014520047/article/details/81940447 java虚拟机ZGC详解 https://vimsky.com/article/4162.html java虚拟机CMS详解 https://juejin.im/post/5c7262a15188252f30484351 java虚拟机G1详解 https://zhuanlan.zhihu.com/p/59861022 解决hash冲突的三种方法 https://blog.csdn.net/qq_32595453/article/details/80660676 为什么要重写hashCode()方法和equals()方法以及如何进行重写 https://blog.csdn.net/xlgen157387/article/details/63683882 动态代理 https://segmentfault.com/a/1190000011291179 红黑树 https://zhuanlan.zhihu.com/p/31805309 hashmap的jdk1.7和jdk1.8区别 https://juejin.im/post/5aa5d8d26fb9a028d2079264 https://blog.csdn.net/qq_36520235/article/details/82417949 concurrenthashmap的jdk1.7和jdk1.8区别 面试题 Java I/O 底层细节，注意是底层细节，而不是怎么用 可以从Java IO底层、JavaIO模型（阻塞、异步等） https://www.cnblogs.com/crazymakercircle/p/10225159.html 如何实现分布式缓存 redis如何实现分布式缓存 https://stor.51cto.com/art/201912/607229.htm 浏览器的缓存机制 说明计算机网络的知识还没有记住 https://www.cnblogs.com/yangyangxxb/p/10218871.html JVM tomcat 容器启动，jvm 加载情况描述 tomcat请求流程：http://objcoding.com/2017/06/12/Tomcat-structure-and-processing-request-process/ 其实就是jvm的类加载情况，非常相似 https://blog.csdn.net/lduzhenlin/article/details/83013143 https://blog.csdn.net/xlgen157387/article/details/53521928 当获取第一个获取锁之后，条件不满足需要释放锁应当怎么做？ https://www.jianshu.com/p/eb112b25b848 HashMap 实现原理，扩容因子过大过小的缺点，扩容过程 采用什么方法能保证每个 bucket 中的数据更均匀 解决冲突的方式，还有没有其他方式（全域哈希） 扩容：https://www.jianshu.com/p/5730bff593ed https://www.cnblogs.com/peizhe123/p/5790252.html Collection 集合类中只能在 Iterator 中删除元素的原因 在Iterator的内部有个expectedModCount 变量， 该变量每次初始化Iterator的时候等于ArrayList的modCount，modCount记录了对ArrayList的结构修改次数， 在通过Iterator对ArrayList进行结构的修改的时候都会将expectedModCount 与modCount同步，但是如果在通过Iterator访问的时候同时又通过索引的方式去修改ArrayList的结构的话，由于通过索引的方式只会修改modCount不会同步修改expectedModCount 就会导致modCount和expectedModCount 不相等就会抛ConcurrentModificationException， 这也就是Iterator的fail-fast，快速失效的。所以只要采取一种方式操作ArrayList就不会出问题， 当然ArrayList不是线程安全的，此处不讨论对线程问题。 更加详细的解释 https://blog.csdn.net/yanshuanche3765/article/details/78917507 java 地址和值传递的例子 https://www.cnblogs.com/zhangyu317/p/11226105.html java NIO，java 多线程、线程池，java 网络编程解决并发量 Java Nio使用：https://blog.csdn.net/forezp/article/details/88414741 Java Nio原理：https://www.cnblogs.com/crazymakercircle/p/10225159.html 线程池：http://cmsblogs.com/?p=2448 为什么nio快：https://blog.csdn.net/yaogao000/article/details/47972143 手写一个线程安全的生产者与消费者。 https://www.cnblogs.com/jun-ma/p/11843394.html https://blog.csdn.net/Virgil_K2017/article/details/89283946 ConcurrentHashMap 和 LinkedHashMap 差异和适用情形 哈希表的原理：https://blog.csdn.net/yyyljw/article/details/80903391 可以以下方面进行回答 （1）使用的数据结构？ （2）添加元素、删除元素的基本逻辑？ （3）是否是fail-fast的？ （4）是否需要扩容？扩容规则？ （5）是否有序？是按插入顺序还是自然顺序还是访问顺序？ （6）是否线程安全？ （7）使用的锁？ （8）优点？缺点？ （9）适用的场景？ （10）时间复杂度？ （11）空间复杂度？ ConcurrentHashMap分段锁是如何实现，ConcurrentHashmap jdk1.8 访问的时候是怎么加锁的，插入的时候是怎么加锁的 访问不加 锁插入的时候对头结点加锁 jdk1.8；https://blog.csdn.net/weixin_42130471/article/details/89813248 ArrayDeque 的使用场景 1、用数组实现的双端队列 2、线程不是安全的 3、可以用来实现栈 JDBC 连接的过程 ，手写 jdbc 连接过程 https://blog.csdn.net/qq_44971038/article/details/103204217 可重入锁,实现原理 ReetrantLock：https://www.jianshu.com/p/f8f6ac49830e Java IO 模型(BIO,NIO 等) ，Tomcat 用的哪一种模型 tomcat支持：https://blog.csdn.net/fd2025/article/details/80007435 ArrayBlockingQueue 源码 http://cmsblogs.com/?p=4755 （1）ArrayBlockingQueue不需要扩容，因为是初始化时指定容量，并循环利用数组； （2）ArrayBlockingQueue利用takeIndex和putIndex循环利用数组； （3）入队和出队各定义了四组方法为满足不同的用途； （4）利用重入锁和两个条件保证并发安全：lock、notEmpty、notFull 多进程和多线程的区别 说出三个遇到过的程序报异常的情况 https://www.cnblogs.com/winnie-man/p/10471338.html Java 无锁原理 https://blog.csdn.net/qq_39291929/article/details/81501829 hashmap 和 treemap 的区别 http://cmsblogs.com/?p=4743 hashmap （1）HashMap是一种散列表，采用（数组 + 链表 + 红黑树）的存储结构； （2）HashMap的默认初始容量为16（1 treemap （1）TreeMap的存储结构只有一颗红黑树； （2）TreeMap中的元素是有序的，按key的顺序排列； （3）TreeMap比HashMap要慢一些，因为HashMap前面还做了一层桶，寻找元素要快很多； （4）TreeMap没有扩容的概念； （5）TreeMap的遍历不是采用传统的递归式遍历； （6）TreeMap可以按范围查找元素，查找最近的元素； rehash 过程 https://www.jianshu.com/p/dde9b12343c1 网络编程的 accept 和 connect HashMap 的负载因子，为什么容量为2^n HashMap为了存取高效，要尽量较少碰撞，就是要尽量把数据分配均匀，每个链表长度大致相同，这个实现就在把数据存到哪个链表中的算法； 这个算法实际就是取模，hash%length，计算机中直接求余效率不如位移运算，源码中做了优化hash&(length-1)， hash%length==hash&(length-1)的前提是length是2的n次方； 为什么这样能均匀分布减少碰撞呢？2的n次方实际就是1后面n个0，2的n次方-1 实际就是n个1； 例如长度为9时候，3&(9-1)=0 2&(9-1)=0 ，都在0上，碰撞了； 例如长度为8时候，3&(8-1)=3 2&(8-1)=2 ，不同位置上，不碰撞； 其实就是按位“与”的时候，每一位都能 &1 ，也就是和1111……1111111进行与运算 try catch finally 可不可以没有 catch（try return,finally return） mapreduce 流程，如何保证 reduce 接受的数据没有丢失，数据如何去重，mapreduce 原理，partion 发生在什么阶段 直接写一个 java 程序，统计 IP 地址的次数 讲讲多线程，多线程的同步方法 1、synchronized 2、reetrantlock list,map,set 之间的区别 https://blog.csdn.net/u012102104/article/details/79235938 socket 是靠什么协议支持的 TCP/IP，协议。socket用于 通信，在实际应用中有im等，因此需要可靠的网络协议，UDP则是不可靠的协议，且服务端与客户端不链接，UDP用于广播，视频流等 java io 用到什么设计模式 装饰模式和适配器模式 serviable 的序列化，其中 uuid 的作用 相当于快递的打包和拆包，里面的东西要保持一致，不能人为的去改变他，不然就交易不成功。序列化与反序列化也是一样，而版本号的存在就是要是里面内容要是不一致，不然就报错。像一个防伪码一样。 什么时候会用到 HashMap 什么情景下会用到反射 注解、Spring 配置文件、动态代理、jdbc 浅克隆与深克隆有什么区别，如何实现深克隆 浅拷贝：仅仅克隆基本类型变量，而不克隆引用类型的变量 深克隆：既克隆基本类型变量，也克隆引用类型变量 1.浅克隆：只复制基本类型的数据，引用类型的数据只复制了引用的地址，引用的对象并没有复制，在新的对象中修改引用类型的数据会影响原对象中的引用。直接使用clone方法，再嵌套的还是浅克隆，因为有些引用类型不能直接克隆。 2.深克隆：是在引用类型的类中也实现了clone，是clone的嵌套，并且在clone方法中又对没有clone方法的引用类型又做差异化复制，克隆后的对象与原对象之间完全不会影响，但是内容完全相同。 常见的线程安全的集合类 Java 8 函数式编程 回调函数 函数式编程，面向对象之间区别 Java 8 中 stream 迭代的优势和区别？ 同步等于可见性吗？ 保证了可见性不等于正确同步，因为还有原子性没考虑。 还了解除 util 其他包下的 List 吗？ CopyOnWriteArrayList （1）CopyOnWriteArrayList使用ReentrantLock重入锁加锁，保证线程安全； （2）CopyOnWriteArrayList的写操作都要先拷贝一份新数组，在新数组中做修改，修改完了再用新数组替换老数组，所以空间复杂度是O(n)，性能比较低下； （3）CopyOnWriteArrayList的读操作支持随机访问，时间复杂度为O(1)； （4）CopyOnWriteArrayList采用读写分离的思想，读操作不加锁，写操作加锁，且写操作占用较大内存空间，所以适用于读多写少的场合； （5）CopyOnWriteArrayList只保证最终一致性，不保证实时一致性； 反射能够使用私有的方法属性吗和底层原理？ https://blog.51cto.com/4247649/2109128 处理器指令优化有些什么考虑？ 禁止重排序 object 对象的常用方法 Stack 和 ArrayList 的区别 statement 和 prestatement 的区别 1、Statement用于执行静态SQL语句，在执行时，必须指定一个事先准备好的SQL语句。 2、PrepareStatement是预编译的SQL语句对象，sql语句被预编译并保存在对象中。被封装的sql语句代表某一类操作，语句中可以包含动态参数“?”，在执行时可以为“?”动态设置参数值。 3、使用PrepareStatement对象执行sql时，sql被数据库进行解析和编译，然后被放到命令缓冲区，每当执行同一个PrepareStatement对象时，它就会被解析一次，但不会被再次编译。在缓冲区可以发现预编译的命令，并且可以重用。 4、PrepareStatement可以减少编译次数提高数据库性能。 手写模拟实现一个阻塞队列 https://www.cnblogs.com/keeya/p/9713686.html 怎么使用父类的方法 util 包下有哪几种接口 cookie 禁用怎么办 https://segmentfault.com/q/1010000007715137 Netty new 实例化过程 socket 实现过程，具体用的方法；怎么实现异步 socket. https://blog.csdn.net/charjay_lin/article/details/81810922 很常见的 Nullpointerexception ，你是怎么排查的，怎么解决的； Binder 的原理 java 线程安全都体现在哪些方面 如果维护线程安全 如果想实现一个线程安全的队列，可以怎么实现？ JUC 包里的 ArrayBlockingQueue 还有 LinkedBlockingQueue 啥的又结合源码说了一 通。 静态内部类和非静态内部类的区别是什么？ 怎么创建静态内部类和非静态内部类？ https://blog.csdn.net/qq_38366777/article/details/78088386 断点续传的原理 Xml 解析方式，原理优缺点 https://segmentfault.com/a/1190000013504078?utm_source=tag-newest 静态变量和全局变量的区别 Java多线程 CountDownLatch、CyclicBarrier、Semaphore 用法总结 https://segmentfault.com/a/1190000012234469 AOS等并发相关面试题 https://cloud.tencent.com/developer/article/1471770 https://zhuanlan.zhihu.com/p/96544118 https://zhuanlan.zhihu.com/p/48295486 threadlocal https://juejin.im/post/5ac2eb52518825555e5e06ee java 线程池达到提交上限的具体情况 ，线程池用法，Java 多线程，线程池有哪几类，每一类的差别 https://blog.csdn.net/github_37130188/article/details/89504500 要你设计的话，如何实现一个线程池 http://www.vcchar.com/thread-29098-1-1.html 线程池的类型，固定大小的线程池内部是如何实现的，等待队列是用了哪一个队列实现 线程池种类和工作流程 重点讲 newcached 线程池 线程池工作原理 比如 corePoolSize 和maxPoolSize 这两个参数该怎么调 http://cmsblogs.com/?p=2448 线程池使用了什么设计模式 线程池使用时一般要考虑哪些问题 https://juejin.im/post/5d1882b1f265da1ba84aa676#heading-14 线程池的配置 Excutor 以及 Connector 的配置 https://www.cnblogs.com/kismetv/p/7806063.html spring&springmvc 面试题： https://mp.weixin.qq.com/s/2Y5X11TycreHgO0R3agK2A https://mp.weixin.qq.com/s/IdjCxumDleLqdU8MgQnrLQ ioc aop总结（概述性） https://juejin.im/post/5b040cf66fb9a07ab7748c8b https://juejin.im/post/5b06bf2df265da0de2574ee1 Spring 的加载流程，Spring 的源码中 Bean 的构造的流程 spring ioc系列文章：http://cmsblogs.com/?p=2806 加载流程（概述）：https://www.jianshu.com/p/5fd1922ccab1 循环依赖问题：https://blog.csdn.net/u010853261/article/details/77940767 Spring 事务源码，IOC 源码，AOP 源码 https://juejin.im/post/5c525968e51d453f5e6b744b ioc、aop系列源码： https://segmentfault.com/a/1190000015319623 http://www.tianxiaobo.com/2018/05/30/Spring-IOC-%E5%AE%B9%E5%99%A8%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%E5%AF%BC%E8%AF%BB/ spring 的作用及理解 事务怎么配置 https://www.jianshu.com/p/e7d59ebf41a3 Spring 的 annotation 如何实现 https://segmentfault.com/a/1190000013258647 SpringMVC 工作原理 https://blog.csdn.net/cswhale/article/details/16941281 了解 SpringMVC 与 Struct2 区别 https://blog.csdn.net/chenleixing/article/details/44570681 springMVC 和 spring 是什么关系 项目中 Spring 的 IOC 和 AOP 具体怎么使用的 https://www.cnblogs.com/xdp-gacl/p/4249939.html https://juejin.im/post/5b06bf2df265da0de2574ee1 spring mvc 底层实现原理 https://blog.csdn.net/weixin_42323802/article/details/84038765 动态代理的原理 https://juejin.im/post/5a3284a75188252970793195 如果使用 spring mvc，那 post 请求跟 put 请求有什么区别啊； 然后开始问 springmvc：描述从 tomcat 开始到 springmvc 返回到前端显示的整个流程,接着问 springmvc 中的 handlerMapping 的内部实现，然后又问 spring 中从载入 xml 文件到 getbean 整个流程，描述一遍 springboot & springcloud springboot springboot面试题 springboot面试题2 springcloud springcloudm面试题 servlet Servlet 知道是做什么的吗？和 JSP 有什么联系？ jsp就是在html里面写java代码，servlet就是在java里面写html代码…其实jsp经过容器解释之后就是servlet.只是我们自己写代码的时候尽量能让它们各司其职，jsp更注重前端显示，servlet更注重模型和业务逻辑。不要写出万能的jsp或servlet来即可。 作者：知乎用户 链接：https://www.zhihu.com/question/37962386/answer/74906895 JSP 的运行原理？ jsp/servlet原理：https://www.jianshu.com/p/93736c3b448b JSP 属于 Java 中 的吗？ Servlet 是线程安全 https://blog.csdn.net/qq_24145735/article/details/52433096 https://www.cnblogs.com/chanshuyi/p/5052426.html servlet 是单例 servlet 和 filter 的区别。 https://blog.csdn.net/weixin_42669555/article/details/81049423 servlet jsp tomcat常见面试题 https://juejin.im/post/5a75ab4b6fb9a063592ba9db https://blog.csdn.net/shxz130/article/details/39735373 hibernate Hibernate 的生成策略 主要说了 native 、uuid https://blog.csdn.net/itmyhome1990/article/details/54863822 Hibernate 与 Mybatis 区别 https://blog.csdn.net/wangpeng047/article/details/17038659 Mybatis原理 https://www.javazhiyin.com/34438.html Redis Redis 数据结构 Redis 持久化机制 Redis 的一致性哈希算法 redis了解多少 redis五种数据类型，当散列类型的 value 值非常大的时候怎么进行压缩 用redis怎么实现摇一摇与附近的人功能 https://blog.csdn.net/smartwu_sir/article/details/80254733 redis 主从复制过程 Redis 如何解决 key 冲突 redis 是怎么存储数据的 redis 使用场景 框架其他 Servlet 的 Filter 用的什么设计模式 https://www.jianshu.com/p/e4197a54828d zookeeper 的常用功能，自己用它来做什么 ibatis 是怎么实现映射的，它的映射原理是什么 mybatis面试题：https://zhuanlan.zhihu.com/p/44464109 redis 的操作是不是原子操作 https://juejin.im/entry/58f9e22044d9040069d40dca 秒杀业务场景设计 WebSocket 长连接问题 如何设计淘宝秒杀系统（重点关注架构，比如数据一致性，数据库集群一致性哈希，缓存， 分库分表等等） List 接口去实例化一个它的实现类（ArrayList）以及直接用 ArrayList 去 new 一个该类的对 象，这两种方式有什么区别 Tomcat 关注哪些参数 （tomcat调优） https://juejin.im/post/5ac034f351882548fe4a4383 https://testerhome.com/topics/16082 对后台的优化有了解吗？比如负载均衡 我给面试官说了 Ngix+Tomcat 负载均 衡，异步处理（消息缓冲服务器），缓存(Redis， Memcache)， NoSQL，数据库优化，存储索引优化 对 Restful 了解 Restful 的认识，优点，以及和 soap 的区别 https://www.ruanyifeng.com/blog/2011/09/restful.html lrucache 的基本原理 设计模式 Java常见设计模式 https://www.jianshu.com/p/61b67ca754a3 单例模式（双检锁模式）、简单工厂、观察者模式、适配器模式、职责链模式等等 享元模式模式 选两个画下 UML 图 手写单例 写的是静态内部类的单例，然后他问我这个地方为什么用 private，这儿为啥用 static, 这就考察你的基本功啦 静态类与单例模式的区别 单例模式 double check 单例模式都有什么，都是否线程安全，怎么改进（从 synchronized 到 双重检验锁 到 枚举 Enum） 基本的设计模式及其核心思想 来，我们写一个单例模式的实现 这里有一个深坑，详情请见《 JVM 》 第 370 页 基本的设计原则 如果有人问你接口里的属性为什么都是 final static 的，记得和他聊一聊设计原则。 数据库 count(1)、count(*)、count(列名) https://blog.csdn.net/iFuMI/article/details/77920767 mysql的undo、redo、binlog的区别 https://mp.weixin.qq.com/s/0z6GmUp0Lb1hDUo0EyYiUg explain解释 https://segmentfault.com/a/1190000010293791 mysql分页查询优化 https://blog.csdn.net/hanchao5272/article/details/102790490 sql注入 https://blog.csdn.net/github_36032947/article/details/78442189 为什么用B+树 https://blog.csdn.net/xlgen157387/article/details/79450295 sql执行流程 https://juejin.im/post/5b7036de6fb9a009c40997eb 聚集索引与非聚集索引 https://juejin.im/post/5cdd701ee51d453a36384939 覆盖索引 https://www.jianshu.com/p/77eaad62f974 sql总结 https://juejin.im/post/5d3f9cc1f265da03a31d1192 有人建议给每张表都建一个自增主键，这样做有什么优点跟缺点 https://blog.csdn.net/yixuandong9010/article/details/72286029 对 MySQL 的了解，和 oracle 的区别 https://juejin.im/post/5cbdbb455188250ab224802d 500万数字排序，内存只能容纳5万个，如何排序，如何优化？ 参考文章：https://juejin.im/entry/5a27cb796fb9a045104a5e8c 平时怎么写数据库的模糊查询（由字典树扯到模糊查询，前缀查询，例如“abc%”，还是索引策略的问题） 数据库里有 10000000 条用户信息，需要给每位用户发送信息（必须发送成功），要求节省内存 项目中如何实现事务 数据库设计一般设计成第几范式 https://blog.csdn.net/hsd2012/article/details/51018631 mysql 用的什么版本 5.7 跟 5.6 有啥区别 提升 MySQL 安全性 https://blog.csdn.net/listen_for/article/details/53907270 问了一个这样的表(三个字段:姓名，id，分数)要求查出平均分大于 80 的 id 然后分数降序排序，然后经过提示用聚合函数 avg。 select id from table group by id having avg(score) > 80 order by avg(score) desc。 为什么 mysql 事务能保证失败回滚 一道算法题，在一个整形数组中，找出第三大的数，注意时间效率 (使用堆) 主键索引底层的实现原理 B+树 经典的01索引问题？ 如何在长文本中快捷的筛选出你的名字？ 全文索引 多列索引及最左前缀原则和其他使用场景 事务隔离级别 索引的最左前缀原则 数据库悲观锁怎么实现的 https://www.jianshu.com/p/f5ff017db62a 建表的原则 索引的内涵和用法 给了两条 SQL 语句，让根据这两条语句建索引（个人想法：主要考虑复合索引只能匹配前缀列的特点） 那么我们来聊一下数据库。A 和 B 两个表做等值连接(Inner join) 怎么优化 https://blog.csdn.net/hguisu/article/details/5731880 哈希 数据库连接池的理解和优化 Sql 语句 分组排序 SQL 语句的 5 个连接概念 数据库优化和架构（主要是主从分离和分库分表相关） 分库分表 跨库join实现 探讨主从分离和分库分表相关 数据库中间件 读写分离在中间件的实现 限流 and 熔断 行锁适用场景 https://cloud.tencent.com/developer/article/1104098 计算机网络 TCP三次握手第三次握手时ACK丢失怎么办 https://www.cnblogs.com/wuyepeng/p/9801470.html dns属于udp还是tcp，原因 https://www.zhihu.com/question/310145373 http的幂等性 https://www.cnblogs.com/weidagang2046/archive/2011/06/04/idempotence.html 建立连接的过程客户端跟服务端会交换什么信息(参考 TCP 报文结构) 丢包如何解决重传的消耗 https://cloud.tencent.com/developer/article/1195037 traceroute 实现原理 https://zhuanlan.zhihu.com/p/36811672 IO多路复用 https://sanyuesha.com/python-server-tutorial/book/ch05.html select 和 poll 区别？ 好文：https://www.jianshu.com/p/dfd940e7fca2 在不使用 WebSocket 情况下怎么实现服务器推送的一种方法 服务器推送：https://juejin.im/post/5c20e5766fb9a049b13e387b 可以使用客户端定时刷新请求或者和 TCP 保持心跳连接实现。 查看磁盘读写吞吐量？ https://www.cnblogs.com/ggjucheng/archive/2013/01/13/2858810.html PING 位于哪一层 ping命令本身相当于一个应用程序，位于应用层，虽然它使用的是ICMP协议，就好像HTTP位于应用层，但是也是使用的TCP协议 网络重定向，说下流程 https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Redirections controller 怎么处理的请求：路由 IP 地址分为几类，每类都代表什么，私网是哪些 https://zhuanlan.zhihu.com/p/54593244 linux linux查找命令 https://blog.51cto.com/whylinux/2043871 项目部署常见linux命令 https://blog.csdn.net/u010938610/article/details/79625988 进程文件里有哪些信息 sed 和 awk 的区别 awk用法：https://www.cnblogs.com/isykw/p/6258781.html 其实sed和awk都是每次读入一行来处理的，区别是：sed 适合简单的文本替换和搜索；而awk除了自动给你分列之外，里面丰富的函数大大增强了awk的功能。数据统计，正则表达式搜索，逻辑处理，前后置脚本等。因此基本上sed能做的，awk可以全部完成并且做的更好。 作者：哩掉掉 链接：https://www.zhihu.com/question/297858714/answer/572046422 linux查看进程并杀死的命令 https://blog.csdn.net/qingmu0803/article/details/38271077 有一个文件被锁住，如何查看锁住它的线程？ 如何查看一个文件第100行到150行的内容 https://blog.csdn.net/zmx19951103/article/details/78575265 如何查看进程消耗的资源 https://www.cnblogs.com/freeweb/p/5407105.html 如何查看每个进程下的线程？ https://blog.csdn.net/inuyashaw/article/details/55095545 linux 如何查找文件 linux命令：https://juejin.im/post/5d3857eaf265da1bd04f2437 select epoll等问题 https://juejin.im/post/5b624f4d518825068302aee9#heading-13 安全加密 http://www.ruanyifeng.com/blog/2011/08/what_is_a_digital_signature.html https://yq.aliyun.com/articles/54155 web安全问题 https://juejin.im/post/5da44c5de51d45783a772a22 分布式 dubbo中的dubbo协议和http协议有什么区别？ https://blog.csdn.net/wjw_77/article/details/99696757 项目及规划 对你来说影响最大的一个项目（该面试中有关项目问题都针对该项目展开）？ 项目哪一部分最难攻克？如何攻克？ 个人建议：大家一定要选自己印象最深的项目回答，首先按模块，然后组成 人员，最后你在项目中的角色和发挥 的作用。全程组织好语言，最好不要有停顿，面试官可以 看出你对项目的熟悉程度 你觉得你在项目运行过程中作为组长是否最大限度发挥了组员的优势？具体事例？ 职业规划，今天想发展的工作方向 项目里我遇到过的最大的困难是什么 实验室的新来的研一，你会给他们什么学习上的建议，例如对于内核源码的枯 燥如何克服 如何协调团队中多人的工作 当团队中有某人的任务没有完成的很好，如何处理 平时看些什么书，技术 综合 项目解决的什么问题 用到了哪些技术 怎么预防 bug 日志 jvm 异常信息 如何找问题的根源（统计表格） 你是怎么学习的，说完会让举个例子 实习投了哪几个公司？为什么，原因 最得意的项目是什么？为什么？(回答因为项目对实际作用大，并得到认可) 最得意的项目内容，讲了会 你简历上写的是最想去的部门不是我们部门，来我们部门的话对你有影响麽？ 你除了在学校还有哪些方式去获取知识和技术？ 你了解阿里文化和阿里开源吗？ 遇到困难解决问题的思路？ 我觉得最成功的一件事了 我说能说几件吗，说了我大学明白明白了 自己想干什么，选择了自己喜欢的事，大学里学会了和自己相处，自己一个人的 时候也不会感觉无聊，精神世界比较丰富，坚持锻炼，健身，有个很不错的身体， 然后顿了顿笑着说，说，有一个对我很好的女朋友算吗？ 压力大的时候怎么调整？多个任务冲突了你怎么协调的？ 家里有几个孩子，父母对你来北京有什么看法? 职业生涯规划 你在什么情况下可能会离职 对你影响最大的人 优点 3 个，以及缺点 2. 说说你应聘这个岗位的优势 3. 说说家庭 4. 为什么 想来网易，用过网易的哪些产品，对比下有什么好的地方 5. 投递了哪些公司，对第一份工 作怎么看待 为什么要选择互联网（楼主偏底层的） 为什么来网易（看你如何夸） 在校期间怎样学习 经常逛的技术性网站有哪些？ 举出你在开发过程中遇到的原先不知道的 bug, 通过各种方式定位 bug 并最终 成功解决的例子 举出一个例子说明你的自学能力 7 次面试记录，除了京东基本上也都走到了很后面的阶段。硬要说经验可能有三点： 不会就不会。我比较爽快，如果遇到的不会的甚至是不确定的，都直接说：“对不起， 我答不上来”之类的。 一技之长。中间件和架构相关的实习经历，让我基本上和面试官都可以聊的很多， 也可以看到，我整个过程没有多少算法题。是因为面试官和你聊完项目就知道你能 做事了。其实，面试官很不愿意出算法题的（BAT 那个档次除外），你能和他扯技 术他当然高兴了。关键很多人只会算法（逃）。 基础非常重要。面试官只要问 Java 相关的基础，我都有自信让一般的面试官感觉 惊讶，甚至学到新知识 "},"zother7-JavaInterview/microservice/微服务相关资料.html":{"url":"zother7-JavaInterview/microservice/微服务相关资料.html","title":"微服务相关资料","keywords":"","body":"springboot教程 springboot学习教程 springcloud教程 springcloud学习教程 "},"zother7-JavaInterview/network/http面试问题全解析.html":{"url":"zother7-JavaInterview/network/http面试问题全解析.html","title":"http面试问题全解析","keywords":"","body":"HTTP协议定义了浏览器怎么向万维网服务器请求万维网文档，以及服务器怎么样把文档传送给浏览器。 举个例子来说，用户单击鼠标后发生的事件按顺序如下（以访问清华大学为例）： 浏览器分析链接指向页面的URL。 浏览器向DNS请求解析www.tsinghua.edu.c的IP地址。 DNS解析出该IP地址。 浏览器与该服务器建立TCP链接（默认port:80）。 浏览器发出HTTP请求：GET/chn/index.htm（HTTP请求通过TCP套接字，发送请求报文，该请求报文作为TCP三次握手第三个报文数据发送给服务器）。 服务器通过HTTP响应把文件index.html发送给服务器。 TCP连接释放（释放连接若connection若为close，服务端主动关闭，客户端被动关闭，若为keepalive，则该连接会保持一段事件，该时间内可以继续接收请求）。 浏览器将文件进行解析，并将Web页面显示给用户（先解析状态行，看状态码是否请求成功，然后解析每一个响应头，告知编码规范，对其进行格式化，有脚本还要加载脚本）。 HTTP 状态码含义： 200 OK 服务器已经成功处理了请求并提供了请求的网页。 202 Accpted 已经接受请求，但处理尚未完成。 204 No Content 没有新文档，浏览器应该继续显示原来的文档。 206 Partial Content 客户端进行了范围请求。响应报文中由Content-Range指定实体内容的范围。实现断点续传 301 Moved Permanently 永久性重定向。请求的网页已永久移动到新位置。 302（307） Moved Temporatily 临时性重定向。 304 Not Moidfied 未修改。自上次请求后，请求的内容未修改过。 401 Unauthorized 客户试图未经授权访问受密码保护的页面。该应答中会包含一个WWW-Authenticate头，浏览器由此显示login框，填写合适Authorization头以后再次发出请求。 403 Forbidden 服务器拒绝请求。 404 Not Found 服务器不存在客户机所请求的资源。 500 Intern Server Error 服务器遇到一个错误，使其无法为请求提供服务。 HTTP的请求方法 GET 向指定的资源发出显示请求，使用GET方法应该只用于在读取数据。 HEAD 与GET方法一样，都是向服务器发出指定资源的请求。只不过HEAD不包含有呈现数据，而仅仅是头部信息（关于该资源的信息）。 PUT 向指定资源位置上传最新位置（与POST相比，PUT指定了存放位置，而PSOT由服务器指定）。 DELETE请求服务器删除某一个资源 POST 向指定资源提交数据，请求服务器进行处理（例如提交表单或者上传文件）。 TRACE 请求服务器回送收到的请求信息，主要用于测试和诊断。 OPTIONS 使服务器传回该资源所支持的所有HTTP请求方法。若请求成功，则会在HTTP头中包含一个名为“Allow”的头，值是支持的方法，如“GET,POST”。 GET和POST的区别： 对资源的影响：GET一般用于获取或者查询资源信息，意味着对同一个URL的多个请求返回的结果一样（幂等），没有修改资源的状态（安全）；而POST一般用于更新资源信息，POST既不是安全的也不是幂等的 传递的信息量：采用GET方法时，客户端把发送的数据添加到URL后面（即HTTP协议头中），使用“？”连接，各个变量用“&”连接，但是由于有些浏览器和服务器对URL的长度和字符格式存在限制，所以传递的信息有限；POST则把需要传递的数据放到请求报文的消息体中，HTTP协议对此没有限制，因此可以传递更多信息。 安全性：GET提交的数据，消息以明文出现在URL上，如密码等信息可能被浏览器缓存，从而从历史记录中得到；POST把消息存放在消息体中，安全性高，但是也存在被抓包软件抓取看到内容。 cookie 和 session ： 　　HTTP协议是无状态：无状态是指协议对于事务处理没有记忆能力，简单来说，即使第一次和服务器连接后并且登陆成功后，第二次请求服务器依然不知道当前请求是哪个用户。为此，cookie和session的使用为此提供了解决方案。 cookie: 　　以文件的形式存在硬盘中的永久性cookie（设置了一定的时限）和停留在浏览器内存中的临时性cookie，当用户访问网站时，浏览器就会在本地寻找相关cookie。如果该cookie存在，浏览器就会将其与页面请求一起通过报头信息发送到站点。 session: 　　session与cookie的作用有点类似，不同的时cookie存储在本地，而session存储在服务器。当程序需要为某个客户端的请求创建一个session的时候，服务器首先会检查这个客户端的请求里是否包含了一个session标识session-id，如果已经包含一个session-id则为此客户端创建过session，服务器就按照session-id把这个session检索出来，如果不包含，就创建一个新的键值对，并把session-id返回客户端保存。 　　浏览器提供了三种方式来保存ssesion-id: cookie url重写（把session-id附加在url后面，即使用GET） 增加隐藏域（使用POST） 　　session什么时候被创建？事实上session并不是在有客户端访问的时候被创建，而是知道某sever端程序调用类似function getSession()方法时才会创建。 HTTP请求报文结构和响应报文结构 HTTP请求报文结构如下： 请求行： 请求头：为请求报文添加了一些附加信息，如 Host（接受请求的服务器地址 ip;port 或者 域名 等） / User-Agent / Connection（连接属性，如Keep-Alive） / Accept-Charset / Accpet-Encoding / Accept-Language 空行：请求头的最后一行会有一个空行，表示请求头部结束，接下俩的请求为正文。 请求正文：可选部分，如GET就没有请求正文 一个报文实体例子如下： GET /562f25980001b1b106000338.jpg HTTP/1.1 Host img.mukewang.com User-Agent Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36 Accept image/webp,image/*,*/*;q=0.8 Referer http://www.imooc.com/ Accept-Encoding gzip, deflate, sdch Accept-Language zh-CN,zh;q=0.8 HTTP响应报文 状态行：协议版本，状态码等 响应头：与请求头类似，为响应报文添加了一些附加信息。 空行 响应正文 一个报文实体例子如下： HTTP/1.1 200 OK Server: nginx Date: Mon, 20 Feb 2017 09:13:59 GMT Content-Type: text/plain;charset=UTF-8 Vary: Accept-Encoding Cache-Control: no-store Pragrma: no-cache Expires: Thu, 01 Jan 1970 00:00:00 GMT Cache-Control: no-cache Content-Encoding: gzip Transfer-Encoding: chunked Proxy-Connection: Keep-alive {\"code\":200,\"notice\":0,\"follow\":0,\"forward\":0,\"msg\":0,\"comment\":0,\"pushMsg\":null,\"friend\":{\"snsCount\":0,\"count\":0,\"celebrityCount\":0},\"lastPrivateMsg\":null,\"event\":0,\"newProgramCount\":0,\"createDJRadioCount\":0,\"newTheme\":true} 参考文章： http详解 https://www.cnblogs.com/an-wen/p/11180076.html cookie和sessions https://www.cnblogs.com/xxtalhr/p/9053906.html 关于HTTP协议 https://www.cnblogs.com/ranyonsue/p/5984001.html Tip：本来有很多我准备的资料的，但是都是外链，或者不合适的分享方式，所以大家去公众号回复【资料】好了。 现在免费送给大家，在我的公众号 好好学java 回复 资料 即可获取。 有收获？希望老铁们来个三连击，给更多的人看到这篇文章 1、老铁们，关注我的原创微信公众号「好好学java」，专注于Java、数据结构和算法、微服务、中间件等技术分享，保证你看完有所收获。 2、给俺一个 star 呗，可以让更多的人看到这篇文章，顺便激励下我继续写作，嘻嘻。 "},"zother7-JavaInterview/operating-system/后端程序员必备的Linux基础知识.html":{"url":"zother7-JavaInterview/operating-system/后端程序员必备的Linux基础知识.html","title":"后端程序员必备的Linux基础知识","keywords":"","body":" Linux使用大全 linux可以看的github "},"zother7-JavaInterview/operating-system/操作系统、计算机网络相关知识.html":{"url":"zother7-JavaInterview/operating-system/操作系统、计算机网络相关知识.html","title":"操作系统、计算机网络相关知识","keywords":"","body":" https://github.com/CyC2018/CS-Notes Tip：本来有很多我准备的资料的，但是都是外链，或者不合适的分享方式，所以大家去公众号回复【资料】好了。 现在免费送给大家，在我的公众号 好好学java 回复 资料 即可获取。 有收获？希望老铁们来个三连击，给更多的人看到这篇文章 1、老铁们，关注我的原创微信公众号「好好学java」，专注于Java、数据结构和算法、微服务、中间件等技术分享，保证你看完有所收获。 2、给俺一个 star 呗，可以让更多的人看到这篇文章，顺便激励下我继续写作，嘻嘻。 "},"zother7-JavaInterview/project/消息中间件面试题.html":{"url":"zother7-JavaInterview/project/消息中间件面试题.html","title":"消息中间件面试题","keywords":"","body":"1 消息中间件 面试官：你在系统里用过消息队列吗？（面试官在随和的语气中展开了面试） 候选人：用过的（此时感觉没啥） 面试官：那你说一下你们在项目里是怎么用消息队列的？ 候选人：巴拉巴拉，我们啥啥系统发送个啥啥消息到队列，别的系统来消费啥啥的（很多同学在这里会进入一个误区，就是你仅仅就是知道以及回答你们是怎么用这个消息队列的，用这个消息队列来干了个什么事情？） 比如我们有个订单系统，订单系统会每次下一个新的订单的时候，就会发送时一条消息到ActiveMQ里面去，后台有个库存系统负责获取了消息然后更新库存。 面试官：那你们为什么使用消息队列啊？ （你的订单系统不发送消息到MQ，直接订单系统调用库存系统一个接口，咔嚓一下，直接就调用成功能了，库存就更新了） 候选人：额。。。（楞了一下，为什么？我没怎么仔细想过啊，老大让用就用了），硬着头皮胡言乱语了几句 （面试官此时听你楞了一下，然后听你胡言乱语了几句，开始心里觉得有点儿那什么了，怀疑你之前就压根儿没思考过这问题） 面试官：那你说说用消息队列都有什么优点和缺点？ （面试官此时心里想的是，你的MQ在项目里为啥要用？你没考虑过，那我稍微简单点儿，我问问你消息队列你之前有没有考虑过如果用的话，优点和缺点分别是啥？） 候选人：这个。。。（确实平时没怎么考虑过这个问题啊。。。胡言乱语了） （面试官此时心里已经更觉得你这哥儿们不行，平时都没什么思考） 面试官：kafka、activemq、rabbitmq、rocketmq都有什么区别？ （面试官问你这个问题，就是说，绕过比较虚的话题，直接看看你对各种MQ中间件是否了解，是否做过功课，是否做过调研） 候选人：我们就用过activemq，所以别的没用过。。。区别，也不太清楚 （面试官此时却是觉得你这哥儿们平时就是瞎用，根本就没什么思考，觉得不行） 为什么使用消息队列啊？消息队列有什么优点和缺点啊？kafka、activemq、rabbitmq、rocketmq都有什么区别以及适合哪些场景？ 2、面试官心理分析 其实面试官主要是想看看： （1）第一，你知道不知道你们系统里为什么要用消息队列这个东西？ 我之前面试就见过大量的候选人，说自己项目里用了redis、mq，但是其实他并不知道自己为什么要用这个东西。其实说白了，就是为了用而用，或者是别人设计的架构，他从头到尾没思考过。 没有对自己的架构问过为什么的人，一定是平时没有思考的人，面试官对这类候选人印象通常很不好。因为进了团队担心你就木头木脑的干呆活儿，不会自己思考。 （2）第二，你既然用了消息队列这个东西，你知道不知道用了有什么好处？ 系统中引入消息队列之后会不会有什么坏处？你要是没考虑过这个，那你盲目弄个MQ进系统里，后面出了问题你是不是就自己溜了给公司留坑？你要是没考虑过引入一个技术可能存在的弊端和风险，面试官把这类候选人招进来了，基本可能就是挖坑型选手。 就怕你干1年挖一堆坑，自己跳槽了，给公司留下后患无穷 （3）第三，既然你用了MQ，可能是某一种MQ，那么你当时做没做过调研啊？ 你别傻乎乎的自己拍脑袋看个人喜好就瞎用了一个MQ，比如kafka。甚至都从没调研过业界到底流行的MQ有哪几种？每一个MQ的优点和缺点是什么？每一个MQ没有绝对的好坏，但是就是看用在哪个场景可以扬长避短，利用其优势，规避其劣势。 如果是一个不考虑技术选型的候选人招进了团队，面试官交给他一个任务，去设计个什么系统，他在里面用一些技术，可能都没考虑过选型，最后选的技术可能并不一定合适，一样是留坑 3、额外的友情提示 同学啊，如果你看到这里，连activemq、rabbitmq、rocketmq、kafka是什么都不知道？连个hello world demo都没写过？那你。。。 通过网上查阅技术资料和博客，用于快速入门，是比较合适的，但是如果要比如系统梳理你的面试技术体系，或者是系统的深入的研究和学习一些东西，看博客实际上是不太合适的 那也没事，我们这个课程的定位是不会去讲这些的，建议你马上暂停一下课程，然后上百度搜一下，这4个东西是什么？每个东西找一个教你hello world的博客，自己跟着做一遍。我保证你1个小时之内就可以快速入门这几个东西。 等你先知道这几个东西是什么，同时写过hello world之后，你再来继续看我们的课程 4、面试题剖析 （1）为什么使用消息队列啊？ 其实就是问问你消息队列都有哪些使用场景，然后你项目里具体是什么场景，说说你在这个场景里用消息队列是什么 面试官问你这个问题，期望的一个回答是说，你们公司有个什么业务场景，这个业务场景有个什么技术挑战，如果不用MQ可能会很麻烦，但是你现在用了MQ之后带给了你很多的好处 先说一下消息队列的常见使用场景吧，其实场景有很多，但是比较核心的有3个：解耦、异步、削峰 解耦：现场画个图来说明一下，A系统发送个数据到BCD三个系统，接口调用发送，那如果E系统也要这个数据呢？那如果C系统现在不需要了呢？现在A系统又要发送第二种数据了呢？A系统负责人濒临崩溃中。。。再来点更加崩溃的事儿，A系统要时时刻刻考虑BCDE四个系统如果挂了咋办？我要不要重发？我要不要把消息存起来？头发都白了啊。。。 面试技巧：你需要去考虑一下你负责的系统中是否有类似的场景，就是一个系统或者一个模块，调用了多个系统或者模块，互相之间的调用很复杂，维护起来很麻烦。但是其实这个调用是不需要直接同步调用接口的，如果用MQ给他异步化解耦，也是可以的，你就需要去考虑在你的项目里，是不是可以运用这个MQ去进行系统的解耦。在简历中体现出来这块东西，用MQ作解耦。 异步：现场画个图来说明一下，A系统接收一个请求，需要在自己本地写库，还需要在BCD三个系统写库，自己本地写库要3ms，BCD三个系统分别写库要300ms、450ms、200ms。最终请求总延时是3 + 300 + 450 + 200 = 953ms，接近1s，用户感觉搞个什么东西，慢死了慢死了。 削峰：每天0点到11点，A系统风平浪静，每秒并发请求数量就100个。结果每次一到11点~1点，每秒并发请求数量突然会暴增到1万条。但是系统最大的处理能力就只能是每秒钟处理1000个请求啊。。。尴尬了，系统会死。。。 （2）消息队列有什么优点和缺点啊？ 优点上面已经说了，就是在特殊场景下有其对应的好处，解耦、异步、削峰 缺点呢？显而易见的 系统可用性降低：系统引入的外部依赖越多，越容易挂掉，本来你就是A系统调用BCD三个系统的接口就好了，人ABCD四个系统好好的，没啥问题，你偏加个MQ进来，万一MQ挂了咋整？MQ挂了，整套系统崩溃了，你不就完了么。 系统复杂性提高：硬生生加个MQ进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已 一致性问题：A系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是BCD三个系统那里，BD两个系统写库成功了，结果C系统写库失败了，咋整？你这数据就不一致了。 所以消息队列实际是一种非常复杂的架构，你引入它有很多好处，但是也得针对它带来的坏处做各种额外的技术方案和架构来规避掉，最好之后，你会发现，妈呀，系统复杂度提升了一个数量级，也许是复杂了10倍。但是关键时刻，用，还是得用的。。。 （3）kafka、activemq、rabbitmq、rocketmq都有什么优点和缺点啊？ 常见的MQ其实就这几种，别的还有很多其他MQ，但是比较冷门的，那么就别多说了 作为一个码农，你起码得知道各种mq的优点和缺点吧，咱们来画个表格看看 特性 ActiveMQ RabbitMQ RocketMQ Kafka 单机吞吐量 万级，吞吐量比RocketMQ和Kafka要低了一个数量级 万级，吞吐量比RocketMQ和Kafka要低了一个数量级 10万级，RocketMQ也是可以支撑高吞吐的一种MQ 10万级别，这是kafka最大的优点，就是吞吐量高。 一般配合大数据类的系统来进行实时数据计算、日志采集等场景 topic数量对吞吐量的影响 topic可以达到几百，几千个的级别，吞吐量会有较小幅度的下降 这是RocketMQ的一大优势，在同等机器下，可以支撑大量的topic topic从几十个到几百个的时候，吞吐量会大幅度下降 所以在同等机器下，kafka尽量保证topic数量不要过多。如果要支撑大规模topic，需要增加更多的机器资源 时效性 ms级 微秒级，这是rabbitmq的一大特点，延迟是最低的 ms级 延迟在ms级以内 可用性 高，基于主从架构实现高可用性 高，基于主从架构实现高可用性 非常高，分布式架构 非常高，kafka是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消息可靠性 有较低的概率丢失数据 经过参数优化配置，可以做到0丢失 经过参数优化配置，消息可以做到0丢失 功能支持 MQ领域的功能极其完备 基于erlang开发，所以并发能力很强，性能极其好，延时很低 MQ功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用，是事实上的标准 优劣势总结 非常成熟，功能强大，在业内大量的公司以及项目中都有应用 偶尔会有较低概率丢失消息 而且现在社区以及国内应用都越来越少，官方社区现在对ActiveMQ 5.x维护越来越少，几个月才发布一个版本 而且确实主要是基于解耦和异步来用的，较少在大规模吞吐的场景中使用 erlang语言开发，性能极其好，延时很低； 吞吐量到万级，MQ功能比较完备 而且开源提供的管理界面非常棒，用起来很好用 社区相对比较活跃，几乎每个月都发布几个版本分 在国内一些互联网公司近几年用rabbitmq也比较多一些 但是问题也是显而易见的，RabbitMQ确实吞吐量会低一些，这是因为他做的实现机制比较重。 而且erlang开发，国内有几个公司有实力做erlang源码级别的研究和定制？如果说你没这个实力的话，确实偶尔会有一些问题，你很难去看懂源码，你公司对这个东西的掌控很弱，基本职能依赖于开源社区的快速维护和修复bug。 而且rabbitmq集群动态扩展会很麻烦，不过这个我觉得还好。其实主要是erlang语言本身带来的问题。很难读源码，很难定制和掌控。 接口简单易用，而且毕竟在阿里大规模应用过，有阿里品牌保障 日处理消息上百亿之多，可以做到大规模吞吐，性能也非常好，分布式扩展也很方便，社区维护还可以，可靠性和可用性都是ok的，还可以支撑大规模的topic数量，支持复杂MQ业务场景 而且一个很大的优势在于，阿里出品都是java系的，我们可以自己阅读源码，定制自己公司的MQ，可以掌控 社区活跃度相对较为一般，不过也还可以，文档相对来说简单一些，然后接口这块不是按照标准JMS规范走的有些系统要迁移需要修改大量代码 还有就是阿里出台的技术，你得做好这个技术万一被抛弃，社区黄掉的风险，那如果你们公司有技术实力我觉得用RocketMQ挺好的 kafka的特点其实很明显，就是仅仅提供较少的核心功能，但是提供超高的吞吐量，ms级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展 同时kafka最好是支撑较少的topic数量即可，保证其超高吞吐量 而且kafka唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略 这个特性天然适合大数据实时计算以及日志收集 综上所述，各种对比之后，我个人倾向于是： 一般的业务系统要引入MQ，最早大家都用ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了； 后来大家开始用RabbitMQ，但是确实erlang语言阻止了大量的java工程师去深入研究和掌控他，对公司而言，几乎处于不可控的状态，但是确实人是开源的，比较稳定的支持，活跃度也高； 不过现在确实越来越多的公司，会去用RocketMQ，确实很不错，但是我提醒一下自己想好社区万一突然黄掉的风险，对自己公司技术实力有绝对自信的，我推荐用RocketMQ，否则回去老老实实用RabbitMQ吧，人是活跃开源社区，绝对不会黄 所以中小型公司，技术实力较为一般，技术挑战不是特别高，用RabbitMQ是不错的选择；大型公司，基础架构研发实力较强，用RocketMQ是很好的选择 如果是大数据领域的实时计算、日志采集等场景，用Kafka是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范 如何保证消息队列的高可用啊？ 2、面试官心理分析 如果有人问到你MQ的知识，高可用是必问的，因为MQ的缺点，我刚才已经说过了，有好多，导致系统可用性降低，等等。所以只要你用了MQ，接下来问的一些要点肯定就是围绕着MQ的那些缺点怎么来解决了。 要是你傻乎乎的就干用了一个MQ，各种问题从来没考虑过，那你就杯具了，面试官对你的印象就是，只会简单实用一些技术，没任何思考，马上对你的印象就不太好了。这样的同学招进来要是做个20k薪资以内的普通小弟还凑合。如果招进来做薪资20多k的高工，那就惨了，让你设计个系统，里面肯定一堆坑，出了事故公司受损失，团队一起背锅。 去年的事儿，非常大的互联网公司，非常核心的系统，就是疏忽了MQ，没考虑MQ如何保证高可用，如果MQ挂了怎么办，导致几个小时系统不可用，公司损失几千万，team背锅，你闹的祸，你老大帮你一起背锅 3、面试题剖析 这个问题这么问是很好的，因为不能问你kafka的高可用性怎么保证啊？ActiveMQ的高可用性怎么保证啊？一个面试官要是这么问就显得很没水平，人家可能用的就是RabbitMQ，没用过Kafka，你上来问人家kafka干什么？这不是摆明了刁难人么。 所以有水平的面试官，问的是MQ的高可用性怎么保证？这样就是你用过哪个MQ，你就说说你对那个MQ的高可用性的理解。 （1）RabbitMQ的高可用性 RabbitMQ是比较有代表性的，因为是基于主从做高可用性的，我们就以他为例子讲解第一种MQ的高可用性怎么实现。 rabbitmq有三种模式：单机模式，普通集群模式，镜像集群模式 1）单机模式 就是demo级别的，一般就是你本地启动了玩玩儿的，没人生产用单机模式 2）普通集群模式 意思就是在多台机器上启动多个rabbitmq实例，每个机器启动一个。但是你创建的queue，只会放在一个rabbtimq实例上，但是每个实例都同步queue的元数据。完了你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从queue所在实例上拉取数据过来。 这种方式确实很麻烦，也不怎么好，没做到所谓的分布式，就是个普通集群。因为这导致你要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个queue所在实例消费数据，前者有数据拉取的开销，后者导致单实例性能瓶颈。 而且如果那个放queue的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果你开启了消息持久化，让rabbitmq落地存储消息的话，消息不一定会丢，得等这个实例恢复了，然后才可以继续从这个queue拉取数据。 所以这个事儿就比较尴尬了，这就没有什么所谓的高可用性可言了，这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个queue的读写操作。 3）镜像集群模式 这种模式，才是所谓的rabbitmq的高可用模式，跟普通集群模式不一样的是，你创建的queue，无论元数据还是queue里的消息都会存在于多个实例上，然后每次你写消息到queue的时候，都会自动把消息到多个实例的queue里进行消息同步。 这样的话，好处在于，你任何一个机器宕机了，没事儿，别的机器都可以用。坏处在于，第一，这个性能开销也太大了吧，消息同步所有机器，导致网络带宽压力和消耗很重！第二，这么玩儿，就没有扩展性可言了，如果某个queue负载很重，你加机器，新增的机器也包含了这个queue的所有数据，并没有办法线性扩展你的queue 那么怎么开启这个镜像集群模式呢？我这里简单说一下，避免面试人家问你你不知道，其实很简单rabbitmq有很好的管理控制台，就是在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候可以要求数据同步到所有节点的，也可以要求就同步到指定数量的节点，然后你再次创建queue的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。 （2）kafka的高可用性 kafka一个最基本的架构认识：多个broker组成，每个broker是一个节点；你创建一个topic，这个topic可以划分为多个partition，每个partition可以存在于不同的broker上，每个partition就放一部分数据。 这就是天然的分布式消息队列，就是说一个topic的数据，是分散放在多个机器上的，每个机器就放一部分数据。 实际上rabbitmq之类的，并不是分布式消息队列，他就是传统的消息队列，只不过提供了一些集群、HA的机制而已，因为无论怎么玩儿，rabbitmq一个queue的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个queue的完整数据。 kafka 0.8以前，是没有HA机制的，就是任何一个broker宕机了，那个broker上的partition就废了，没法写也没法读，没有什么高可用性可言。 kafka 0.8以后，提供了HA机制，就是replica副本机制。每个partition的数据都会同步到吉他机器上，形成自己的多个replica副本。然后所有replica会选举一个leader出来，那么生产和消费都跟这个leader打交道，然后其他replica就是follower。写的时候，leader会负责把数据同步到所有follower上去，读的时候就直接读leader上数据即可。只能读写leader？很简单，要是你可以随意读写每个follower，那么就要care数据一致性的问题，系统复杂度太高，很容易出问题。kafka会均匀的将一个partition的所有replica分布在不同的机器上，这样才可以提高容错性。 这么搞，就有所谓的高可用性了，因为如果某个broker宕机了，没事儿，那个broker上面的partition在其他机器上都有副本的，如果这上面有某个partition的leader，那么此时会重新选举一个新的leader出来，大家继续读写那个新的leader即可。这就有所谓的高可用性了。 写数据的时候，生产者就写leader，然后leader将数据落地写本地磁盘，接着其他follower自己主动从leader来pull数据。一旦所有follower同步好数据了，就会发送ack给leader，leader收到所有follower的ack之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为） 消费的时候，只会从leader去读，但是只有一个消息已经被所有follower都同步成功返回ack的时候，这个消息才会被消费者读到。 实际上这块机制，讲深了，是可以非常之深入的，但是我还是回到我们这个课程的主题和定位，聚焦面试，至少你听到这里大致明白了kafka是如何保证高可用机制的了，对吧？不至于一无所知，现场还能给面试官画画图。要遇上面试官确实是kafka高手，深挖了问，那你只能说不好意思，太深入的你没研究过。 但是大家一定要明白，这个事情是要权衡的，你现在是要快速突击常见面试题体系，而不是要深入学习kafka，要深入学习kafka，你是没那么多时间的。你只能确保，你之前也许压根儿不知道这块，但是现在你知道了，面试被问到，你大概可以说一说。然后很多其他的候选人，也许还不如你，没看过这个，被问到了压根儿答不出来，相比之下，你还能说点出来，大概就是这个意思了。 如何保证消息不被重复消费啊（如何保证消息消费时的幂等性）？ 2、面试官心里分析 其实这个很常见的一个问题，这俩问题基本可以连起来问。既然是消费消息，那肯定要考虑考虑会不会重复消费？能不能避免重复消费？或者重复消费了也别造成系统异常可以吗？这个是MQ领域的基本问题，其实本质上还是问你使用消息队列如何保证幂等性，这个是你架构里要考虑的一个问题。 面试官问你，肯定是必问的，这是你要考虑的实际生产上的系统设计问题。 3、面试题剖析 回答这个问题，首先你别听到重复消息这个事儿，就一无所知吧，你先大概说一说可能会有哪些重复消费的问题。 首先就是比如rabbitmq、rocketmq、kafka，都有可能会出现消费重复消费的问题，正常。因为这问题通常不是mq自己保证的，是给你保证的。然后我们挑一个kafka来举个例子，说说怎么重复消费吧。 kafka实际上有个offset的概念，就是每个消息写进去，都有一个offset，代表他的序号，然后consumer消费了数据之后，每隔一段时间，会把自己消费过的消息的offset提交一下，代表我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的offset来继续消费吧。 但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接kill进程了，再重启。这会导致consumer有些消息处理了，但是没来得及提交offset，尴尬了。重启之后，少数消息会再次消费一次。 其实重复消费不可怕，可怕的是你没考虑到重复消费之后，怎么保证幂等性。 给你举个例子吧。假设你有个系统，消费一条往数据库里插入一条，要是你一个消息重复两次，你不就插入了两条，这数据不就错了？但是你要是消费到第二次的时候，自己判断一下已经消费过了，直接扔了，不就保留了一条数据？ 一条数据重复出现两次，数据库里就只有一条数据，这就保证了系统的幂等性 幂等性，我通俗点说，就一个数据，或者一个请求，给你重复来多次，你得确保对应的数据是不会改变的，不能出错。 那所以第二个问题来了，怎么保证消息队列消费的幂等性？ 其实还是得结合业务来思考，我这里给几个思路： （1）比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update一下好吧 （2）比如你是写redis，那没问题了，反正每次都是set，天然幂等性 （3）比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的id，类似订单id之类的东西，然后你这里消费到了之后，先根据这个id去比如redis里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个id写redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。 还有比如基于数据库的唯一键来保证重复数据不会重复插入多条，我们之前线上系统就有这个问题，就是拿到数据的时候，每次重启可能会有重复，因为kafka消费者还没来得及提交offset，重复数据拿到了以后我们插入的时候，因为有唯一键约束了，所以重复数据只会插入报错，不会导致数据库中出现脏数据 如何保证MQ的消费是幂等性的，需要结合具体的业务来看 如何保证消息的可靠性传输（如何处理消息丢失的问题）？ 2、面试官心里分析 这个是肯定的，用mq有个基本原则，就是数据不能多一条，也不能少一条，不能多，就是刚才说的重复消费和幂等性问题。不能少，就是说这数据别搞丢了。那这个问题你必须得考虑一下。 如果说你这个是用mq来传递非常核心的消息，比如说计费，扣费的一些消息，因为我以前设计和研发过一个公司非常核心的广告平台，计费系统，计费系统是很重的一个业务，操作是很耗时的。所以说广告系统整体的架构里面，实际上是将计费做成异步化的，然后中间就是加了一个MQ。 我们当时为了确保说这个MQ传递过程中绝对不会把计费消息给弄丢，花了很多的精力。广告主投放了一个广告，明明说好了，用户点击一次扣费1块钱。结果要是用户动不动点击了一次，扣费的时候搞的消息丢了，我们公司就会不断的少几块钱，几块钱，积少成多，这个就对公司是一个很大的损失。 3、面试题剖析 这个丢数据，mq一般分为两种，要么是mq自己弄丢了，要么是我们消费的时候弄丢了。咱们从rabbitmq和kafka分别来分析一下吧 rabbitmq这种mq，一般来说都是承载公司的核心业务的，数据是绝对不能弄丢的 （1）rabbitmq 1）生产者弄丢了数据 生产者将数据发送到rabbitmq的时候，可能数据就在半路给搞丢了，因为网络啥的问题，都有可能。 此时可以选择用rabbitmq提供的事务功能，就是生产者发送数据之前开启rabbitmq事务（channel.txSelect），然后发送消息，如果消息没有成功被rabbitmq接收到，那么生产者会收到异常报错，此时就可以回滚事务（channel.txRollback），然后重试发送消息；如果收到了消息，那么可以提交事务（channel.txCommit）。但是问题是，rabbitmq事务机制一搞，基本上吞吐量会下来，因为太耗性能。 所以一般来说，如果你要确保说写rabbitmq的消息别丢，可以开启confirm模式，在生产者那里设置开启confirm模式之后，你每次写的消息都会分配一个唯一的id，然后如果写入了rabbitmq中，rabbitmq会给你回传一个ack消息，告诉你说这个消息ok了。如果rabbitmq没能处理这个消息，会回调你一个nack接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息id的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。 事务机制和cnofirm机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是confirm机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息rabbitmq接收了之后会异步回调你一个接口通知你这个消息接收到了。 所以一般在生产者这块避免数据丢失，都是用confirm机制的。 2）rabbitmq弄丢了数据 就是rabbitmq自己弄丢了数据，这个你必须开启rabbitmq的持久化，就是消息写入之后会持久化到磁盘，哪怕是rabbitmq自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，rabbitmq还没持久化，自己就挂了，可能导致少量数据会丢失的，但是这个概率较小。 设置持久化有两个步骤，第一个是创建queue的时候将其设置为持久化的，这样就可以保证rabbitmq持久化queue的元数据，但是不会持久化queue里的数据；第二个是发送消息的时候将消息的deliveryMode设置为2，就是将消息设置为持久化的，此时rabbitmq就会将消息持久化到磁盘上去。必须要同时设置这两个持久化才行，rabbitmq哪怕是挂了，再次重启，也会从磁盘上重启恢复queue，恢复这个queue里的数据。 而且持久化可以跟生产者那边的confirm机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者ack了，所以哪怕是在持久化到磁盘之前，rabbitmq挂了，数据丢了，生产者收不到ack，你也是可以自己重发的。 哪怕是你给rabbitmq开启了持久化机制，也有一种可能，就是这个消息写到了rabbitmq中，但是还没来得及持久化到磁盘上，结果不巧，此时rabbitmq挂了，就会导致内存里的一点点数据会丢失。 3）消费端弄丢了数据 rabbitmq如果丢失了数据，主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，rabbitmq认为你都消费了，这数据就丢了。 这个时候得用rabbitmq提供的ack机制，简单来说，就是你关闭rabbitmq自动ack，可以通过一个api来调用就行，然后每次你自己代码里确保处理完的时候，再程序里ack一把。这样的话，如果你还没处理完，不就没有ack？那rabbitmq就认为你还没处理完，这个时候rabbitmq会把这个消费分配给别的consumer去处理，消息是不会丢的。 （2）kafka 1）消费端弄丢了数据 唯一可能导致消费者弄丢数据的情况，就是说，你那个消费到了这个消息，然后消费者那边自动提交了offset，让kafka以为你已经消费好了这个消息，其实你刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。 这不是一样么，大家都知道kafka会自动提交offset，那么只要关闭自动提交offset，在处理完之后自己手动提交offset，就可以保证数据不会丢。但是此时确实还是会重复消费，比如你刚处理完，还没提交offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。 生产环境碰到的一个问题，就是说我们的kafka消费者消费到了数据之后是写到一个内存的queue里先缓冲一下，结果有的时候，你刚把消息写入内存queue，然后消费者会自动提交offset。 然后此时我们重启了系统，就会导致内存queue里还没来得及处理的数据就丢失了 2）kafka弄丢了数据 这块比较常见的一个场景，就是kafka某个broker宕机，然后重新选举partiton的leader时。大家想想，要是此时其他的follower刚好还有些数据没有同步，结果此时leader挂了，然后选举某个follower成leader之后，他不就少了一些数据？这就丢了一些数据啊。 生产环境也遇到过，我们也是，之前kafka的leader机器宕机了，将follower切换为leader之后，就会发现说这个数据就丢了 所以此时一般是要求起码设置如下4个参数： 给这个topic设置replication.factor参数：这个值必须大于1，要求每个partition必须有至少2个副本 在kafka服务端设置min.insync.replicas参数：这个值必须大于1，这个是要求一个leader至少感知到有至少一个follower还跟自己保持联系，没掉队，这样才能确保leader挂了还有一个follower吧 在producer端设置acks=all：这个是要求每条数据，必须是写入所有replica之后，才能认为是写成功了 在producer端设置retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了 我们生产环境就是按照上述要求配置的，这样配置之后，至少在kafka broker端就可以保证在leader所在broker发生故障，进行leader切换时，数据不会丢失 3）生产者会不会弄丢数据 如果按照上述的思路设置了ack=all，一定不会丢，要求是，你的leader接收到消息，所有的follower都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。 如何保证消息的顺序性？ 2、面试官心里分析 其实这个也是用MQ的时候必问的话题，第一看看你了解不了解顺序这个事儿？第二看看你有没有办法保证消息是有顺序的？这个生产系统中常见的问题。 3、面试题剖析 我举个例子，我们以前做过一个mysql binlog同步的系统，压力还是非常大的，日同步数据要达到上亿。mysql -> mysql，常见的一点在于说大数据team，就需要同步一个mysql库过来，对公司的业务系统的数据做各种复杂的操作。 你在mysql里增删改一条数据，对应出来了增删改3条binlog，接着这三条binlog发送到MQ里面，到消费出来依次执行，起码得保证人家是按照顺序来的吧？不然本来是：增加、修改、删除；你楞是换了顺序给执行成删除、修改、增加，不全错了么。 本来这个数据同步过来，应该最后这个数据被删除了；结果你搞错了这个顺序，最后这个数据保留下来了，数据同步就出错了。 先看看顺序会错乱的俩场景 （1）rabbitmq：一个queue，多个consumer，这不明显乱了 （2）kafka：一个topic，一个partition，一个consumer，内部多线程，这不也明显乱了 那如何保证消息的顺序性呢？简单简单 （1）rabbitmq：拆分多个queue，每个queue一个consumer，就是多一些queue而已，确实是麻烦点；或者就一个queue但是对应一个consumer，然后这个consumer内部用内存队列做排队，然后分发给底层不同的worker来处理 （2）kafka：一个topic，一个partition，一个consumer，内部单线程消费，写N个内存queue，然后N个线程分别消费一个内存queue即可 如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？ 2、面试官心里分析 你看这问法，其实本质针对的场景，都是说，可能你的消费端出了问题，不消费了，或者消费的极其极其慢。接着就坑爹了，可能你的消息队列集群的磁盘都快写满了，都没人消费，这个时候怎么办？或者是整个这就积压了几个小时，你这个时候怎么办？或者是你积压的时间太长了，导致比如rabbitmq设置了消息过期时间后就没了怎么办？ 所以就这事儿，其实线上挺常见的，一般不出，一出就是大case，一般常见于，举个例子，消费端每次消费之后要写mysql，结果mysql挂了，消费端hang那儿了，不动了。或者是消费端出了个什么叉子，导致消费速度极其慢。 3、面试题分析 关于这个事儿，我们一个一个来梳理吧，先假设一个场景，我们现在消费端出故障了，然后大量消息在mq里积压，现在事故了，慌了 （1）大量消息在mq里积压了几个小时了还没解决 几千万条数据在MQ里积压了七八个小时，从下午4点多，积压到了晚上很晚，10点多，11点多 这个是我们真实遇到过的一个场景，确实是线上故障了，这个时候要不然就是修复consumer的问题，让他恢复消费速度，然后傻傻的等待几个小时消费完毕。这个肯定不能在面试的时候说吧。 一个消费者一秒是1000条，一秒3个消费者是3000条，一分钟是18万条，1000多万条 所以如果你积压了几百万到上千万的数据，即使消费者恢复了，也需要大概1小时的时间才能恢复过来 一般这个时候，只能操作临时紧急扩容了，具体操作步骤和思路如下： 1）先修复consumer的问题，确保其恢复消费速度，然后将现有cnosumer都停掉 2）新建一个topic，partition是原来的10倍，临时建立好原先10倍或者20倍的queue数量 3）然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue 4）接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据 5）这种做法相当于是临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据 6）等快速消费完积压数据之后，得恢复原先部署架构，重新用原先的consumer机器来消费消息 （2）这里我们假设再来第二个坑 假设你用的是rabbitmq，rabbitmq是可以设置过期时间的，就是TTL，如果消息在queue中积压超过一定的时间就会被rabbitmq给清理掉，这个数据就没了。那这就是第二个坑了。这就不是说数据会大量积压在mq里，而是大量的数据会直接搞丢。 这个情况下，就不是说要增加consumer消费积压的消息，因为实际上没啥积压，而是丢了大量的消息。我们可以采取一个方案，就是批量重导，这个我们之前线上也有类似的场景干过。就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，比如大家一起喝咖啡熬夜到晚上12点以后，用户都睡觉了。 这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入mq里面去，把白天丢的数据给他补回来。也只能是这样了。 假设1万个订单积压在mq里面，没有处理，其中1000个订单都丢了，你只能手动写程序把那1000个订单给查出来，手动发到mq里去再补一次 （3）然后我们再来假设第三个坑 如果走的方式是消息积压在mq里，那么如果你很长时间都没处理掉，此时导致mq都快写满了，咋办？这个还有别的办法吗？没有，谁让你第一个方案执行的太慢了，你临时写程序，接入数据来消费，消费一个丢弃一个，都不要了，快速消费掉所有的消息。然后走第二个方案，到了晚上再补数据吧。 如果让你写一个消息队列，该如何进行架构设计啊？说一下你的思路 2、面试官心里分析 其实聊到这个问题，一般面试官要考察两块： （1）你有没有对某一个消息队列做过较为深入的原理的了解，或者从整体了解把握住一个mq的架构原理 （2）看看你的设计能力，给你一个常见的系统，就是消息队列系统，看看你能不能从全局把握一下整体架构设计，给出一些关键点出来 说实话，我一般面类似问题的时候，大部分人基本都会蒙，因为平时从来没有思考过类似的问题，大多数人就是平时埋头用，从来不去思考背后的一些东西。类似的问题，我经常问的还有，如果让你来设计一个spring框架你会怎么做？如果让你来设计一个dubbo框架你会怎么做？如果让你来设计一个mybatis框架你会怎么做？ 3、面试题剖析 其实回答这类问题，说白了，起码不求你看过那技术的源码，起码你大概知道那个技术的基本原理，核心组成部分，基本架构构成，然后参照一些开源的技术把一个系统设计出来的思路说一下就好 比如说这个消息队列系统，我们来从以下几个角度来考虑一下 （1）首先这个mq得支持可伸缩性吧，就是需要的时候快速扩容，就可以增加吞吐量和容量，那怎么搞？设计个分布式的系统呗，参照一下kafka的设计理念，broker -> topic -> partition，每个partition放一个机器，就存一部分数据。如果现在资源不够了，简单啊，给topic增加partition，然后做数据迁移，增加机器，不就可以存放更多数据，提供更高的吞吐量了？ （2）其次你得考虑一下这个mq的数据要不要落地磁盘吧？那肯定要了，落磁盘，才能保证别进程挂了数据就丢了。那落磁盘的时候怎么落啊？顺序写，这样就没有磁盘随机读写的寻址开销，磁盘顺序读写的性能是很高的，这就是kafka的思路。 （3）其次你考虑一下你的mq的可用性啊？这个事儿，具体参考我们之前可用性那个环节讲解的kafka的高可用保障机制。多副本 -> leader & follower -> broker挂了重新选举leader即可对外服务。 （4）能不能支持数据0丢失啊？可以的，参考我们之前说的那个kafka数据零丢失方案 其实一个mq肯定是很复杂的，面试官问你这个问题，其实是个开放题，他就是看看你有没有从架构角度整体构思和设计的思维以及能力。确实这个问题可以刷掉一大批人，因为大部分人平时不思考这些东西。 2 Redis 在项目中缓存是如何使用的？缓存如果使用不当会造成什么后果？ 2、面试官心里分析 这个问题，互联网公司必问，要是一个人连缓存都不太清楚，那确实比较尴尬 只要问到缓存，上来第一个问题，肯定能是先问问你项目哪里用了缓存？为啥要用？不用行不行？如果用了以后可能会有什么不良的后果？ 这就是看看你对你用缓存这个东西背后，有没有思考，如果你就是傻乎乎的瞎用，没法给面试官一个合理的解答。那我只能说，面试官对你印象肯定不太好，觉得你平时思考太少，就知道干活儿。 3、面试题剖析 一个一个来看 （1）在项目中缓存是如何使用的？ 这个，你结合你自己项目的业务来，你如果用了那恭喜你，你如果没用那不好意思，你硬加也得加一个场景吧 （2）为啥在项目里要用缓存呢？ 用缓存，主要是俩用途，高性能和高并发 1）高性能 假设这么个场景，你有个操作，一个请求过来，吭哧吭哧你各种乱七八糟操作mysql，半天查出来一个结果，耗时600ms。但是这个结果可能接下来几个小时都不会变了，或者变了也可以不用立即反馈给用户。那么此时咋办？ 缓存啊，折腾600ms查出来的结果，扔缓存里，一个key对应一个value，下次再有人查，别走mysql折腾600ms了。直接从缓存里，通过一个key查出来一个value，2ms搞定。性能提升300倍。 这就是所谓的高性能。 就是把你一些复杂操作耗时查出来的结果，如果确定后面不咋变了，然后但是马上还有很多读请求，那么直接结果放缓存，后面直接读缓存就好了。 2）高并发 mysql这么重的数据库，压根儿设计不是让你玩儿高并发的，虽然也可以玩儿，但是天然支持不好。mysql单机支撑到2000qps也开始容易报警了。 所以要是你有个系统，高峰期一秒钟过来的请求有1万，那一个mysql单机绝对会死掉。你这个时候就只能上缓存，把很多数据放缓存，别放mysql。缓存功能简单，说白了就是key-value式操作，单机支撑的并发量轻松一秒几万十几万，支撑高并发so easy。单机承载并发量是mysql单机的几十倍。 3）所以你要结合这俩场景考虑一下，你为啥要用缓存？ 一般很多同学项目里没啥高并发场景，那就别折腾了，直接用高性能那个场景吧，就思考有没有可以缓存结果的复杂查询场景，后续可以大幅度提升性能，优化用户体验，有，就说这个理由，没有？？那你也得编一个出来吧，不然你不是在搞笑么 （3）用了缓存之后会有啥不良的后果？ 呵呵。。。你要是没考虑过这个问题，那你就尴尬了，面试官会觉得你头脑简单，四肢也不发达。你别光是傻用一个东西，多考虑考虑背后的一些事儿。 常见的缓存问题有仨（当然其实有很多，我这里就说仨，你能说出来也可以了） 1）缓存与数据库双写不一致 2）缓存雪崩 3）缓存穿透 4）缓存并发竞争 这仨问题是常见面试题，后面我要讲，大家看到后面自然就知道了，但是人要是问你，你至少自己能说出来，并且给出对应的解决方案 redis和memcached有什么区别？redis的线程模型是什么？为什么单线程的redis比多线程的memcached效率要高得多（为什么redis是单线程的但是还可以支撑高并发）？ https://www.jianshu.com/p/6264fa82ac33 2、面试官心里分析 这个是问redis的时候，最基本的问题吧，redis最基本的一个内部原理和特点，就是redis实际上是个单线程工作模型，你要是这个都不知道，那后面玩儿redis的时候，出了问题岂不是什么都不知道？ 还有可能面试官会问问你redis和memcached的区别，不过说实话，最近这两年，我作为面试官都不太喜欢这么问了，memched是早些年各大互联网公司常用的缓存方案，但是现在近几年基本都是redis，没什么公司用memcached了 3、额外的友情提示 同学，你要是现在还不知道redis和memcached是啥？那你赶紧百度一下redis入门和memcahced入门，简单启动一下，然后试一下几个简单操作，先感受一下。接着回来继续听课，我觉得1小时以内你就搞定了。 另外一个友情提示，要听明白redis的线程模型，你需要了解socket网络相关的基本知识，如果不懂。。。那我觉得你java没学好吧。初学者都该学习java的socket网络通信相关知识的。。。 4、面试题剖析 （1）redis和memcached有啥区别 这个事儿吧，你可以比较出N多个区别来，但是我还是采取redis作者给出的几个比较吧 1）Redis支持服务器端的数据操作：Redis相比Memcached来说，拥有更多的数据结构和并支持更丰富的数据操作，通常在Memcached里，你需要将数据拿到客户端来进行类似的修改再set回去。这大大增加了网络IO的次数和数据体积。在Redis中，这些复杂的操作通常和一般的GET/SET一样高效。所以，如果需要缓存能够支持更复杂的结构和操作，那么Redis会是不错的选择。 2）内存使用效率对比：使用简单的key-value存储的话，Memcached的内存利用率更高，而如果Redis采用hash结构来做key-value存储，由于其组合式的压缩，其内存利用率会高于Memcached。 3）性能对比：由于Redis只使用单核，而Memcached可以使用多核，所以平均每一个核上Redis在存储小数据时比Memcached性能更高。而在100k以上的数据中，Memcached性能要高于Redis，虽然Redis最近也在存储大数据的性能上进行优化，但是比起Memcached，还是稍有逊色。 4）集群模式：memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是redis目前是原生支持cluster模式的，redis官方就是支持redis cluster集群模式的，比memcached来说要更好 （2）redis的线程模型 1）文件事件处理器 redis基于reactor模式开发了网络事件处理器，这个处理器叫做文件事件处理器，file event handler。这个文件事件处理器，是单线程的，redis才叫做单线程的模型，采用IO多路复用机制同时监听多个socket，根据socket上的事件来选择对应的事件处理器来处理这个事件。 如果被监听的socket准备好执行accept、read、write、close等操作的时候，跟操作对应的文件事件就会产生，这个时候文件事件处理器就会调用之前关联好的事件处理器来处理这个事件。 文件事件处理器是单线程模式运行的，但是通过IO多路复用机制监听多个socket，可以实现高性能的网络通信模型，又可以跟内部其他单线程的模块进行对接，保证了redis内部的线程模型的简单性。 文件事件处理器的结构包含4个部分：多个socket，IO多路复用程序，文件事件分派器，事件处理器（命令请求处理器、命令回复处理器、连接应答处理器，等等）。 多个socket可能并发的产生不同的操作，每个操作对应不同的文件事件，但是IO多路复用程序会监听多个socket，但是会将socket放入一个队列中排队，每次从队列中取出一个socket给事件分派器，事件分派器把socket给对应的事件处理器。 然后一个socket的事件处理完之后，IO多路复用程序才会将队列中的下一个socket给事件分派器。文件事件分派器会根据每个socket当前产生的事件，来选择对应的事件处理器来处理。 2）文件事件 当socket变得可读时（比如客户端对redis执行write操作，或者close操作），或者有新的可以应答的sccket出现时（客户端对redis执行connect操作），socket就会产生一个AE_READABLE事件。 当socket变得可写的时候（客户端对redis执行read操作），socket会产生一个AE_WRITABLE事件。 IO多路复用程序可以同时监听AE_REABLE和AE_WRITABLE两种事件，要是一个socket同时产生了AE_READABLE和AE_WRITABLE两种事件，那么文件事件分派器优先处理AE_REABLE事件，然后才是AE_WRITABLE事件。 3）文件事件处理器 如果是客户端要连接redis，那么会为socket关联连接应答处理器 如果是客户端要写数据到redis，那么会为socket关联命令请求处理器 如果是客户端要从redis读数据，那么会为socket关联命令回复处理器 4）客户端与redis通信的一次流程 在redis启动初始化的时候，redis会将连接应答处理器跟AE_READABLE事件关联起来，接着如果一个客户端跟redis发起连接，此时会产生一个AE_READABLE事件，然后由连接应答处理器来处理跟客户端建立连接，创建客户端对应的socket，同时将这个socket的AE_READABLE事件跟命令请求处理器关联起来。 当客户端向redis发起请求的时候（不管是读请求还是写请求，都一样），首先就会在socket产生一个AE_READABLE事件，然后由对应的命令请求处理器来处理。这个命令请求处理器就会从socket中读取请求相关数据，然后进行执行和处理。 接着redis这边准备好了给客户端的响应数据之后，就会将socket的AE_WRITABLE事件跟命令回复处理器关联起来，当客户端这边准备好读取响应数据时，就会在socket上产生一个AE_WRITABLE事件，会由对应的命令回复处理器来处理，就是将准备好的响应数据写入socket，供客户端来读取。 命令回复处理器写完之后，就会删除这个socket的AE_WRITABLE事件和命令回复处理器的关联关系。 （3）为啥redis单线程模型也能效率这么高？ 1）纯内存操作 2）核心是基于非阻塞的IO多路复用机制 3）单线程反而避免了多线程的频繁上下文切换问题（百度） redis都有哪些数据类型？分别在哪些场景下使用比较合适？ 2、面试官心里分析 除非是我感觉看你简历，就是工作3年以内的比较初级的一个同学，可能对技术没有很深入的研究过，我才会问这类问题，在宝贵的面试时间里，我实在是不想多问 其实问这个问题呢。。。主要就俩原因 第一，看看你到底有没有全面的了解redis有哪些功能，一般怎么来用，啥场景用什么，就怕你别就会最简单的kv操作 第二，看看你在实际项目里都怎么玩儿过redis 要是你回答的不好，没说出几种数据类型，也没说什么场景，你完了，面试官对你印象肯定不好，觉得你平时就是做个简单的set和get。 3、面试题剖析 （1）string 这是最基本的类型了，没啥可说的，就是普通的set和get，做简单的kv缓存 （2）hash 这个是类似map的一种结构，这个一般就是可以将结构化的数据，比如一个对象（前提是这个对象没嵌套其他的对象）给缓存在redis里，然后每次读写缓存的时候，可以就操作hash里的某个字段。 key=150 value={ “id”: 150, “name”: “zhangsan”, “age”: 20 } hash类的数据结构，主要是用来存放一些对象，把一些简单的对象给缓存起来，后续操作的时候，你可以直接仅仅修改这个对象中的某个字段的值 value={ “id”: 150, “name”: “zhangsan”, “age”: 21 } （3）list 有序列表，这个是可以玩儿出很多花样的 微博，某个大v的粉丝，就可以以list的格式放在redis里去缓存 key=某大v value=[zhangsan, lisi, wangwu] 比如可以通过list存储一些列表型的数据结构，类似粉丝列表了、文章的评论列表了之类的东西 比如可以通过lrange命令，就是从某个元素开始读取多少个元素，可以基于list实现分页查询，这个很棒的一个功能，基于redis实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西，性能高，就一页一页走 比如可以搞个简单的消息队列，从list头怼进去，从list尾巴那里弄出来 （4）set 无序集合，自动去重 直接基于set将系统里需要去重的数据扔进去，自动就给去重了，如果你需要对一些数据进行快速的全局去重，你当然也可以基于jvm内存里的HashSet进行去重，但是如果你的某个系统部署在多台机器上呢？ 得基于redis进行全局的set去重 可以基于set玩儿交集、并集、差集的操作，比如交集吧，可以把两个人的粉丝列表整一个交集，看看俩人的共同好友是谁？对吧 把两个大v的粉丝都放在两个set中，对两个set做交集 （5）sorted set 排序的set，去重但是可以排序，写进去的时候给一个分数，自动根据分数排序，这个可以玩儿很多的花样，最大的特点是有个分数可以自定义排序规则 比如说你要是想根据时间对数据排序，那么可以写入进去的时候用某个时间作为分数，人家自动给你按照时间排序了 排行榜：将每个用户以及其对应的什么分数写入进去，zadd board score username，接着zrevrange board 0 99，就可以获取排名前100的用户；zrank board username，可以看到用户在排行榜里的排名 zadd board 85 zhangsan zadd board 72 wangwu zadd board 96 lisi zadd board 62 zhaoliu 96 lisi 85 zhangsan 72 wangwu 62 zhaoliu zrevrange board 0 3 获取排名前3的用户 96 lisi 85 zhangsan 72 wangwu zrank board zhaoliu redis的过期策略都有哪些？内存淘汰机制都有哪些？手写一下LRU代码实现？ 2、面试官心里分析 1）老师啊，我往redis里写的数据怎么没了？ 之前有同学问过我，说我们生产环境的redis怎么经常会丢掉一些数据？写进去了，过一会儿可能就没了。我的天，同学，你问这个问题就说明redis你就没用对啊。redis是缓存，你给当存储了是吧？ 啥叫缓存？用内存当缓存。内存是无限的吗，内存是很宝贵而且是有限的，磁盘是廉价而且是大量的。可能一台机器就几十个G的内存，但是可以有几个T的硬盘空间。redis主要是基于内存来进行高性能、高并发的读写操作的。 那既然内存是有限的，比如redis就只能用10个G，你要是往里面写了20个G的数据，会咋办？当然会干掉10个G的数据，然后就保留10个G的数据了。那干掉哪些数据？保留哪些数据？当然是干掉不常用的数据，保留常用的数据了。 所以说，这是缓存的一个最基本的概念，数据是会过期的，要么是你自己设置个过期时间，要么是redis自己给干掉。 set key value 过期时间（1小时） set进去的key，1小时之后就没了，就失效了 2）老师，我的数据明明都过期了，怎么还占用着内存啊？ 还有一种就是如果你设置好了一个过期时间，你知道redis是怎么给你弄成过期的吗？什么时候删除掉？如果你不知道，之前有个学员就问了，为啥好多数据明明应该过期了，结果发现redis内存占用还是很高？那是因为你不知道redis是怎么删除那些过期key的。 redis 内存一共是10g，你现在往里面写了5g的数据，结果这些数据明明你都设置了过期时间，要求这些数据1小时之后都会过期，结果1小时之后，你回来一看，redis机器，怎么内存占用还是50%呢？5g数据过期了，我从redis里查，是查不到了，结果过期的数据还占用着redis的内存。 如果你连这个问题都不知道，上来就懵了，回答不出来，那线上你写代码的时候，想当然的认为写进redis的数据就一定会存在，后面导致系统各种漏洞和bug，谁来负责？ 3、面试题剖析 （1）设置过期时间 我们set key的时候，都可以给一个expire time，就是过期时间，指定这个key比如说只能存活1个小时？10分钟？这个很有用，我们自己可以指定缓存到期就失效。 如果假设你设置一个一批key只能存活1个小时，那么接下来1小时后，redis是怎么对这批key进行删除的？ 答案是：定期删除+惰性删除 所谓定期删除，指的是redis默认是每隔100ms就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除。假设redis里放了10万个key，都设置了过期时间，你每隔几百毫秒，就检查10万个key，那redis基本上就死了，cpu负载会很高的，消耗在你的检查过期key上了。注意，这里可不是每隔100ms就遍历所有的设置过期时间的key，那样就是一场性能上的灾难。实际上redis是每隔100ms随机抽取一些key来检查和删除的。 但是问题是，定期删除可能会导致很多过期key到了时间并没有被删除掉，那咋整呢？所以就是惰性删除了。这就是说，在你获取某个key的时候，redis会检查一下 ，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除，不会给你返回任何东西。 并不是key到时间就被删除掉，而是你查询这个key的时候，redis再懒惰的检查一下 通过上述两种手段结合起来，保证过期的key一定会被干掉。 很简单，就是说，你的过期key，靠定期删除没有被删除掉，还停留在内存里，占用着你的内存呢，除非你的系统去查一下那个key，才会被redis给删除掉。 但是实际上这还是有问题的，如果定期删除漏掉了很多过期key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期key堆积在内存里，导致redis内存块耗尽了，咋整？ 答案是：走内存淘汰机制。 （2）内存淘汰 如果redis的内存占用过多的时候，此时会进行内存淘汰，有如下一些策略： redis 10个key，现在已经满了，redis需要删除掉5个key 1个key，最近1分钟被查询了100次 1个key，最近10分钟被查询了50次 1个key，最近1个小时倍查询了1次 1）noeviction：当内存不足以容纳新写入数据时，新写入操作会报错，这个一般没人用吧，实在是太恶心了 2）allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key（这个是最常用的） 3）allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key，这个一般没人用吧，为啥要随机，肯定是把最近最少使用的key给干掉啊 4）volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key（这个一般不太合适） 5）volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key 6）volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除 百度，问题啊，网上鱼龙混杂 如果百度一些api操作，入门的知识，ok的，随便找一个博客都可以 一些高级别的，redis单线程模型 很简单，你写的数据太多，内存满了，或者触发了什么条件，redis lru，自动给你清理掉了一些最近很少使用的数据 （3）要不你手写一个LRU算法？ https://blog.csdn.net/nakiri_arisu/article/details/79205660 我确实有时会问这个，因为有些候选人如果确实过五关斩六将，前面的问题都答的很好，那么其实让他写一下LRU算法，可以考察一下编码功底 你可以现场手写最原始的LRU算法，那个代码量太大了，我觉得不太现实 public class LRUCache extends LinkedHashMap { private final int CACHE_SIZE; // 这里就是传递进来最多能缓存多少数据 public LRUCache(int cacheSize) { super((int) Math.ceil(cacheSize / 0.75) + 1, 0.75f, true); // 这块就是设置一个hashmap的初始大小，同时最后一个true指的是让linkedhashmap按照访问顺序来进行排序，最近访问的放在头，最老访问的就在尾 CACHE_SIZE = cacheSize; } @Override protected boolean removeEldestEntry(Map.Entry eldest) { return size() > CACHE_SIZE; // 这个意思就是说当map中的数据量大于指定的缓存个数的时候，就自动删除最老的数据 } } 我给你看上面的代码，是告诉你最起码你也得写出来上面那种代码，不求自己纯手工从底层开始打造出自己的LRU，但是起码知道如何利用已有的jdk数据结构实现一个java版的LRU 如何保证Redis的高并发和高可用？redis的主从复制原理能介绍一下么？redis的哨兵原理能介绍一下么？ 1、redis高并发跟整个系统的高并发之间的关系 redis，你要搞高并发的话，不可避免，要把底层的缓存搞得很好 mysql，高并发，做到了，那么也是通过一系列复杂的分库分表，订单系统，事务要求的，QPS到几万，比较高了 要做一些电商的商品详情页，真正的超高并发，QPS上十万，甚至是百万，一秒钟百万的请求量 光是redis是不够的，但是redis是整个大型的缓存架构中，支撑高并发的架构里面，非常重要的一个环节 首先，你的底层的缓存中间件，缓存系统，必须能够支撑的起我们说的那种高并发，其次，再经过良好的整体的缓存架构的设计（多级缓存架构、热点缓存），支撑真正的上十万，甚至上百万的高并发 2、redis不能支撑高并发的瓶颈在哪里？ 单机 3、如果redis要支撑超过10万+的并发，那应该怎么做？ 单机的redis几乎不太可能说QPS超过10万+，除非一些特殊情况，比如你的机器性能特别好，配置特别高，物理机，维护做的特别好，而且你的整体的操作不是太复杂 单机在几万 读写分离，一般来说，对缓存，一般都是用来支撑读高并发的，写的请求是比较少的，可能写请求也就一秒钟几千，一两千 大量的请求都是读，一秒钟二十万次读 读写分离 主从架构 -> 读写分离 -> 支撑10万+读QPS的架构 4、接下来要讲解的一个topic redis replication redis主从架构 -> 读写分离架构 -> 可支持水平扩展的读高并发架构 课程大纲 1、图解redis replication基本原理 2、redis replication的核心机制 3、master持久化对于主从架构的安全保障的意义 redis replication -> 主从架构 -> 读写分离 -> 水平扩容支撑读高并发 redis replication的最最基本的原理，铺垫 1、图解redis replication基本原理 2、redis replication的核心机制 （1）redis采用异步方式复制数据到slave节点，不过redis 2.8开始，slave node会周期性地确认自己每次复制的数据量 （2）一个master node是可以配置多个slave node的 （3）slave node也可以连接其他的slave node （4）slave node做复制的时候，是不会block master node的正常工作的 （5）slave node在做复制的时候，也不会block对自己的查询操作，它会用旧的数据集来提供服务; 但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了 （6）slave node主要用来进行横向扩容，做读写分离，扩容的slave node可以提高读的吞吐量 slave，高可用性，有很大的关系 3、master持久化对于主从架构的安全保障的意义 如果采用了主从架构，那么建议必须开启master node的持久化！ 不建议用slave node作为master node的数据热备，因为那样的话，如果你关掉master的持久化，可能在master宕机重启的时候数据是空的，然后可能一经过复制，salve node数据也丢了 master -> RDB和AOF都关闭了 -> 全部在内存中 master宕机，重启，是没有本地数据可以恢复的，然后就会直接认为自己IDE数据是空的 master就会将空的数据集同步到slave上去，所有slave的数据全部清空 100%的数据丢失 master节点，必须要使用持久化机制 第二个，master的各种备份方案，要不要做，万一说本地的所有文件丢失了; 从备份中挑选一份rdb去恢复master; 这样才能确保master启动的时候，是有数据的 即使采用了后续讲解的高可用机制，slave node可以自动接管master node，但是也可能sentinal还没有检测到master failure，master node就自动重启了，还是可能导致上面的所有slave node数据清空故障 课程大纲 1、主从架构的核心原理 当启动一个slave node的时候，它会发送一个PSYNC命令给master node 如果这是slave node重新连接master node，那么master node仅仅会复制给slave部分缺少的数据; 否则如果是slave node第一次连接master node，那么会触发一次full resynchronization 开始full resynchronization的时候，master会启动一个后台线程，开始生成一份RDB快照文件，同时还会将从客户端收到的所有写命令缓存在内存中。RDB文件生成完毕之后，master会将这个RDB发送给slave，slave会先写入本地磁盘，然后再从本地磁盘加载到内存中。然后master会将内存中缓存的写命令发送给slave，slave也会同步这些数据。 slave node如果跟master node有网络故障，断开了连接，会自动重连。master如果发现有多个slave node都来重新连接，仅仅会启动一个rdb save操作，用一份数据服务所有slave node。 2、主从复制的断点续传 从redis 2.8开始，就支持主从复制的断点续传，如果主从复制过程中，网络连接断掉了，那么可以接着上次复制的地方，继续复制下去，而不是从头开始复制一份 master node会在内存中常见一个backlog，master和slave都会保存一个replica offset还有一个master id，offset就是保存在backlog中的。如果master和slave网络连接断掉了，slave会让master从上次的replica offset开始继续复制 但是如果没有找到对应的offset，那么就会执行一次resynchronization 3、无磁盘化复制 master在内存中直接创建rdb，然后发送给slave，不会在自己本地落地磁盘了 repl-diskless-sync repl-diskless-sync-delay，等待一定时长再开始复制，因为要等更多slave重新连接过来 4、过期key处理 slave不会过期key，只会等待master过期key。如果master过期了一个key，或者通过LRU淘汰了一个key，那么会模拟一条del命令发送给slave。 1、复制的完整流程 （1）slave node启动，仅仅保存master node的信息，包括master node的host和ip，但是复制流程没开始 master host和ip是从哪儿来的，redis.conf里面的slaveof配置的 （2）slave node内部有个定时任务，每秒检查是否有新的master node要连接和复制，如果发现，就跟master node建立socket网络连接 （3）slave node发送ping命令给master node （4）口令认证，如果master设置了requirepass，那么salve node必须发送masterauth的口令过去进行认证 （5）master node第一次执行全量复制，将所有数据发给slave node （6）master node后续持续将写命令，异步复制给slave node 2、数据同步相关的核心机制 指的就是第一次slave连接msater的时候，执行的全量复制，那个过程里面你的一些细节的机制 （1）master和slave都会维护一个offset master会在自身不断累加offset，slave也会在自身不断累加offset slave每秒都会上报自己的offset给master，同时master也会保存每个slave的offset 这个倒不是说特定就用在全量复制的，主要是master和slave都要知道各自的数据的offset，才能知道互相之间的数据不一致的情况 （2）backlog master node有一个backlog，默认是1MB大小 master node给slave node复制数据时，也会将数据在backlog中同步写一份 backlog主要是用来做全量复制中断候的增量复制的 （3）master run id info server，可以看到master run id 如果根据host+ip定位master node，是不靠谱的，如果master node重启或者数据出现了变化，那么slave node应该根据不同的run id区分，run id不同就做全量复制 如果需要不更改run id重启redis，可以使用redis-cli debug reload命令 （4）psync 从节点使用psync从master node进行复制，psync runid offset master node会根据自身的情况返回响应信息，可能是FULLRESYNC runid offset触发全量复制，可能是CONTINUE触发增量复制 3、全量复制 （1）master执行bgsave，在本地生成一份rdb快照文件 （2）master node将rdb快照文件发送给salve node，如果rdb复制时间超过60秒（repl-timeout），那么slave node就会认为复制失败，可以适当调节大这个参数 （3）对于千兆网卡的机器，一般每秒传输100MB，6G文件，很可能超过60s （4）master node在生成rdb时，会将所有新的写命令缓存在内存中，在salve node保存了rdb之后，再将新的写命令复制给salve node （5）client-output-buffer-limit slave 256MB 64MB 60，如果在复制期间，内存缓冲区持续消耗超过64MB，或者一次性超过256MB，那么停止复制，复制失败 （6）slave node接收到rdb之后，清空自己的旧数据，然后重新加载rdb到自己的内存中，同时基于旧的数据版本对外提供服务 （7）如果slave node开启了AOF，那么会立即执行BGREWRITEAOF，重写AOF rdb生成、rdb通过网络拷贝、slave旧数据的清理、slave aof rewrite，很耗费时间 如果复制的数据量在4G~6G之间，那么很可能全量复制时间消耗到1分半到2分钟 4、增量复制 （1）如果全量复制过程中，master-slave网络连接断掉，那么salve重新连接master时，会触发增量复制 （2）master直接从自己的backlog中获取部分丢失的数据，发送给slave node，默认backlog就是1MB （3）msater就是根据slave发送的psync中的offset来从backlog中获取数据的 5、heartbeat 主从节点互相都会发送heartbeat信息 master默认每隔10秒发送一次heartbeat，salve node每隔1秒发送一个heartbeat 6、异步复制 master每次接收到写命令之后，现在内部写入数据，然后异步发送给slave node 1、哨兵的介绍 sentinal，中文名是哨兵 哨兵是redis集群架构中非常重要的一个组件，主要功能如下 （1）集群监控，负责监控redis master和slave进程是否正常工作 （2）消息通知，如果某个redis实例有故障，那么哨兵负责发送消息作为报警通知给管理员 （3）故障转移，如果master node挂掉了，会自动转移到slave node上 （4）配置中心，如果故障转移发生了，通知client客户端新的master地址 哨兵本身也是分布式的，作为一个哨兵集群去运行，互相协同工作 （1）故障转移时，判断一个master node是宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题 （2）即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就很坑爹了 目前采用的是sentinal 2版本，sentinal 2相对于sentinal 1来说，重写了很多代码，主要是让故障转移的机制和算法变得更加健壮和简单 2、哨兵的核心知识 （1）哨兵至少需要3个实例，来保证自己的健壮性 （2）哨兵 + redis主从的部署架构，是不会保证数据零丢失的，只能保证redis集群的高可用性 （3）对于哨兵 + redis主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练 3、为什么redis哨兵集群只有2个节点无法正常工作？ 哨兵集群必须部署2个以上节点 如果哨兵集群仅仅部署了个2个哨兵实例，quorum=1 +----+ +----+ | M1 |---------| R1 | | S1 | | S2 | +----+ +----+ Configuration: quorum = 1 master宕机，s1和s2中只要有1个哨兵认为master宕机就可以还行切换，同时s1和s2中会选举出一个哨兵来执行故障转移 同时这个时候，需要majority，也就是大多数哨兵都是运行的，2个哨兵的majority就是2（2的majority=2，3的majority=2，5的majority=3，4的majority=2），2个哨兵都运行着，就可以允许执行故障转移 但是如果整个M1和S1运行的机器宕机了，那么哨兵只有1个了，此时就没有majority来允许执行故障转移，虽然另外一台机器还有一个R1，但是故障转移不会执行 4、经典的3节点哨兵集群 +----+ | M1 | | S1 | +----+ | +----+ | +----+ | R2 |----+----| R3 | | S2 | | S3 | +----+ +----+ Configuration: quorum = 2，majority 如果M1所在机器宕机了，那么三个哨兵还剩下2个，S2和S3可以一致认为master宕机，然后选举出一个来执行故障转移 同时3个哨兵的majority是2，所以还剩下的2个哨兵运行着，就可以允许执行故障转移 课程大纲 1、两种数据丢失的情况 2、解决异步复制和脑裂导致的数据丢失 1、两种数据丢失的情况 主备切换的过程，可能会导致数据丢失 （1）异步复制导致的数据丢失 因为master -> slave的复制是异步的，所以可能有部分数据还没复制到slave，master就宕机了，此时这些部分数据就丢失了 （2）脑裂导致的数据丢失 脑裂，也就是说，某个master所在机器突然脱离了正常的网络，跟其他slave机器不能连接，但是实际上master还运行着 此时哨兵可能就会认为master宕机了，然后开启选举，将其他slave切换成了master 这个时候，集群里就会有两个master，也就是所谓的脑裂 此时虽然某个slave被切换成了master，但是可能client还没来得及切换到新的master，还继续写向旧master的数据可能也丢失了 因此旧master再次恢复的时候，会被作为一个slave挂到新的master上去，自己的数据会清空，重新从新的master复制数据 2、解决异步复制和脑裂导致的数据丢失 min-slaves-to-write 1 min-slaves-max-lag 10 要求至少有1个slave，数据复制和同步的延迟不能超过10秒 如果说一旦所有的slave，数据复制和同步的延迟都超过了10秒钟，那么这个时候，master就不会再接收任何请求了 上面两个配置可以减少异步复制和脑裂导致的数据丢失 （1）减少异步复制的数据丢失 有了min-slaves-max-lag这个配置，就可以确保说，一旦slave复制数据和ack延时太长，就认为可能master宕机后损失的数据太多了，那么就拒绝写请求，这样可以把master宕机时由于部分数据未同步到slave导致的数据丢失降低的可控范围内 （2）减少脑裂的数据丢失 如果一个master出现了脑裂，跟其他slave丢了连接，那么上面两个配置可以确保说，如果不能继续给指定数量的slave发送数据，而且slave超过10秒没有给自己ack消息，那么就直接拒绝客户端的写请求 这样脑裂后的旧master就不会接受client的新数据，也就避免了数据丢失 上面的配置就确保了，如果跟任何一个slave丢了连接，在10秒后发现没有slave给自己ack，那么就拒绝新的写请求 因此在脑裂场景下，最多就丢失10秒的数据 1、sdown和odown转换机制 sdown和odown两种失败状态 sdown是主观宕机，就一个哨兵如果自己觉得一个master宕机了，那么就是主观宕机 odown是客观宕机，如果quorum数量的哨兵都觉得一个master宕机了，那么就是客观宕机 sdown达成的条件很简单，如果一个哨兵ping一个master，超过了is-master-down-after-milliseconds指定的毫秒数之后，就主观认为master宕机 sdown到odown转换的条件很简单，如果一个哨兵在指定时间内，收到了quorum指定数量的其他哨兵也认为那个master是sdown了，那么就认为是odown了，客观认为master宕机 2、哨兵集群的自动发现机制 哨兵互相之间的发现，是通过redis的pub/sub系统实现的，每个哨兵都会往sentinel:hello这个channel里发送一个消息，这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在 每隔两秒钟，每个哨兵都会往自己监控的某个master+slaves对应的sentinel:hello channel里发送一个消息，内容是自己的host、ip和runid还有对这个master的监控配置 每个哨兵也会去监听自己监控的每个master+slaves对应的sentinel:hello channel，然后去感知到同样在监听这个master+slaves的其他哨兵的存在 每个哨兵还会跟其他哨兵交换对master的监控配置，互相进行监控配置的同步 3、slave配置的自动纠正 哨兵会负责自动纠正slave的一些配置，比如slave如果要成为潜在的master候选人，哨兵会确保slave在复制现有master的数据; 如果slave连接到了一个错误的master上，比如故障转移之后，那么哨兵会确保它们连接到正确的master上 4、slave->master选举算法 如果一个master被认为odown了，而且majority哨兵都允许了主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个slave来 会考虑slave的一些信息 （1）跟master断开连接的时长 （2）slave优先级 （3）复制offset （4）run id 如果一个slave跟master断开连接已经超过了down-after-milliseconds的10倍，外加master宕机的时长，那么slave就被认为不适合选举为master (down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state 接下来会对slave进行排序 （1）按照slave优先级进行排序，slave priority越低，优先级就越高 （2）如果slave priority相同，那么看replica offset，哪个slave复制了越多的数据，offset越靠后，优先级就越高 （3）如果上面两个条件都相同，那么选择一个run id比较小的那个slave 5、quorum和majority 每次一个哨兵要做主备切换，首先需要quorum数量的哨兵认为odown，然后选举出一个哨兵来做切换，这个哨兵还得得到majority哨兵的授权，才能正式执行切换 如果quorum 但是如果quorum >= majority，那么必须quorum数量的哨兵都授权，比如5个哨兵，quorum是5，那么必须5个哨兵都同意授权，才能执行切换 6、configuration epoch 哨兵会对一套redis master+slave进行监控，有相应的监控的配置 执行切换的那个哨兵，会从要切换到的新master（salve->master）那里得到一个configuration epoch，这就是一个version号，每次切换的version号都必须是唯一的 如果第一个选举出的哨兵切换失败了，那么其他哨兵，会等待failover-timeout时间，然后接替继续执行切换，此时会重新获取一个新的configuration epoch，作为新的version号 7、configuraiton传播 哨兵完成切换之后，会在自己本地更新生成最新的master配置，然后同步给其他的哨兵，就是通过之前说的pub/sub消息机制 这里之前的version号就很重要了，因为各种消息都是通过一个channel去发布和监听的，所以一个哨兵完成一次新的切换之后，新的master配置是跟着新的version号的 其他的哨兵都是根据版本号的大小来更新自己的master配置的 redis的持久化有哪几种方式？不同的持久化机制都有什么优缺点？持久化机制具体底层是如何实现的？ 课程大纲 1、故障发生的时候会怎么样 2、如何应对故障的发生 很多同学，自己也看过一些redis的资料和书籍，当然可能也看过一些redis视频课程 所有的资料，其实都会讲解redis持久化，但是有个问题，我到目前为止，没有看到有人很仔细的去讲解，redis的持久化意义 redis的持久化，RDB，AOF，区别，各自的特点是什么，适合什么场景 redis的企业级的持久化方案是什么，是用来跟哪些企业级的场景结合起来使用的？？？ redis持久化的意义，在于故障恢复 比如你部署了一个redis，作为cache缓存，当然也可以保存一些较为重要的数据 如果没有持久化的话，redis遇到灾难性故障的时候，就会丢失所有的数据 如果通过持久化将数据搞一份儿在磁盘上去，然后定期比如说同步和备份到一些云存储服务上去，那么就可以保证数据不丢失全部，还是可以恢复一部分数据回来的 课程大纲 1、RDB和AOF两种持久化机制的介绍 2、RDB持久化机制的优点 3、RDB持久化机制的缺点 4、AOF持久化机制的优点 5、AOF持久化机制的缺点 6、RDB和AOF到底该如何选择 我们已经知道对于一个企业级的redis架构来说，持久化是不可减少的 企业级redis集群架构：海量数据、高并发、高可用 持久化主要是做灾难恢复，数据恢复，也可以归类到高可用的一个环节里面去 比如你redis整个挂了，然后redis就不可用了，你要做的事情是让redis变得可用，尽快变得可用 重启redis，尽快让它对外提供服务，但是就像上一讲说，如果你没做数据备份，这个时候redis启动了，也不可用啊，数据都没了 很可能说，大量的请求过来，缓存全部无法命中，在redis里根本找不到数据，这个时候就死定了，缓存雪崩问题，所有请求，没有在redis命中，就会去mysql数据库这种数据源头中去找，一下子mysql承接高并发，然后就挂了 mysql挂掉，你都没法去找数据恢复到redis里面去，redis的数据从哪儿来？从mysql来。。。 具体的完整的缓存雪崩的场景，还有企业级的解决方案，到后面讲 如果你把redis的持久化做好，备份和恢复方案做到企业级的程度，那么即使你的redis故障了，也可以通过备份数据，快速恢复，一旦恢复立即对外提供服务 redis的持久化，跟高可用，是有关系的，企业级redis架构中去讲解 redis持久化：RDB，AOF 1、RDB和AOF两种持久化机制的介绍 RDB持久化机制，对redis中的数据执行周期性的持久化 AOF机制对每条写入命令作为日志，以append-only的模式写入一个日志文件中，在redis重启的时候，可以通过回放AOF日志中的写入指令来重新构建整个数据集 如果我们想要redis仅仅作为纯内存的缓存来用，那么可以禁止RDB和AOF所有的持久化机制 通过RDB或AOF，都可以将redis内存中的数据给持久化到磁盘上面来，然后可以将这些数据备份到别的地方去，比如说阿里云，云服务 如果redis挂了，服务器上的内存和磁盘上的数据都丢了，可以从云服务上拷贝回来之前的数据，放到指定的目录中，然后重新启动redis，redis就会自动根据持久化数据文件中的数据，去恢复内存中的数据，继续对外提供服务 如果同时使用RDB和AOF两种持久化机制，那么在redis重启的时候，会使用AOF来重新构建数据，因为AOF中的数据更加完整 2、RDB持久化机制的优点 （1）RDB会生成多个数据文件，每个数据文件都代表了某一个时刻中redis的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说Amazon的S3云服务上去，在国内可以是阿里云的ODPS分布式存储上，以预定好的备份策略来定期备份redis中的数据 （2）RDB对redis对外提供的读写服务，影响非常小，可以让redis保持高性能，因为redis主进程只需要fork一个子进程，让子进程执行磁盘IO操作来进行RDB持久化即可 （3）相对于AOF持久化机制来说，直接基于RDB数据文件来重启和恢复redis进程，更加快速 3、RDB持久化机制的缺点 （1）如果想要在redis故障时，尽可能少的丢失数据，那么RDB没有AOF好。一般来说，RDB数据快照文件，都是每隔5分钟，或者更长时间生成一次，这个时候就得接受一旦redis进程宕机，那么会丢失最近5分钟的数据 （2）RDB每次在fork子进程来执行RDB快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒 4、AOF持久化机制的优点 （1）AOF可以更好的保护数据不丢失，一般AOF会每隔1秒，通过一个后台线程执行一次fsync操作，最多丢失1秒钟的数据 （2）AOF日志文件以append-only模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复 （3）AOF日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在rewrite log的时候，会对其中的指导进行压缩，创建出一份需要恢复数据的最小日志出来。再创建新日志文件的时候，老的日志文件还是照常写入。当新的merge后的日志文件ready的时候，再交换新老日志文件即可。 （4）AOF日志文件的命令通过非常可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据 5、AOF持久化机制的缺点 （1）对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大 （2）AOF开启后，支持的写QPS会比RDB支持的写QPS低，因为AOF一般会配置成每秒fsync一次日志文件，当然，每秒一次fsync，性能也还是很高的 （3）以前AOF发生过bug，就是通过AOF记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似AOF这种较为复杂的基于命令日志/merge/回放的方式，比基于RDB每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有bug。不过AOF就是为了避免rewrite过程导致的bug，因此每次rewrite并不是基于旧的指令日志进行merge的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。 6、RDB和AOF到底该如何选择 （1）不要仅仅使用RDB，因为那样会导致你丢失很多数据 （2）也不要仅仅使用AOF，因为那样有两个问题，第一，你通过AOF做冷备，没有RDB做冷备，来的恢复速度更快; 第二，RDB每次简单粗暴生成数据快照，更加健壮，可以避免AOF这种复杂的备份和恢复机制的bug （3）综合使用AOF和RDB两种持久化机制，用AOF来保证数据不丢失，作为数据恢复的第一选择; 用RDB来做不同程度的冷备，在AOF文件都丢失或损坏不可用的时候，还可以使用RDB来进行快速的数据恢复 redis集群模式的工作原理能说一下么？在集群模式下，redis的key是如何寻址的？分布式寻址都有哪些算法？了解一致性hash算法吗？ 讲解分布式数据存储的核心算法，数据分布的算法 hash算法 -> 一致性hash算法（memcached） -> redis cluster，hash slot算法 用不同的算法，就决定了在多个master节点的时候，数据如何分布到这些节点上去，解决这个问题 1、redis cluster介绍 redis cluster （1）自动将数据进行分片，每个master上放一部分数据 （2）提供内置的高可用支持，部分master不可用时，还是可以继续工作的 在redis cluster架构下，每个redis要放开两个端口号，比如一个是6379，另外一个就是加10000的端口号，比如16379 16379端口号是用来进行节点间通信的，也就是cluster bus的东西，集群总线。cluster bus的通信，用来进行故障检测，配置更新，故障转移授权 cluster bus用了另外一种二进制的协议，主要用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间 2、最老土的hash算法和弊端（大量缓存重建） 3、一致性hash算法（自动缓存迁移）+虚拟节点（自动负载均衡） 4、redis cluster的hash slot算法 redis cluster有固定的16384个hash slot，对每个key计算CRC16值，然后对16384取模，可以获取key对应的hash slot redis cluster中每个master都会持有部分slot，比如有3个master，那么可能每个master持有5000多个hash slot hash slot让node的增加和移除很简单，增加一个master，就将其他master的hash slot移动部分过去，减少一个master，就将它的hash slot移动到其他master上去 移动hash slot的成本是非常低的 客户端的api，可以对指定的数据，让他们走同一个hash slot，通过hash tag来实现 一、节点间的内部通信机制 1、基础通信原理 （1）redis cluster节点间采取gossip协议进行通信 跟集中式不同，不是将集群元数据（节点信息，故障，等等）集中存储在某个节点上，而是互相之间不断通信，保持整个集群所有节点的数据是完整的 维护集群的元数据用得，集中式，一种叫做gossip 集中式：好处在于，元数据的更新和读取，时效性非常好，一旦元数据出现了变更，立即就更新到集中式的存储中，其他节点读取的时候立即就可以感知到; 不好在于，所有的元数据的跟新压力全部集中在一个地方，可能会导致元数据的存储有压力 gossip：好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续，打到所有节点上去更新，有一定的延时，降低了压力; 缺点，元数据更新有延时，可能导致集群的一些操作会有一些滞后 我们刚才做reshard，去做另外一个操作，会发现说，configuration error，达成一致 （2）10000端口 每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号+10000，比如7001，那么用于节点间通信的就是17001端口 每隔节点每隔一段时间都会往另外几个节点发送ping消息，同时其他几点接收到ping之后返回pong （3）交换的信息 故障信息，节点的增加和移除，hash slot信息，等等 2、gossip协议 gossip协议包含多种消息，包括ping，pong，meet，fail，等等 meet: 某个节点发送meet给新加入的节点，让新节点加入集群中，然后新节点就会开始与其他节点进行通信 redis-trib.rb add-node 其实内部就是发送了一个gossip meet消息，给新加入的节点，通知那个节点去加入我们的集群 ping: 每个节点都会频繁给其他节点发送ping，其中包含自己的状态还有自己维护的集群元数据，互相通过ping交换元数据 每个节点每秒都会频繁发送ping给其他的集群，ping，频繁的互相之间交换数据，互相进行元数据的更新 pong: 返回ping和meet，包含自己的状态和其他信息，也可以用于信息广播和更新 fail: 某个节点判断另一个节点fail之后，就发送fail给其他节点，通知其他节点，指定的节点宕机了 3、ping消息深入 ping很频繁，而且要携带一些元数据，所以可能会加重网络负担 每个节点每秒会执行10次ping，每次会选择5个最久没有通信的其他节点 当然如果发现某个节点通信延时达到了cluster_node_timeout / 2，那么立即发送ping，避免数据交换延时过长，落后的时间太长了 比如说，两个节点之间都10分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题 所以cluster_node_timeout可以调节，如果调节比较大，那么会降低发送的频率 每次ping，一个是带上自己节点的信息，还有就是带上1/10其他节点的信息，发送出去，进行数据交换 至少包含3个其他节点的信息，最多包含总节点-2个其他节点的信息 二、面向集群的jedis内部实现原理 开发，jedis，redis的java client客户端，redis cluster，jedis cluster api jedis cluster api与redis cluster集群交互的一些基本原理 1、基于重定向的客户端 redis-cli -c，自动重定向 （1）请求重定向 客户端可能会挑选任意一个redis实例去发送命令，每个redis实例接收到命令，都会计算key对应的hash slot 如果在本地就在本地处理，否则返回moved给客户端，让客户端进行重定向 cluster keyslot mykey，可以查看一个key对应的hash slot是什么 用redis-cli的时候，可以加入-c参数，支持自动的请求重定向，redis-cli接收到moved之后，会自动重定向到对应的节点执行命令 （2）计算hash slot 计算hash slot的算法，就是根据key计算CRC16值，然后对16384取模，拿到对应的hash slot 用hash tag可以手动指定key对应的slot，同一个hash tag下的key，都会在一个hash slot中，比如set mykey1:{100}和set mykey2:{100} （3）hash slot查找 节点间通过gossip协议进行数据交换，就知道每个hash slot在哪个节点上 2、smart jedis （1）什么是smart jedis 基于重定向的客户端，很消耗网络IO，因为大部分情况下，可能都会出现一次请求重定向，才能找到正确的节点 所以大部分的客户端，比如java redis客户端，就是jedis，都是smart的 本地维护一份hashslot -> node的映射表，缓存，大部分情况下，直接走本地缓存就可以找到hashslot -> node，不需要通过节点进行moved重定向 （2）JedisCluster的工作原理 在JedisCluster初始化的时候，就会随机选择一个node，初始化hashslot -> node映射表，同时为每个节点创建一个JedisPool连接池 每次基于JedisCluster执行操作，首先JedisCluster都会在本地计算key的hashslot，然后在本地映射表找到对应的节点 如果那个node正好还是持有那个hashslot，那么就ok; 如果说进行了reshard这样的操作，可能hashslot已经不在那个node上了，就会返回moved 如果JedisCluter API发现对应的节点返回moved，那么利用该节点的元数据，更新本地的hashslot -> node映射表缓存 重复上面几个步骤，直到找到对应的节点，如果重试超过5次，那么就报错，JedisClusterMaxRedirectionException jedis老版本，可能会出现在集群某个节点故障还没完成自动切换恢复时，频繁更新hash slot，频繁ping节点检查活跃，导致大量网络IO开销 jedis最新版本，对于这些过度的hash slot更新和ping，都进行了优化，避免了类似问题 （3）hashslot迁移和ask重定向 如果hash slot正在迁移，那么会返回ask重定向给jedis jedis接收到ask重定向之后，会重新定位到目标节点去执行，但是因为ask发生在hash slot迁移过程中，所以JedisCluster API收到ask是不会更新hashslot本地缓存 已经可以确定说，hashslot已经迁移完了，moved是会更新本地hashslot->node映射表缓存的 三、高可用性与主备切换原理 redis cluster的高可用的原理，几乎跟哨兵是类似的 1、判断节点宕机 如果一个节点认为另外一个节点宕机，那么就是pfail，主观宕机 如果多个节点都认为另外一个节点宕机了，那么就是fail，客观宕机，跟哨兵的原理几乎一样，sdown，odown 在cluster-node-timeout内，某个节点一直没有返回pong，那么就被认为pfail 如果一个节点认为某个节点pfail了，那么会在gossip ping消息中，ping给其他节点，如果超过半数的节点都认为pfail了，那么就会变成fail 2、从节点过滤 对宕机的master node，从其所有的slave node中，选择一个切换成master node 检查每个slave node与master node断开连接的时间，如果超过了cluster-node-timeout * cluster-slave-validity-factor，那么就没有资格切换成master 这个也是跟哨兵是一样的，从节点超时过滤的步骤 3、从节点选举 哨兵：对所有从节点进行排序，slave priority，offset，run id 每个从节点，都根据自己对master复制数据的offset，来设置一个选举时间，offset越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举 所有的master node开始slave选举投票，给要进行选举的slave进行投票，如果大部分master node（N/2 + 1）都投票给了某个从节点，那么选举通过，那个从节点可以切换成master 从节点执行主备切换，从节点切换为主节点 4、与哨兵比较 整个流程跟哨兵相比，非常类似，所以说，redis cluster功能强大，直接集成了replication和sentinal的功能 没有办法去给大家深入讲解redis底层的设计的细节，核心原理和设计的细节，那个除非单独开一门课，redis底层原理深度剖析，redis源码 对于咱们这个架构课来说，主要关注的是架构，不是底层的细节，对于架构来说，核心的原理的基本思路，是要梳理清晰的 了解什么是redis的雪崩和穿透？redis崩溃之后会怎么样？系统该如何应对这种情况？如何处理redis的穿透？ 2、面试官心里分析 其实这是问到缓存必问的，因为缓存雪崩和穿透，那是缓存最大的两个问题，要么不出现，一旦出现就是致命性的问题。所以面试官一定会问你。 3、面试题剖析 缓存雪崩发生的现象 缓存雪崩的事前事中事后的解决方案 事前：redis高可用，主从+哨兵，redis cluster，避免全盘崩溃 事中：本地ehcache缓存 + hystrix限流&降级，避免MySQL被打死 事后：redis持久化，快速恢复缓存数据 缓存穿透的现象 缓存穿透的解决方法 如何保证缓存与数据库的双写一致性？ 最经典的缓存+数据库读写的模式，cache aside pattern 1、Cache Aside Pattern （1）读的时候，先读缓存，缓存没有的话，那么就读数据库，然后取出数据后放入缓存，同时返回响应 （2）更新的时候，先删除缓存，然后再更新数据库 2、为什么是删除缓存，而不是更新缓存呢？ 原因很简单，很多时候，复杂点的缓存的场景，因为缓存有的时候，不简单是数据库中直接取出来的值 商品详情页的系统，修改库存，只是修改了某个表的某些字段，但是要真正把这个影响的最终的库存计算出来，可能还需要从其他表查询一些数据，然后进行一些复杂的运算，才能最终计算出 现在最新的库存是多少，然后才能将库存更新到缓存中去 比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据，并进行运算，才能计算出缓存最新的值的 更新缓存的代价是很高的 是不是说，每次修改数据库的时候，都一定要将其对应的缓存去跟新一份？也许有的场景是这样的，但是对于比较复杂的缓存数据计算的场景，就不是这样了 如果你频繁修改一个缓存涉及的多个表，那么这个缓存会被频繁的更新，频繁的更新缓存 但是问题在于，这个缓存到底会不会被频繁访问到？？？ 举个例子，一个缓存涉及的表的字段，在1分钟内就修改了20次，或者是100次，那么缓存跟新20次，100次; 但是这个缓存在1分钟内就被读取了1次，有大量的冷数据 28法则，黄金法则，20%的数据，占用了80%的访问量 实际上，如果你只是删除缓存的话，那么1分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低 每次数据过来，就只是删除缓存，然后修改数据库，如果这个缓存，在1分钟内只是被访问了1次，那么只有那1次，缓存是要被重新计算的，用缓存才去算缓存 其实删除缓存，而不是更新缓存，就是一个lazy计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算 mybatis，hibernate，懒加载，思想 查询一个部门，部门带了一个员工的list，没有必要说每次查询部门，都里面的1000个员工的数据也同时查出来啊 80%的情况，查这个部门，就只是要访问这个部门的信息就可以了 先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询1000个员工 1、最初级的缓存不一致问题以及解决方案 问题：先修改数据库，再删除缓存，如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据出现不一致 解决思路 先删除缓存，再修改数据库，如果删除缓存成功了，如果修改数据库失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致 因为读的时候缓存没有，则读数据库中旧数据，然后更新到缓存中 2、比较复杂的数据不一致问题分析 数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改 一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中 数据变更的程序完成了数据库的修改 完了，数据库和缓存中的数据不一样了。。。。 3、为什么上亿流量高并发场景下，缓存会出现这个问题？ 只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题 其实如果说你的并发量很低的话，特别是读并发很低，每天访问量就1万次，那么很少的情况下，会出现刚才描述的那种不一致的场景 但是问题是，如果每天的是上亿的流量，每秒并发读是几万，每秒只要有数据更新的请求，就可能会出现上述的数据库+缓存不一致的情况 高并发了以后，问题是很多的 4、数据库与缓存更新与读取操作进行异步串行化 更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个jvm内部的队列中 读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个jvm内部的队列中 一个队列对应一个工作线程 每个工作线程串行拿到对应的操作，然后一条一条的执行 这样的话，一个数据变更的操作，先执行，删除缓存，然后再去更新数据库，但是还没完成更新 此时如果一个读请求过来，读到了空的缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成 这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可 待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中 如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回; 如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值 5、高并发的场景下，该解决方案要注意的问题 （1）读请求长时阻塞 由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回 该解决方案，最大的风险点在于说，可能数据更新很频繁，导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库 务必通过一些模拟真实的测试，看看更新数据的频繁是怎样的 另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要部署多个服务，每个服务分摊一些数据的更新操作 如果一个内存队列里居然会挤压100个商品的库存修改操作，每隔库存修改操作要耗费10ms区完成，那么最后一个商品的读请求，可能等待10 * 100 = 1000ms = 1s后，才能得到数据 这个时候就导致读请求的长时阻塞 一定要做根据实际业务系统的运行情况，去进行一些压力测试，和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求，会hang多少时间，如果读请求在200ms返回，如果你计算过后，哪怕是最繁忙的时候，积压10个更新操作，最多等待200ms，那还可以的 如果一个内存队列可能积压的更新操作特别多，那么你就要加机器，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少 其实根据之前的项目经验，一般来说数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的 针对读高并发，读缓存架构的项目，一般写请求相对读来说，是非常非常少的，每秒的QPS能到几百就不错了 一秒，500的写操作，5份，每200ms，就100个写操作 单机器，20个内存队列，每个内存队列，可能就积压5个写操作，每个写操作性能测试后，一般在20ms左右就完成 那么针对每个内存队列中的数据的读请求，也就最多hang一会儿，200ms以内肯定能返回了 写QPS扩大10倍，但是经过刚才的测算，就知道，单机支撑写QPS几百没问题，那么就扩容机器，扩容10倍的机器，10台机器，每个机器20个队列，200个队列 大部分的情况下，应该是这样的，大量的读请求过来，都是直接走缓存取到数据的 少量情况下，可能遇到读跟数据更新冲突的情况，如上所述，那么此时更新操作如果先入队列，之后可能会瞬间来了对这个数据大量的读请求，但是因为做了去重的优化，所以也就一个更新缓存的操作跟在它后面 等数据更新完了，读请求触发的缓存更新操作也完成，然后临时等待的读请求全部可以读到缓存中的数据 （2）读请求并发量过高 这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时hang在服务上，看服务能不能抗的住，需要多少机器才能抗住最大的极限情况的峰值 但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大 按1:99的比例计算读和写的请求，每秒5万的读QPS，可能只有500次更新操作 如果一秒有500的写QPS，那么要测算好，可能写操作影响的数据有500条，这500条数据在缓存中失效后，可能导致多少读请求，发送读请求到库存服务来，要求更新缓存 一般来说，1:1，1:2，1:3，每秒钟有1000个读请求，会hang在库存服务上，每个读请求最多hang多少时间，200ms就会返回 在同一时间最多hang住的可能也就是单机200个读请求，同时hang住 单机hang200个读请求，还是ok的 1:20，每秒更新500条数据，这500秒数据对应的读请求，会有20 * 500 = 1万 1万个读请求全部hang在库存服务上，就死定了 （3）多服务实例部署的请求路由 可能这个服务部署了多个实例，那么必须保证说，执行数据更新操作，以及执行缓存更新操作的请求，都通过nginx服务器路由到相同的服务实例上 （4）热点商品的路由问题，导致请求的倾斜 万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能造成某台机器的压力过大 就是说，因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以更新频率不是太高的话，这个问题的影响并不是特别大 但是的确可能某些机器的负载会高一些 redis的并发竞争问题是什么？如何解决这个问题？了解Redis事务的CAS方案吗？ 生产环境中的redis是怎么部署的？ 2、面试官心里分析 看看你了解不了解你们公司的redis生产集群的部署架构，如果你不了解，那么确实你就很失职了，你的redis是主从架构？集群架构？用了哪种集群方案？有没有做高可用保证？有没有开启持久化机制确保可以进行数据恢复？线上redis给几个G的内存？设置了哪些参数？压测后你们redis集群承载多少QPS？ 兄弟，这些你必须是门儿清的，否则你确实是没好好思考过 3、面试题剖析 redis cluster，10台机器，5台机器部署了redis主实例，另外5台机器部署了redis的从实例，每个主实例挂了一个从实例，5个节点对外提供读写服务，每个节点的读写高峰qps可能可以达到每秒5万，5台机器最多是25万读写请求/s。 机器是什么配置？32G内存+8核CPU+1T磁盘，但是分配给redis进程的是10g内存，一般线上生产环境，redis的内存尽量不要超过10g，超过10g可能会有问题。 5台机器对外提供读写，一共有50g内存。 因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，redis从实例会自动变成主实例继续提供读写服务 你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是10kb。100条数据是1mb，10万条数据是1g。常驻内存的是200万条商品数据，占用内存是20g，仅仅不到总内存的50%。 目前高峰期每秒就是3500左右的请求量 比如我们吧，大型的公司，其实基础架构的team，会负责缓存集群的运维 Dubbo分布式系统 为什么要进行系统拆分？如何进行系统拆分？拆分后不用dubbo可以吗？ （1）为什么要将系统进行拆分？ 网上查查，答案极度零散和复杂，很琐碎，原因一大坨。但是我这里给大家直观的感受： 1）要是不拆分，一个大系统几十万行代码，20个人维护一份代码，简直是悲剧啊。代码经常改着改着就冲突了，各种代码冲突和合并要处理，非常耗费时间；经常我改动了我的代码，你调用了我，导致你的代码也得重新测试，麻烦的要死；然后每次发布都是几十万行代码的系统一起发布，大家得一起提心吊胆准备上线，几十万行代码的上线，可能每次上线都要做很多的检查，很多异常问题的处理，简直是又麻烦又痛苦；而且如果我现在打算把技术升级到最新的spring版本，还不行，因为这可能导致你的代码报错，我不敢随意乱改技术。 假设一个系统是20万行代码，其中小A在里面改了1000行代码，但是此时发布的时候是这个20万行代码的大系统一块儿发布。就意味着20万上代码在线上就可能出现各种变化，20个人，每个人都要紧张地等在电脑面前，上线之后，检查日志，看自己负责的那一块儿有没有什么问题。 小A就检查了自己负责的1万行代码对应的功能，确保ok就闪人了；结果不巧的是，小A上线的时候不小心修改了线上机器的某个配置，导致另外小B和小C负责的2万行代码对应的一些功能，出错了 几十个人负责维护一个几十万行代码的单块应用，每次上线，准备几个礼拜，上线 -> 部署 -> 检查自己负责的功能 最近从2013年到现在，5年的时间里，2013年以前，基本上都是BAT的天下；2013年开始，有几个小巨头开始快速的发展，上市，几百亿美金，估值都几百亿美金；2015年，出现了除了BAT以外，又有几个互联网行业的小巨头出现。 BAT工作，在市值几百亿美金的小巨头工作 有某一个小巨头，现在估值几百亿美金的小巨头，5年前刚开始搞的时候，核心的业务，几十个人，维护一个单块的应用 维护单块的应用，在从0到1的环节里，是很合适的，因为那个时候，是系统都没上线，没什么技术挑战，大家有条不紊的开发。ssh + mysql + tomcat，可能会部署几台机器吧。 结果不行了，后来系统上线了，业务快速发展，10万用户 -> 100万用户 -> 1000万用户 -> 上亿用户了。 2）拆分了以后，整个世界清爽了，几十万行代码的系统，拆分成20个服务，平均每个服务就1~2万行代码，每个服务部署到单独的机器上。20个工程，20个git代码仓库里，20个码农，每个人维护自己的那个服务就可以了，是自己独立的代码，跟别人没关系。再也没有代码冲突了，爽。每次就测试我自己的代码就可以了，爽。每次就发布我自己的一个小服务就可以了，爽。技术上想怎么升级就怎么升级，保持接口不变就可以了，爽。 所以简单来说，一句话总结，如果是那种代码量多达几十万行的中大型项目，团队里有几十个人，那么如果不拆分系统，开发效率极其低下，问题很多。但是拆分系统之后，每个人就负责自己的一小部分就好了，可以随便玩儿随便弄。分布式系统拆分之后，可以大幅度提升复杂系统大型团队的开发效率。 但是同时，也要提醒的一点是，系统拆分成分布式系统之后，大量的分布式系统面临的问题也是接踵而来，所以后面的问题都是在围绕分布式系统带来的复杂技术挑战在说。 （2）如何进行系统拆分？ 这个问题说大可以很大，可以扯到领域驱动模型设计上去，说小了也很小，我不太想给大家太过于学术的说法，因为你也不可能背这个答案，过去了直接说吧。还是说的简单一点，大家自己到时候知道怎么回答就行了。 系统拆分分布式系统，拆成多个服务，拆成微服务的架构，拆很多轮的。上来一个架构师第一轮就给拆好了，第一轮；团队继续扩大，拆好的某个服务，刚开始是1个人维护1万行代码，后来业务系统越来越复杂，这个服务是10万行代码，5个人；第二轮，1个服务 -> 5个服务，每个服务2万行代码，每人负责一个服务 如果是多人维护一个服务， 我个人建议，一个服务的代码不要太多，1万行左右，两三万撑死了吧 大部分的系统，是要进行多轮拆分的，第一次拆分，可能就是将以前的多个模块该拆分开来了，比如说将电商系统拆分成订单系统、商品系统、采购系统、仓储系统、用户系统，等等吧。 但是后面可能每个系统又变得越来越复杂了，比如说采购系统里面又分成了供应商管理系统、采购单管理系统，订单系统又拆分成了购物车系统、价格系统、订单管理系统。 扯深了实在很深，所以这里先给大家举个例子，你自己感受一下，核心意思就是根据情况，先拆分一轮，后面如果系统更复杂了，可以继续分拆。你根据自己负责系统的例子，来考虑一下就好了。 （3）拆分后不用dubbo可以吗？ 当然可以了，大不了最次，就是各个系统之间，直接基于spring mvc，就纯http接口互相通信呗，还能咋样。但是这个肯定是有问题的，因为http接口通信维护起来成本很高，你要考虑超时重试、负载均衡等等各种乱七八糟的问题，比如说你的订单系统调用商品系统，商品系统部署了5台机器，你怎么把请求均匀地甩给那5台机器？这不就是负载均衡？你要是都自己搞那是可以的，但是确实很痛苦。 所以dubbo说白了，是一种rpc框架，就是本地就是进行接口调用，但是dubbo会代理这个调用请求，跟远程机器网络通信，给你处理掉负载均衡了、服务实例上下线自动感知了、超时重试了，等等乱七八糟的问题。那你就不用自己做了，用dubbo就可以了。 说一下的dubbo的工作原理？注册中心挂了可以继续通信吗？说说一次rpc请求的流程？ MQ、ES、Redis、Dubbo，上来先问你一些思考的问题，原理（kafka高可用架构原理、es分布式架构原理、redis线程模型原理、Dubbo工作原理），生产环境里可能会碰到的一些问题（每种技术引入之后生产环境都可能会碰到一些问题），系统设计（设计MQ，设计搜索引擎，设计一个缓存，设计rpc框架） 当然比如说，hard面试官，死扣，结合项目死扣细节，百度（深入底层，基础性），阿里（结合项目死扣细节，扣很深的技术底层），小米（数据结构和算法）。 （1）dubbo工作原理 第一层：service层，接口层，给服务提供者和消费者来实现的 第二层：config层，配置层，主要是对dubbo进行各种配置的 第三层：proxy层，服务代理层，透明生成客户端的stub和服务单的skeleton 第四层：registry层，服务注册层，负责服务的注册与发现 第五层：cluster层，集群层，封装多个服务提供者的路由以及负载均衡，将多个实例组合成一个服务 第六层：monitor层，监控层，对rpc接口的调用次数和调用时间进行监控 第七层：protocol层，远程调用层，封装rpc调用 第八层：exchange层，信息交换层，封装请求响应模式，同步转异步 第九层：transport层，网络传输层，抽象mina和netty为统一接口 第十层：serialize层，数据序列化层 工作流程： 1）第一步，provider向注册中心去注册 2）第二步，consumer从注册中心订阅服务，注册中心会通知consumer注册好的服务 3）第三步，consumer调用provider 4）第四步，consumer和provider都异步的通知监控中心 （2）注册中心挂了可以继续通信吗？ 可以，因为刚开始初始化的时候，消费者会将提供者的地址等信息拉取到本地缓存，所以注册中心挂了可以继续通信 dubbo支持哪些通信协议？支持哪些序列化协议？ 2、面试官心里分析 上一个问题，说说dubbo的基本工作原理，那是你必须知道的，至少知道dubbo分成哪些层，然后平时怎么发起rpc请求的，注册、发现、调用，这些是基本的。 接着就可以针对底层进行深入的问问了，比如第一步就可以先问问序列化协议这块，就是平时rpc的时候怎么走的？ 3、面试题剖析 （1）dubbo支持不同的通信协议 1）dubbo协议 dubbo://192.168.0.1:20188 默认就是走dubbo协议的，单一长连接，NIO异步通信，基于hessian作为序列化协议 适用的场景就是：传输数据量很小（每次请求在100kb以内），但是并发量很高 为了要支持高并发场景，一般是服务提供者就几台机器，但是服务消费者有上百台，可能每天调用量达到上亿次！此时用长连接是最合适的，就是跟每个服务消费者维持一个长连接就可以，可能总共就100个连接。然后后面直接基于长连接NIO异步通信，可以支撑高并发请求。 否则如果上亿次请求每次都是短连接的话，服务提供者会扛不住。 而且因为走的是单一长连接，所以传输数据量太大的话，会导致并发能力降低。所以一般建议是传输数据量很小，支撑高并发访问。 2）rmi协议 走java二进制序列化，多个短连接，适合消费者和提供者数量差不多，适用于文件的传输，一般较少用 3）hessian协议 走hessian序列化协议，多个短连接，适用于提供者数量比消费者数量还多，适用于文件的传输，一般较少用 4）http协议 走json序列化 5）webservice 走SOAP文本序列化 （2）dubbo支持的序列化协议 所以dubbo实际基于不同的通信协议，支持hessian、java二进制序列化、json、SOAP文本序列化多种序列化协议。但是hessian是其默认的序列化协议。 dubbo负载均衡策略和集群容错策略都有哪些？动态代理策略呢？ 2、面试官心里分析 继续深问吧，这些都是用dubbo必须知道的一些东西，你得知道基本原理，知道序列化是什么协议，还得知道具体用dubbo的时候，如何负载均衡，如何高可用，如何动态代理。 说白了，就是看你对dubbo熟悉不熟悉： （1）dubbo工作原理：服务注册，注册中心，消费者，代理通信，负载均衡 （2）网络通信、序列化：dubbo协议，长连接，NIO，hessian序列化协议 （3）负载均衡策略，集群容错策略，动态代理策略：dubbo跑起来的时候一些功能是如何运转的，怎么做负载均衡？怎么做集群容错？怎么生成动态代理？ （4）dubbo SPI机制：你了解不了解dubbo的SPI机制？如何基于SPI机制对dubbo进行扩展？ 3、面试题剖析 （1）dubbo负载均衡策略 1）random loadbalance 默认情况下，dubbo是random load balance随机调用实现负载均衡，可以对provider不同实例设置不同的权重，会按照权重来负载均衡，权重越大分配流量越高，一般就用这个默认的就可以了。 2）roundrobin loadbalance 还有roundrobin loadbalance，这个的话默认就是均匀地将流量打到各个机器上去，但是如果各个机器的性能不一样，容易导致性能差的机器负载过高。所以此时需要调整权重，让性能差的机器承载权重小一些，流量少一些。 跟运维同学申请机器，有的时候，我们运气，正好公司资源比较充足，刚刚有一批热气腾腾，刚刚做好的一批虚拟机新鲜出炉，配置都比较高。8核+16g，机器，2台。过了一段时间，我感觉2台机器有点不太够，我去找运维同学，哥儿们，你能不能再给我1台机器，4核+8G的机器。我还是得要。 3）leastactive loadbalance 这个就是自动感知一下，如果某个机器性能越差，那么接收的请求越少，越不活跃，此时就会给不活跃的性能差的机器更少的请求 4）consistanthash loadbalance 一致性Hash算法，相同参数的请求一定分发到一个provider上去，provider挂掉的时候，会基于虚拟节点均匀分配剩余的流量，抖动不会太大。如果你需要的不是随机负载均衡，是要一类请求都到一个节点，那就走这个一致性hash策略。 （2）dubbo集群容错策略 1）failover cluster模式 失败自动切换，自动重试其他机器，默认就是这个，常见于读操作 2）failfast cluster模式 一次调用失败就立即失败，常见于写操作 3）failsafe cluster模式 出现异常时忽略掉，常用于不重要的接口调用，比如记录日志 4）failbackc cluster模式 失败了后台自动记录请求，然后定时重发，比较适合于写消息队列这种 5）forking cluster 并行调用多个provider，只要一个成功就立即返回 6）broadcacst cluster 逐个调用所有的provider （3）dubbo动态代理策略 默认使用javassist动态字节码生成，创建代理类 但是可以通过spi扩展机制配置自己的动态代理策略 dubbo的spi思想是什么？ dubbo的spi原理1 dubbo的spi原理2 2、面试官心里分析 继续深入问呗，前面一些基础性的东西问完了，确定你应该都ok了解dubbo的一些基本东西，那么问个稍微难一点点的问题，就是spi，先问问你spi是啥？然后问问你dubbo的spi是怎么实现的？ 其实就是看看你对dubbo的掌握如何 3、面试题剖析 spi，简单来说，就是service provider interface，说白了是什么意思呢，比如你有个接口，现在这个接口有3个实现类，那么在系统运行的时候对这个接口到底选择哪个实现类呢？这就需要spi了，需要根据指定的配置或者是默认的配置，去找到对应的实现类加载进来，然后用这个实现类的实例对象。 接口A -> 实现A1，实现A2，实现A3 配置一下，接口A = 实现A2 在系统实际运行的时候，会加载你的配置，用实现A2实例化一个对象来提供服务 比如说你要通过jar包的方式给某个接口提供实现，然后你就在自己jar包的META-INF/services/目录下放一个跟接口同名的文件，里面指定接口的实现里是自己这个jar包里的某个类。ok了，别人用了一个接口，然后用了你的jar包，就会在运行的时候通过你的jar包的那个文件找到这个接口该用哪个实现类。 这是jdk提供的一个功能。 比如说你有个工程A，有个接口A，接口A在工程A里是没有实现类的 -> 系统在运行的时候，怎么给接口A选择一个实现类呢？ 你就可以自己搞一个jar包，META-INF/services/，放上一个文件，文件名就是接口名，接口A，接口A的实现类=com.zhss.service.实现类A2。让工程A来依赖你的这个jar包，然后呢在系统运行的时候，工程A跑起来，对接口A，就会扫描自己依赖的所有的jar包，在每个jar里找找，有没有META-INF/services文件夹，如果有，在里面找找，有没有接口A这个名字的文件，如果有在里面找一下你指定的接口A的实现是你的jar包里的哪个类？ SPI机制，一般来说用在哪儿？插件扩展的场景，比如说你开发的是一个给别人使用的开源框架，如果你想让别人自己写个插件，插到你的开源框架里面来，扩展某个功能。 经典的思想体现，大家平时都在用，比如说jdbc java定义了一套jdbc的接口，但是java是没有提供jdbc的实现类 但是实际上项目跑的时候，要使用jdbc接口的哪些实现类呢？一般来说，我们要根据自己使用的数据库，比如msyql，你就将mysql-jdbc-connector.jar，引入进来；oracle，你就将oracle-jdbc-connector.jar，引入进来。 在系统跑的时候，碰到你使用jdbc的接口，他会在底层使用你引入的那个jar中提供的实现类 但是dubbo也用了spi思想，不过没有用jdk的spi机制，是自己实现的一套spi机制。 Protocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension(); Protocol接口，dubbo要判断一下，在系统运行的时候，应该选用这个Protocol接口的哪个实现类来实例化对象来使用呢？ 他会去找一个你配置的Protocol，他就会将你配置的Protocol实现类，加载到jvm中来，然后实例化对象，就用你的那个Protocol实现类就可以了 微内核，可插拔，大量的组件，Protocol负责rpc调用的东西，你可以实现自己的rpc调用组件，实现Protocol接口，给自己的一个实现类即可。 这行代码就是dubbo里大量使用的，就是对很多组件，都是保留一个接口和多个实现，然后在系统运行的时候动态根据配置去找到对应的实现类。如果你没配置，那就走默认的实现好了，没问题。 @SPI(\"dubbo\") public interface Protocol { int getDefaultPort(); @Adaptive Exporter export(Invoker invoker) throws RpcException; @Adaptive Invoker refer(Class type, URL url) throws RpcException; void destroy(); } 在dubbo自己的jar里，在/META_INF/dubbo/internal/com.alibaba.dubbo.rpc.Protocol文件中： dubbo=com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocol http=com.alibaba.dubbo.rpc.protocol.http.HttpProtocol hessian=com.alibaba.dubbo.rpc.protocol.hessian.HessianProtocol 所以说，这就看到了dubbo的spi机制默认是怎么玩儿的了，其实就是Protocol接口，@SPI(“dubbo”)说的是，通过SPI机制来提供实现类，实现类是通过dubbo作为默认key去配置文件里找到的，配置文件名称与接口全限定名一样的，通过dubbo作为key可以找到默认的实现了就是com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocol。 dubbo的默认网络通信协议，就是dubbo协议，用的DubboProtocol 如果想要动态替换掉默认的实现类，需要使用@Adaptive接口，Protocol接口中，有两个方法加了@Adaptive注解，就是说那俩接口会被代理实现。 啥意思呢？ 比如这个Protocol接口搞了俩@Adaptive注解标注了方法，在运行的时候会针对Protocol生成代理类，这个代理类的那俩方法里面会有代理代码，代理代码会在运行的时候动态根据url中的protocol来获取那个key，默认是dubbo，你也可以自己指定，你如果指定了别的key，那么就会获取别的实现类的实例了。 通过这个url中的参数不通，就可以控制动态使用不同的组件实现类 好吧，那下面来说说怎么来自己扩展dubbo中的组件 自己写个工程，要是那种可以打成jar包的，里面的src/main/resources目录下，搞一个META-INF/services，里面放个文件叫：com.alibaba.dubbo.rpc.Protocol，文件里搞一个my=com.zhss.MyProtocol。自己把jar弄到nexus私服里去。 然后自己搞一个dubbo provider工程，在这个工程里面依赖你自己搞的那个jar，然后在spring配置文件里给个配置： 这个时候provider启动的时候，就会加载到我们jar包里的my=com.zhss.MyProtocol这行配置里，接着会根据你的配置使用你定义好的MyProtocol了，这个就是简单说明一下，你通过上述方式，可以替换掉大量的dubbo内部的组件，就是扔个你自己的jar包，然后配置一下即可。 dubbo里面提供了大量的类似上面的扩展点，就是说，你如果要扩展一个东西，只要自己写个jar，让你的consumer或者是provider工程，依赖你的那个jar，在你的jar里指定目录下配置好接口名称对应的文件，里面通过key=实现类。 然后对对应的组件，用类似用你的哪个key对应的实现类来实现某个接口，你可以自己去扩展dubbo的各种功能，提供你自己的实现。 如何基于dubbo进行服务治理、服务降级、失败重试以及超时重试？ Dubbo官方源码分析 2、面试官心里分析 服务治理，这个问题如果问你，其实就是看看你有没有服务治理的思想，因为这个是做过复杂微服务的人肯定会遇到的一个问题。 服务降级，这个是涉及到复杂分布式系统中必备的一个话题，因为分布式系统互相来回调用，任何一个系统故障了，你不降级，直接就全盘崩溃？那就太坑爹了吧 失败重试，分布式系统中网络请求如此频繁，要是因为网络问题不小心失败了一次，是不是要重试？ 超时重试，同上，如果不小心网络慢一点，超时了，如何重试？ （1）dubbo工作原理：服务注册，注册中心，消费者，代理通信，负载均衡 （2）网络通信、序列化：dubbo协议，长连接，NIO，hessian序列化协议 （3）负载均衡策略，集群容错策略，动态代理策略：dubbo跑起来的时候一些功能是如何运转的，怎么做负载均衡？怎么做集群容错？怎么生成动态代理？ （4）dubbo SPI机制：你了解不了解dubbo的SPI机制？如何基于SPI机制对dubbo进行扩展？ （5）dubbo的服务治理、降级、重试 3、面试题剖析 （1）服务治理 1）调用链路自动生成 一个大型的分布式系统，或者说是用现在流行的微服务架构来说吧，分布式系统由大量的服务组成。那么这些服务之间互相是如何调用的？调用链路是啥？说实话，几乎到后面没人搞的清楚了，因为服务实在太多了，可能几百个甚至几千个服务。 那就需要基于dubbo做的分布式系统中，对各个服务之间的调用自动记录下来，然后自动将各个服务之间的依赖关系和调用链路生成出来，做成一张图，显示出来，大家才可以看到对吧。 服务A -> 服务B -> 服务C -> 服务E -> 服务D -> 服务F -> 服务W 2）服务访问压力以及时长统计 需要自动统计各个接口和服务之间的调用次数以及访问延时，而且要分成两个级别。一个级别是接口粒度，就是每个服务的每个接口每天被调用多少次，TP50，TP90，TP99，三个档次的请求延时分别是多少；第二个级别是从源头入口开始，一个完整的请求链路经过几十个服务之后，完成一次请求，每天全链路走多少次，全链路请求延时的TP50，TP90，TP99，分别是多少。 这些东西都搞定了之后，后面才可以来看当前系统的压力主要在哪里，如何来扩容和优化啊 3）其他的 服务分层（避免循环依赖），调用链路失败监控和报警，服务鉴权，每个服务的可用性的监控（接口调用成功率？几个9？）99.99%，99.9%，99% （2）服务降级 比如说服务A调用服务B，结果服务B挂掉了，服务A重试几次调用服务B，还是不行，直接降级，走一个备用的逻辑，给用户返回响应 public interface HelloService { void sayHello(); } public class HelloServiceImpl implements HelloService { public void sayHello() { System.out.println(\"hello world......\"); } } 现在就是mock，如果调用失败统一返回null 但是可以将mock修改为true，然后在跟接口同一个路径下实现一个Mock类，命名规则是接口名称加Mock后缀。然后在Mock类里实现自己的降级逻辑。 public class HelloServiceMock implements HelloService { public void sayHello() { // 降级逻辑 } } （3）失败重试和超时重试 所谓失败重试，就是consumer调用provider要是失败了，比如抛异常了，此时应该是可以重试的，或者调用超时了也可以重试。 某个服务的接口，要耗费5s，你这边不能干等着，你这边配置了timeout之后，我等待2s，还没返回，我直接就撤了，不能干等你 如果是超时了，timeout就会设置超时时间；如果是调用失败了自动就会重试指定的次数 你就结合你们公司的具体的场景来说说你是怎么设置这些参数的，timeout，一般设置为200ms，我们认为不能超过200ms还没返回 retries，3次，设置retries，还一般是在读请求的时候，比如你要查询个数据，你可以设置个retries，如果第一次没读到，报错，重试指定的次数，尝试再次读取2次 分布式服务接口的幂等性如何设计（比如不能重复扣款）？ 2、面试官心里分析 从这个问题开始，面试官就已经进入了实际的生产问题的面试了 一个分布式系统中的某个接口，要保证幂等性，该如何保证？这个事儿其实是你做分布式系统的时候必须要考虑的一个生产环境的技术问题。啥意思呢？ 你看，假如你有个服务提供一个接口，结果这服务部署在了5台机器上，接着有个接口就是付款接口。然后人家用户在前端上操作的时候，不知道为啥，总之就是一个订单不小心发起了两次支付请求，然后这俩请求分散在了这个服务部署的不同的机器上，好了，结果一个订单扣款扣两次？尴尬了。。。 或者是订单系统调用支付系统进行支付，结果不小心因为网络超时了，然后订单系统走了前面我们看到的那个重试机制，咔嚓给你重试了一把，好，支付系统收到一个支付请求两次，而且因为负载均衡算法落在了不同的机器上，尴尬了。。。 所以你肯定得知道这事儿，否则你做出来的分布式系统恐怕容易埋坑 网络问题很常见，100次请求，都ok；1万次，可能1次是超时会重试；10万，10次；100万，100次；如果有100个请求重复了，你没处理，导致订单扣款2次，100个订单都扣错了；每天被100个用户投诉；一个月被3000个用户投诉 我们之前生产就遇到过，是往数据库里写入数据，重复的请求，就导致我们的数据经常会错，出现一些重复数据，就会导致一些问题 3、面试题剖析 这个不是技术问题，这个没有通用的一个方法，这个是结合业务来看应该如何保证幂等性的，你的经验。 所谓幂等性，就是说一个接口，多次发起同一个请求，你这个接口得保证结果是准确的，比如不能多扣款，不能多插入一条数据，不能将统计值多加了1。这就是幂等性，不给大家来学术性词语了。 其实保证幂等性主要是三点： （1）对于每个请求必须有一个唯一的标识，举个例子：订单支付请求，肯定得包含订单id，一个订单id最多支付一次，对吧 （2）每次处理完请求之后，必须有一个记录标识这个请求处理过了，比如说常见的方案是在mysql中记录个状态啥的，比如支付之前记录一条这个订单的支付流水，而且支付流水采 （3）每次接收请求需要进行判断之前是否处理过的逻辑处理，比如说，如果有一个订单已经支付了，就已经有了一条支付流水，那么如果重复发送这个请求，则此时先插入支付流水，orderId已经存在了，唯一键约束生效，报错插入不进去的。然后你就不用再扣款了。 （4）上面只是给大家举个例子，实际运作过程中，你要结合自己的业务来，比如说用redis用orderId作为唯一键。只有成功插入这个支付流水，才可以执行实际的支付扣款。 要求是支付一个订单，必须插入一条支付流水，order_id建一个唯一键，unique key 所以你在支付一个订单之前，先插入一条支付流水，order_id就已经进去了 你就可以写一个标识到redis里面去，set order_id payed，下一次重复请求过来了，先查redis的order_id对应的value，如果是payed就说明已经支付过了，你就别重复支付了 然后呢，你再重复支付这个订单的时候，你写尝试插入一条支付流水，数据库给你报错了，说unique key冲突了，整个事务回滚就可以了 来保存一个是否处理过的标识也可以，服务的不同实例可以一起操作redis。 分布式服务接口请求的顺序性如何保证？ 2、面试官心里分析 其实分布式系统接口的调用顺序，也是个问题，一般来说是不用保证顺序的。但是有的时候可能确实是需要严格的顺序保证。给大家举个例子，你服务A调用服务B，先插入再删除。好，结果俩请求过去了，落在不同机器上，可能插入请求因为某些原因执行慢了一些，导致删除请求先执行了，此时因为没数据所以啥效果也没有；结果这个时候插入请求过来了，好，数据插入进去了，那就尴尬了。 本来应该是先插入 -> 再删除，这条数据应该没了，结果现在先删除 -> 再插入，数据还存在，最后你死都想不明白是怎么回事。 所以这都是分布式系统一些很常见的问题 3、面试题剖析 首先，一般来说，我个人给你的建议是，你们从业务逻辑上最好设计的这个系统不需要这种顺序性的保证，因为一旦引入顺序性保障，会导致系统复杂度上升，而且会带来效率低下，热点数据压力过大，等问题。 下面我给个我们用过的方案吧，简单来说，首先你得用dubbo的一致性hash负载均衡策略，将比如某一个订单id对应的请求都给分发到某个机器上去，接着就是在那个机器上因为可能还是多线程并发执行的，你可能得立即将某个订单id对应的请求扔一个内存队列里去，强制排队，这样来确保他们的顺序性。 但是这样引发的后续问题就很多，比如说要是某个订单对应的请求特别多，造成某台机器成热点怎么办？解决这些问题又要开启后续一连串的复杂技术方案。。。曾经这类问题弄的我们头疼不已，所以，还是建议什么呢？ 最好是比如说刚才那种，一个订单的插入和删除操作，能不能合并成一个操作，就是一个删除，或者是什么，避免这种问题的产生。 如何自己设计一个类似dubbo的rpc框架？ 2、面试官心里分析 说实话，就这问题，其实就跟问你，如何自己设计一个MQ，一样的道理，就考两个： （1）你有没有对某个rpc框架原理有非常深入的理解 （2）你能不能从整体上来思考一下，如何设计一个rpc框架，考考你的系统设计能力 3、面试题剖析 其实一般问到你这问题，你起码不能认怂，因为既然咱们这个课程是短期的面试突击训练课程，那我不可能给你深入讲解什么kafka源码剖析，dubbo源码剖析，何况我就算讲了，你要真的消化理解和吸收，起码个把月以后了。 所以我给大家一个建议，遇到这类问题，起码从你了解的类似框架的原理入手，自己说说参照dubbo的原理，你来设计一下，举个例子，dubbo不是有那么多分层么？而且每个分层是干啥的，你大概是不是知道？那就按照这个思路大致说一下吧，起码你不能懵逼，要比那些上来就懵，啥也说不出来的人要好一些 举个例子，我给大家说个最简单的回答思路： （1）上来你的服务就得去注册中心注册吧，你是不是得有个注册中心，保留各个服务的信心，可以用zookeeper来做，对吧 （2）然后你的消费者需要去注册中心拿对应的服务信息吧，对吧，而且每个服务可能会存在于多台机器上 （3）接着你就该发起一次请求了，咋发起？蒙圈了是吧。当然是基于动态代理了，你面向接口获取到一个动态代理，这个动态代理就是接口在本地的一个代理，然后这个代理会找到服务对应的机器地址 （4）然后找哪个机器发送请求？那肯定得有个负载均衡算法了，比如最简单的可以随机轮询是不是 （5）接着找到一台机器，就可以跟他发送请求了，第一个问题咋发送？你可以说用netty了，nio方式；第二个问题发送啥格式数据？你可以说用hessian序列化协议了，或者是别的，对吧。然后请求过去了。。 （6）服务器那边一样的，需要针对你自己的服务生成一个动态代理，监听某个网络端口了，然后代理你本地的服务代码。接收到请求的时候，就调用对应的服务代码，对吧。 这就是一个最最基本的rpc框架的思路，先不说你有多牛逼的技术功底，哪怕这个最简单的思路你先给出来行不行？好，突击课程，那就到这儿结束了，我这课程定位是帮你快速梳理一遍，扫清盲点，不是打通你任督二脉，给你九阳神功的。 zk都有哪些使用场景？ 大致来说，zk的使用场景如下，我就举几个简单的，大家能说几个就好了： （1）分布式协调：这个其实是zk很经典的一个用法，简单来说，就好比，你A系统发送个请求到mq，然后B消息消费之后处理了。那A系统如何知道B系统的处理结果？用zk就可以实现分布式系统之间的协调工作。A系统发送请求之后可以在zk上对某个节点的值注册个监听器，一旦B系统处理完了就修改zk那个节点的值，A立马就可以收到通知，完美解决。 （2）分布式锁：对某一个数据连续发出两个修改操作，两台机器同时收到了请求，但是只能一台机器先执行另外一个机器再执行。那么此时就可以使用zk分布式锁，一个机器接收到了请求之后先获取zk上的一把分布式锁，就是可以去创建一个znode，接着执行操作；然后另外一个机器也尝试去创建那个znode，结果发现自己创建不了，因为被别人创建了。。。。那只能等着，等第一个机器执行完了自己再执行。 （3）元数据/配置信息管理：zk可以用作很多系统的配置信息的管理，比如kafka、storm等等很多分布式系统都会选用zk来做一些元数据、配置信息的管理，包括dubbo注册中心不也支持zk么 （4）HA高可用性：这个应该是很常见的，比如hadoop、hdfs、yarn等很多大数据系统，都选择基于zk来开发HA高可用机制，就是一个重要进程一般会做主备两个，主进程挂了立马通过zk感知到切换到备用进程 一般实现分布式锁都有哪些方式？使用redis如何设计分布式锁？使用zk来设计分布式锁可以吗？这两种分布式锁的实现方式哪种效率比较高？ （1）redis分布式锁 官方叫做RedLock算法，是redis官方支持的分布式锁算法。 这个分布式锁有3个重要的考量点，互斥（只能有一个客户端获取锁），不能死锁，容错（大部分redis节点或者这个锁就可以加可以释放） 第一个最普通的实现方式，如果就是在redis里创建一个key算加锁 SET my:lock 随机值 NX PX 30000，这个命令就ok，这个的NX的意思就是只有key不存在的时候才会设置成功，PX 30000的意思是30秒后锁自动释放。别人创建的时候如果发现已经有了就不能加锁了。 释放锁就是删除key，但是一般可以用lua脚本删除，判断value一样才删除： 关于redis如何执行lua脚本，自行百度 if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1]) else return 0 end 为啥要用随机值呢？因为如果某个客户端获取到了锁，但是阻塞了很长时间才执行完，此时可能已经自动释放锁了，此时可能别的客户端已经获取到了这个锁，要是你这个时候直接删除key的话会有问题，所以得用随机值加上面的lua脚本来释放锁。 但是这样是肯定不行的。因为如果是普通的redis单实例，那就是单点故障。或者是redis普通主从，那redis主从异步复制，如果主节点挂了，key还没同步到从节点，此时从节点切换为主节点，别人就会拿到锁。 第二个问题，RedLock算法 这个场景是假设有一个redis cluster，有5个redis master实例。然后执行如下步骤获取一把锁： 1）获取当前时间戳，单位是毫秒 2）跟上面类似，轮流尝试在每个master节点上创建锁，过期时间较短，一般就几十毫秒 3）尝试在大多数节点上建立一个锁，比如5个节点就要求是3个节点（n / 2 +1） 4）客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了 5）要是锁建立失败了，那么就依次删除这个锁 6）只要别人建立了一把分布式锁，你就得不断轮询去尝试获取锁 （2）zk分布式锁 zk分布式锁，其实可以做的比较简单，就是某个节点尝试创建临时znode，此时创建成功了就获取了这个锁；这个时候别的客户端来创建锁会失败，只能注册个监听器监听这个锁。释放锁就是删除这个znode，一旦释放掉就会通知客户端，然后有一个等待着的客户端就可以再次重新枷锁。 /** * ZooKeeperSession * @author Administrator * */ public class ZooKeeperSession { private static CountDownLatch connectedSemaphore = new CountDownLatch(1); private ZooKeeper zookeeper; private CountDownLatch latch; public ZooKeeperSession() { try { this.zookeeper = new ZooKeeper( \"192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181\", 50000, new ZooKeeperWatcher()); try { connectedSemaphore.await(); } catch(InterruptedException e) { e.printStackTrace(); } System.out.println(\"ZooKeeper session established......\"); } catch (Exception e) { e.printStackTrace(); } } /** * 获取分布式锁 * @param productId */ public Boolean acquireDistributedLock(Long productId) { String path = \"/product-lock-\" + productId; try { zookeeper.create(path, \"\".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); return true; } catch (Exception e) { while(true) { try { Stat stat = zk.exists(path, true); // 相当于是给node注册一个监听器，去看看这个监听器是否存在 if(stat != null) { this.latch = new CountDownLatch(1); this.latch.await(waitTime, TimeUnit.MILLISECONDS); this.latch = null; } zookeeper.create(path, \"\".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); return true; } catch(Exception e) { continue; } } // 很不优雅，我呢就是给大家来演示这么一个思路 // 比较通用的，我们公司里我们自己封装的基于zookeeper的分布式锁，我们基于zookeeper的临时顺序节点去实现的，比较优雅的 } return true; } /** * 释放掉一个分布式锁 * @param productId */ public void releaseDistributedLock(Long productId) { String path = \"/product-lock-\" + productId; try { zookeeper.delete(path, -1); System.out.println(\"release the lock for product[id=\" + productId + \"]......\"); } catch (Exception e) { e.printStackTrace(); } } /** * 建立zk session的watcher * @author Administrator * */ private class ZooKeeperWatcher implements Watcher { public void process(WatchedEvent event) { System.out.println(\"Receive watched event: \" + event.getState()); if(KeeperState.SyncConnected == event.getState()) { connectedSemaphore.countDown(); } if(this.latch != null) { this.latch.countDown(); } } } /** * 封装单例的静态内部类 * @author Administrator * */ private static class Singleton { private static ZooKeeperSession instance; static { instance = new ZooKeeperSession(); } public static ZooKeeperSession getInstance() { return instance; } } /** * 获取单例 * @return */ public static ZooKeeperSession getInstance() { return Singleton.getInstance(); } /** * 初始化单例的便捷方法 */ public static void init() { getInstance(); } } （3）redis分布式锁和zk分布式锁的对比 redis分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能 zk分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小 另外一点就是，如果是redis获取锁的那个客户端bug了或者挂了，那么只能等待超时时间之后才能释放锁；而zk的话，因为创建的是临时znode，只要客户端挂了，znode就没了，此时就自动释放锁 redis分布式锁大家每发现好麻烦吗？遍历上锁，计算时间等等。。。zk的分布式锁语义清晰实现简单 所以先不分析太多的东西，就说这两点，我个人实践认为zk的分布式锁比redis的分布式锁牢靠、而且模型简单易用 集群部署时的分布式session如何实现？ session是啥？浏览器有个cookie，在一段时间内这个cookie都存在，然后每次发请求过来都带上一个特殊的jsessionid cookie，就根据这个东西，在服务端可以维护一个对应的session域，里面可以放点儿数据。 一般只要你没关掉浏览器，cookie还在，那么对应的那个session就在，但是cookie没了，session就没了。常见于什么购物车之类的东西，还有登录状态保存之类的。 这个不多说了，懂java的都该知道这个。 但是你单块系统的时候这么玩儿session没问题啊，但是你要是分布式系统了呢，那么多的服务，session状态在哪儿维护啊？ 其实方法很多，但是常见常用的是两种： （1）tomcat + redis 这个其实还挺方便的，就是使用session的代码跟以前一样，还是基于tomcat原生的session支持即可，然后就是用一个叫做Tomcat RedisSessionManager的东西，让所有我们部署的tomcat都将session数据存储到redis即可。 在tomcat的配置文件中，配置一下 搞一个类似上面的配置即可，你看是不是就是用了RedisSessionManager，然后指定了redis的host和 port就ok了。 :26379,:26379,:26379\" maxInactiveInterval=\"60\"/> 还可以用上面这种方式基于redis哨兵支持的redis高可用集群来保存session数据，都是ok的 （2）spring session + redis 分布式会话的这个东西重耦合在tomcat中，如果我要将web容器迁移成jetty，难道你重新把jetty都配置一遍吗？ 因为上面那种tomcat + redis的方式好用，但是会严重依赖于web容器，不好将代码移植到其他web容器上去，尤其是你要是换了技术栈咋整？比如换成了spring cloud或者是spring boot之类的。还得好好思忖思忖。 所以现在比较好的还是基于java一站式解决方案，spring了。人家spring基本上包掉了大部分的我们需要使用的框架了，spirng cloud做微服务了，spring boot做脚手架了，所以用sping session是一个很好的选择。 pom.xml org.springframework.session spring-session-data-redis 1.2.1.RELEASE redis.clients jedis 2.8.1 spring配置文件中 web.xml springSessionRepositoryFilter org.springframework.web.filter.DelegatingFilterProxy springSessionRepositoryFilter /* 示例代码 @Controller @RequestMapping(\"/test\") public class TestController { @RequestMapping(\"/putIntoSession\") @ResponseBody public String putIntoSession(HttpServletRequest request, String username){ request.getSession().setAttribute(\"name\", “leo”); return \"ok\"; } @RequestMapping(\"/getFromSession\") @ResponseBody public String getFromSession(HttpServletRequest request, Model model){ String name = request.getSession().getAttribute(\"name\"); return name; } } 上面的代码就是ok的，给sping session配置基于redis来存储session数据，然后配置了一个spring session的过滤器，这样的话，session相关操作都会交给spring session来管了。接着在代码中，就用原生的session操作，就是直接基于spring sesion从redis中获取数据了。 实现分布式的会话，有很多种很多种方式，我说的只不过比较常见的两种方式，tomcat + redis早期比较常用；近些年，重耦合到tomcat中去，通过spring session来实现。 分布式事务了解吗？你们如何解决分布式事务问题的？ 分布式系统里的事务 （1）两阶段提交方案/XA方案 两阶段提交方案 也叫做两阶段提交事务方案，这个举个例子，比如说咱们公司里经常tb是吧（就是团建），然后一般会有个tb主席（就是负责组织团建的那个人）。 tb，team building，团建 第一个阶段，一般tb主席会提前一周问一下团队里的每个人，说，大家伙，下周六我们去滑雪+烧烤，去吗？这个时候tb主席开始等待每个人的回答，如果所有人都说ok，那么就可以决定一起去这次tb。如果这个阶段里，任何一个人回答说，我有事不去了，那么tb主席就会取消这次活动。 第二个阶段，那下周六大家就一起去滑雪+烧烤了 所以这个就是所谓的XA事务，两阶段提交，有一个事务管理器的概念，负责协调多个数据库（资源管理器）的事务，事务管理器先问问各个数据库你准备好了吗？如果每个数据库都回复ok，那么就正式提交事务，在各个数据库上执行操作；如果任何一个数据库回答不ok，那么就回滚事务。 这种分布式事务方案，比较适合单块应用里，跨多个库的分布式事务，而且因为严重依赖于数据库层面来搞定复杂的事务，效率很低，绝对不适合高并发的场景。如果要玩儿，那么基于spring + JTA就可以搞定，自己随便搜个demo看看就知道了。 这个方案，我们很少用，一般来说某个系统内部如果出现跨多个库的这么一个操作，是不合规的。我可以给大家介绍一下， 现在微服务，一个大的系统分成几百个服务，几十个服务。一般来说，我们的规定和规范，是要求说每个服务只能操作自己对应的一个数据库。 如果你要操作别的服务对应的库，不允许直连别的服务的库，违反微服务架构的规范，你随便交叉胡乱访问，几百个服务的话，全体乱套，这样的一套服务是没法管理的，没法治理的，经常数据被别人改错，自己的库被别人写挂。 如果你要操作别人的服务的库，你必须是通过调用别的服务的接口来实现，绝对不允许你交叉访问别人的数据库！ （2）TCC方案 TCC方案 TCC的全程是：Try、Confirm、Cancel。 这个其实是用到了补偿的概念，分为了三个阶段： 1）Try阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行锁定或者预留 2）Confirm阶段：这个阶段说的是在各个服务中执行实际的操作 3）Cancel阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚操作 给大家举个例子吧，比如说跨银行转账的时候，要涉及到两个银行的分布式事务，如果用TCC方案来实现，思路是这样的： 1）Try阶段：先把两个银行账户中的资金给它冻结住就不让操作了 2）Confirm阶段：执行实际的转账操作，A银行账户的资金扣减，B银行账户的资金增加 3）Cancel阶段：如果任何一个银行的操作执行失败，那么就需要回滚进行补偿，就是比如A银行账户如果已经扣减了，但是B银行账户资金增加失败了，那么就得把A银行账户资金给加回去 这种方案说实话几乎很少用人使用，我们用的也比较少，但是也有使用的场景。因为这个事务回滚实际上是严重依赖于你自己写代码来回滚和补偿了，会造成补偿代码巨大，非常之恶心。 比如说我们，一般来说跟钱相关的，跟钱打交道的，支付、交易相关的场景，我们会用TCC，严格严格保证分布式事务要么全部成功，要么全部自动回滚，严格保证资金的正确性，在资金上出现问题 比较适合的场景：这个就是除非你是真的一致性要求太高，是你系统中核心之核心的场景，比如常见的就是资金类的场景，那你可以用TCC方案了，自己编写大量的业务逻辑，自己判断一个事务中的各个环节是否ok，不ok就执行补偿/回滚代码。 而且最好是你的各个业务执行的时间都比较短。 但是说实话，一般尽量别这么搞，自己手写回滚逻辑，或者是补偿逻辑，实在太恶心了，那个业务代码很难维护。 （3）本地消息表 本地消息表方案 国外的ebay搞出来的这么一套思想 这个大概意思是这样的 1）A系统在自己本地一个事务里操作同时，插入一条数据到消息表 2）接着A系统将这个消息发送到MQ中去 3）B系统接收到消息之后，在一个事务里，往自己本地消息表里插入一条数据，同时执行其他的业务操作，如果这个消息已经被处理过了，那么此时这个事务会回滚，这样保证不会重复处理消息 4）B系统执行成功之后，就会更新自己本地消息表的状态以及A系统消息表的状态 5）如果B系统处理失败了，那么就不会更新消息表状态，那么此时A系统会定时扫描自己的消息表，如果有没处理的消息，会再次发送到MQ中去，让B再次处理 6）这个方案保证了最终一致性，哪怕B事务失败了，但是A会不断重发消息，直到B那边成功为止 这个方案说实话最大的问题就在于严重依赖于数据库的消息表来管理事务啥的？？？这个会导致如果是高并发场景咋办呢？咋扩展呢？所以一般确实很少用 （4）可靠消息最终一致性方案 可靠消息最终一致性方案 这个的意思，就是干脆不要用本地的消息表了，直接基于MQ来实现事务。比如阿里的RocketMQ就支持消息事务。 大概的意思就是： 1）A系统先发送一个prepared消息到mq，如果这个prepared消息发送失败那么就直接取消操作别执行了 2）如果这个消息发送成功过了，那么接着执行本地事务，如果成功就告诉mq发送确认消息，如果失败就告诉mq回滚消息 3）如果发送了确认消息，那么此时B系统会接收到确认消息，然后执行本地的事务 4）mq会自动定时轮询所有prepared消息回调你的接口，问你，这个消息是不是本地事务处理失败了，所有没发送确认消息？那是继续重试还是回滚？一般来说这里你就可以查下数据库看之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，别确认消息发送失败了。 5）这个方案里，要是系统B的事务失败了咋办？重试咯，自动不断重试直到成功，如果实在是不行，要么就是针对重要的资金类业务进行回滚，比如B系统本地回滚后，想办法通知系统A也回滚；或者是发送报警由人工来手工回滚和补偿 这个还是比较合适的，目前国内互联网公司大都是这么玩儿的，要不你举用RocketMQ支持的，要不你就自己基于类似ActiveMQ？RabbitMQ？自己封装一套类似的逻辑出来，总之思路就是这样子的 （5）最大努力通知方案 最大努力通知方案 这个方案的大致意思就是： 1）系统A本地事务执行完之后，发送个消息到MQ 2）这里会有个专门消费MQ的最大努力通知服务，这个服务会消费MQ然后写入数据库中记录下来，或者是放入个内存队列也可以，接着调用系统B的接口 3）要是系统B执行成功就ok了；要是系统B执行失败了，那么最大努力通知服务就定时尝试重新调用系统B，反复N次，最后还是不行就放弃 （6）你们公司是如何处理分布式事务的？ 这个，说真的，确实我们这个课程没法带着大家来实战，因为定位不是这个。但是如果你真的被问到，你可以这么说，我们某某特别严格的场景，用的是TCC来保证强一致性；然后其他的一些场景基于了阿里的RocketMQ来实现了分布式事务。 你找一个严格资金要求绝对不能错的场景，你可以说你是用的TCC方案；如果是一般的分布式事务场景，订单插入之后要调用库存服务更新库存，库存数据没有资金那么的敏感，可以用可靠消息最终一致性方案 友情提示一下，rocketmq 3.2.6之前的版本，是可以按照上面的思路来的，但是之后接口做了一些改变，我这里不再赘述了。 当然如果你愿意，你可以参考可靠消息最终一致性方案来自己实现一套分布式事务，比如基于rabbitmq来玩儿。 如何设计一个高并发系统？ 高并发系统的架构组成 其实所谓的高并发，如果你要理解这个问题呢，其实就得从高并发的根源出发，为啥会有高并发？为啥高并发就很牛逼？ 我说的浅显一点，很简单，就是因为刚开始系统都是连接数据库的，但是要知道数据库支撑到每秒并发两三千的时候，基本就快完了。所以才有说，很多公司，刚开始干的时候，技术比较low，结果业务发展太快，有的时候系统扛不住压力就挂了。 当然会挂了，凭什么不挂？你数据库如果瞬间承载每秒5000,8000，甚至上万的并发，一定会宕机，因为比如mysql就压根儿扛不住这么高的并发量。 所以为啥高并发牛逼？就是因为现在用互联网的人越来越多，很多app、网站、系统承载的都是高并发请求，可能高峰期每秒并发量几千，很正常的。如果是什么双十一了之类的，每秒并发几万几十万都有可能。 那么如此之高的并发量，加上原本就如此之复杂的业务，咋玩儿？真正厉害的，一定是在复杂业务系统里玩儿过高并发架构的人，但是你没有，那么我给你说一下你该怎么回答这个问题： （1）系统拆分，将一个系统拆分为多个子系统，用dubbo来搞。然后每个系统连一个数据库，这样本来就一个库，现在多个数据库，不也可以抗高并发么。 （2）缓存，必须得用缓存。大部分的高并发场景，都是读多写少，那你完全可以在数据库和缓存里都写一份，然后读的时候大量走缓存不就得了。毕竟人家redis轻轻松松单机几万的并发啊。没问题的。所以你可以考虑考虑你的项目里，那些承载主要请求的读场景，怎么用缓存来抗高并发。 （3）MQ，必须得用MQ。可能你还是会出现高并发写的场景，比如说一个业务操作里要频繁搞数据库几十次，增删改增删改，疯了。那高并发绝对搞挂你的系统，你要是用redis来承载写那肯定不行，人家是缓存，数据随时就被LRU了，数据格式还无比简单，没有事务支持。所以该用mysql还得用mysql啊。那你咋办？用MQ吧，大量的写请求灌入MQ里，排队慢慢玩儿，后边系统消费后慢慢写，控制在mysql承载范围之内。所以你得考虑考虑你的项目里，那些承载复杂写业务逻辑的场景里，如何用MQ来异步写，提升并发性。MQ单机抗几万并发也是ok的，这个之前还特意说过。 （4）分库分表，可能到了最后数据库层面还是免不了抗高并发的要求，好吧，那么就将一个数据库拆分为多个库，多个库来抗更高的并发；然后将一个表拆分为多个表，每个表的数据量保持少一点，提高sql跑的性能。 （5）读写分离，这个就是说大部分时候数据库可能也是读多写少，没必要所有请求都集中在一个库上吧，可以搞个主从架构，主库写入，从库读取，搞一个读写分离。读流量太多的时候，还可以加更多的从库。 （6）Elasticsearch，可以考虑用es。es是分布式的，可以随便扩容，分布式天然就可以支撑高并发，因为动不动就可以扩容加机器来抗更高的并发。那么一些比较简单的查询、统计类的操作，可以考虑用es来承载，还有一些全文搜索类的操作，也可以考虑用es来承载。 上面的6点，基本就是高并发系统肯定要干的一些事儿，大家可以仔细结合之前讲过的知识考虑一下，到时候你可以系统的把这块阐述一下，然后每个部分要注意哪些问题，之前都讲过了，你都可以阐述阐述，表明你对这块是有点积累的。 分库分表 为什么要分库分表？（设计高并发系统的时候，数据库层面该如何设计？） 01_分库分表的由来 说白了，分库分表是两回事儿，大家可别搞混了，可能是光分库不分表，也可能是光分表不分库，都有可能。我先给大家抛出来一个场景。 假如我们现在是一个小创业公司（或者是一个BAT公司刚兴起的一个新部门），现在注册用户就20万，每天活跃用户就1万，每天单表数据量就1000，然后高峰期每秒钟并发请求最多就10。。。天，就这种系统，随便找一个有几年工作经验的，然后带几个刚培训出来的，随便干干都可以。 结果没想到我们运气居然这么好，碰上个CEO带着我们走上了康庄大道，业务发展迅猛，过了几个月，注册用户数达到了2000万！每天活跃用户数100万！每天单表数据量10万条！高峰期每秒最大请求达到1000！同时公司还顺带着融资了两轮，紧张了几个亿人民币啊！公司估值达到了惊人的几亿美金！这是小独角兽的节奏！ 好吧，没事，现在大家感觉压力已经有点大了，为啥呢？因为每天多10万条数据，一个月就多300万条数据，现在咱们单表已经几百万数据了，马上就破千万了。但是勉强还能撑着。高峰期请求现在是1000，咱们线上部署了几台机器，负载均衡搞了一下，数据库撑1000 QPS也还凑合。但是大家现在开始感觉有点担心了，接下来咋整呢。。。。。。 再接下来几个月，我的天，CEO太牛逼了，公司用户数已经达到1亿，公司继续融资几十亿人民币啊！公司估值达到了惊人的几十亿美金，成为了国内今年最牛逼的明星创业公司！天，我们太幸运了。 但是我们同时也是不幸的，因为此时每天活跃用户数上千万，每天单表新增数据多达50万，目前一个表总数据量都已经达到了两三千万了！扛不住啊！数据库磁盘容量不断消耗掉！高峰期并发达到惊人的5000~8000！别开玩笑了，哥。我跟你保证，你的系统支撑不到现在，已经挂掉了！ 好吧，所以看到你这里你差不多就理解分库分表是怎么回事儿了，实际上这是跟着你的公司业务发展走的，你公司业务发展越好，用户就越多，数据量越大，请求量越大，那你单个数据库一定扛不住。 比如你单表都几千万数据了，你确定你能抗住么？绝对不行，单表数据量太大，会极大影响你的sql执行的性能，到了后面你的sql可能就跑的很慢了。一般来说，就以我的经验来看，单表到几百万的时候，性能就会相对差一些了，你就得分表了。 分表是啥意思？就是把一个表的数据放到多个表中，然后查询的时候你就查一个表。比如按照用户id来分表，将一个用户的数据就放在一个表中。然后操作的时候你对一个用户就操作那个表就好了。这样可以控制每个表的数据量在可控的范围内，比如每个表就固定在200万以内。 分库是啥意思？就是你一个库一般我们经验而言，最多支撑到并发2000，一定要扩容了，而且一个健康的单库并发值你最好保持在每秒1000左右，不要太大。那么你可以将一个库的数据拆分到多个库中，访问的时候就访问一个库好了。 这就是所谓的分库分表，为啥要分库分表？你明白了吧 用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？ 这个其实就是看看你了解哪些分库分表的中间件，各个中间件的优缺点是啥？然后你用过哪些分库分表的中间件。 比较常见的包括：cobar、TDDL、atlas、sharding-jdbc、mycat cobar：阿里b2b团队开发和开源的，属于proxy层方案。早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库join和分页等操作。 TDDL：淘宝团队开发的，属于client层方案。不支持join、多表查询等语法，就是基本的crud语法是ok，但是支持读写分离。目前使用的也不多，因为还依赖淘宝的diamond配置管理系统。 atlas：360开源的，属于proxy层方案，以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在5年前了。所以，现在用的公司基本也很少了。 sharding-jdbc：当当开源的，属于client层方案。确实之前用的还比较多一些，因为SQL语法支持也比较多，没有太多限制，而且目前推出到了2.0版本，支持分库分表、读写分离、分布式id生成、柔性事务（最大努力送达型事务、TCC事务）。而且确实之前使用的公司会比较多一些（这个在官网有登记使用的公司，可以看到从2017年一直到现在，是不少公司在用的），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也可以选择的方案。 mycat：基于cobar改造的，属于proxy层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于sharding jdbc来说，年轻一些，经历的锤炼少一些。 所以综上所述，现在其实建议考量的，就是sharding-jdbc和mycat，这两个都可以去考虑使用。 sharding-jdbc这种client层方案的优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合sharding-jdbc的依赖； mycat这种proxy层方案的缺点在于需要部署，自己及运维一套中间件，运维成本高，但是好处在于对于各个项目是透明的，如果遇到升级之类的都是自己中间件那里搞就行了。 通常来说，这两个方案其实都可以选用，但是我个人建议中小型公司选用sharding-jdbc，client层方案轻便，而且维护成本低，不需要额外增派人手，而且中小型公司系统复杂度会低一些，项目也没那么多； 但是中大型公司最好还是选用mycat这类proxy层方案，因为可能大公司系统和项目非常多，团队很大，人员充足，那么最好是专门弄个人来研究和维护mycat，然后大量项目直接透明使用即可。 我们，数据库中间件都是自研的，也用过proxy层，后来也用过client层 你们具体是如何对数据库如何进行垂直拆分或水平拆分的？ 02_数据库如何拆分 水平拆分的意思，就是把一个表的数据给弄到多个库的多个表里去，但是每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据。水平拆分的意义，就是将数据均匀放更多的库里，然后用多个库来抗更高的并发，还有就是用多个库的存储容量来进行扩容。 垂直拆分的意思，就是把一个有很多字段的表给拆分成多个表，或者是多个库上去。每个库表的结构都不一样，每个库表都包含部分字段。一般来说，会将较少的访问频率很高的字段放到一个表里去，然后将较多的访问频率很低的字段放到另外一个表里去。因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。 这个其实挺常见的，不一定我说，大家很多同学可能自己都做过，把一个大表拆开，订单表、订单支付表、订单商品表。 还有表层面的拆分，就是分表，将一个表变成N个表，就是让每个表的数据量控制在一定范围内，保证SQL的性能。否则单表数据量越大，SQL性能就越差。一般是200万行左右，不要太多，但是也得看具体你怎么操作，也可能是500万，或者是100万。你的SQL越复杂，就最好让单表行数越少。 好了，无论是分库了还是分表了，上面说的那些数据库中间件都是可以支持的。就是基本上那些中间件可以做到你分库分表之后，中间件可以根据你指定的某个字段值，比如说userid，自动路由到对应的库上去，然后再自动路由到对应的表里去。 你就得考虑一下，你的项目里该如何分库分表？一般来说，垂直拆分，你可以在表层面来做，对一些字段特别多的表做一下拆分；水平拆分，你可以说是并发承载不了，或者是数据量太大，容量承载不了，你给拆了，按什么字段来拆，你自己想好；分表，你考虑一下，你如果哪怕是拆到每个库里去，并发和容量都ok了，但是每个库的表还是太大了，那么你就分表，将这个表分开，保证每个表的数据量并不是很大。 而且这儿还有两种分库分表的方式，一种是按照range来分，就是每个库一段连续的数据，这个一般是按比如时间范围来的，但是这种一般较少用，因为很容易产生热点问题，大量的流量都打在最新的数据上了；或者是按照某个字段hash一下均匀分散，这个较为常用。 range来分，好处在于说，后面扩容的时候，就很容易，因为你只要预备好，给每个月都准备一个库就可以了，到了一个新的月份的时候，自然而然，就会写新的库了；缺点，但是大部分的请求，都是访问最新的数据。实际生产用range，要看场景，你的用户不是仅仅访问最新的数据，而是均匀的访问现在的数据以及历史的数据 hash分法，好处在于说，可以平均分配没给库的数据量和请求压力；坏处在于说扩容起来比较麻烦，会有一个数据迁移的这么一个过程 现在有一个未分库分表的系统，未来要分库分表，如何设计才可以让系统从未分库分表动态切换到分库分表上？ 假设，你现有有一个单库单表的系统，在线上在跑，假设单表有600万数据 3个库，每个库里分了4个表，每个表要放50万的数据量 假设你已经选择了一个分库分表的数据库中间件，sharding-jdbc，mycat，都可以 你怎么把线上系统平滑地迁移到分库分表上面去 sharding-jdbc：自己上官网，找一个官网最基本的例子，自己写一下，试一下，跑跑看，是非常简单的 mycat：自己上官网，找一个官网最基本的例子，自己写一下，试一下看看 1个小时以内就可以搞定了 3、面试题剖析 这个其实从low到高大上有好几种方案，我们都玩儿过，我都给你说一下 （1）停机迁移方案 01_长时间停机分库分表 我先给你说一个最low的方案，就是很简单，大家伙儿凌晨12点开始运维，网站或者app挂个公告，说0点到早上6点进行运维，无法访问。。。。。。 接着到0点，停机，系统挺掉，没有流量写入了，此时老的单库单表数据库静止了。然后你之前得写好一个导数的一次性工具，此时直接跑起来，然后将单库单表的数据哗哗哗读出来，写到分库分表里面去。 导数完了之后，就ok了，修改系统的数据库连接配置啥的，包括可能代码和SQL也许有修改，那你就用最新的代码，然后直接启动连到新的分库分表上去。 验证一下，ok了，完美，大家伸个懒腰，看看看凌晨4点钟的北京夜景，打个滴滴回家吧 但是这个方案比较low，谁都能干，我们来看看高大上一点的方案 （2）双写迁移方案 02_不停机双写方案 这个是我们常用的一种迁移方案，比较靠谱一些，不用停机，不用看北京凌晨4点的风景 简单来说，就是在线上系统里面，之前所有写库的地方，增删改操作，都除了对老库增删改，都加上对新库的增删改，这就是所谓双写，同时写俩库，老库和新库。 然后系统部署之后，新库数据差太远，用之前说的导数工具，跑起来读老库数据写新库，写的时候要根据gmt_modified这类字段判断这条数据最后修改的时间，除非是读出来的数据在新库里没有，或者是比新库的数据新才会写。 接着导万一轮之后，有可能数据还是存在不一致，那么就程序自动做一轮校验，比对新老库每个表的每条数据，接着如果有不一样的，就针对那些不一样的，从老库读数据再次写。反复循环，直到两个库每个表的数据都完全一致为止。 接着当数据完全一致了，就ok了，基于仅仅使用分库分表的最新代码，重新部署一次，不就仅仅基于分库分表在操作了么，还没有几个小时的停机时间，很稳。所以现在基本玩儿数据迁移之类的，都是这么干了。 如何设计可以动态扩容缩容的分库分表方案？ 01_分库分表扩容方案 （1）停机扩容 这个方案就跟停机迁移一样，步骤几乎一致，唯一的一点就是那个导数的工具，是把现有库表的数据抽出来慢慢倒入到新的库和表里去。但是最好别这么玩儿，有点不太靠谱，因为既然分库分表就说明数据量实在是太大了，可能多达几亿条，甚至几十亿，你这么玩儿，可能会出问题。 从单库单表迁移到分库分表的时候，数据量并不是很大，单表最大也就两三千万 写个工具，多弄几台机器并行跑，1小时数据就导完了 3个库+12个表，跑了一段时间了，数据量都1亿~2亿了。光是导2亿数据，都要导个几个小时，6点，刚刚导完数据，还要搞后续的修改配置，重启系统，测试验证，10点才可以搞完 （2）优化后的方案 一开始上来就是32个库，每个库32个表，1024张表 我可以告诉各位同学说，这个分法，第一，基本上国内的互联网肯定都是够用了，第二，无论是并发支撑还是数据量支撑都没问题 每个库正常承载的写入并发量是1000，那么32个库就可以承载32 1000 = 32000的写并发，如果每个库承载1500的写并发，32 1500 = 48000的写并发，接近5万/s的写入并发，前面再加一个MQ，削峰，每秒写入MQ 8万条数据，每秒消费5万条数据。 有些除非是国内排名非常靠前的这些公司，他们的最核心的系统的数据库，可能会出现几百台数据库的这么一个规模，128个库，256个库，512个库 1024张表，假设每个表放500万数据，在MySQL里可以放50亿条数据 每秒的5万写并发，总共50亿条数据，对于国内大部分的互联网公司来说，其实一般来说都够了 谈分库分表的扩容，第一次分库分表，就一次性给他分个够，32个库，1024张表，可能对大部分的中小型互联网公司来说，已经可以支撑好几年了 一个实践是利用32 * 32来分库分表，即分为32个库，每个库里一个表分为32张表。一共就是1024张表。根据某个id先根据32取模路由到库，再根据32取模路由到库里的表。 刚开始的时候，这个库可能就是逻辑库，建在一个数据库上的，就是一个mysql服务器可能建了n个库，比如16个库。后面如果要拆分，就是不断在库和mysql服务器之间做迁移就可以了。然后系统配合改一下配置即可。 比如说最多可以扩展到32个数据库服务器，每个数据库服务器是一个库。如果还是不够？最多可以扩展到1024个数据库服务器，每个数据库服务器上面一个库一个表。因为最多是1024个表么。 这么搞，是不用自己写代码做数据迁移的，都交给dba来搞好了，但是dba确实是需要做一些库表迁移的工作，但是总比你自己写代码，抽数据导数据来的效率高得多了。 哪怕是要减少库的数量，也很简单，其实说白了就是按倍数缩容就可以了，然后修改一下路由规则。 对2 ^ n取模 orderId 模 32 = 库 orderId / 32 模 32 = 表 259 3 8 1189 5 5 352 0 11 4593 17 15 1、设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是32库 * 32表，对于大部分公司来说，可能几年都够了 2、路由的规则，orderId 模 32 = 库，orderId / 32 模 32 = 表 3、扩容的时候，申请增加更多的数据库服务器，装好mysql，倍数扩容，4台服务器，扩到8台服务器，16台服务器 4、由dba负责将原先数据库服务器的库，迁移到新的数据库服务器上去，很多工具，库迁移，比较便捷 5、我们这边就是修改一下配置，调整迁移的库所在数据库服务器的地址 6、重新发布系统，上线，原先的路由规则变都不用变，直接可以基于2倍的数据库服务器的资源，继续进行线上系统的提供服务 分库分表之后，id主键如何处理？ （1）数据库自增id 这个就是说你的系统里每次得到一个id，都是往一个库的一个表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个id。拿到这个id之后再往对应的分库分表里去写入。 这个方案的好处就是方便简单，谁都会用；缺点就是单库生成自增id，要是高并发的话，就会有瓶颈的；如果你硬是要改进一下，那么就专门开一个服务出来，这个服务每次就拿到当前id最大值，然后自己递增几个id，一次性返回一批id，然后再把当前最大id值修改成递增几个id之后的一个值；但是无论怎么说都是基于单个数据库。 适合的场景：你分库分表就俩原因，要不就是单库并发太高，要不就是单库数据量太大；除非是你并发不高，但是数据量太大导致的分库分表扩容，你可以用这个方案，因为可能每秒最高并发最多就几百，那么就走单独的一个库和表生成自增主键即可。 并发很低，几百/s，但是数据量大，几十亿的数据，所以需要靠分库分表来存放海量的数据 （2）uuid 好处就是本地生成，不要基于数据库来了；不好之处就是，uuid太长了，作为主键性能太差了，不适合用于主键。 适合的场景：如果你是要随机生成个什么文件名了，编号之类的，你可以用uuid，但是作为主键是不能用uuid的。 UUID.randomUUID().toString().replace(“-”, “”) -> sfsdf23423rr234sfdaf （3）获取系统当前时间 这个就是获取当前时间即可，但是问题是，并发很高的时候，比如一秒并发几千，会有重复的情况，这个是肯定不合适的。基本就不用考虑了。 适合的场景：一般如果用这个方案，是将当前时间跟很多其他的业务字段拼接起来，作为一个id，如果业务上你觉得可以接受，那么也是可以的。你可以将别的业务字段值跟当前时间拼接起来，组成一个全局唯一的编号，订单编号，时间戳 + 用户id + 业务含义编码 （4）snowflake算法 twitter开源的分布式id生成算法，就是把一个64位的long型的id，1个bit是不用的，用其中的41 bit作为毫秒数，用10 bit作为工作机器id，12 bit作为序列号 1 bit：不用，为啥呢？因为二进制里第一个bit为如果是1，那么都是负数，但是我们生成的id都是正数，所以第一个bit统一都是0 41 bit：表示的是时间戳，单位是毫秒。41 bit可以表示的数字多达2^41 - 1，也就是可以标识2 ^ 41 - 1个毫秒值，换算成年就是表示69年的时间。 10 bit：记录工作机器id，代表的是这个服务最多可以部署在2^10台机器上哪，也就是1024台机器。但是10 bit里5个bit代表机房id，5个bit代表机器id。意思就是最多代表2 ^ 5个机房（32个机房），每个机房里可以代表2 ^ 5个机器（32台机器）。 12 bit：这个是用来记录同一个毫秒内产生的不同id，12 bit可以代表的最大正整数是2 ^ 12 - 1 = 4096，也就是说可以用这个12bit代表的数字来区分同一个毫秒内的4096个不同的id 64位的long型的id，64位的long -> 二进制 0 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 1 1001 | 0000 00000000 2018-01-01 10:00:00 -> 做了一些计算，再换算成一个二进制，41bit来放 -> 0001100 10100010 10111110 10001001 01011100 00 机房id，17 -> 换算成一个二进制 -> 10001 机器id，25 -> 换算成一个二进制 -> 11001 snowflake算法服务，会判断一下，当前这个请求是否是，机房17的机器25，在2175/11/7 12:12:14时间点发送过来的第一个请求，如果是第一个请求 假设，在2175/11/7 12:12:14时间里，机房17的机器25，发送了第二条消息，snowflake算法服务，会发现说机房17的机器25，在2175/11/7 12:12:14时间里，在这一毫秒，之前已经生成过一个id了，此时如果你同一个机房，同一个机器，在同一个毫秒内，再次要求生成一个id，此时我只能把加1 0 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 1 1001 | 0000 00000001 比如我们来观察上面的那个，就是一个典型的二进制的64位的id，换算成10进制就是910499571847892992。 public class IdWorker{ private long workerId; private long datacenterId; private long sequence; public IdWorker(long workerId, long datacenterId, long sequence){ // sanity check for workerId // 这儿不就检查了一下，要求就是你传递进来的机房id和机器id不能超过32，不能小于0 if (workerId > maxWorkerId || workerId maxDatacenterId || datacenterId 1 if (lastTimestamp == timestamp) { sequence = (sequence + 1) & sequenceMask; // 这个意思是说一个毫秒内最多只能有4096个数字，无论你传递多少进来，这个位运算保证始终就是在4096这个范围内，避免你自己传递个sequence超过了4096这个范围 if (sequence == 0) { timestamp = tilNextMillis(lastTimestamp); } } else { sequence = 0; } // 这儿记录一下最近一次生成id的时间戳，单位是毫秒 lastTimestamp = timestamp; // 这儿就是将时间戳左移，放到41 bit那儿；将机房id左移放到5 bit那儿；将机器id左移放到5 bit那儿；将序号放最后10 bit；最后拼接起来成一个64 bit的二进制数字，转换成10进制就是个long型 return ((timestamp - twepoch) 怎么说呢，大概这个意思吧，就是说41 bit，就是当前毫秒单位的一个时间戳，就这意思；然后5 bit是你传递进来的一个机房id（但是最大只能是32以内），5 bit是你传递进来的机器id（但是最大只能是32以内），剩下的那个10 bit序列号，就是如果跟你上次生成id的时间还在一个毫秒内，那么会把顺序给你累加，最多在4096个序号以内。 所以你自己利用这个工具类，自己搞一个服务，然后对每个机房的每个机器都初始化这么一个东西，刚开始这个机房的这个机器的序号就是0。然后每次接收到一个请求，说这个机房的这个机器要生成一个id，你就找到对应的Worker，生成。 他这个算法生成的时候，会把当前毫秒放到41 bit中，然后5 bit是机房id，5 bit是机器id，接着就是判断上一次生成id的时间如果跟这次不一样，序号就自动从0开始；要是上次的时间跟现在还是在一个毫秒内，他就把seq累加1，就是自动生成一个毫秒的不同的序号。 这个算法那，可以确保说每个机房每个机器每一毫秒，最多生成4096个不重复的id。 利用这个snowflake算法，你可以开发自己公司的服务，甚至对于机房id和机器id，反正给你预留了5 bit + 5 bit，你换成别的有业务含义的东西也可以的。 这个snowflake算法相对来说还是比较靠谱的，所以你要真是搞分布式id生成，如果是高并发啥的，那么用这个应该性能比较好，一般每秒几万并发的场景，也足够你用了。 你们有没有做MySQL读写分离？如何实现mysql的读写分离？MySQL主从复制原理的是啥？如何解决mysql主从同步的延时问题？ 01_为什么MySQL要读写分离 （1）如何实现mysql的读写分离？ 02_MySQL主从复制原理 其实很简单，就是基于主从复制架构，简单来说，就搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。 （2）MySQL主从复制原理的是啥？ 主库将变更写binlog日志，然后从库连接到主库之后，从库有一个IO线程，将主库的binlog日志拷贝到自己本地，写入一个中继日志中。接着从库中有一个SQL线程会从中继日志读取binlog，然后执行binlog日志中的内容，也就是在自己本地再次执行一遍SQL，这样就可以保证自己跟主库的数据是一样的。 这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行SQL的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。 而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。 所以mysql实际上在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；一个是并行复制，用来解决主从同步延时问题。 这个所谓半同步复制，semi-sync复制，指的就是主库写入binlog日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的relay log之后，接着会返回一个ack给主库，主库接收到至少一个从库的ack之后才会认为写操作完成了。 所谓并行复制，指的是从库开启多个线程，并行读取relay log中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。 1）主从复制的原理 2）主从延迟问题产生的原因 3）主从复制的数据丢失问题，以及半同步复制的原理 4）并行复制的原理，多库并发重放relay日志，缓解主从延迟问题 （3）mysql主从同步延时问题（精华） 03_MySQL主从延迟导致的生产环境的问题 线上确实处理过因为主从同步延时问题，导致的线上的bug，小型的生产事故 show status，Seconds_Behind_Master，你可以看到从库复制主库的数据落后了几ms 其实这块东西我们经常会碰到，就比如说用了mysql主从架构之后，可能会发现，刚写入库的数据结果没查到，结果就完蛋了。。。。 所以实际上你要考虑好应该在什么场景下来用这个mysql主从同步，建议是一般在读远远多于写，而且读的时候一般对数据时效性要求没那么高的时候，用mysql主从同步 所以这个时候，我们可以考虑的一个事情就是，你可以用mysql的并行复制，但是问题是那是库级别的并行，所以有时候作用不是很大 所以这个时候。。通常来说，我们会对于那种写了之后立马就要保证可以查到的场景，采用强制读主库的方式，这样就可以保证你肯定的可以读到数据了吧。其实用一些数据库中间件是没问题的。 一般来说，如果主从延迟较为严重 1、分库，将一个主库拆分为4个主库，每个主库的写并发就500/s，此时主从延迟可以忽略不计 2、打开mysql支持的并行复制，多个库并行复制，如果说某个库的写入并发就是特别高，单库写并发达到了2000/s，并行复制还是没意义。28法则，很多时候比如说，就是少数的几个订单表，写入了2000/s，其他几十个表10/s。 3、重写代码，写代码的同学，要慎重，当时我们其实短期是让那个同学重写了一下代码，插入数据之后，直接就更新，不要查询 4、如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，你这么搞导致读写分离的意义就丧失了 "},"zother7-JavaInterview/project/秒杀项目总结.html":{"url":"zother7-JavaInterview/project/秒杀项目总结.html","title":"秒杀项目总结","keywords":"","body":"1 介绍项目 1）项目整体设计的感受（可以画架构图） 2）你负责了什么，承担了什么角色，做了什么 3）项目描述，最好体现自己的综合素质：如何协调开发，遇到问题如何解决，用什么技术实现了什么功能 2 缓存雪崩 3 问题：秒杀地址不应该爆漏给用户 写脚本，LRU缓存控制请求数量 4 不同数据库的数据读写 分布式锁，消息队列（提高性能，降低耦合度，流量削峰） 5 分布式事物解决方案 6 数据库与redis缓存如何保持一致性 7 秒杀多个步骤如何保证线程安全性 客观锁，悲观锁 8 用的什么测试工具 Redis缓存服务 缓存商品和商品详情信息，如果不存在，则从数据库查询，并且加入到redis缓存，如果存在，直接返回。 RabbitMQ 1）异步订单 2）异步支付 3）订单超时处理（quartz） 缓存的信息：商品信息，商品详情信息，订单信息 查询订单优化 面杀系统架构设计思路 面试经验相关 https://github.com/AobingJava/JavaFamily 互联网公司的面试官是如何360°无死角考察候选人的？（上篇） 互联网公司面试官是如何360°无死角考察候选人的？（下篇） 记一位朋友斩获BAT技术专家Offer的面试经历 秒杀系统相关 分布式架构图 秒杀架构设计 秒杀架构设计思路详解 秒杀关键细节设计 项目技术点 ssm activiti lombok、日志log4j 分布式相关 分布式相关面试题 扎心！线上服务宕机时，如何保证数据100%不丢失？ ehcache EhCache在项目中的使用 Redis Redis设计与实现总结文章 Redis面试题必备：基础，面试题 Redis面试相关：其中包含redis知识 Redis源码分析 redis其他数据结构 其他 gossip协议 Raft协议 dubbo dubbo教程 dubbo源码分析 dubbo面试题 dubbo面试题2 zookeeper 什么是zookeeper？ Zookeeper教程 zookeeper源码分析 zookeeeper面试题 zookeeper面试题2 消息队列 为什么要用消息队列？ * 你们的系统架构中为什么要引入消息中间件？ 哥们，那你说说系统架构引入消息中间件有什么缺点？ 哥们，消息中间件在你们项目里是如何落地的？ 消息中间件集群崩溃，如何保证百万生产数据不丢失？ RocketMQ RocketMQ简单教程 RocketMQ教程 RocketMQ源码分析 RocketMQ面试题 RabbitMQ RabbitMQ教程 RabbitMQ面试题 RabbitMQ面试题2 RabbitMQ面试题3 kafka 全网最通俗易懂的Kafka入门 全网最通俗易懂的Kafka入门2 kafka入门教程 kafka面试题 kafka面试题2 分布式解决方案 如果20万用户同时访问一个热点缓存，如何优化你的缓存架构？ 高并发场景下，如何保证生产者投递到消息中间件的消息不丢失？ 从团队自研的百万并发中间件系统的内核设计看Java并发性能优化 支撑日活百万用户的高并发系统，应该如何设计其数据库架构？ 支撑百万连接的系统应该如何设计其高并发架构？ 如何保证消息中间件全链路数据100%不丢失（1） 如何保证消息中间件全链路数据100%不丢失（2） 消息中间件如何实现消费吞吐量的百倍优化？ 优雅的告诉面试官消息中间件该如何实现高可用架构？ 消息中间件如何实现每秒几十万的高并发写入？ 请谈谈写入消息中间件的数据，如何保证不丢失？ 如果让你设计一个消息中间件，如何将其网络通信性能优化10倍以上？ 分布式锁 拜托，面试请不要再问我Redis分布式锁的实现原理 每秒上千订单场景下的分布式锁高并发优化实践！ 彻底讲清楚ZooKeeper分布式锁的实现原理 分布式事务 拜托，面试请不要再问我TCC分布式事务的实现原理！ 分布式事务如何保障实际生产中99.99%高可用？ TCC-Transaction实战 微服务 拜托！面试请不要再问我Spring Cloud底层原理 微服务注册中心如何承载大型系统的千万级访问？ 每秒上万并发下的Spring Cloud参数优化实战 微服务架构如何保障双11狂欢下的99.99%高可用 分布式session 分库分表 读写分离 亿级流量架构设计方案 分布式系统的唯一id生成算法你了解吗？ 用大白话给你讲小白都能看懂的分布式系统容错架构 亿级流量系统架构之如何支撑百亿级数据的存储与计算 亿级流量系统架构之如何设计高容错分布式计算系统 亿级流量系统架构之如何设计承载百亿流量的高性能架构 亿级流量系统架构之如何设计每秒十万查询的高并发架构 亿级流量系统架构之如何设计全链路99.99%高可用架构 亿级流量系统架构之如何在上万并发场景下设计可扩展架构（上）？ 亿级流量系统架构之如何在上万并发场景下设计可扩展架构（中）？ 亿级流量系统架构之如何在上万并发场景下设计可扩展架构（下）？ 亿级流量系统架构之如何保证百亿流量下的数据一致性（上） 亿级流量系统架构之如何保证百亿流量下的数据一致性（中）？ 亿级流量系统架构之如何保证百亿流量下的数据一致性（下）？ 项目工具 git 实际开发中的git命令大全 maven 项目功能点 单点登录 什么是单点登录 单点登录机制原理 前后端分离 前后端分离概述 前后端分离之JWT用户认证 mysql（优化） MySQL高频面试题 MySQL查询优化过程 生成数据工具：mockaroo 1、如果有子查询，改为连接语句 2、在where条件建立索引：user的id 3、如果有连接关键词，也建立索引：user的id 4、如果有分组或者排序，建立索引：order by time 5、同时也可以建立联合索引 需求1 某个部门查看审核人员的所有审批信息：先查询某个部门所有的审核人员，根据审批建议、审批时间及审批人查询审批信息(审批数据70w) select u.* , c.* from user u inner join checklog c on c.uid == u.id where u.deptmentId = 1 and time > {1} and time ①子查询：先查询部门的用户id然后再查询相关的审批信息 ②没有加索引：70w数据查询用了3w s。 ③给user的id建立索引，时间减少到了1.003s ④然后考虑到子查询，所以，改成连接操作，时间变为0.057s ⑤再给连接字段建立索引，时间变为0.001s ⑥模拟数据增加到300w时间增加到了0.016s，在部门id和用户id建立联合索引，时间降到了0.005s。 ⑦用时间进行排序，建立时间索引。 MVCC原理 MySQL锁 权限控制（设计、shiro） 权限控制设计 shiro相关教程 springboot+vue+shiro前后端分离实战项目 shiro挺好的教程 线上问题调优（虚拟机，tomcat） 垃圾收集器ZGC jvm系列文章 * 一次JVM FullGC的背后，竟隐藏着惊心动魄的线上生产事故！ Java虚拟机调优文章 利用VisualVM对高并发项目进行性能分析 JVM性能调优 百亿吞吐量服务的JVM性能调优实战 一次线上JVM调优实践，FullGC40次/天到10天一次的优化过程 JVM调优工具 并发问题 Tip：本来有很多我准备的资料的，但是都是外链，或者不合适的分享方式，所以大家去公众号回复【资料】好了。 现在免费送给大家，在我的公众号 好好学java 回复 资料 即可获取。 有收获？希望老铁们来个三连击，给更多的人看到这篇文章 1、老铁们，关注我的原创微信公众号「好好学java」，专注于Java、数据结构和算法、微服务、中间件等技术分享，保证你看完有所收获。 2、给俺一个 star 呗，可以让更多的人看到这篇文章，顺便激励下我继续写作，嘻嘻。 "},"学习路线.html":{"url":"学习路线.html","title":"学习路线","keywords":"","body":" https://www.processon.com/view/link/5cb6c8a4e4b059e209fbf369#outline 框架源码专题 应用框架Spring Spring IOC源码剖析 整体认知spring 体系结构 理解Spring IOC 容器设计原理 掌握Bean生命周期 初始化InitializingBean/@PostConstruct Bean的后置处理器BeanPostProcessor源码分析 销毁DisposableBean/@PreDestroy Spring Context 装载过程源码分析 BeanFactoryPostProcessor源码分析 BeanDefinitionRegistryPostProcessor源码分析 Factorybean与Beanfactory区别 Spring Aop源码剖析 深入理解AOP的底层实现 掌握Spring AOP 编程概念 AOP注解编程 @EnableAspectJAutoProxy @Before/@After/@AfterReturning/@AfterThrowing/@Around @Pointcut 基于Spring AOP 实现应用插件机制 Spring AOP源码分析 ProxyFactory源码解析 AOP代理源码解析 JdkDynamicAopProxy Cglib2AopProxy 拦截器链与织入源码解析 Advice Interceptor Spring事务控制与底层源码分析 @EnableTransactionManagement @Transactional Spring MVC源码剖析 理解MVC设计思想 从DispatchServlet 出发 讲述MVC体系结构组成 基于示例展开DispatchServlet 核心类结构 MVC执行流程讲解 RequestMaping 实现原理 MVC初始化过程源码深度解读 熟悉MVC组件体系 映射器原理实现 执行适配器原理实现 视图解析器原理实现 异常捕捉器原理实现 Spring注解式开发 @ComponentScan @Bean @Configuration @Component/@Service@/Controller/@Repository @Conditional @Lazy @Scope @Import @Value @Autowired/@Resources/@Inject @Profile Spring 5新特性 新特性详解 响应式编程模型 函数式风格的ApplicationContext Kotlin表达式的支持 SpringWebFlux模块讲解 ORM框架MyBatis MyBatis快速掌握 MyBatis 与 Hibernate 对比 传统JDBC弊端 mybatis全局参数详解 逆向工程 详解configuration 、properties、 settings、 typeAliases、 mapper 掌握xml和annotations和Criteria差异 Mybatis 源码分析 整体认识mybatis源码包 Mybatis核心概念 Spring 与MyBatis 集成 Configuration、Mapper、SqlSession、Executor源码解析 Mybatis徒手实现 熟悉MyBatis内部运行机制 熟悉myBatis 初始化过程 源码debug一行行详细讲解 MyBatis二级缓存应用 手写实现一套mybatis框架 学习源码中的优秀设计模式 工厂模式 单例模式 代理模式 模板模式 装饰器模式 责任链模式 观察者模式 策略模式 构建模式 原型模式 并发编程专题 JMM内存模型 现代计算机模型基础理论知识 什么是线程 深入理解Java线程 JMM Volatile 缓存一致性协议 指令重排、可见性、原子性、顺序一致性、happens-beofre详解 as-if-serial 并发同步处理 Synchronized JVM内置锁实现原理 对象加锁过程 锁的膨胀升级过程分析 AbstractQueuedSynchronizer（AQS） 乐观锁 悲观锁 重入锁 公平锁 非公平锁 锁的粒度 并发包之locks锁 ReentrantLock ReentrantReadWriteLock ReadWriteLock Condition 条件队列 同步队列 深入讲解、源码分析 并发包之tools 限制 CountDownLatch Semaphore CyclicBarrier 并发包之atomic原子 atomic类 ThreadLocal ABA JMM cas算法乐观锁 Unsafe魔法类详解 并发包之collections容器 并发Queue BlockingQueue ArrayBlockingQueue 数组有界队列 ConcurrentLinkedQueue 链表有界队列 PriorityBlockingQueue 优先级排序无界队列 DelayQueue 延时无界队列 Map ConcurrentHashMap HashMap 并发List Set CopyOnWriteArrayList CopyOnWriteArraySet ArrayList LinkedList 并发包之executor线程池 Futrue RunnableFuture RunnableFuture RunnableScheduledFuture ScheduledFuture Thread Runable Callable Executor AbstractExecutorService ThreadPoolExecutor ScheduledExecutorService ScheduledThreadPoolExecutor 线程详解与源码剖析 并发之ForkJoin框架 ForkJoin框架介绍 ForkJoin案列讲解 ForkJoin原理解析 分布式框架专题 初识分布式 初识分布式系统定义与意义 分布式系统的基础知识 淘宝电商架构演变过程 大型网站架构模式 大型网站的分层、分割模式 大型分布式、集群模式 分布式中缓存、异步模式 分布式系统冗余 、扩展模式 大型网站架构要素 分布式系统中高并发原子：无状态、拆分、服务化、消息队列 分布式系统之高可用原子:降级、限流、备份、监听 分布式中间件 分布式服务治理中间件(Zookeeper，Dubbo) 分布式下应用系统服务化通讯技术 从集中到分布式特点 ACID到CAP/BASE基础 分布式协同框架Zookeeper Zookeeper快速入门 Zookeeper多节点集群部署实战 深入Zookeeper典型应用场景 服务注册与订阅 分布式配置中心 分布式锁 深入Zookeeper中znode、watcher、ACL、客户端API详解 深入Zookeeper客户端服务端源码分析 深入熟悉Zookeeper迁移、扩容、监控详解 RPC服务框架Dubbo 从0到1整体认知分布式系统 分布式架构的发展历史与背景 如何着手架构一套分布式系统 分布式架构所带来的成本与风险 快速掌握Dubbo常规应用 Dubbo的作用简要说明、快速演示Dubbo 调用示例 Dubbo 架构与基本角色说明 Dubbo基本应用与配置说明 Dubbo 企业级应用进阶 分布式项目开发与联调 Dubbo控制管理后台使用 Dubbo注册中心使用 RPC协议底层原理与实现 RPC协议基本组成 RPC协议报文编码与实现详解 Dubbo中所支持RPC协议使用 Dubbo 调用模块详解 Dubbo 调用模块详解（容错、负载均衡、异步调用、过滤器） Dubbo 其它使用场景详解（泛化调用与引用、隐示传参、令牌验证） Dubbo 路由功能使用 分布式消息中间件(RockerMq，Rabbitmq，Kafka) 初识消息中间件特性与规划 阿里双十一交易系统与统计系统场景讲解 常见消息中间件Rabbitmq、Kafka、ActiveMq、RocketMq对比 分布式消息框架之RocketMq 解密Rocket,Mq集群部署与快速入门 深入分析RocketMq模块划分与集群原理讲解 详解普通消息、顺序消息、事务消息、定时消息 深入RocketMq Broker、Consumer、Producer源码分析 详解RocketMq监控与运维 企业实战RocketMq消息中间件API架构开发 分布式消息框架之Rabbitmq Rabbitmq入门与高可用集群部署实战 详解Rabbitmq消息分发与主题消息分布功能 Rabbitmq消息路由机制详解 Rabbitmq消息确认机制详解 Rabbitmq Web监控平台使用 Rabbitmq镜像队列详解 分布式消息框架之Kafka Kafka发展介绍与对比 Kafka集群搭建与使用 Kafka副本机制与选举原理详解 Kafka架构设计原理分析 基于Kafka的大规模日志系统实现原理分析 分布式缓存中间件(Redis，MongoDB，FastDFS) 关系型数据库瓶颈与优化 非关系型数据库数据中间件mongoDb、Redis、Tair、Memcache、Neo4j、FastDFS对比 分布式数据库存储之Redis 行互联网业使用Redis场景详解与演变过程 微博与微博Redis使用场景 电商Redis使用场景 快速搜索Redis使用场景 解密Redis基本数据类型、哨兵机制、复制、常用命令 快速开始Redis Cluster集群与原理 深入详解集群分配算法详解与动态水平扩容与监控 深入详解Jedis cluster开发与通讯协议详解 Redis持久化机制与安全机制详解 Redis整合lua脚本 实战企业级项目Redis框架gcache架构与开发 分布式数据库存储之MongoDB MongoDB基础概念 MongoDB高可用集群搭建实战 MongoDB最佳实践与使用注意事项 分布式文件存储之FastDFS 文件存储实战 文件同步实战 文件查询实战 分布式部署实战 分布式数据存储(Sharding-Sphere) 初识分布式下数据库瓶颈 为何要读写分离、分库分表 常见分片算法hash、list、range、tag详解 常见数据库中间件Mycat和Sharding-Jdbc对比 分布式数据中间件之Sharding-Sphere 讲最新技术sharding-sphere 解密Sharding-jdbc核心概念与快速开始 深入Sharding-jdbc特性详解与模块划分 实战订单交易中orders和ordersItem分库分表开发 深入Sharding-jdbc源码之sql解析、sql路由、sql改写、sql执行、结果合并 Atlas 深入熟悉Atlas原理与配置搭建 深入剖析Atlas实战与优缺点 分布式通信(Netty) 深入IO与NIO线程模型Reactor模型Netty 深入netty的线程模型源码分析 深入高性能序列化协议protobuf及源码分析 深入粘包分包现象及解决方案、编解码器源码分析 Netty之Http协议开发应用实践（弹幕系统） Netty之WebSocket协议开发应用实践二（贪吃蛇多人联机网游实现） 分布式搜索引擎(ELK) 涉及技术点为Elasticsearch、Logstash、Kibana ELK集群搭建实践 ELK架构与原理分析 Elasticsearch进阶深入浅出 性能调优专题 Jvm性能调优 JVM内存模型 线程共享区 堆空间 生命周期 分代机制 常用JVM参数 方法区(元空间) 线程私有区 程序计数器 线程栈 栈帧 局部变量表详解 操作数栈 动态链接解析 方法出口（方法返回地址） 本地方法栈 直接内存 JVM内存管理 垃圾收集机制详解 垃圾收集器 G1收集器 Serial收集器 ParNew收集器 ParallelScavenge收集器 Serial Old收集器 CMS(Concurrent Mark Sweep)收集器 垃圾收集算法 标记-清除（Mark-Sweep） 复制算法 标记-整理 分代收集 调优工具详解 JDK命令 jps jstat jinfo jmap jhat jstack jconsole详细使用 jvisualvm详细使用 JVM类加载机制详解 类加载器分类 启动类加载器（Bootstrap ClassLoader） 扩展类加载器 应用程序加载器 双亲委派模型（Parents Delegation Model） JVM调优实战 GC日志详细分析 gceasy日志分析工具使用 GCViewer日志分析工具使用 JVM参数调优分析 日均百万交易系统JVM堆栈大小设置策略与调优 亿级流量电商系统堆内年轻代与老年代垃圾回收参数设置与调优 高并发系统如何基于G1垃圾回收期优化性能 每秒10万并发的秒杀系统为什么会频繁发生GC 电商大促活动时，严重Full GC导致系统直接卡死的优化实战 线上生产系统OOM监控及定位与解决 Mysql性能调优 Mysql索引数据结构深度讲解 B+树 Hash 红黑树 Mysql 执行计划与索引讲解 explain工具深度使用 索引优化最佳实践 Mysql锁机制与事务隔离级别详解 Mysql锁 性能 乐观锁 悲观锁 操作 读锁 写锁 粒度 表锁 行锁 死锁以及优化解决 事务隔离级别 读未提交 读已提交 可重复读 MVCC机制详解 串行化 慢查询Sql调优实战 Nginx调优 Nginx快速掌握 核心模块 标准Http模块 可选Http模块 第三方模块 nginx 事件驱动模型及特性 熟练掌握Nginx核心配置 基本配置 虚拟主机配置 upstream location 静态目录配置 掌握Nginx负载算法配置 轮循+权重 ip hash url hash least_conn least_time Tomcat调优 整体认知Tomcat项目架构 理解Tomat启动流程 理解对Http请求解析与处理流程 核心组件认知 wrapper context host engine container Tomcat 8 与Tomcat7 对比 生产环境配置 Tomcat server.xml 配置详解 Tomcat集群与会话复制方案实现 Tomcat虚拟主机配置 掌握Tomcat 线程模型背后原理 Tomcat 支持四种线程模型介绍 通过压测演示Nio与 Bio模型的区别 Tomcat Bio实现源码解读 Tomcat Nio 实现源码解读 Tomcat connector 并发参数解读 微服务系列专题 微服务架构变迁史 Spring Boot源码剖析 Spring boot 快速开始及核心配置详解 Spring boot 部署方式及热部署详解，Web开发模板引擎Thymeleaf,Freemarker Spring Boot集成mybatis，Redis缓存，RabbitMq, 多数据源路由实战及分布式事务处理 Spring Boot底层源码分析 Spring Boot启动过程源码分析 Spring Boot自动装配源码分析 Spring Cloud Netflix源码剖析 eureka服务注册与发现详解及源码分析 ribbon 客户端负载均衡详解及源码分析 fegin 声明式服务调用详解及源码分析 hystrix实现服务限流，降级，熔断详解及源码分析 hystrix实现自定义接口降级,监控数据及监控数据聚合 Zuul/Gateway统一网关详解，服务路由，过滤器使用及源码分析 分布式配置中心Config详解 分布式链路跟踪Sleuth详解 Zuul 应用之统一异常处理，Cookie和重定向处理 Spring Cloud Alibaba源码剖析 Nacos服务注册与发现详解及源码分析 Sentinel限流，降级详解及底层源码剖析 虚拟容器 虚拟服务之Docker Docker的镜像，仓库，容器详解 快速开始搭建Docker环境 DockerFile使用详解 DockerCompose集成式应用组合 Docker服务编排实现 Kubernetes容器管理 Kubernetes介绍与快速开始 Kubernetes生产集群环境搭建与使用 项目实战专题 互联网工程专题 拓展技术专题 "}}